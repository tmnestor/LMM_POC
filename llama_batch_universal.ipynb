{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama Universal Extraction - Method 1\n",
    "\n",
    "**Strategy:** Extract all 17 fields from every document regardless of type.\n",
    "\n",
    "**Key Differences from Two-Stage:**\n",
    "- No document type classification step\n",
    "- Single universal prompt for all documents\n",
    "- Evaluation uses ground truth doc type to select relevant fields (Option C)\n",
    "- Tracks false positives (irrelevant fields extracted)\n",
    "\n",
    "**Output:** `llama_universal_batch_results_*.csv` compatible with model_comparison.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 1\n",
    "# Enable autoreload for module changes\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.environ['EVALUATION_METHOD'] = 'order_aware_f1'  # or 'f1', 'kieval', 'order_aware_f1', 'correlation'\n",
    "\n",
    "# Standard library imports\n",
    "import warnings\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display, Image\n",
    "from rich import print as rprint\n",
    "from rich.console import Console\n",
    "\n",
    "# Project-specific imports\n",
    "from common.batch_analytics import BatchAnalytics\n",
    "from common.batch_reporting import BatchReporter\n",
    "from common.batch_visualizations import BatchVisualizer\n",
    "from common.evaluation_metrics import load_ground_truth\n",
    "from common.extraction_parser import discover_images\n",
    "from common.gpu_optimization import emergency_cleanup\n",
    "from common.llama_model_loader_robust import load_llama_model_robust\n",
    "\n",
    "print(\"‚úÖ All imports loaded successfully\")\n",
    "print(\"‚úÖ Universal extraction mode\")\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-emptive Memory Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 2\n",
    "# Initialize console for rich output\n",
    "console = Console()\n",
    "\n",
    "# Pre-emptive V100 Memory Cleanup - Run FIRST to prevent OOM errors\n",
    "rprint(\"[bold red]üßπ PRE-EMPTIVE V100 MEMORY CLEANUP[/bold red]\")\n",
    "rprint(\"[yellow]Clearing any existing model caches before loading...[/yellow]\")\n",
    "\n",
    "# Emergency cleanup to ensure clean slate\n",
    "emergency_cleanup(verbose=True)\n",
    "\n",
    "rprint(\"[green]‚úÖ Memory cleanup complete - ready for model loading[/green]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 3\n",
    "# Environment-specific base paths\n",
    "ENVIRONMENT_BASES = {\n",
    "    'sandbox': '/home/jovyan/nfs_share/tod',\n",
    "    'efs': '/efs/shared/PoC_data'\n",
    "}\n",
    "base_data_path = ENVIRONMENT_BASES['sandbox']\n",
    "\n",
    "CONFIG = {\n",
    "    # Model settings\n",
    "    'MODEL_PATH': \"/home/jovyan/nfs_share/models/Llama-3.2-11B-Vision-Instruct\",\n",
    "\n",
    "    # Batch settings\n",
    "    'DATA_DIR': f'{base_data_path}/evaluation_data',\n",
    "    'GROUND_TRUTH': f'{base_data_path}/evaluation_data/ground_truth.csv',\n",
    "    'OUTPUT_BASE': f'{base_data_path}/LMM_POC/output',\n",
    "    'MAX_IMAGES': None,  # None for all, or set limit\n",
    "    'DOCUMENT_TYPES': None,  # None for all\n",
    "    \n",
    "    # EXTRACTION METHOD - Universal\n",
    "    'EXTRACTION_METHOD': 'universal',  # Extract all 17 fields for every document\n",
    "    'ENABLE_MATH_ENHANCEMENT': False,\n",
    "    \n",
    "    # Inference and evaluation mode\n",
    "    'INFERENCE_ONLY': False,  # Set to True to skip evaluation\n",
    "\n",
    "    # Verbosity control\n",
    "    'VERBOSE': True,\n",
    "    'SHOW_PROMPTS': True,\n",
    "\n",
    "    # GPU optimization settings\n",
    "    'USE_QUANTIZATION': False,\n",
    "    'DEVICE_MAP': 'auto',\n",
    "    'MAX_NEW_TOKENS': 2000,\n",
    "    'TORCH_DTYPE': 'bfloat16',\n",
    "    'LOW_CPU_MEM_USAGE': True,\n",
    "    \n",
    "    # Image preprocessing settings\n",
    "    'ENABLE_PREPROCESSING': True,\n",
    "    'PREPROCESSING_MODE': 'adaptive',  # 'light', 'moderate', 'aggressive', 'adaptive'\n",
    "    'SAVE_PREPROCESSED': False,\n",
    "    'PREPROCESSED_DIR': None,\n",
    "}\n",
    "\n",
    "# Make GROUND_TRUTH conditional based on INFERENCE_ONLY mode\n",
    "if CONFIG['INFERENCE_ONLY']:\n",
    "    CONFIG['GROUND_TRUTH'] = None\n",
    "\n",
    "print(\"‚úÖ Configuration set up successfully\")\n",
    "print(f\"üìÇ Data directory: {CONFIG['DATA_DIR']}\")\n",
    "print(f\"üìä Ground truth: {CONFIG['GROUND_TRUTH']}\")\n",
    "print(f\"ü§ñ Model path: {CONFIG['MODEL_PATH']}\")\n",
    "print(f\"üìÅ Output base: {CONFIG['OUTPUT_BASE']}\")\n",
    "print(f\"üî¨ Extraction method: {CONFIG['EXTRACTION_METHOD']} (all 17 fields)\")\n",
    "print(f\"üéØ Mode: {'Inference-only' if CONFIG['INFERENCE_ONLY'] else 'Evaluation mode'}\")\n",
    "print(f\"üîß Preprocessing: {'Enabled (' + CONFIG['PREPROCESSING_MODE'] + ')' if CONFIG['ENABLE_PREPROCESSING'] else 'Disabled'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Directory Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 4\n",
    "# Setup output directories\n",
    "OUTPUT_BASE = Path(CONFIG['OUTPUT_BASE'])\n",
    "if not OUTPUT_BASE.is_absolute():\n",
    "    OUTPUT_BASE = Path.cwd() / OUTPUT_BASE\n",
    "\n",
    "BATCH_TIMESTAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "OUTPUT_DIRS = {\n",
    "    'base': OUTPUT_BASE,\n",
    "    'batch': OUTPUT_BASE / 'batch_results',\n",
    "    'csv': OUTPUT_BASE / 'csv',\n",
    "    'visualizations': OUTPUT_BASE / 'visualizations',\n",
    "    'reports': OUTPUT_BASE / 'reports'\n",
    "}\n",
    "\n",
    "for dir_path in OUTPUT_DIRS.values():\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"‚úÖ Output directories created at: {OUTPUT_BASE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 5\n",
    "# Load model once for entire batch\n",
    "rprint(\"[bold green]Loading model with robust multi-GPU detection...[/bold green]\")\n",
    "\n",
    "model, processor = load_llama_model_robust(\n",
    "    model_path=CONFIG['MODEL_PATH'],\n",
    "    use_quantization=CONFIG['USE_QUANTIZATION'],\n",
    "    device_map=CONFIG['DEVICE_MAP'],\n",
    "    max_new_tokens=CONFIG['MAX_NEW_TOKENS'],\n",
    "    torch_dtype=CONFIG['TORCH_DTYPE'],\n",
    "    low_cpu_mem_usage=CONFIG['LOW_CPU_MEM_USAGE'],\n",
    "    verbose=CONFIG['VERBOSE']\n",
    ")\n",
    "\n",
    "rprint(\"[bold green]‚úÖ Model ready for universal extraction[/bold green]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Universal Extraction Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 6\n",
    "# Load universal extraction prompt from YAML\n",
    "prompt_file = Path('prompts/universal.yaml')\n",
    "\n",
    "with open(prompt_file, 'r') as f:\n",
    "    prompt_data = yaml.safe_load(f)\n",
    "\n",
    "UNIVERSAL_PROMPT = prompt_data['prompts']['universal']['prompt']\n",
    "\n",
    "rprint(\"[green]‚úÖ Universal extraction prompt loaded from prompts/universal.yaml[/green]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 6.25\n",
    "# Load universal field list from field_definitions.yaml\n",
    "field_defs_path = Path('config/field_definitions.yaml')\n",
    "\n",
    "with open(field_defs_path, 'r') as f:\n",
    "    field_defs = yaml.safe_load(f)\n",
    "\n",
    "# Get universal fields from YAML (19 total)\n",
    "all_universal_fields = field_defs['document_fields']['universal']['fields']\n",
    "\n",
    "# Remove fields no longer extracted (TRANSACTION_AMOUNTS_RECEIVED, ACCOUNT_BALANCE)\n",
    "# Final 17 fields for universal extraction\n",
    "EXCLUDED_FIELDS = ['TRANSACTION_AMOUNTS_RECEIVED', 'ACCOUNT_BALANCE']\n",
    "UNIVERSAL_FIELDS = [f for f in all_universal_fields if f not in EXCLUDED_FIELDS]\n",
    "\n",
    "rprint(f\"[green]‚úÖ Loaded {len(UNIVERSAL_FIELDS)} universal fields (excluded {len(EXCLUDED_FIELDS)} fields)[/green]\")\n",
    "rprint(f\"[cyan]Excluded: {', '.join(EXCLUDED_FIELDS)}[/cyan]\")\n",
    "rprint(f\"[dim]Fields: {', '.join(UNIVERSAL_FIELDS[:5])}... (and {len(UNIVERSAL_FIELDS)-5} more)[/dim]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 6.5\n",
    "# Display universal prompt contents\n",
    "if CONFIG.get('SHOW_PROMPTS', True):\n",
    "    console.rule(\"[bold cyan]Universal Extraction Prompt[/bold cyan]\")\n",
    "    print(UNIVERSAL_PROMPT)\n",
    "    console.rule(\"[bold cyan]End of Prompt[/bold cyan]\")\n",
    "else:\n",
    "    rprint(\"[dim]Prompt display disabled (set CONFIG['SHOW_PROMPTS'] = True to view)[/dim]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Discovery and Ground Truth Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 7\n",
    "# Discover images\n",
    "data_dir = Path(CONFIG['DATA_DIR'])\n",
    "if not data_dir.is_absolute():\n",
    "    data_dir = Path.cwd() / data_dir\n",
    "\n",
    "all_images = discover_images(str(data_dir))\n",
    "\n",
    "# Image preprocessing (if enabled)\n",
    "if CONFIG['ENABLE_PREPROCESSING']:\n",
    "    import tempfile\n",
    "    from common.image_preprocessing import (\n",
    "        enhance_statement_quality,\n",
    "        enhance_for_llama,\n",
    "        preprocess_statement_for_llama,\n",
    "        adaptive_enhance,\n",
    "        preprocess_recommended\n",
    "    )\n",
    "    \n",
    "    preprocess_functions = {\n",
    "        'light': enhance_statement_quality,\n",
    "        'moderate': enhance_for_llama,\n",
    "        'aggressive': preprocess_statement_for_llama,\n",
    "        'adaptive': adaptive_enhance,\n",
    "        'recommended': preprocess_recommended\n",
    "    }\n",
    "    \n",
    "    preprocess_fn = preprocess_functions[CONFIG['PREPROCESSING_MODE']]\n",
    "    preprocessed_images = []\n",
    "    \n",
    "    rprint(f\"[cyan]üîß Preprocessing {len(all_images)} images (mode: {CONFIG['PREPROCESSING_MODE']})[/cyan]\")\n",
    "    \n",
    "    if CONFIG['SAVE_PREPROCESSED']:\n",
    "        preprocessed_dir = Path(CONFIG['PREPROCESSED_DIR'] or 'preprocessed_images')\n",
    "        preprocessed_dir.mkdir(parents=True, exist_ok=True)\n",
    "    else:\n",
    "        preprocessed_dir = Path(tempfile.mkdtemp(prefix='preprocessed_'))\n",
    "    \n",
    "    for img_path in all_images:\n",
    "        original_filename = Path(img_path).name\n",
    "        try:\n",
    "            preprocessed_img = preprocess_fn(img_path)\n",
    "            preprocessed_path = preprocessed_dir / original_filename\n",
    "            preprocessed_img.save(preprocessed_path)\n",
    "            preprocessed_images.append(str(preprocessed_path))\n",
    "        except Exception as e:\n",
    "            rprint(f\"[yellow]‚ö†Ô∏è  Preprocessing failed for {original_filename}: {e}[/yellow]\")\n",
    "            preprocessed_images.append(img_path)\n",
    "    \n",
    "    all_images = preprocessed_images\n",
    "    rprint(f\"[green]‚úÖ Preprocessing complete[/green]\")\n",
    "\n",
    "# Load ground truth\n",
    "ground_truth = {}\n",
    "if not CONFIG['INFERENCE_ONLY'] and CONFIG['GROUND_TRUTH']:\n",
    "    ground_truth_path = Path(CONFIG['GROUND_TRUTH'])\n",
    "    if not ground_truth_path.is_absolute():\n",
    "        ground_truth_path = Path.cwd() / ground_truth_path\n",
    "    \n",
    "    ground_truth = load_ground_truth(str(ground_truth_path), verbose=CONFIG['VERBOSE'])\n",
    "    rprint(f\"[green]‚úÖ Ground truth loaded for {len(ground_truth)} images[/green]\")\n",
    "else:\n",
    "    rprint(\"[cyan]üìã Running in inference-only mode (no ground truth required)[/cyan]\")\n",
    "\n",
    "# Apply filters\n",
    "if CONFIG['DOCUMENT_TYPES'] and ground_truth:\n",
    "    filtered = []\n",
    "    for img in all_images:\n",
    "        img_name = Path(img).name\n",
    "        if img_name in ground_truth:\n",
    "            doc_type = ground_truth[img_name].get('DOCUMENT_TYPE', '').lower()\n",
    "            if any(dt.lower() in doc_type for dt in CONFIG['DOCUMENT_TYPES']):\n",
    "                filtered.append(img)\n",
    "    all_images = filtered\n",
    "\n",
    "if CONFIG['MAX_IMAGES']:\n",
    "    all_images = all_images[:CONFIG['MAX_IMAGES']]\n",
    "\n",
    "rprint(f\"[bold green]Ready to process {len(all_images)} images with universal extraction[/bold green]\")\n",
    "for i, img in enumerate(all_images[:5], 1):\n",
    "    print(f\"  {i}. {Path(img).name}\")\n",
    "if len(all_images) > 5:\n",
    "    print(f\"  ... and {len(all_images) - 5} more\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 7.5\n",
    "# Debug: Check ground truth keys vs image filenames\n",
    "if ground_truth:\n",
    "    rprint(\"\\n[bold yellow]üîç Ground Truth Debug Info[/bold yellow]\")\n",
    "    rprint(f\"[cyan]Total ground truth entries: {len(ground_truth)}[/cyan]\")\n",
    "    rprint(f\"[cyan]Total images to process: {len(all_images)}[/cyan]\")\n",
    "    \n",
    "    # Show first 3 ground truth keys\n",
    "    gt_keys = list(ground_truth.keys())[:3]\n",
    "    rprint(f\"[cyan]Sample GT keys: {gt_keys}[/cyan]\")\n",
    "    \n",
    "    # Show first 3 image filenames (with and without extensions)\n",
    "    img_names_full = [Path(img).name for img in all_images[:3]]\n",
    "    img_names_no_ext = [Path(img).stem for img in all_images[:3]]\n",
    "    rprint(f\"[cyan]Sample image names (full): {img_names_full}[/cyan]\")\n",
    "    rprint(f\"[cyan]Sample image names (no ext): {img_names_no_ext}[/cyan]\")\n",
    "    \n",
    "    # Check for mismatches using filename WITHOUT extension (Path.stem)\n",
    "    missing_gt = []\n",
    "    for img in all_images:\n",
    "        img_name_no_ext = Path(img).stem  # Strip extension for GT lookup\n",
    "        if img_name_no_ext not in ground_truth:\n",
    "            missing_gt.append(Path(img).name)  # Show full name in error\n",
    "    \n",
    "    if missing_gt:\n",
    "        rprint(f\"[red]‚ö†Ô∏è  WARNING: {len(missing_gt)} images missing from ground truth![/red]\")\n",
    "        rprint(f\"[red]First 5 missing: {missing_gt[:5]}[/red]\")\n",
    "    else:\n",
    "        rprint(f\"[green]‚úÖ All {len(all_images)} images have ground truth entries (using stem lookup)[/green]\")\n",
    "    \n",
    "    console.rule()\n",
    "else:\n",
    "    rprint(\"[yellow]‚ö†Ô∏è  No ground truth loaded (inference-only mode)[/yellow]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Universal Batch Processing\n",
    "\n",
    "Process all images with the same universal prompt (no document type detection)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#Cell 8\nimport torch\nfrom PIL import Image as PILImage\nfrom tqdm import tqdm\n\n# Import the correct parser\nfrom common.extraction_parser import parse_extraction_response\n\n# Universal batch processing function\ndef process_with_universal_prompt(image_paths, model, processor, prompt, ground_truth_data, verbose=False):\n    \"\"\"\n    Process images using universal extraction (all 17 fields).\n    \n    Returns:\n        batch_results: List of result dictionaries\n        processing_times: List of processing times\n    \"\"\"\n    batch_results = []\n    processing_times = []\n    \n    for img_path in tqdm(image_paths, desc=\"Processing images\"):\n        start_time = datetime.now()\n        img_name = Path(img_path).name\n        \n        # CRITICAL FIX: Strip file extension for ground truth lookup\n        # Ground truth keys don't have extensions (e.g., \"invoice_001\" not \"invoice_001.jpg\")\n        img_name_no_ext = Path(img_path).stem\n        \n        try:\n            # Load image\n            image = PILImage.open(img_path)\n            \n            # Create message for Llama\n            messages = [\n                {\n                    \"role\": \"user\",\n                    \"content\": [\n                        {\"type\": \"image\"},\n                        {\"type\": \"text\", \"text\": prompt}\n                    ]\n                }\n            ]\n            \n            # Process with model\n            input_text = processor.apply_chat_template(messages, add_generation_prompt=True)\n            inputs = processor(image, input_text, return_tensors=\"pt\").to(model.device)\n            \n            # Generate\n            with torch.no_grad():\n                output = model.generate(**inputs, max_new_tokens=2000)\n            \n            # Decode response\n            response = processor.decode(output[0], skip_special_tokens=True)\n            \n            # Extract only the assistant's response (after \"assistant\\n\\n\")\n            if \"assistant\\n\\n\" in response:\n                response = response.split(\"assistant\\n\\n\", 1)[1]\n            \n            # Parse the field-by-field response format (NOT JSON)\n            extracted_data = parse_extraction_response(response)\n            \n            # Get ground truth for this image (using name WITHOUT extension)\n            gt = ground_truth_data.get(img_name_no_ext, {})\n            \n            # Calculate processing time\n            processing_time = (datetime.now() - start_time).total_seconds()\n            processing_times.append(processing_time)\n            \n            # Store result\n            result = {\n                'image_name': img_name,  # Keep full filename for display\n                'image_name_no_ext': img_name_no_ext,  # For ground truth lookup\n                'image_path': img_path,\n                'extracted_data': extracted_data,\n                'raw_response': response,\n                'processing_time': processing_time,\n                'ground_truth': gt\n            }\n            \n            batch_results.append(result)\n            \n            if verbose:\n                gt_found = \"‚úÖ\" if gt else \"‚ùå\"\n                fields_found = sum(1 for v in extracted_data.values() if v != \"NOT_FOUND\")\n                rprint(f\"[green]{gt_found} {img_name}: {processing_time:.2f}s ({fields_found} fields)[/green]\")\n                \n        except Exception as e:\n            rprint(f\"[red]‚ùå Error processing {img_name}: {e}[/red]\")\n            import traceback\n            rprint(f\"[dim]{traceback.format_exc()}[/dim]\")\n            batch_results.append({\n                'image_name': img_name,\n                'image_name_no_ext': img_name_no_ext,\n                'image_path': img_path,\n                'error': str(e),\n                'processing_time': 0\n            })\n            processing_times.append(0)\n    \n    return batch_results, processing_times\n\n# Process batch\nrprint(\"[bold cyan]Starting universal batch extraction...[/bold cyan]\")\nbatch_results, processing_times = process_with_universal_prompt(\n    all_images,\n    model,\n    processor,\n    UNIVERSAL_PROMPT,\n    ground_truth,\n    verbose=CONFIG['VERBOSE']\n)\n\nrprint(f\"[bold green]‚úÖ Processed {len(batch_results)} images[/bold green]\")\nrprint(f\"[cyan]Average time: {np.mean(processing_times):.2f}s[/cyan]\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if batch_results:\n",
    "    first_result = batch_results[0]\n",
    "    print(f\"Image: {first_result['image_name']}\")\n",
    "    print(f\"\\nExtracted data:\")\n",
    "    for field, value in first_result['extracted_data'].items():\n",
    "        print(f\"  {field}: {value}\")\n",
    "\n",
    "    print(f\"\\nGround truth:\")\n",
    "    for field, value in first_result['ground_truth'].items():\n",
    "        print(f\"  {field}: {value}\")\n",
    "\n",
    "    print(f\"\\nRaw response (first 500 chars):\")\n",
    "    print(first_result['raw_response'][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation with Option C (Hybrid Strategy)\n",
    "\n",
    "Evaluate only fields that should exist based on ground truth document type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 9\n",
    "from common.evaluation_metrics import calculate_field_accuracy_with_method\n",
    "\n",
    "# Load document-specific field mappings directly from YAML\n",
    "field_defs_path = Path('config/field_definitions.yaml')\n",
    "with open(field_defs_path, 'r') as f:\n",
    "    field_defs = yaml.safe_load(f)\n",
    "\n",
    "# Create document type to fields mapping\n",
    "DOC_TYPE_FIELDS = {\n",
    "    'invoice': field_defs['document_fields']['invoice']['fields'],\n",
    "    'receipt': field_defs['document_fields']['receipt']['fields'],\n",
    "    'bank_statement': field_defs['document_fields']['bank_statement']['fields'],\n",
    "    'statement': field_defs['document_fields']['bank_statement']['fields'],  # Alias\n",
    "}\n",
    "\n",
    "rprint(f\"[green]‚úÖ Loaded document-specific field mappings from YAML[/green]\")\n",
    "rprint(f\"[cyan]  Invoice: {len(DOC_TYPE_FIELDS['invoice'])} fields[/cyan]\")\n",
    "rprint(f\"[cyan]  Receipt: {len(DOC_TYPE_FIELDS['receipt'])} fields[/cyan]\")\n",
    "rprint(f\"[cyan]  Bank Statement: {len(DOC_TYPE_FIELDS['bank_statement'])} fields[/cyan]\")\n",
    "\n",
    "def evaluate_universal_extraction(batch_results, inference_only_mode=False, evaluation_method='order_aware_f1'):\n",
    "    \"\"\"\n",
    "    Evaluate universal extraction using Option C strategy.\n",
    "    Only evaluates fields relevant to the ground truth document type.\n",
    "    Tracks false positives separately.\n",
    "    \"\"\"\n",
    "    evaluated_results = []\n",
    "    \n",
    "    for result in batch_results:\n",
    "        if 'error' in result:\n",
    "            evaluated_results.append(result)\n",
    "            continue\n",
    "        \n",
    "        extracted_data = result['extracted_data']\n",
    "        ground_truth = result.get('ground_truth', {})\n",
    "        \n",
    "        # Check global inference_only mode OR missing ground truth for this image\n",
    "        if inference_only_mode or not ground_truth:\n",
    "            # Inference only mode OR no ground truth for this specific image\n",
    "            result['evaluation'] = {\n",
    "                'inference_only': True,\n",
    "                'reason': 'global_config' if inference_only_mode else 'no_ground_truth_for_image'\n",
    "            }\n",
    "            evaluated_results.append(result)\n",
    "            continue\n",
    "        \n",
    "        # Get document type from ground truth\n",
    "        doc_type = ground_truth.get('DOCUMENT_TYPE', 'invoice').lower().replace(' ', '_')\n",
    "        \n",
    "        # Get relevant fields for this document type from YAML mapping\n",
    "        relevant_fields = DOC_TYPE_FIELDS.get(doc_type, DOC_TYPE_FIELDS['invoice'])\n",
    "        irrelevant_fields = set(UNIVERSAL_FIELDS) - set(relevant_fields)\n",
    "        \n",
    "        # Evaluate relevant fields only (PRIMARY METRIC)\n",
    "        field_scores = {}\n",
    "        total_f1 = 0.0\n",
    "        fields_evaluated = 0\n",
    "        fields_matched = 0\n",
    "        \n",
    "        for field in relevant_fields:\n",
    "            extracted_value = extracted_data.get(field, \"NOT_FOUND\")\n",
    "            gt_value = ground_truth.get(field, \"NOT_FOUND\")\n",
    "            \n",
    "            if extracted_value == \"NOT_FOUND\" and gt_value == \"NOT_FOUND\":\n",
    "                continue\n",
    "            \n",
    "            fields_evaluated += 1\n",
    "            \n",
    "            try:\n",
    "                metrics = calculate_field_accuracy_with_method(\n",
    "                    extracted_value, gt_value, field, method=evaluation_method\n",
    "                )\n",
    "            except Exception as e:\n",
    "                metrics = {'f1_score': 0.0, 'precision': 0.0, 'recall': 0.0}\n",
    "            \n",
    "            field_scores[field] = metrics\n",
    "            total_f1 += metrics.get('f1_score', 0.0)\n",
    "            \n",
    "            if metrics.get('f1_score', 0.0) > 0.9:\n",
    "                fields_matched += 1\n",
    "        \n",
    "        # Calculate primary accuracy\n",
    "        primary_accuracy = (total_f1 / fields_evaluated) if fields_evaluated > 0 else 0.0\n",
    "        \n",
    "        # Track false positives (SECONDARY METRIC)\n",
    "        false_positives = []\n",
    "        for field in irrelevant_fields:\n",
    "            if extracted_data.get(field, \"NOT_FOUND\") != \"NOT_FOUND\":\n",
    "                false_positives.append(field)\n",
    "        \n",
    "        false_positive_rate = len(false_positives) / len(irrelevant_fields) if irrelevant_fields else 0.0\n",
    "        \n",
    "        # Store evaluation\n",
    "        result['evaluation'] = {\n",
    "            'overall_accuracy': primary_accuracy,\n",
    "            'fields_evaluated': fields_evaluated,\n",
    "            'fields_matched': fields_matched,\n",
    "            'total_fields': len(relevant_fields),\n",
    "            'field_scores': field_scores,\n",
    "            'false_positive_count': len(false_positives),\n",
    "            'false_positive_rate': false_positive_rate,\n",
    "            'false_positive_fields': false_positives,\n",
    "            'document_type': doc_type,\n",
    "            'inference_only': False\n",
    "        }\n",
    "        \n",
    "        evaluated_results.append(result)\n",
    "    \n",
    "    return evaluated_results\n",
    "\n",
    "# Run evaluation\n",
    "if not CONFIG['INFERENCE_ONLY']:\n",
    "    rprint(\"[bold cyan]Evaluating with Option C (hybrid strategy)...[/bold cyan]\")\n",
    "    batch_results = evaluate_universal_extraction(batch_results, inference_only_mode=CONFIG['INFERENCE_ONLY'])\n",
    "    \n",
    "    # Count how many were actually evaluated vs skipped\n",
    "    evaluated_count = sum(1 for r in batch_results if 'evaluation' in r and not r['evaluation'].get('inference_only'))\n",
    "    skipped_count = sum(1 for r in batch_results if 'evaluation' in r and r['evaluation'].get('inference_only'))\n",
    "    \n",
    "    if evaluated_count > 0:\n",
    "        avg_accuracy = np.mean([r.get('evaluation', {}).get('overall_accuracy', 0) * 100 \n",
    "                                for r in batch_results if 'evaluation' in r and not r['evaluation'].get('inference_only')])\n",
    "        avg_false_positives = np.mean([r.get('evaluation', {}).get('false_positive_count', 0) \n",
    "                                        for r in batch_results if 'evaluation' in r and not r['evaluation'].get('inference_only')])\n",
    "        \n",
    "        rprint(f\"[green]‚úÖ Evaluation complete[/green]\")\n",
    "        rprint(f\"[cyan]Evaluated: {evaluated_count} images[/cyan]\")\n",
    "        if skipped_count > 0:\n",
    "            rprint(f\"[yellow]Skipped: {skipped_count} images (no ground truth)[/yellow]\")\n",
    "        rprint(f\"[cyan]Average accuracy: {avg_accuracy:.2f}%[/cyan]\")\n",
    "        rprint(f\"[cyan]Average false positives: {avg_false_positives:.2f} fields/image[/cyan]\")\n",
    "    else:\n",
    "        rprint(f\"[red]‚ö†Ô∏è  No images evaluated - all {skipped_count} images missing ground truth[/red]\")\n",
    "else:\n",
    "    rprint(\"[cyan]üìã Skipping evaluation (inference-only mode)[/cyan]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Output CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 10\n",
    "# Create model-specific CSV (compatible with model_comparison.ipynb)\n",
    "csv_data = []\n",
    "\n",
    "for i, result in enumerate(batch_results):\n",
    "    image_name = Path(result['image_path']).name\n",
    "    extracted_data = result.get('extracted_data', {})\n",
    "    evaluation = result.get('evaluation', {})\n",
    "    processing_time = result.get('processing_time', 0)\n",
    "    \n",
    "    # Determine document type (from ground truth if available, else from extraction)\n",
    "    doc_type = result.get('ground_truth', {}).get('DOCUMENT_TYPE', \n",
    "                                                    extracted_data.get('DOCUMENT_TYPE', 'unknown')).lower()\n",
    "    \n",
    "    # Count fields\n",
    "    found_fields = sum(1 for v in extracted_data.values() if v != \"NOT_FOUND\")\n",
    "    field_coverage = (found_fields / len(UNIVERSAL_FIELDS) * 100)\n",
    "    \n",
    "    # Create row\n",
    "    row = {\n",
    "        'image_file': image_name,\n",
    "        'image_name': image_name,\n",
    "        'document_type': doc_type,\n",
    "        'processing_time': processing_time,\n",
    "        'field_count': evaluation.get('total_fields', len(UNIVERSAL_FIELDS)),\n",
    "        'found_fields': found_fields,\n",
    "        'field_coverage': field_coverage,\n",
    "        'prompt_used': 'llama_universal',\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'overall_accuracy': evaluation.get('overall_accuracy', 0) * 100 if not CONFIG['INFERENCE_ONLY'] else None,\n",
    "        'fields_extracted': evaluation.get('fields_evaluated', 0),\n",
    "        'fields_matched': evaluation.get('fields_matched', 0),\n",
    "        'total_fields': evaluation.get('total_fields', len(UNIVERSAL_FIELDS)),\n",
    "        'false_positive_count': evaluation.get('false_positive_count', 0),\n",
    "        'false_positive_rate': evaluation.get('false_positive_rate', 0),\n",
    "        'inference_only': CONFIG['INFERENCE_ONLY']\n",
    "    }\n",
    "    \n",
    "    # Add all field values\n",
    "    for field in UNIVERSAL_FIELDS:\n",
    "        row[field] = extracted_data.get(field, 'NOT_FOUND')\n",
    "    \n",
    "    csv_data.append(row)\n",
    "\n",
    "# Create DataFrame and save\n",
    "df = pd.DataFrame(csv_data)\n",
    "csv_path = OUTPUT_DIRS['csv'] / f\"llama_universal_batch_results_{BATCH_TIMESTAMP}.csv\"\n",
    "df.to_csv(csv_path, index=False)\n",
    "\n",
    "rprint(\"[bold green]‚úÖ Universal extraction CSV exported:[/bold green]\")\n",
    "rprint(f\"[cyan]üìÑ File: {csv_path}[/cyan]\")\n",
    "rprint(f\"[cyan]üìä Structure: {len(df)} rows √ó {len(df.columns)} columns[/cyan]\")\n",
    "\n",
    "# Display sample\n",
    "if CONFIG['INFERENCE_ONLY']:\n",
    "    sample_cols = ['image_file', 'document_type', 'processing_time', 'found_fields', \n",
    "                   'field_coverage', 'false_positive_count']\n",
    "else:\n",
    "    sample_cols = ['image_file', 'document_type', 'overall_accuracy', 'processing_time', \n",
    "                   'found_fields', 'false_positive_count']\n",
    "\n",
    "rprint(\"\\n[bold blue]üìã Sample data:[/bold blue]\")\n",
    "display(df[sample_cols].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 11\n",
    "# Summary statistics\n",
    "console.rule(\"[bold green]Universal Extraction Summary[/bold green]\")\n",
    "\n",
    "rprint(f\"[bold green]‚úÖ Total images processed: {len(batch_results)}[/bold green]\")\n",
    "rprint(f\"[cyan]Average processing time: {np.mean(processing_times):.2f}s[/cyan]\")\n",
    "rprint(f\"[cyan]Average fields extracted: {df['found_fields'].mean():.1f} / {len(UNIVERSAL_FIELDS)}[/cyan]\")\n",
    "rprint(f\"[cyan]Average field coverage: {df['field_coverage'].mean():.1f}%[/cyan]\")\n",
    "\n",
    "if not CONFIG['INFERENCE_ONLY']:\n",
    "    rprint(f\"[cyan]Average accuracy (relevant fields): {df['overall_accuracy'].mean():.2f}%[/cyan]\")\n",
    "    rprint(f\"[cyan]Average false positives: {df['false_positive_count'].mean():.2f} fields/image[/cyan]\")\n",
    "    rprint(f\"[cyan]False positive rate: {df['false_positive_rate'].mean()*100:.1f}%[/cyan]\")\n",
    "\n",
    "rprint(f\"\\n[cyan]Output saved to: {OUTPUT_DIRS['csv']}[/cyan]\")\n",
    "rprint(\"[green]‚úÖ Ready for comparison with two-stage and oracle methods[/green]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}