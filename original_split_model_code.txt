def split_model(model_path):
    """Create custom device map for InternVL3-8B across multiple GPUs.
    
    This function intelligently distributes the model layers:
    - Vision model goes to GPU 0 (uses ~50% capacity)
    - LLM layers are distributed across all GPUs
    - Critical components stay on GPU 0 for efficiency
    """
    device_map = {}
    world_size = torch.cuda.device_count()
    
    # Load config to get model architecture details
    config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)
    num_layers = config.llm_config.num_hidden_layers
    
    # Since the first GPU will be used for ViT, treat it as half a GPU
    num_layers_per_gpu = math.ceil(num_layers / (world_size - 0.5))
    num_layers_per_gpu = [num_layers_per_gpu] * world_size
    num_layers_per_gpu[0] = math.ceil(num_layers_per_gpu[0] * 0.5)
    
    # Distribute layers across GPUs
    layer_cnt = 0
    for i, num_layer in enumerate(num_layers_per_gpu):
        for j in range(num_layer):
            if layer_cnt < num_layers:
                device_map[f'language_model.model.layers.{layer_cnt}'] = i
                layer_cnt += 1
    
    # Place vision model and critical components on GPU 0
    device_map['vision_model'] = 0
    device_map['mlp1'] = 0
    device_map['language_model.model.embed_tokens'] = 0
    device_map['language_model.model.norm'] = 0
    device_map['language_model.model.rotary_emb'] = 0
    device_map['language_model.lm_head'] = 0
    
    # Move last layer to GPU 0 for better communication
    if num_layers > 0:
        device_map[f'language_model.model.layers.{num_layers - 1}'] = 0
    
    print(f"ðŸ“Š Device map created for {num_layers} layers across {world_size} GPUs")
    print(f"   GPU 0: Vision model + {num_layers_per_gpu[0]} LLM layers + critical components")
    for i in range(1, world_size):
        print(f"   GPU {i}: {num_layers_per_gpu[i]} LLM layers")
    
    return device_map