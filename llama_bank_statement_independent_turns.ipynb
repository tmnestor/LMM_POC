{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Llama 3.2 Vision: Independent Single-Turn Bank Statement Extraction\n\n**Protocol**: Three independent single-turn prompts (NOT multi-turn conversation)\n\n**Key Insight**: Multi-turn conversation degrades accuracy due to context accumulation. Each turn runs with fresh context.\n\n---\n\n## Complete Workflow\n\n```\nTurn 0: Image + Prompt ‚Üí Headers (fresh context)\n        ‚Üì (Python pattern matching)\nTurn 1: Image + Prompt ‚Üí Full Table (fresh context, uses headers from Turn 0)\n        ‚Üì (Python parsing)  \nTurn 2: Image + Prompt ‚Üí Filtered Table (fresh context, uses headers from Turn 0)\n        ‚Üì (Python schema extraction)\n```\n\n### Pipeline Stages:\n1. **Turn 0 (Independent)**: Image + prompt to identify column headers\n2. **Pattern Matching (Python)**: Map headers to generic concepts (Date, Description, Debit)\n3. **Turn 1 (Independent)**: Image + prompt to extract full table using identified headers\n4. **Turn 2 (Independent)**: Image + prompt to filter withdrawal transactions\n5. **Python Parsing**: Convert markdown to schema format\n\n### Key Differences from Multi-Turn:\n- ‚ùå **No conversation history** - each turn starts fresh\n- ‚ùå **No context accumulation** - prevents attention dilution\n- ‚úÖ **Direct image access** - every turn has full image attention\n- ‚úÖ **Better accuracy** - empirically proven to work better than multi-turn"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports\n",
    "\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, MllamaForConditionalGeneration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Random Seed for Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Random seed set to 42 for reproducibility\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Set random seed\n",
    "\n",
    "from common.reproducibility import set_seed\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Load Llama-3.2-Vision model\n",
    "# Update this path to your local Llama model\n",
    "# model_id = \"/home/jovyan/shared_PTM/Llama-3.2-11B-Vision-Instruct\"\n",
    "model_id = \"/home/jovyan/nfs_share/models/Llama-3.2-11B-Vision-Instruct\"\n",
    "\n",
    "print(\"üîß Loading Llama-3.2-Vision model...\")\n",
    "# model = MllamaForConditionalGeneration.from_pretrained(\n",
    "#     model_id,\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     device_map=\"auto\",\n",
    "# )\n",
    "# processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "from common.llama_model_loader_robust import load_llama_model_robust\n",
    "\n",
    "model, processor = load_llama_model_robust(\n",
    "    model_path=model_id,\n",
    "    use_quantization=False,\n",
    "    device_map='auto',\n",
    "    max_new_tokens=2000,\n",
    "    torch_dtype='bfloat16',\n",
    "    low_cpu_mem_usage=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Add tie_weights() call\n",
    "try:\n",
    "    model.tie_weights()\n",
    "    print(\"‚úÖ Model weights tied successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è tie_weights() warning: {e}\")\n",
    "\n",
    "# processor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Load bank statement image\n",
    "# Update this path to your test image\n",
    "# imageName = \"/home/jovyan/shared_PoC_data/evaluation_data/synthetic_bank_images/cba_amount_balance.png\"\n",
    "imageName = \"/home/jovyan/nfs_share/tod/LMM_POC/evaluation_data/images/image_008.png\"\n",
    "print(\"üìÅ Loading image...\")\n",
    "image = Image.open(imageName)\n",
    "\n",
    "# CRITICAL: Store as list for multi-turn compatibility\n",
    "images = [image]\n",
    "\n",
    "print(f\"‚úÖ Image loaded: {image.size}\")\n",
    "print(f\"‚úÖ Images list created with {len(images)} image(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Turn Bank Statement Protocol\n",
    "- Turn 0: Identify actual table headers\n",
    "- Turn 1: Extract full table using those headers\n",
    "- Turn 2: Filter using the actual column names found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Turn 0 - Identify table headers (prompt)\n",
    "# TURN 0: Identify Table Headers\n",
    "# First, identify the actual column headers used in this specific bank statement\n",
    "\n",
    "prompt = \"\"\"\n",
    "Look at the transaction table in this bank statement image.\n",
    "\n",
    "IMPORTANT STRUCTURAL NOTE:\n",
    "Some bank statements show dates as section headings with multiple transactions underneath.\n",
    "If you see this structure, remember that each transaction needs its explicit date in the final output.\n",
    "\n",
    "What are the exact column header names used in the transaction table?\n",
    "\n",
    "List each column header exactly as it appears, in order from left to right.\n",
    "Do not interpret or rename them - use the EXACT text from the image.\n",
    "\"\"\"\n",
    "\n",
    "# Create message structure for Llama\n",
    "messageDataStructure = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\"},\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": prompt,\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"üí¨ TURN 0: Identifying actual table headers\")\n",
    "print(\"ü§ñ Generating response with Llama-3.2-Vision...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Turn 0 - Execute and parse headers\n",
    "# Process the input using the CORRECT multi-turn pattern\n",
    "# Based on: https://medium.com/data-science/chat-with-your-images-using-multimodal-llms-60af003e8bfa\n",
    "\n",
    "textInput = processor.apply_chat_template(\n",
    "    messageDataStructure, add_generation_prompt=True\n",
    ")\n",
    "\n",
    "# CRITICAL: Use named parameter 'images=' with list\n",
    "inputs = processor(images=images, text=textInput, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Generate response with deterministic parameters\n",
    "output = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=2000,\n",
    "    do_sample=False,\n",
    "    temperature=None,\n",
    "    top_p=None,\n",
    ")\n",
    "\n",
    "# CRITICAL: Trim input tokens from output (this is the key to clean responses!)\n",
    "generate_ids = output[:, inputs['input_ids'].shape[1]:-1]\n",
    "cleanedOutput = processor.decode(generate_ids[0], clean_up_tokenization_spaces=False)\n",
    "\n",
    "print(\"‚úÖ Response generated successfully!\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TURN 0 - IDENTIFIED TABLE HEADERS:\")\n",
    "print(\"=\" * 60)\n",
    "print(cleanedOutput)\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# CRITICAL: Parse the identified headers for use in subsequent turns\n",
    "# Extract column names from the response\n",
    "header_lines = [line.strip() for line in cleanedOutput.split('\\n') if line.strip()]\n",
    "identified_headers = []\n",
    "\n",
    "# Look for numbered list or bullet points\n",
    "for line in header_lines:\n",
    "    # Remove common list markers\n",
    "    cleaned = line.lstrip('0123456789.-‚Ä¢* ').strip()\n",
    "    \n",
    "    # Strip markdown bold formatting\n",
    "    cleaned = cleaned.replace('**', '').replace('__', '')\n",
    "    \n",
    "    # Skip section headers (lines ending with colon)\n",
    "    if cleaned.endswith(':'):\n",
    "        continue\n",
    "    \n",
    "    # Skip long sentences (likely explanatory text, not headers)\n",
    "    if len(cleaned) > 40:\n",
    "        continue\n",
    "        \n",
    "    if cleaned and len(cleaned) > 2:  # Ignore very short strings\n",
    "        identified_headers.append(cleaned)\n",
    "\n",
    "print(f\"\\nüìã Parsed {len(identified_headers)} column headers:\")\n",
    "for i, header in enumerate(identified_headers, 1):\n",
    "    print(f\"  {i}. '{header}'\")\n",
    "\n",
    "# Store headers for use in subsequent turns\n",
    "table_headers = identified_headers\n",
    "\n",
    "# Save the table headers\n",
    "output_path = Path(\"llama_table_headers.txt\")\n",
    "with output_path.open(\"w\", encoding=\"utf-8\") as text_file:\n",
    "    text_file.write(cleanedOutput)\n",
    "\n",
    "print(f\"\\n‚úÖ Table headers saved to: {output_path}\")\n",
    "print(\"üí° These LITERAL header names will be used in Turn 1 & 2 prompts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern Matching: Map Generic Concepts to Actual Headers\n",
    "\n",
    "Different bank statements use different column names. Use pattern matching to identify:\n",
    "- Which header represents **Date**\n",
    "- Which header represents **Description/Details**  \n",
    "- Which header represents **Debit/Withdrawal**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Pattern Matching - Map headers to generic columns\n",
    "# Pattern Matching: Map extracted headers to generic concepts\n",
    "# This handles variety in bank statement column naming conventions\n",
    "\n",
    "# Pattern keywords for each concept (in priority order)\n",
    "DATE_PATTERNS = ['date', 'day', 'transaction date', 'trans date']\n",
    "DESCRIPTION_PATTERNS = [\n",
    "    'description', 'details', 'transaction details', 'trans details',\n",
    "    'particulars', 'narrative', 'transaction', 'trans'\n",
    "]\n",
    "DEBIT_PATTERNS = ['debit', 'withdrawal', 'withdrawals', 'paid', 'paid out', 'spent', 'dr']\n",
    "CREDIT_PATTERNS = ['credit', 'deposit', 'deposits', 'received', 'cr']\n",
    "BALANCE_PATTERNS = ['balance', 'bal', 'running balance']\n",
    "\n",
    "# NEW: Pattern for single-column transaction formats (e.g., \"Amount\" instead of separate Debit/Credit)\n",
    "AMOUNT_PATTERNS = ['amount', 'amt', 'value', 'total']\n",
    "\n",
    "def match_header(headers, patterns, fallback=None):\n",
    "    \"\"\"Match a header using pattern keywords.\n",
    "    \n",
    "    Matching strategy:\n",
    "    1. Exact match (case-insensitive)\n",
    "    2. Substring match (only for patterns with length > 2 to avoid false positives)\n",
    "    \"\"\"\n",
    "    headers_lower = [h.lower() for h in headers]\n",
    "    \n",
    "    # Try exact match first\n",
    "    for pattern in patterns:\n",
    "        for i, header_lower in enumerate(headers_lower):\n",
    "            if pattern == header_lower:\n",
    "                return headers[i]\n",
    "    \n",
    "    # Try substring match (only for patterns longer than 2 chars)\n",
    "    for pattern in patterns:\n",
    "        if len(pattern) > 2:  # Avoid false positives like 'cr' matching 'description'\n",
    "            for i, header_lower in enumerate(headers_lower):\n",
    "                if pattern in header_lower:\n",
    "                    return headers[i]\n",
    "    \n",
    "    return fallback\n",
    "\n",
    "# Perform pattern matching on extracted headers\n",
    "date_col = match_header(table_headers, DATE_PATTERNS, fallback=table_headers[0] if table_headers else 'Date')\n",
    "desc_col = match_header(table_headers, DESCRIPTION_PATTERNS, fallback=table_headers[1] if len(table_headers) > 1 else 'Description')\n",
    "\n",
    "# NEW: First try to match a generic \"Amount\" column (for 4-column formats)\n",
    "amount_col = match_header(table_headers, AMOUNT_PATTERNS, fallback=None)\n",
    "\n",
    "# Use amount_col as fallback if no separate debit/credit columns exist\n",
    "# This handles formats like: Date | Description | Amount | Balance\n",
    "debit_col = match_header(table_headers, DEBIT_PATTERNS, fallback=amount_col if amount_col else 'Debit')\n",
    "credit_col = match_header(table_headers, CREDIT_PATTERNS, fallback=amount_col if amount_col else 'Credit')\n",
    "balance_col = match_header(table_headers, BALANCE_PATTERNS, fallback='Balance')\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PATTERN MATCHING RESULTS:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"üìã Extracted Headers: {table_headers}\")\n",
    "print(f\"\\nüîç Mapped Columns:\")\n",
    "print(f\"  Date        ‚Üí '{date_col}'\")\n",
    "print(f\"  Description ‚Üí '{desc_col}'\")\n",
    "print(f\"  Debit       ‚Üí '{debit_col}'\")\n",
    "print(f\"  Credit      ‚Üí '{credit_col}'\")\n",
    "print(f\"  Balance     ‚Üí '{balance_col}'\")\n",
    "if amount_col:\n",
    "    print(f\"\\nüí° Single-column format detected: '{amount_col}' used for both debit and credit\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n‚úÖ These literal column names will be used in Turn 1 and Turn 2\")\n",
    "print(\"üí° Adjust patterns above if matching fails for your bank statement format\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### üîë Independent Single-Turn Pattern (NOT Multi-Turn Conversation)\n\n**CRITICAL INSIGHT**: Multi-turn conversation accumulates context and degrades accuracy.\n\nInstead, we use **three independent single-turn prompts**, each with fresh context:\n\n#### Key Principles:\n\n1. **No Conversation History**: Each turn is completely independent\n2. **Fresh Image Attention**: Each turn processes the image directly\n3. **No Context Accumulation**: Prevents attention dilution\n4. **Headers as Parameters**: Turn 0 headers are passed as text parameters to Turn 1 and Turn 2\n\n#### Message Structure for Each Turn:\n\nEvery turn uses the same fresh structure:\n```python\nmessageDataStructure = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\"},\n            {\"type\": \"text\", \"text\": \"<prompt with headers from Turn 0>\"}\n        ]\n    }\n]\n```\n\n**No assistant responses in history. No conversation accumulation.**\n\n#### Why This Works Better:\n\n- **Turn 0**: Clean context ‚Üí accurate header identification\n- **Turn 1**: Clean context + headers ‚Üí accurate table extraction  \n- **Turn 2**: Clean context + headers ‚Üí accurate filtering\n\nEach turn has **full attention** on the image, not diluted by conversation history."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 15: NO conversation history (independent turns)\n# \n# CRITICAL: We do NOT use conversation history in this notebook.\n# Each turn is completely independent with fresh context.\n#\n# Why? Multi-turn conversation accumulates context and degrades accuracy:\n# - Turn 0: 50 tokens ‚Üí accurate\n# - Turn 1 with history: 350 tokens ‚Üí attention diluted ‚Üí less accurate\n# - Turn 2 with history: 2000 tokens ‚Üí attention heavily diluted ‚Üí poor accuracy\n#\n# Instead: Each turn starts fresh with just image + prompt\n\nprint(\"‚úÖ Independent turn approach - NO conversation history\")\nprint(\"üí° Each turn will have fresh context with direct image access\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TURN 1: Extract Full Table in Markdown\n",
    "\n",
    "Now that we know the actual column headers, extract the complete table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 17: Turn 1 - Extract full table (INDEPENDENT, fresh context)\n\n# Build the header string using LITERAL names from Turn 0\nheader_string = \" | \".join(table_headers)\n\nfollow_up_prompt = f\"\"\"\nExtract the entire transaction table from this bank statement image in markdown format.\n\nUse these EXACT column headers in this order:\n{header_string}\n\nFormat requirements:\n- Standard markdown table syntax with | delimiters\n- Header row: | {header_string} |\n- Separator row: | {\" | \".join([\"---\"] * len(table_headers))} |\n\nCRITICAL EXTRACTION RULES:\n1. Extract EVERY transaction as a separate row\n2. Each transaction MUST have its explicit date in the date column\n3. If multiple transactions share a date heading, repeat that date for each transaction row\n4. Do NOT skip or combine any rows\n5. Keep all amounts with decimal values intact\n6. Do NOT add explanatory text - only output the markdown table\n\nCRITICAL COLUMN POSITION RULES:\n7. Place each amount in the EXACT column where it appears in the original image\n8. Do NOT move amounts between columns based on transaction type\n9. Do NOT interpret whether a transaction should be a debit or credit\n10. If an amount appears under the \"{debit_col}\" column in the image, put it in the {debit_col} column in your table\n11. If an amount appears under the \"{credit_col}\" column in the image, put it in the {credit_col} column in your table\n12. If a column is empty for a row in the image, leave it empty in your table\n13. Preserve the EXACT visual layout - read amounts strictly by their column position\n\nExample: If you see:\n  01/06/2024\n    Transaction A    $100\n    Transaction B    $50\n    \nOutput as TWO rows:\n  | 01/06/2024 | Transaction A | $100 | ... |\n  | 01/06/2024 | Transaction B | $50  | ... |\n\"\"\"\n\n# CRITICAL: Create FRESH message structure (NOT appending to conversation history)\nmessageDataStructure_turn1 = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\"},\n            {\"type\": \"text\", \"text\": follow_up_prompt}\n        ]\n    }\n]\n\nprint(f\"üí¨ TURN 1: Extract full markdown table (INDEPENDENT)\")\nprint(f\"üìã Using literal headers: {table_headers}\")\nprint(f\"üí° Column position rules reference: '{debit_col}' and '{credit_col}'\")\nprint(f\"üîÑ Fresh context (no conversation history)\")\nprint(\"ü§ñ Generating response with Llama-3.2-Vision...\")\n\n# Process with FRESH context\ntextInput = processor.apply_chat_template(\n    messageDataStructure_turn1, add_generation_prompt=True\n)\n\n# CRITICAL: Use named parameter 'images=' with list\ninputs = processor(images=images, text=textInput, return_tensors=\"pt\").to(model.device)\n\n# Generate response\noutput = model.generate(\n    **inputs,\n    max_new_tokens=2000,\n    do_sample=False,\n    temperature=None,\n    top_p=None,\n)\n\n# CRITICAL: Trim input tokens from output\ngenerate_ids = output[:, inputs['input_ids'].shape[1]:-1]\ncleanedOutput2 = processor.decode(generate_ids[0], clean_up_tokenization_spaces=False)\n\nprint(\"\\n‚úÖ Follow-up response generated successfully!\")\nprint(\"\\n\" + \"=\" * 60)\nprint(\"TURN 1 - FULL MARKDOWN TABLE:\")\nprint(\"=\" * 60)\nprint(cleanedOutput2)\nprint(\"=\" * 60)\n\n# Save the markdown table\noutput_path = Path(\"llama_markdown_table_extraction.txt\")\nwith output_path.open(\"w\", encoding=\"utf-8\") as text_file:\n    text_file.write(cleanedOutput2)\n\nprint(f\"\\n‚úÖ Markdown table saved to: {output_path}\")\nprint(\"üí° Each transaction has explicit date, even if grouped by date heading\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TURN 2: Filter Using Actual Column Names\n",
    "\n",
    "Filter the extracted table using the specific column names identified in Turn 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 19: Turn 2 - Filter withdrawals (INDEPENDENT, fresh context)\n\nfollow_up_prompt_3 = f\"\"\"\nExtract ONLY the withdrawal/debit transactions from this bank statement image in markdown format.\n\nUse these EXACT column names:\n- {date_col}\n- {desc_col}  \n- {debit_col}\n\nFilter rules:\n- Only include rows where '{debit_col}' column has a value (not empty)\n- Exclude credit/deposit transactions  \n- Keep the markdown table format with header: | {date_col} | {desc_col} | {debit_col} |\n\nOutput only the filtered markdown table showing withdrawal transactions.\n\"\"\"\n\n# CRITICAL: Create FRESH message structure (NOT appending to conversation history)\nmessageDataStructure_turn2 = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\"},\n            {\"type\": \"text\", \"text\": follow_up_prompt_3}\n        ]\n    }\n]\n\nprint(f\"üí¨ TURN 2: Filter withdrawal transactions (INDEPENDENT)\")\nprint(f\"üìã Filter columns: '{date_col}' | '{desc_col}' | '{debit_col}'\")\nprint(f\"üîÑ Fresh context (no conversation history)\")\nprint(\"ü§ñ Generating response with Llama-3.2-Vision...\")\n\n# Process with FRESH context\ntextInput = processor.apply_chat_template(\n    messageDataStructure_turn2, add_generation_prompt=True\n)\n\n# Use named parameter 'images=' with list\ninputs = processor(images=images, text=textInput, return_tensors=\"pt\").to(model.device)\n\n# Generate response\noutput = model.generate(\n    **inputs,\n    max_new_tokens=2000,\n    do_sample=False,\n    temperature=None,\n    top_p=None,\n)\n\n# Trim input tokens from output\ngenerate_ids = output[:, inputs['input_ids'].shape[1]:-1]\ncleanedOutput3 = processor.decode(generate_ids[0], clean_up_tokenization_spaces=False)\n\nprint(\"\\n‚úÖ Follow-up response generated successfully!\")\nprint(\"\\n\" + \"=\" * 60)\nprint(\"TURN 2 - FILTERED WITHDRAWALS:\")\nprint(\"=\" * 60)\nprint(cleanedOutput3)\nprint(\"=\" * 60)\n\n# Save filtered results\noutput_path = Path(\"llama_filtered_withdrawals.txt\")\nwith output_path.open(\"w\", encoding=\"utf-8\") as text_file:\n    text_file.write(cleanedOutput3)\n\nprint(f\"\\n‚úÖ Filtered table saved to: {output_path}\")\nprint(f\"\\n‚úÖ Complete Protocol:\")\nprint(f\"   Turn 0: Identify headers ‚Üí {table_headers}\")\nprint(f\"   Pattern Match: Date='{date_col}', Desc='{desc_col}', Debit='{debit_col}'\")\nprint(f\"   Turn 1: Extract full table (independent, fresh context)\")\nprint(f\"   Turn 2: Filter withdrawals (independent, fresh context)\")\nprint(\"\\n‚úÖ Independent turns - better accuracy than multi-turn conversation!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Schema Fields from Markdown Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 21: Parse markdown and extract schema fields\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "def parse_markdown_table(markdown_text):\n",
    "    \"\"\"Parse markdown table into list of dictionaries.\"\"\"\n",
    "    lines = [line.strip() for line in markdown_text.strip().split('\\n') if line.strip()]\n",
    "    \n",
    "    # Find header row (first line with pipes)\n",
    "    header_idx = None\n",
    "    for i, line in enumerate(lines):\n",
    "        if '|' in line and not set(line.replace('|', '').replace('-', '').strip()) == set():\n",
    "            header_idx = i\n",
    "            break\n",
    "    \n",
    "    if header_idx is None:\n",
    "        return []\n",
    "    \n",
    "    # Parse headers\n",
    "    header_line = lines[header_idx]\n",
    "    headers = [h.strip() for h in header_line.split('|') if h.strip()]\n",
    "    \n",
    "    # Parse data rows (skip header and separator)\n",
    "    rows = []\n",
    "    for line in lines[header_idx + 1:]:\n",
    "        if '|' in line:\n",
    "            # Skip separator rows (contain only pipes, hyphens, and spaces)\n",
    "            cleaned = line.replace('|', '').replace('-', '').replace(' ', '')\n",
    "            if not cleaned:\n",
    "                continue\n",
    "            \n",
    "            values = [v.strip() for v in line.split('|') if v.strip()]\n",
    "            if len(values) == len(headers):\n",
    "                rows.append(dict(zip(headers, values)))\n",
    "    \n",
    "    return rows\n",
    "\n",
    "def extract_schema_fields(rows, date_col, desc_col, debit_col):\n",
    "    \"\"\"Extract fields in universal.yaml schema format.\"\"\"\n",
    "    if not rows:\n",
    "        return {\n",
    "            'TRANSACTION_DATES': 'NOT_FOUND',\n",
    "            'LINE_ITEM_DESCRIPTIONS': 'NOT_FOUND',\n",
    "            'TRANSACTION_AMOUNTS_PAID': 'NOT_FOUND',\n",
    "            'STATEMENT_DATE_RANGE': 'NOT_FOUND'\n",
    "        }\n",
    "    \n",
    "    # Extract lists\n",
    "    dates = []\n",
    "    descriptions = []\n",
    "    amounts = []\n",
    "    \n",
    "    for row in rows:\n",
    "        date = row.get(date_col, '').strip()\n",
    "        desc = row.get(desc_col, '').strip()\n",
    "        amount = row.get(debit_col, '').strip()\n",
    "        \n",
    "        if date:\n",
    "            dates.append(date)\n",
    "        if desc:\n",
    "            descriptions.append(desc)\n",
    "        if amount:\n",
    "            amounts.append(amount)\n",
    "    \n",
    "    # Calculate statement date range\n",
    "    date_range = 'NOT_FOUND'\n",
    "    if dates:\n",
    "        try:\n",
    "            # Try to parse dates to find min/max\n",
    "            parsed_dates = []\n",
    "            for d in dates:\n",
    "                # Try common formats\n",
    "                for fmt in ['%d/%m/%Y', '%d/%m/%y', '%Y-%m-%d', '%d-%m-%Y']:\n",
    "                    try:\n",
    "                        parsed_dates.append(datetime.strptime(d, fmt))\n",
    "                        break\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "            \n",
    "            if parsed_dates:\n",
    "                min_date = min(parsed_dates)\n",
    "                max_date = max(parsed_dates)\n",
    "                date_range = f\"{min_date.strftime('%d/%m/%Y')} to {max_date.strftime('%d/%m/%Y')}\"\n",
    "        except Exception:\n",
    "            # If parsing fails, use first and last date as-is\n",
    "            date_range = f\"{dates[0]} to {dates[-1]}\"\n",
    "    \n",
    "    return {\n",
    "        'TRANSACTION_DATES': ' | '.join(dates) if dates else 'NOT_FOUND',\n",
    "        'LINE_ITEM_DESCRIPTIONS': ' | '.join(descriptions) if descriptions else 'NOT_FOUND',\n",
    "        'TRANSACTION_AMOUNTS_PAID': ' | '.join(amounts) if amounts else 'NOT_FOUND',\n",
    "        'STATEMENT_DATE_RANGE': date_range\n",
    "    }\n",
    "\n",
    "# Parse the filtered markdown table from Turn 2\n",
    "parsed_rows = parse_markdown_table(cleanedOutput3)\n",
    "\n",
    "print(f\"üìä Parsed {len(parsed_rows)} transactions from markdown table\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SAMPLE ROWS:\")\n",
    "print(\"=\" * 60)\n",
    "for i, row in enumerate(parsed_rows[:3], 1):\n",
    "    print(f\"\\nRow {i}:\")\n",
    "    for key, value in row.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# Extract schema fields using the LITERAL column names from pattern matching\n",
    "schema_fields = extract_schema_fields(parsed_rows, date_col, desc_col, debit_col)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EXTRACTED SCHEMA FIELDS:\")\n",
    "print(\"=\" * 60)\n",
    "for field, value in schema_fields.items():\n",
    "    print(f\"{field}: {value}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Save to file\n",
    "output_path = Path(\"llama_extracted_fields.txt\")\n",
    "with output_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    for field, value in schema_fields.items():\n",
    "        f.write(f\"{field}: {value}\\n\")\n",
    "\n",
    "print(f\"\\n‚úÖ Schema fields saved to: {output_path}\")\n",
    "print(f\"üí° Fields extracted from columns: '{date_col}' | '{desc_col}' | '{debit_col}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (unified_vision_processor)",
   "language": "python",
   "name": "unified_vision_processor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}