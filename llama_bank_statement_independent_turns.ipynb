{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama 3.2 Vision: Independent Single-Turn Bank Statement Extraction\n",
    "\n",
    "**Protocol**: Three independent single-turn prompts (NOT multi-turn conversation)\n",
    "\n",
    "**Key Insight**: Multi-turn conversation degrades accuracy due to context accumulation. Each turn runs with fresh context.\n",
    "\n",
    "---\n",
    "\n",
    "## Complete Workflow\n",
    "\n",
    "```\n",
    "Turn 0: Image + Prompt â†’ Headers (fresh context)\n",
    "        â†“ (Python pattern matching)\n",
    "Turn 1: Image + Prompt â†’ Full Table (fresh context, uses headers from Turn 0)\n",
    "        â†“ (Python parsing)  \n",
    "Turn 2: Image + Prompt â†’ Filtered Table (fresh context, uses headers from Turn 0)\n",
    "        â†“ (Python schema extraction)\n",
    "```\n",
    "\n",
    "### Pipeline Stages:\n",
    "1. **Turn 0 (Independent)**: Image + prompt to identify column headers\n",
    "2. **Pattern Matching (Python)**: Map headers to generic concepts (Date, Description, Debit)\n",
    "3. **Turn 1 (Independent)**: Image + prompt to extract full table using identified headers\n",
    "4. **Turn 2 (Independent)**: Image + prompt to filter withdrawal transactions\n",
    "5. **Python Parsing**: Convert markdown to schema format\n",
    "\n",
    "### Key Differences from Multi-Turn:\n",
    "- âŒ **No conversation history** - each turn starts fresh\n",
    "- âŒ **No context accumulation** - prevents attention dilution\n",
    "- âœ… **Direct image access** - every turn has full image attention\n",
    "- âœ… **Better accuracy** - empirically proven to work better than multi-turn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports\n",
    "\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, MllamaForConditionalGeneration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Random Seed for Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Random seed set to 42 for reproducibility\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Set random seed\n",
    "\n",
    "from common.reproducibility import set_seed\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ Loading Llama-3.2-Vision model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">ğŸš€ Loading Llama Vision model with robust multi-GPU optimization...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34mğŸš€ Loading Llama Vision model with robust multi-GPU optimization\u001b[0m\u001b[1;34m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Features: Smart quantization, memory management, V100 support</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mFeatures: Smart quantization, memory management, V100 support\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">ğŸ”§ Configuring CUDA memory for Llama...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34mğŸ”§ Configuring CUDA memory for Llama\u001b[0m\u001b[34m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ CUDA memory allocation configured: max_split_size_mb:64\n",
      "ğŸ’¡ Using 64MB memory blocks to reduce fragmentation\n",
      "ğŸ“Š Initial CUDA state (Multi-GPU Total): Allocated=0.00GB, Reserved=0.00GB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">ğŸ” Performing robust GPU memory detection...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34mğŸ” Performing robust GPU memory detection\u001b[0m\u001b[34m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Starting robust GPU memory detection...\n",
      "ğŸ“Š Detected 2 GPU(s), analyzing each device...\n",
      "   GPU 0 (NVIDIA L40S): 44.5GB total, 44.5GB available\n",
      "   GPU 1 (NVIDIA L40S): 44.5GB total, 44.5GB available\n",
      "\n",
      "======================================================================\n",
      "ğŸ” ROBUST GPU MEMORY DETECTION REPORT\n",
      "======================================================================\n",
      "âœ… Success: 2/2 GPUs detected\n",
      "ğŸ“Š Total Memory: 89.04GB\n",
      "ğŸ’¾ Available Memory: 89.04GB\n",
      "âš¡ Allocated Memory: 0.00GB\n",
      "ğŸ”„ Reserved Memory: 0.00GB\n",
      "ğŸ“¦ Fragmentation: 0.00GB\n",
      "ğŸ–¥ï¸  Multi-GPU: Yes\n",
      "âš–ï¸  Balanced Distribution: Yes\n",
      "\n",
      "ğŸ“‹ Per-GPU Breakdown:\n",
      "   GPU 0 (NVIDIA L40S): 44.5GB total, 44.5GB available (0.0% used)\n",
      "   GPU 1 (NVIDIA L40S): 44.5GB total, 44.5GB available (0.0% used)\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">ğŸ“Š GPU Hardware: NVIDIA L40S </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">(</span><span style=\"color: #000080; text-decoration-color: #000080\">2x 45GB = 89GB total</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34mğŸ“Š GPU Hardware: NVIDIA L40S \u001b[0m\u001b[1;34m(\u001b[0m\u001b[34m2x 45GB = 89GB total\u001b[0m\u001b[1;34m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">ğŸ—ï¸ Architecture: workstation_high_memory </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">(</span><span style=\"color: #000080; text-decoration-color: #000080\">dynamic detection</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34mğŸ—ï¸ Architecture: workstation_high_memory \u001b[0m\u001b[1;34m(\u001b[0m\u001b[34mdynamic detection\u001b[0m\u001b[1;34m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">ğŸ¯ Model: Llama-</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">3.2</span><span style=\"color: #000080; text-decoration-color: #000080\">-11B-Vision </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">(</span><span style=\"color: #000080; text-decoration-color: #000080\">estimated need: 22GB + </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">6.</span><span style=\"color: #000080; text-decoration-color: #000080\">0GB buffer</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34mğŸ¯ Model: Llama-\u001b[0m\u001b[1;34m3.2\u001b[0m\u001b[34m-11B-Vision \u001b[0m\u001b[1;34m(\u001b[0m\u001b[34mestimated need: 22GB + \u001b[0m\u001b[1;34m6.\u001b[0m\u001b[34m0GB buffer\u001b[0m\u001b[1;34m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">ğŸ’¾ Available Memory: </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">89.</span><span style=\"color: #000080; text-decoration-color: #000080\">0GB across </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">2</span><span style=\"color: #000080; text-decoration-color: #000080\"> </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">GPU(</span><span style=\"color: #000080; text-decoration-color: #000080\">s</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34mğŸ’¾ Available Memory: \u001b[0m\u001b[1;34m89.\u001b[0m\u001b[34m0GB across \u001b[0m\u001b[1;34m2\u001b[0m\u001b[34m \u001b[0m\u001b[1;34mGPU\u001b[0m\u001b[1;34m(\u001b[0m\u001b[34ms\u001b[0m\u001b[1;34m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">ğŸ’¡ Memory sufficient: âœ… Yes</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34mğŸ’¡ Memory sufficient: âœ… Yes\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">âœ… workstation_high_memory with 89GB - running in full precision as requested</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mâœ… workstation_high_memory with 89GB - running in full precision as requested\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">ğŸ“Š FINAL QUANTIZATION DECISION: DISABLED (full precision)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36mğŸ“Š FINAL QUANTIZATION DECISION: DISABLED \u001b[0m\u001b[1;36m(\u001b[0m\u001b[1;36mfull precision\u001b[0m\u001b[1;36m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">   Total GPU Memory: 89GB</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36m   Total GPU Memory: 89GB\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">   Available Memory: 89GB</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36m   Available Memory: 89GB\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   Model needs: ~22GB + <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6.</span>0GB buffer for Llama-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.2</span>-11B-Vision\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   Model needs: ~22GB + \u001b[1;36m6.\u001b[0m0GB buffer for Llama-\u001b[1;36m3.2\u001b[0m-11B-Vision\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">   Working GPUs: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #008080; text-decoration-color: #008080\">/</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36m   Working GPUs: \u001b[0m\u001b[1;36m2\u001b[0m\u001b[36m/\u001b[0m\u001b[1;36m2\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">ğŸš€ Using </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">16</span><span style=\"color: #008000; text-decoration-color: #008000\">-bit precision for optimal performance</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mğŸš€ Using \u001b[0m\u001b[1;32m16\u001b[0m\u001b[32m-bit precision for optimal performance\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Loading Llama Vision model...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mLoading Llama Vision model\u001b[0m\u001b[36m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">ğŸ”„ Auto-distributing model across </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">2</span><span style=\"color: #000080; text-decoration-color: #000080\"> GPUs...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34mğŸ”„ Auto-distributing model across \u001b[0m\u001b[1;34m2\u001b[0m\u001b[34m GPUs\u001b[0m\u001b[34m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "460f4b36363e48d480a5b86ff37bf885",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Loading processor...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mLoading processor\u001b[0m\u001b[36m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">âœ… Model and processor loaded successfully!</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mâœ… Model and processor loaded successfully!\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">ğŸ”„ Multi-GPU Distribution Analysis </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">(</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">2</span><span style=\"color: #000080; text-decoration-color: #000080\"> GPUs</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">)</span><span style=\"color: #000080; text-decoration-color: #000080\">:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34mğŸ”„ Multi-GPU Distribution Analysis \u001b[0m\u001b[1;34m(\u001b[0m\u001b[1;34m2\u001b[0m\u001b[34m GPUs\u001b[0m\u001b[1;34m)\u001b[0m\u001b[34m:\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   GPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> <span style=\"font-weight: bold\">(</span>NVIDIA L40S<span style=\"font-weight: bold\">)</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9.</span>8GB/48GB <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20.8</span>%<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   GPU \u001b[1;36m0\u001b[0m \u001b[1m(\u001b[0mNVIDIA L40S\u001b[1m)\u001b[0m: \u001b[1;36m9.\u001b[0m8GB/48GB \u001b[1m(\u001b[0m\u001b[1;36m20.8\u001b[0m%\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   GPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> <span style=\"font-weight: bold\">(</span>NVIDIA L40S<span style=\"font-weight: bold\">)</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11.</span>6GB/48GB <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">24.4</span>%<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   GPU \u001b[1;36m1\u001b[0m \u001b[1m(\u001b[0mNVIDIA L40S\u001b[1m)\u001b[0m: \u001b[1;36m11.\u001b[0m6GB/48GB \u001b[1m(\u001b[0m\u001b[1;36m24.4\u001b[0m%\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">ğŸ“Š Total across all GPUs: </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">21.</span><span style=\"color: #000080; text-decoration-color: #000080\">3GB allocated, </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">21.</span><span style=\"color: #000080; text-decoration-color: #000080\">6GB reserved, 96GB capacity</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34mğŸ“Š Total across all GPUs: \u001b[0m\u001b[1;34m21.\u001b[0m\u001b[34m3GB allocated, \u001b[0m\u001b[1;34m21.\u001b[0m\u001b[34m6GB reserved, 96GB capacity\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">âœ… Model successfully distributed across GPUs</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mâœ… Model successfully distributed across GPUs\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">18</span> modules\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   \u001b[1;36m0\u001b[0m: \u001b[1;36m18\u001b[0m modules\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">28</span> modules\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   \u001b[1;36m1\u001b[0m: \u001b[1;36m28\u001b[0m modules\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                            ğŸ”§ Llama Vision Model Configuration                            </span>\n",
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Setting             </span>â”ƒ<span style=\"font-weight: bold\"> Value                         </span>â”ƒ<span style=\"font-weight: bold\"> Llama Status                      </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\"> Model Path          </span>â”‚<span style=\"color: #808000; text-decoration-color: #808000\"> Llama-3.2-11B-Vision-Instruct </span>â”‚<span style=\"color: #008000; text-decoration-color: #008000\"> âœ… Valid                          </span>â”‚\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\"> Device Placement    </span>â”‚<span style=\"color: #808000; text-decoration-color: #808000\"> cuda:0                        </span>â”‚<span style=\"color: #008000; text-decoration-color: #008000\"> âœ… Loaded                         </span>â”‚\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\"> Quantization Method </span>â”‚<span style=\"color: #808000; text-decoration-color: #808000\"> 16-bit                        </span>â”‚<span style=\"color: #008000; text-decoration-color: #008000\"> âœ… 16-bit (Performance Optimized) </span>â”‚\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\"> Data Type           </span>â”‚<span style=\"color: #808000; text-decoration-color: #808000\"> bfloat16                      </span>â”‚<span style=\"color: #008000; text-decoration-color: #008000\"> âœ… Recommended                    </span>â”‚\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\"> Max New Tokens      </span>â”‚<span style=\"color: #808000; text-decoration-color: #808000\"> 2000                          </span>â”‚<span style=\"color: #008000; text-decoration-color: #008000\"> âœ… Generation Ready               </span>â”‚\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\"> GPU Configuration   </span>â”‚<span style=\"color: #808000; text-decoration-color: #808000\"> 2x NVIDIA L40S (96GB)         </span>â”‚<span style=\"color: #008000; text-decoration-color: #008000\"> âœ… 96GB Total                     </span>â”‚\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\"> Model Parameters    </span>â”‚<span style=\"color: #808000; text-decoration-color: #808000\"> 10,670,220,835                </span>â”‚<span style=\"color: #008000; text-decoration-color: #008000\"> âœ… Loaded                         </span>â”‚\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\"> Memory Optimization </span>â”‚<span style=\"color: #808000; text-decoration-color: #808000\"> Llama Robust                  </span>â”‚<span style=\"color: #008000; text-decoration-color: #008000\"> âœ… V100 Compatible                </span>â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                            ğŸ”§ Llama Vision Model Configuration                            \u001b[0m\n",
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mSetting            \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mValue                        \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mLlama Status                     \u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36mModel Path         \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[33m \u001b[0m\u001b[33mLlama-3.2-11B-Vision-Instruct\u001b[0m\u001b[33m \u001b[0mâ”‚\u001b[32m \u001b[0m\u001b[32mâœ… Valid                         \u001b[0m\u001b[32m \u001b[0mâ”‚\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36mDevice Placement   \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[33m \u001b[0m\u001b[33mcuda:0                       \u001b[0m\u001b[33m \u001b[0mâ”‚\u001b[32m \u001b[0m\u001b[32mâœ… Loaded                        \u001b[0m\u001b[32m \u001b[0mâ”‚\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36mQuantization Method\u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[33m \u001b[0m\u001b[33m16-bit                       \u001b[0m\u001b[33m \u001b[0mâ”‚\u001b[32m \u001b[0m\u001b[32mâœ… 16-bit (Performance Optimized)\u001b[0m\u001b[32m \u001b[0mâ”‚\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36mData Type          \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[33m \u001b[0m\u001b[33mbfloat16                     \u001b[0m\u001b[33m \u001b[0mâ”‚\u001b[32m \u001b[0m\u001b[32mâœ… Recommended                   \u001b[0m\u001b[32m \u001b[0mâ”‚\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36mMax New Tokens     \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[33m \u001b[0m\u001b[33m2000                         \u001b[0m\u001b[33m \u001b[0mâ”‚\u001b[32m \u001b[0m\u001b[32mâœ… Generation Ready              \u001b[0m\u001b[32m \u001b[0mâ”‚\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36mGPU Configuration  \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[33m \u001b[0m\u001b[33m2x NVIDIA L40S (96GB)        \u001b[0m\u001b[33m \u001b[0mâ”‚\u001b[32m \u001b[0m\u001b[32mâœ… 96GB Total                    \u001b[0m\u001b[32m \u001b[0mâ”‚\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36mModel Parameters   \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[33m \u001b[0m\u001b[33m10,670,220,835               \u001b[0m\u001b[33m \u001b[0mâ”‚\u001b[32m \u001b[0m\u001b[32mâœ… Loaded                        \u001b[0m\u001b[32m \u001b[0mâ”‚\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36mMemory Optimization\u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[33m \u001b[0m\u001b[33mLlama Robust                 \u001b[0m\u001b[33m \u001b[0mâ”‚\u001b[32m \u001b[0m\u001b[32mâœ… V100 Compatible               \u001b[0m\u001b[32m \u001b[0mâ”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Running model compatibility test...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mRunning model compatibility test\u001b[0m\u001b[36m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">âœ… Model compatibility test passed</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mâœ… Model compatibility test passed\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Performing initial memory cleanup...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mPerforming initial memory cleanup\u001b[0m\u001b[36m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">ğŸ§¹ Memory cleanup completed\n",
       "</pre>\n"
      ],
      "text/plain": [
       "ğŸ§¹ Memory cleanup completed\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">ğŸ’¾ Final state <span style=\"font-weight: bold\">(</span>Multi-GPU Total<span style=\"font-weight: bold\">)</span>: <span style=\"color: #808000; text-decoration-color: #808000\">Allocated</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">21.</span>34GB, <span style=\"color: #808000; text-decoration-color: #808000\">Reserved</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">21.</span>65GB, <span style=\"color: #808000; text-decoration-color: #808000\">Fragmentation</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.</span>30GB\n",
       "</pre>\n"
      ],
      "text/plain": [
       "ğŸ’¾ Final state \u001b[1m(\u001b[0mMulti-GPU Total\u001b[1m)\u001b[0m: \u001b[33mAllocated\u001b[0m=\u001b[1;36m21\u001b[0m\u001b[1;36m.\u001b[0m34GB, \u001b[33mReserved\u001b[0m=\u001b[1;36m21\u001b[0m\u001b[1;36m.\u001b[0m65GB, \u001b[33mFragmentation\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.\u001b[0m30GB\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">ğŸ‰ Llama Vision model loading and validation complete!</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mğŸ‰ Llama Vision model loading and validation complete!\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">ğŸ”§ Llama optimizations active: </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">16</span><span style=\"color: #000080; text-decoration-color: #000080\">-bit precision, memory management, vision preservation</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34mğŸ”§ Llama optimizations active: \u001b[0m\u001b[1;34m16\u001b[0m\u001b[34m-bit precision, memory management, vision preservation\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model weights tied successfully\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Load Llama-3.2-Vision model\n",
    "# Update this path to your local Llama model\n",
    "# model_id = \"/home/jovyan/shared_PTM/Llama-3.2-11B-Vision-Instruct\"\n",
    "model_id = \"/home/jovyan/nfs_share/models/Llama-3.2-11B-Vision-Instruct\"\n",
    "\n",
    "print(\"ğŸ”§ Loading Llama-3.2-Vision model...\")\n",
    "# model = MllamaForConditionalGeneration.from_pretrained(\n",
    "#     model_id,\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     device_map=\"auto\",\n",
    "# )\n",
    "# processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "from common.llama_model_loader_robust import load_llama_model_robust\n",
    "\n",
    "model, processor = load_llama_model_robust(\n",
    "    model_path=model_id,\n",
    "    use_quantization=False,\n",
    "    device_map='auto',\n",
    "    max_new_tokens=2000,\n",
    "    torch_dtype='bfloat16',\n",
    "    low_cpu_mem_usage=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Add tie_weights() call\n",
    "try:\n",
    "    model.tie_weights()\n",
    "    print(\"âœ… Model weights tied successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ tie_weights() warning: {e}\")\n",
    "\n",
    "# processor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Loading image...\n",
      "âœ… Image loaded: (785, 769)\n",
      "âœ… Images list created with 1 image(s)\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Load bank statement image\n",
    "# Update this path to your test image\n",
    "# imageName = \"/home/jovyan/nfs_share/tod/LMM_POC/evaluation_data/synthetic_bank_images/cba_amount_balance.png\"\n",
    "# imageName = \"/home/jovyan/nfs_share/tod/LMM_POC/evaluation_data/synthetic_bank_images/cba_date_grouped_cont.png\"\n",
    "# imageName = \"/home/jovyan/nfs_share/tod/LMM_POC/evaluation_data/synthetic_bank_images/cba_debit_credit.png\"\n",
    "imageName = \"/home/jovyan/nfs_share/tod/LMM_POC/evaluation_data/synthetic_bank_images/cba_highligted.png\"\n",
    "# imageName = \"/home/jovyan/nfs_share/tod/LMM_POC/evaluation_data/synthetic_bank_images/low_contrast.png\"\n",
    "# imageName = \"/home/jovyan/nfs_share/tod/LMM_POC/evaluation_data/synthetic_bank_images/nab_classic_highligted.png\"\n",
    "# imageName = \"/home/jovyan/nfs_share/tod/LMM_POC/evaluation_data/synthetic_bank_images/westpac_debit_credit.png\"\n",
    "# imageName = \"/home/jovyan/nfs_share/tod/LMM_POC/evaluation_data/synthetic_bank_images/transaction_summary.png\"\n",
    "\n",
    "\n",
    "\n",
    "# imageName = \"/home/jovyan/nfs_share/tod/LMM_POC/evaluation_data/images/image_009.png\"\n",
    "print(\"ğŸ“ Loading image...\")\n",
    "image = Image.open(imageName)\n",
    "\n",
    "# CRITICAL: Store as list for multi-turn compatibility\n",
    "images = [image]\n",
    "\n",
    "print(f\"âœ… Image loaded: {image.size}\")\n",
    "print(f\"âœ… Images list created with {len(images)} image(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Turn Bank Statement Protocol\n",
    "- Turn 0: Identify actual table headers\n",
    "- Turn 1: Extract full table using those headers\n",
    "- Turn 2: Filter using the actual column names found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¬ TURN 0: Identifying actual table headers\n",
      "ğŸ¤– Generating response with Llama-3.2-Vision...\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Turn 0 - Identify table headers (prompt)\n",
    "# TURN 0: Identify Table Headers\n",
    "# First, identify the actual column headers used in this specific bank statement\n",
    "\n",
    "prompt = \"\"\"\n",
    "Look at the transaction table in this bank statement image.\n",
    "\n",
    "IMPORTANT STRUCTURAL NOTE:\n",
    "Some bank statements show dates as section headings with multiple transactions underneath.\n",
    "If you see this structure, remember that each transaction needs its explicit date in the final output.\n",
    "\n",
    "What are the exact column header names used in the transaction table?\n",
    "\n",
    "List each column header exactly as it appears, in order from left to right.\n",
    "Do not interpret or rename them - use the EXACT text from the image.\n",
    "\"\"\"\n",
    "\n",
    "# Create message structure for Llama\n",
    "messageDataStructure = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\"},\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": prompt,\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"ğŸ’¬ TURN 0: Identifying actual table headers\")\n",
    "print(\"ğŸ¤– Generating response with Llama-3.2-Vision...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Response generated successfully!\n",
      "\n",
      "============================================================\n",
      "TURN 0 - IDENTIFIED TABLE HEADERS:\n",
      "============================================================\n",
      "**Transaction Table Column Headers:**\n",
      "\n",
      "*   **Date**\n",
      "*   **Transaction**\n",
      "*   **Debit**\n",
      "*   **Credit**\n",
      "*   **Balance**\n",
      "============================================================\n",
      "\n",
      "ğŸ“‹ Parsed 5 column headers:\n",
      "  1. 'Date'\n",
      "  2. 'Transaction'\n",
      "  3. 'Debit'\n",
      "  4. 'Credit'\n",
      "  5. 'Balance'\n",
      "\n",
      "âœ… Table headers saved to: llama_table_headers.txt\n",
      "ğŸ’¡ These LITERAL header names will be used in Turn 1 & 2 prompts\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Turn 0 - Execute and parse headers\n",
    "# Process the input using the CORRECT multi-turn pattern\n",
    "# Based on: https://medium.com/data-science/chat-with-your-images-using-multimodal-llms-60af003e8bfa\n",
    "\n",
    "textInput = processor.apply_chat_template(\n",
    "    messageDataStructure, add_generation_prompt=True\n",
    ")\n",
    "\n",
    "# CRITICAL: Use named parameter 'images=' with list\n",
    "inputs = processor(images=images, text=textInput, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Generate response with deterministic parameters\n",
    "output = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=2000,\n",
    "    do_sample=False,\n",
    "    temperature=None,\n",
    "    top_p=None,\n",
    ")\n",
    "\n",
    "# CRITICAL: Trim input tokens from output (this is the key to clean responses!)\n",
    "generate_ids = output[:, inputs['input_ids'].shape[1]:-1]\n",
    "cleanedOutput = processor.decode(generate_ids[0], clean_up_tokenization_spaces=False)\n",
    "\n",
    "print(\"âœ… Response generated successfully!\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TURN 0 - IDENTIFIED TABLE HEADERS:\")\n",
    "print(\"=\" * 60)\n",
    "print(cleanedOutput)\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# CRITICAL: Parse the identified headers for use in subsequent turns\n",
    "# Extract column names from the response\n",
    "header_lines = [line.strip() for line in cleanedOutput.split('\\n') if line.strip()]\n",
    "identified_headers = []\n",
    "\n",
    "# Look for numbered list or bullet points\n",
    "for line in header_lines:\n",
    "    # Remove common list markers\n",
    "    cleaned = line.lstrip('0123456789.-â€¢* ').strip()\n",
    "    \n",
    "    # Strip markdown bold formatting\n",
    "    cleaned = cleaned.replace('**', '').replace('__', '')\n",
    "    \n",
    "    # Skip section headers (lines ending with colon)\n",
    "    if cleaned.endswith(':'):\n",
    "        continue\n",
    "    \n",
    "    # Skip long sentences (likely explanatory text, not headers)\n",
    "    if len(cleaned) > 40:\n",
    "        continue\n",
    "        \n",
    "    if cleaned and len(cleaned) > 2:  # Ignore very short strings\n",
    "        identified_headers.append(cleaned)\n",
    "\n",
    "print(f\"\\nğŸ“‹ Parsed {len(identified_headers)} column headers:\")\n",
    "for i, header in enumerate(identified_headers, 1):\n",
    "    print(f\"  {i}. '{header}'\")\n",
    "\n",
    "# Store headers for use in subsequent turns\n",
    "table_headers = identified_headers\n",
    "\n",
    "# Save the table headers\n",
    "output_path = Path(\"llama_table_headers.txt\")\n",
    "with output_path.open(\"w\", encoding=\"utf-8\") as text_file:\n",
    "    text_file.write(cleanedOutput)\n",
    "\n",
    "print(f\"\\nâœ… Table headers saved to: {output_path}\")\n",
    "print(\"ğŸ’¡ These LITERAL header names will be used in Turn 1 & 2 prompts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern Matching: Map Generic Concepts to Actual Headers\n",
    "\n",
    "Different bank statements use different column names. Use pattern matching to identify:\n",
    "- Which header represents **Date**\n",
    "- Which header represents **Description/Details**  \n",
    "- Which header represents **Debit/Withdrawal**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PATTERN MATCHING RESULTS:\n",
      "============================================================\n",
      "ğŸ“‹ Extracted Headers: ['Date', 'Transaction', 'Debit', 'Credit', 'Balance']\n",
      "\n",
      "ğŸ” Mapped Columns:\n",
      "  Date        â†’ 'Date'\n",
      "  Description â†’ 'Transaction'\n",
      "  Debit       â†’ 'Debit'\n",
      "  Credit      â†’ 'Credit'\n",
      "  Balance     â†’ 'Balance'\n",
      "============================================================\n",
      "\n",
      "âœ… These literal column names will be used in Turn 1 and Turn 2\n",
      "ğŸ’¡ Adjust patterns above if matching fails for your bank statement format\n"
     ]
    }
   ],
   "source": [
    "# Cell 13: Pattern Matching - Map headers to generic columns\n",
    "# Pattern Matching: Map extracted headers to generic concepts\n",
    "# This handles variety in bank statement column naming conventions\n",
    "\n",
    "# Pattern keywords for each concept (in priority order)\n",
    "DATE_PATTERNS = ['date', 'day', 'transaction date', 'trans date']\n",
    "DESCRIPTION_PATTERNS = [\n",
    "    'description', 'details', 'transaction details', 'trans details',\n",
    "    'particulars', 'narrative', 'transaction', 'trans'\n",
    "]\n",
    "DEBIT_PATTERNS = ['debit', 'withdrawal', 'withdrawals', 'paid', 'paid out', 'spent', 'dr']\n",
    "CREDIT_PATTERNS = ['credit', 'deposit', 'deposits', 'received', 'cr']\n",
    "BALANCE_PATTERNS = ['balance', 'bal', 'running balance']\n",
    "\n",
    "# NEW: Pattern for single-column transaction formats (e.g., \"Amount\" instead of separate Debit/Credit)\n",
    "AMOUNT_PATTERNS = ['amount', 'amt', 'value', 'total']\n",
    "\n",
    "def match_header(headers, patterns, fallback=None):\n",
    "    \"\"\"Match a header using pattern keywords.\n",
    "    \n",
    "    Matching strategy:\n",
    "    1. Exact match (case-insensitive)\n",
    "    2. Substring match (only for patterns with length > 2 to avoid false positives)\n",
    "    \"\"\"\n",
    "    headers_lower = [h.lower() for h in headers]\n",
    "    \n",
    "    # Try exact match first\n",
    "    for pattern in patterns:\n",
    "        for i, header_lower in enumerate(headers_lower):\n",
    "            if pattern == header_lower:\n",
    "                return headers[i]\n",
    "    \n",
    "    # Try substring match (only for patterns longer than 2 chars)\n",
    "    for pattern in patterns:\n",
    "        if len(pattern) > 2:  # Avoid false positives like 'cr' matching 'description'\n",
    "            for i, header_lower in enumerate(headers_lower):\n",
    "                if pattern in header_lower:\n",
    "                    return headers[i]\n",
    "    \n",
    "    return fallback\n",
    "\n",
    "# Perform pattern matching on extracted headers\n",
    "date_col = match_header(table_headers, DATE_PATTERNS, fallback=table_headers[0] if table_headers else 'Date')\n",
    "desc_col = match_header(table_headers, DESCRIPTION_PATTERNS, fallback=table_headers[1] if len(table_headers) > 1 else 'Description')\n",
    "\n",
    "# NEW: First try to match a generic \"Amount\" column (for 4-column formats)\n",
    "amount_col = match_header(table_headers, AMOUNT_PATTERNS, fallback=None)\n",
    "\n",
    "# Use amount_col as fallback if no separate debit/credit columns exist\n",
    "# This handles formats like: Date | Description | Amount | Balance\n",
    "debit_col = match_header(table_headers, DEBIT_PATTERNS, fallback=amount_col if amount_col else 'Debit')\n",
    "credit_col = match_header(table_headers, CREDIT_PATTERNS, fallback=amount_col if amount_col else 'Credit')\n",
    "balance_col = match_header(table_headers, BALANCE_PATTERNS, fallback='Balance')\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PATTERN MATCHING RESULTS:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"ğŸ“‹ Extracted Headers: {table_headers}\")\n",
    "print(f\"\\nğŸ” Mapped Columns:\")\n",
    "print(f\"  Date        â†’ '{date_col}'\")\n",
    "print(f\"  Description â†’ '{desc_col}'\")\n",
    "print(f\"  Debit       â†’ '{debit_col}'\")\n",
    "print(f\"  Credit      â†’ '{credit_col}'\")\n",
    "print(f\"  Balance     â†’ '{balance_col}'\")\n",
    "if amount_col:\n",
    "    print(f\"\\nğŸ’¡ Single-column format detected: '{amount_col}' used for both debit and credit\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nâœ… These literal column names will be used in Turn 1 and Turn 2\")\n",
    "print(\"ğŸ’¡ Adjust patterns above if matching fails for your bank statement format\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ”‘ Independent Single-Turn Pattern (NOT Multi-Turn Conversation)\n",
    "\n",
    "**CRITICAL INSIGHT**: Multi-turn conversation accumulates context and degrades accuracy.\n",
    "\n",
    "Instead, we use **three independent single-turn prompts**, each with fresh context:\n",
    "\n",
    "#### Key Principles:\n",
    "\n",
    "1. **No Conversation History**: Each turn is completely independent\n",
    "2. **Fresh Image Attention**: Each turn processes the image directly\n",
    "3. **No Context Accumulation**: Prevents attention dilution\n",
    "4. **Headers as Parameters**: Turn 0 headers are passed as text parameters to Turn 1 and Turn 2\n",
    "\n",
    "#### Message Structure for Each Turn:\n",
    "\n",
    "Every turn uses the same fresh structure:\n",
    "```python\n",
    "messageDataStructure = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\"},\n",
    "            {\"type\": \"text\", \"text\": \"<prompt with headers from Turn 0>\"}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "```\n",
    "\n",
    "**No assistant responses in history. No conversation accumulation.**\n",
    "\n",
    "#### Why This Works Better:\n",
    "\n",
    "- **Turn 0**: Clean context â†’ accurate header identification\n",
    "- **Turn 1**: Clean context + headers â†’ accurate table extraction  \n",
    "- **Turn 2**: Clean context + headers â†’ accurate filtering\n",
    "\n",
    "Each turn has **full attention** on the image, not diluted by conversation history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Independent turn approach - NO conversation history\n",
      "ğŸ’¡ Each turn will have fresh context with direct image access\n"
     ]
    }
   ],
   "source": [
    "# Cell 15: NO conversation history (independent turns)\n",
    "# \n",
    "# CRITICAL: We do NOT use conversation history in this notebook.\n",
    "# Each turn is completely independent with fresh context.\n",
    "#\n",
    "# Why? Multi-turn conversation accumulates context and degrades accuracy:\n",
    "# - Turn 0: 50 tokens â†’ accurate\n",
    "# - Turn 1 with history: 350 tokens â†’ attention diluted â†’ less accurate\n",
    "# - Turn 2 with history: 2000 tokens â†’ attention heavily diluted â†’ poor accuracy\n",
    "#\n",
    "# Instead: Each turn starts fresh with just image + prompt\n",
    "\n",
    "print(\"âœ… Independent turn approach - NO conversation history\")\n",
    "print(\"ğŸ’¡ Each turn will have fresh context with direct image access\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TURN 1: Extract Full Table in Markdown\n",
    "\n",
    "Now that we know the actual column headers, extract the complete table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¬ TURN 1: Extract full markdown table (INDEPENDENT)\n",
      "ğŸ“‹ Using literal headers: ['Date', 'Transaction', 'Debit', 'Credit', 'Balance']\n",
      "ğŸ’¡ Column position rules reference: 'Debit' and 'Credit'\n",
      "ğŸ”„ Fresh context (no conversation history)\n",
      "ğŸ¤– Generating response with Llama-3.2-Vision...\n",
      "\n",
      "âœ… Follow-up response generated successfully!\n",
      "\n",
      "============================================================\n",
      "TURN 1 - FULL MARKDOWN TABLE:\n",
      "============================================================\n",
      "| Date | Transaction | Debit | Credit | Balance |\n",
      "| --- | --- | --- | --- | --- |\n",
      "| 21 Mar | Direct Credit 890543 FOOD DELIVERY |  | 458.20 | $6,842.15 CR |\n",
      "| 23 Mar | Salary CORPORATION PTY LTD BANK ACCOUNT | 2,145.80 |  | $4,696.35 CR |\n",
      "| 24 Mar | Salary OpenMinds 456789 ASP345678 |  | 850.00 | $5,546.35 CR |\n",
      "| 27 Mar | Direct Credit 678901 LOGISTICS CO (A Paid by didi |  | 325.45 | $5,871.80 CR |\n",
      "| 28 Mar | Direct Credit 890432 RIDESHARE BV Store ID: ABC_J | 178.90 |  | $6,050.70 CR |\n",
      "| 30 Mar | Salary ENTERPRISE WAGES |  | 1,285.65 | $7,336.35 CR |\n",
      "| 02 Apr | Transfer to xx9876 CommBank app | 4,200.00 |  | $3,136.35 CR |\n",
      "| 03 Apr 2024 | CLOSING BALANCE |  |  | $3,136.35 CR |\n",
      "============================================================\n",
      "\n",
      "âœ… Markdown table saved to: llama_markdown_table_extraction.txt\n",
      "ğŸ’¡ Each transaction has explicit date, even if grouped by date heading\n"
     ]
    }
   ],
   "source": [
    "# Cell 17: Turn 1 - Extract full table (INDEPENDENT, fresh context)\n",
    "\n",
    "# Build the header string using LITERAL names from Turn 0\n",
    "header_string = \" | \".join(table_headers)\n",
    "\n",
    "# follow_up_prompt = f\"\"\"\n",
    "# Extract the entire transaction table from this bank statement image in markdown format.\n",
    "\n",
    "# Use these EXACT column headers in this order:\n",
    "# {header_string}\n",
    "\n",
    "# Format requirements:\n",
    "# - Standard markdown table syntax with | delimiters\n",
    "# - Header row: | {header_string} |\n",
    "# - Separator row: | {\" | \".join([\"---\"] * len(table_headers))} |\n",
    "\n",
    "# CRITICAL EXTRACTION RULES:\n",
    "# 1. Extract EVERY transaction as a separate row\n",
    "# 2. Each transaction MUST have its explicit date in the date column\n",
    "# 3. If multiple transactions share a date heading, repeat that date for each transaction row\n",
    "# 4. Do NOT skip or combine any rows\n",
    "# 5. Keep all amounts with decimal values intact\n",
    "# 6. Do NOT add explanatory text - only output the markdown table\n",
    "\n",
    "# CRITICAL COLUMN POSITION RULES:\n",
    "# 7. Place each amount in the EXACT column where it appears in the original image\n",
    "# 8. Do NOT move amounts between columns based on transaction type\n",
    "# 9. Do NOT interpret whether a transaction should be a debit or credit\n",
    "# 10. If an amount appears under the \"{debit_col}\" column in the image, put it in the {debit_col} column in your table\n",
    "# 11. If an amount appears under the \"{credit_col}\" column in the image, put it in the {credit_col} column in your table\n",
    "# 12. If a column is empty for a row in the image, leave it empty in your table\n",
    "# 13. Preserve the EXACT visual layout - read amounts strictly by their column position\n",
    "\n",
    "# Example: If you see:\n",
    "#   01/06/2024\n",
    "#     Transaction A    $100\n",
    "#     Transaction B    $50\n",
    "    \n",
    "# Output as TWO rows:\n",
    "#   | 01/06/2024 | Transaction A | $100 | ... |\n",
    "#   | 01/06/2024 | Transaction B | $50  | ... |\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "# CRITICAL EXTRACTION RULES:\n",
    "# 1. Extract EVERY transaction as a separate row\n",
    "# 2. Each transaction MUST have its explicit date in the date column\n",
    "# 3. If multiple transactions share a date heading, repeat that date for each transaction row\n",
    "# 4. Do NOT skip or combine any rows\n",
    "# 5. Do NOT add explanatory text - only output the markdown table\n",
    "\n",
    "follow_up_prompt = f\"\"\"\n",
    "Extract the ONLY the MAIN transaction table from this bank statement image in markdown format.\n",
    "CRITICAL EXTRACTION RULES:\n",
    "1. EACH ROW MUST be ALIGNED UNDER the CORRECT COLUMN HEADING. \n",
    "2. If you see a transaction row that looks like:\n",
    "      a date |  line 1  |  | amount1 | amount2\n",
    "             |  line 2  |  |         |\n",
    "    you must extract it as\n",
    "      a date |  line 1 line 2 |  | amount1 | amount2\n",
    "3. If you see a transaction row that looks like:\n",
    "      a date |  line 1  | amount1 |  | amount2\n",
    "             |  line 2  |         |  |\n",
    "    you must extract it as\n",
    "      a date |  line 1 line 2 | amount1 |  | amount2\n",
    "4. IGNORE ANY OTHER TABLES that may exist\n",
    "5. DO NOT ADD ANY further information or explanation.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# CRITICAL: Create FRESH message structure (NOT appending to conversation history)\n",
    "messageDataStructure_turn1 = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\"},\n",
    "            {\"type\": \"text\", \"text\": follow_up_prompt}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"ğŸ’¬ TURN 1: Extract full markdown table (INDEPENDENT)\")\n",
    "print(f\"ğŸ“‹ Using literal headers: {table_headers}\")\n",
    "print(f\"ğŸ’¡ Column position rules reference: '{debit_col}' and '{credit_col}'\")\n",
    "print(f\"ğŸ”„ Fresh context (no conversation history)\")\n",
    "print(\"ğŸ¤– Generating response with Llama-3.2-Vision...\")\n",
    "\n",
    "# Process with FRESH context\n",
    "textInput = processor.apply_chat_template(\n",
    "    messageDataStructure_turn1, add_generation_prompt=True\n",
    ")\n",
    "\n",
    "# CRITICAL: Use named parameter 'images=' with list\n",
    "inputs = processor(images=images, text=textInput, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Generate response\n",
    "output = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=2000,\n",
    "    do_sample=False,\n",
    "    temperature=None,\n",
    "    top_p=None,\n",
    ")\n",
    "\n",
    "# CRITICAL: Trim input tokens from output\n",
    "generate_ids = output[:, inputs['input_ids'].shape[1]:-1]\n",
    "cleanedOutput2 = processor.decode(generate_ids[0], clean_up_tokenization_spaces=False)\n",
    "\n",
    "print(\"\\nâœ… Follow-up response generated successfully!\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TURN 1 - FULL MARKDOWN TABLE:\")\n",
    "print(\"=\" * 60)\n",
    "print(cleanedOutput2)\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Save the markdown table\n",
    "output_path = Path(\"llama_markdown_table_extraction.txt\")\n",
    "with output_path.open(\"w\", encoding=\"utf-8\") as text_file:\n",
    "    text_file.write(cleanedOutput2)\n",
    "\n",
    "print(f\"\\nâœ… Markdown table saved to: {output_path}\")\n",
    "print(\"ğŸ’¡ Each transaction has explicit date, even if grouped by date heading\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TURN 2: Filter Using Actual Column Names\n",
    "\n",
    "Filter the extracted table using the specific column names identified in Turn 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¬ TURN 2: Filter withdrawal transactions (INDEPENDENT)\n",
      "ğŸ“‹ Filter columns: 'Date' | 'Transaction' | 'Debit'\n",
      "ğŸ”„ Fresh context (no conversation history)\n",
      "ğŸ¤– Generating response with Llama-3.2-Vision...\n",
      "\n",
      "âœ… Follow-up response generated successfully!\n",
      "\n",
      "============================================================\n",
      "TURN 2 - FILTERED WITHDRAWALS:\n",
      "============================================================\n",
      "Here is the data from the image in markdown format:\n",
      "\n",
      "* **21 Mar**\n",
      "\t+ Transaction: Direct Credit 890543 FOOD DELIVERY\n",
      "\t+ Debit: 2,145.80\n",
      "* **23 Mar**\n",
      "\t+ Transaction: Salary CORPORATION PTY LTD BANK ACCOUNT\n",
      "\t+ Debit: 2,145.80\n",
      "* **24 Mar**\n",
      "\t+ Transaction: Salary OpenMinds 456789 ASP345678\n",
      "\t+ Debit: 850.00\n",
      "* **27 Mar**\n",
      "\t+ Transaction: Direct Credit 678901 LOGISTICS CO (A Paid by didi\n",
      "\t+ Debit: 325.45\n",
      "* **28 Mar**\n",
      "\t+ Transaction: Direct Credit 890432 RIDESHARE BV Store ID: ABC_J\n",
      "\t+ Debit: 178.90\n",
      "* **30 Mar**\n",
      "\t+ Transaction: Salary ENTERPRISE WAGES\n",
      "\t+ Debit: 1,285.65\n",
      "* **02 Apr**\n",
      "\t+ Transaction: Transfer to xx9876 CommBank app\n",
      "\t+ Debit: 4,200.00\n",
      "* **03 Apr 2024**\n",
      "\t+ Transaction: CLOSING BALANCE\n",
      "\t+ Debit: 3,136.35\n",
      "============================================================\n",
      "\n",
      "âœ… Filtered table saved to: llama_filtered_withdrawals.txt\n",
      "\n",
      "âœ… Complete Protocol:\n",
      "   Turn 0: Identify headers â†’ ['Date', 'Transaction', 'Debit', 'Credit', 'Balance']\n",
      "   Pattern Match: Date='Date', Desc='Transaction', Debit='Debit'\n",
      "   Turn 1: Extract full table (independent, fresh context)\n",
      "   Turn 2: Filter withdrawals (independent, fresh context)\n",
      "\n",
      "âœ… Independent turns - better accuracy than multi-turn conversation!\n"
     ]
    }
   ],
   "source": [
    "# Cell 19: Turn 2 - Filter withdrawals (INDEPENDENT, fresh context)\n",
    "\n",
    "# follow_up_prompt_3 = f\"\"\"\n",
    "# Extract ONLY the withdrawal/debit transactions from this bank statement image in markdown format.\n",
    "\n",
    "# Use these EXACT column names:\n",
    "# - {date_col}\n",
    "# - {desc_col}  \n",
    "# - {debit_col}\n",
    "\n",
    "# Filter rules:\n",
    "# - Only include rows where '{debit_col}' column has a value (not empty)\n",
    "# - Exclude credit/deposit transactions  \n",
    "# - Keep the markdown table format with header: | {date_col} | {desc_col} | {debit_col} |\n",
    "\n",
    "# Output only the filtered markdown table showing withdrawal transactions.\n",
    "# \"\"\"\n",
    "\n",
    "follow_up_prompt_3 = f\"\"\"\n",
    "SELECT ONLY the {date_col}, {desc_col} and {debit_col} columns from this bank statement image in markdown format.\n",
    "\"\"\"\n",
    "\n",
    "# CRITICAL: Create FRESH message structure (NOT appending to conversation history)\n",
    "messageDataStructure_turn2 = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\"},\n",
    "            {\"type\": \"text\", \"text\": follow_up_prompt_3}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"ğŸ’¬ TURN 2: Filter withdrawal transactions (INDEPENDENT)\")\n",
    "print(f\"ğŸ“‹ Filter columns: '{date_col}' | '{desc_col}' | '{debit_col}'\")\n",
    "print(f\"ğŸ”„ Fresh context (no conversation history)\")\n",
    "print(\"ğŸ¤– Generating response with Llama-3.2-Vision...\")\n",
    "\n",
    "# Process with FRESH context\n",
    "textInput = processor.apply_chat_template(\n",
    "    messageDataStructure_turn2, add_generation_prompt=True\n",
    ")\n",
    "\n",
    "# Use named parameter 'images=' with list\n",
    "inputs = processor(images=images, text=textInput, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Generate response\n",
    "output = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=2000,\n",
    "    do_sample=False,\n",
    "    temperature=None,\n",
    "    top_p=None,\n",
    ")\n",
    "\n",
    "# Trim input tokens from output\n",
    "generate_ids = output[:, inputs['input_ids'].shape[1]:-1]\n",
    "cleanedOutput3 = processor.decode(generate_ids[0], clean_up_tokenization_spaces=False)\n",
    "\n",
    "print(\"\\nâœ… Follow-up response generated successfully!\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TURN 2 - FILTERED WITHDRAWALS:\")\n",
    "print(\"=\" * 60)\n",
    "print(cleanedOutput3)\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Save filtered results\n",
    "output_path = Path(\"llama_filtered_withdrawals.txt\")\n",
    "with output_path.open(\"w\", encoding=\"utf-8\") as text_file:\n",
    "    text_file.write(cleanedOutput3)\n",
    "\n",
    "print(f\"\\nâœ… Filtered table saved to: {output_path}\")\n",
    "print(f\"\\nâœ… Complete Protocol:\")\n",
    "print(f\"   Turn 0: Identify headers â†’ {table_headers}\")\n",
    "print(f\"   Pattern Match: Date='{date_col}', Desc='{desc_col}', Debit='{debit_col}'\")\n",
    "print(f\"   Turn 1: Extract full table (independent, fresh context)\")\n",
    "print(f\"   Turn 2: Filter withdrawals (independent, fresh context)\")\n",
    "print(\"\\nâœ… Independent turns - better accuracy than multi-turn conversation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Schema Fields from Markdown Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Parsed 0 transactions from markdown table\n",
      "\n",
      "============================================================\n",
      "SAMPLE ROWS:\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "EXTRACTED SCHEMA FIELDS:\n",
      "============================================================\n",
      "TRANSACTION_DATES: NOT_FOUND\n",
      "LINE_ITEM_DESCRIPTIONS: NOT_FOUND\n",
      "TRANSACTION_AMOUNTS_PAID: NOT_FOUND\n",
      "STATEMENT_DATE_RANGE: NOT_FOUND\n",
      "============================================================\n",
      "\n",
      "âœ… Schema fields saved to: llama_extracted_fields.txt\n",
      "ğŸ’¡ Fields extracted from columns: 'Date' | 'Transaction' | 'Debit'\n"
     ]
    }
   ],
   "source": [
    "# Cell 21: Parse markdown and extract schema fields\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "def parse_markdown_table(markdown_text):\n",
    "    \"\"\"Parse markdown table into list of dictionaries.\"\"\"\n",
    "    lines = [line.strip() for line in markdown_text.strip().split('\\n') if line.strip()]\n",
    "    \n",
    "    # Find header row (first line with pipes)\n",
    "    header_idx = None\n",
    "    for i, line in enumerate(lines):\n",
    "        if '|' in line and not set(line.replace('|', '').replace('-', '').strip()) == set():\n",
    "            header_idx = i\n",
    "            break\n",
    "    \n",
    "    if header_idx is None:\n",
    "        return []\n",
    "    \n",
    "    # Parse headers\n",
    "    header_line = lines[header_idx]\n",
    "    headers = [h.strip() for h in header_line.split('|') if h.strip()]\n",
    "    \n",
    "    # Parse data rows (skip header and separator)\n",
    "    rows = []\n",
    "    for line in lines[header_idx + 1:]:\n",
    "        if '|' in line:\n",
    "            # Skip separator rows (contain only pipes, hyphens, and spaces)\n",
    "            cleaned = line.replace('|', '').replace('-', '').replace(' ', '')\n",
    "            if not cleaned:\n",
    "                continue\n",
    "            \n",
    "            values = [v.strip() for v in line.split('|') if v.strip()]\n",
    "            if len(values) == len(headers):\n",
    "                rows.append(dict(zip(headers, values)))\n",
    "    \n",
    "    return rows\n",
    "\n",
    "def extract_schema_fields(rows, date_col, desc_col, debit_col):\n",
    "    \"\"\"Extract fields in universal.yaml schema format.\"\"\"\n",
    "    if not rows:\n",
    "        return {\n",
    "            'TRANSACTION_DATES': 'NOT_FOUND',\n",
    "            'LINE_ITEM_DESCRIPTIONS': 'NOT_FOUND',\n",
    "            'TRANSACTION_AMOUNTS_PAID': 'NOT_FOUND',\n",
    "            'STATEMENT_DATE_RANGE': 'NOT_FOUND'\n",
    "        }\n",
    "    \n",
    "    # Extract lists\n",
    "    dates = []\n",
    "    descriptions = []\n",
    "    amounts = []\n",
    "    \n",
    "    for row in rows:\n",
    "        date = row.get(date_col, '').strip()\n",
    "        desc = row.get(desc_col, '').strip()\n",
    "        amount = row.get(debit_col, '').strip()\n",
    "        \n",
    "        if date:\n",
    "            dates.append(date)\n",
    "        if desc:\n",
    "            descriptions.append(desc)\n",
    "        if amount:\n",
    "            amounts.append(amount)\n",
    "    \n",
    "    # Calculate statement date range\n",
    "    date_range = 'NOT_FOUND'\n",
    "    if dates:\n",
    "        try:\n",
    "            # Try to parse dates to find min/max\n",
    "            parsed_dates = []\n",
    "            for d in dates:\n",
    "                # Try common formats\n",
    "                for fmt in ['%d/%m/%Y', '%d/%m/%y', '%Y-%m-%d', '%d-%m-%Y']:\n",
    "                    try:\n",
    "                        parsed_dates.append(datetime.strptime(d, fmt))\n",
    "                        break\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "            \n",
    "            if parsed_dates:\n",
    "                min_date = min(parsed_dates)\n",
    "                max_date = max(parsed_dates)\n",
    "                date_range = f\"{min_date.strftime('%d/%m/%Y')} to {max_date.strftime('%d/%m/%Y')}\"\n",
    "        except Exception:\n",
    "            # If parsing fails, use first and last date as-is\n",
    "            date_range = f\"{dates[0]} to {dates[-1]}\"\n",
    "    \n",
    "    return {\n",
    "        'TRANSACTION_DATES': ' | '.join(dates) if dates else 'NOT_FOUND',\n",
    "        'LINE_ITEM_DESCRIPTIONS': ' | '.join(descriptions) if descriptions else 'NOT_FOUND',\n",
    "        'TRANSACTION_AMOUNTS_PAID': ' | '.join(amounts) if amounts else 'NOT_FOUND',\n",
    "        'STATEMENT_DATE_RANGE': date_range\n",
    "    }\n",
    "\n",
    "# Parse the filtered markdown table from Turn 2\n",
    "parsed_rows = parse_markdown_table(cleanedOutput3)\n",
    "\n",
    "print(f\"ğŸ“Š Parsed {len(parsed_rows)} transactions from markdown table\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SAMPLE ROWS:\")\n",
    "print(\"=\" * 60)\n",
    "for i, row in enumerate(parsed_rows[:3], 1):\n",
    "    print(f\"\\nRow {i}:\")\n",
    "    for key, value in row.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# Extract schema fields using the LITERAL column names from pattern matching\n",
    "schema_fields = extract_schema_fields(parsed_rows, date_col, desc_col, debit_col)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EXTRACTED SCHEMA FIELDS:\")\n",
    "print(\"=\" * 60)\n",
    "for field, value in schema_fields.items():\n",
    "    print(f\"{field}: {value}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Save to file\n",
    "output_path = Path(\"llama_extracted_fields.txt\")\n",
    "with output_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    for field, value in schema_fields.items():\n",
    "        f.write(f\"{field}: {value}\\n\")\n",
    "\n",
    "print(f\"\\nâœ… Schema fields saved to: {output_path}\")\n",
    "print(f\"ğŸ’¡ Fields extracted from columns: '{date_col}' | '{desc_col}' | '{debit_col}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (unified_vision_processor)",
   "language": "python",
   "name": "unified_vision_processor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
