{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from pathlib import Path\nimport random\nimport math\n\nimport numpy as np\nimport torch\nfrom PIL import Image\nfrom transformers import AutoModel, AutoTokenizer\nimport torchvision.transforms as T\nfrom torchvision.transforms.functional import InterpolationMode\n\n# Set random seed for reproducibility\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\nset_seed(42)\nprint(\"‚úÖ Random seed set to 42 for reproducibility\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Update this path to your local InternVL3 model\nmodel_id = \"/path/to/InternVL3-8B\"\n# Update this path to your test image\nimageName = \"sample_data/synthetic_invoice_014.png\"\n\nprint(\"üîß Loading InternVL3-8B model for 4x V100 GPUs...\")\n\n# Load model\nmodel = AutoModel.from_pretrained(\n    model_id,\n    torch_dtype=torch.bfloat16,\n    low_cpu_mem_usage=True,\n    trust_remote_code=True\n).eval()\n\n# Use DataParallel for 4x V100 multi-GPU utilization\nif torch.cuda.device_count() > 1:\n    model = torch.nn.DataParallel(model, device_ids=[0, 1, 2, 3])\n    model = model.cuda()\n    print(f\"‚úÖ Using {torch.cuda.device_count()} V100 GPUs with DataParallel\")\nelse:\n    model = model.cuda()\n    print(\"‚úÖ Using single GPU\")\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\n    model_id, \n    trust_remote_code=True, \n    use_fast=False\n)\n\nprint(\"‚úÖ Model and tokenizer loaded successfully\")\nif hasattr(model, 'module'):\n    print(f\"‚úÖ Model replicated across {torch.cuda.device_count()} GPUs\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Official InternVL3 image preprocessing (from docs)\nIMAGENET_MEAN = (0.485, 0.456, 0.406)\nIMAGENET_STD = (0.229, 0.224, 0.225)\n\ndef build_transform(input_size):\n    transform = T.Compose([\n        T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),\n        T.ToTensor(),\n        T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n    ])\n    return transform\n\ndef find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):\n    best_ratio_diff = float('inf')\n    best_ratio = (1, 1)\n    area = width * height\n    for ratio in target_ratios:\n        target_aspect_ratio = ratio[0] / ratio[1]\n        ratio_diff = abs(aspect_ratio - target_aspect_ratio)\n        if ratio_diff < best_ratio_diff:\n            best_ratio_diff = ratio_diff\n            best_ratio = ratio\n        elif ratio_diff == best_ratio_diff:\n            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\n                best_ratio = ratio\n    return best_ratio\n\ndef dynamic_preprocess(image, min_num=1, max_num=12, image_size=448, use_thumbnail=False):\n    orig_width, orig_height = image.size\n    aspect_ratio = orig_width / orig_height\n\n    target_ratios = set(\n        (i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if\n        i * j <= max_num and i * j >= min_num)\n    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n\n    target_aspect_ratio = find_closest_aspect_ratio(\n        aspect_ratio, target_ratios, orig_width, orig_height, image_size)\n\n    target_width = image_size * target_aspect_ratio[0]\n    target_height = image_size * target_aspect_ratio[1]\n    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]\n\n    resized_img = image.resize((target_width, target_height))\n    processed_images = []\n    for i in range(blocks):\n        box = (\n            (i % (target_width // image_size)) * image_size,\n            (i // (target_width // image_size)) * image_size,\n            ((i % (target_width // image_size)) + 1) * image_size,\n            ((i // (target_width // image_size)) + 1) * image_size\n        )\n        split_img = resized_img.crop(box)\n        processed_images.append(split_img)\n    assert len(processed_images) == blocks\n    if use_thumbnail and len(processed_images) != 1:\n        thumbnail_img = image.resize((image_size, image_size))\n        processed_images.append(thumbnail_img)\n    return processed_images\n\ndef load_image(image_file, input_size=448, max_num=12):\n    image = Image.open(image_file).convert('RGB')\n    transform = build_transform(input_size=input_size)\n    images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)\n    pixel_values = [transform(image) for image in images]\n    pixel_values = torch.stack(pixel_values)\n    return pixel_values\n\n# Process image and move to GPU\nprint(\"üñºÔ∏è  Processing image...\")\npixel_values = load_image(imageName, max_num=12).to(torch.bfloat16).cuda()\nprint(f\"‚úÖ Image processed: {pixel_values.shape}\")\nprint(f\"‚úÖ Number of image tiles: {pixel_values.shape[0]}\")\n\n# Visual Question Answering - ask a simple question about the image\nquestion = 'How much did Jessica pay?'\n# InternVL3 format: <image>\\n + question\nformatted_question = f'<image>\\n{question}'\nprint(f\"‚ùì Question: {question}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Deterministic generation config with multi-turn support\ngeneration_config = dict(\n    max_new_tokens=2000,\n    do_sample=False  # Pure greedy decoding for deterministic output\n)\n\n# Clear CUDA cache before generation\ntorch.cuda.empty_cache()\n\n# Generate response using InternVL3 API\nprint(\"ü§ñ Generating response with InternVL3...\")\nprint(f\"üíæ GPU Memory before generation:\")\nfor i in range(torch.cuda.device_count()):\n    allocated = torch.cuda.memory_allocated(i) / 1e9\n    reserved = torch.cuda.memory_reserved(i) / 1e9\n    print(f\"   GPU {i}: {allocated:.2f}GB allocated, {reserved:.2f}GB reserved\")\n\ntry:\n    # Access chat() via .module when using DataParallel\n    chat_method = model.module.chat if hasattr(model, 'module') else model.chat\n    \n    response, history = chat_method(\n        tokenizer, \n        pixel_values, \n        formatted_question, \n        generation_config,\n        history=None,\n        return_history=True\n    )\n    \n    print(\"‚úÖ Response generated successfully!\")\n    print(\"\\n\" + \"=\"*60)\n    print(\"INTERNVL3 RESPONSE:\")\n    print(\"=\"*60)\n    print(response)\n    print(\"=\"*60)\n    \nexcept Exception as e:\n    print(f\"‚ùå Error during inference: {e}\")\n    print(f\"Error type: {type(e).__name__}\")\n    import traceback\n    traceback.print_exc()\nfinally:\n    # Clean up GPU memory\n    torch.cuda.empty_cache()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Save response to file\n",
    "try:\n",
    "    output_path = Path(\"internvl3_vqa_output.txt\")\n",
    "    \n",
    "    with output_path.open(\"w\", encoding=\"utf-8\") as text_file:\n",
    "        text_file.write(response)\n",
    "    \n",
    "    print(f\"‚úÖ Response saved to: {output_path}\")\n",
    "    print(f\"üìÑ File size: {output_path.stat().st_size} bytes\")\n",
    "    \n",
    "except NameError:\n",
    "    print(\"‚ùå Error: 'response' variable not defined.\")\n",
    "    print(\"üí° Please run the previous cell first to generate the response.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error saving file: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Vision Notebooks)",
   "language": "python",
   "name": "vision_notebooks"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}