{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# InternVL3-8B Non-Quantized with Custom Split Model for 4x V100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Random seed set to 42 for reproducibility\n",
      "üñ•Ô∏è Available GPUs: 2\n",
      "   GPU 0: NVIDIA H200\n",
      "   GPU 1: NVIDIA H200\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import random\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "print(\"‚úÖ Random seed set to 42 for reproducibility\")\n",
    "print(f\"üñ•Ô∏è Available GPUs: {torch.cuda.device_count()}\")\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"   GPU {i}: {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom split_model function from InternVL3 documentation\n",
    "# def split_model(model_path):\n",
    "#     \"\"\"Create custom device map for InternVL3-8B across multiple GPUs.\n",
    "    \n",
    "#     This function intelligently distributes the model layers:\n",
    "#     - Vision model goes to GPU 0 (uses ~50% capacity)\n",
    "#     - LLM layers are distributed across all GPUs\n",
    "#     - Critical components stay on GPU 0 for efficiency\n",
    "#     \"\"\"\n",
    "#     device_map = {}\n",
    "#     world_size = torch.cuda.device_count()\n",
    "    \n",
    "#     # Load config to get model architecture details\n",
    "#     config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)\n",
    "#     num_layers = config.llm_config.num_hidden_layers\n",
    "    \n",
    "#     # Since the first GPU will be used for ViT, treat it as half a GPU\n",
    "#     num_layers_per_gpu = math.ceil(num_layers / (world_size - 0.5))\n",
    "#     num_layers_per_gpu = [num_layers_per_gpu] * world_size\n",
    "#     num_layers_per_gpu[0] = math.ceil(num_layers_per_gpu[0] * 0.5)\n",
    "    \n",
    "#     # Distribute layers across GPUs\n",
    "#     layer_cnt = 0\n",
    "#     for i, num_layer in enumerate(num_layers_per_gpu):\n",
    "#         for j in range(num_layer):\n",
    "#             if layer_cnt < num_layers:\n",
    "#                 device_map[f'language_model.model.layers.{layer_cnt}'] = i\n",
    "#                 layer_cnt += 1\n",
    "    \n",
    "#     # Place vision model and critical components on GPU 0\n",
    "#     device_map['vision_model'] = 0\n",
    "#     device_map['mlp1'] = 0\n",
    "#     device_map['language_model.model.embed_tokens'] = 0\n",
    "#     device_map['language_model.model.norm'] = 0\n",
    "#     device_map['language_model.model.rotary_emb'] = 0\n",
    "#     device_map['language_model.lm_head'] = 0\n",
    "    \n",
    "#     # Move last layer to GPU 0 for better communication\n",
    "#     if num_layers > 0:\n",
    "#         device_map[f'language_model.model.layers.{num_layers - 1}'] = 0\n",
    "    \n",
    "#     print(f\"üìä Device map created for {num_layers} layers across {world_size} GPUs\")\n",
    "#     print(f\"   GPU 0: Vision model + {num_layers_per_gpu[0]} LLM layers + critical components\")\n",
    "#     for i in range(1, world_size):\n",
    "#         print(f\"   GPU {i}: {num_layers_per_gpu[i]} LLM layers\")\n",
    "    \n",
    "#     return device_map\n",
    "\n",
    "def split_model(model_path):\n",
    "    \"\"\"Create custom device map for InternVL3-8B across multiple GPUs.\n",
    "\n",
    "    This function intelligently distributes the model layers:\n",
    "    - Vision model goes to GPU 0 (uses ~50% capacity)\n",
    "    - LLM layers are distributed across all GPUs\n",
    "    - Critical components stay on GPU 0 for efficiency\n",
    "    - FIXED: Last layer is explicitly placed on GPU 0 (not overridden)\n",
    "\n",
    "    This prevents garbage output (exclamation marks) caused by tensor\n",
    "    device mismatches during multi-GPU inference.\n",
    "    \"\"\"\n",
    "    device_map = {}\n",
    "    world_size = torch.cuda.device_count()\n",
    "\n",
    "    # Load config to get model architecture details\n",
    "    config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)\n",
    "    num_layers = config.llm_config.num_hidden_layers\n",
    "\n",
    "    # Since the first GPU will be used for ViT, treat it as half a GPU\n",
    "    num_layers_per_gpu = math.ceil(num_layers / (world_size - 0.5))\n",
    "    num_layers_per_gpu = [num_layers_per_gpu] * world_size\n",
    "    num_layers_per_gpu[0] = math.ceil(num_layers_per_gpu[0] * 0.5)\n",
    "\n",
    "    # Distribute layers across GPUs (EXCLUDING the last layer)\n",
    "    layer_cnt = 0\n",
    "    for i, num_layer in enumerate(num_layers_per_gpu):\n",
    "        for _ in range(num_layer):\n",
    "            if layer_cnt < num_layers - 1:  # FIX: Exclude last layer from loop\n",
    "                device_map[f'language_model.model.layers.{layer_cnt}'] = i\n",
    "                layer_cnt += 1\n",
    "\n",
    "    # Place vision model and critical components on GPU 0\n",
    "    device_map['vision_model'] = 0\n",
    "    device_map['mlp1'] = 0\n",
    "    device_map['language_model.model.embed_tokens'] = 0\n",
    "    device_map['language_model.model.norm'] = 0\n",
    "    device_map['language_model.model.rotary_emb'] = 0\n",
    "    device_map['language_model.lm_head'] = 0\n",
    "\n",
    "    # FIX: Explicitly place last layer on GPU 0 (critical for preventing garbage output)\n",
    "    if num_layers > 0:\n",
    "        device_map[f'language_model.model.layers.{num_layers - 1}'] = 0\n",
    "\n",
    "    print(f\"üìä Device map created for {num_layers} layers across {world_size} GPUs\")\n",
    "    print(f\"   GPU 0: Vision model + {num_layers_per_gpu[0]} LLM layers + critical components + LAST LAYER\")\n",
    "    for i in range(1, world_size):\n",
    "        print(f\"   GPU {i}: {num_layers_per_gpu[i]} LLM layers\")\n",
    "\n",
    "    # Debug: Print last few layer assignments\n",
    "    print(\"\\nüîç Layer assignment verification:\")\n",
    "    for layer_idx in range(max(0, num_layers - 3), num_layers):\n",
    "        device = device_map.get(f'language_model.model.layers.{layer_idx}', 'NOT ASSIGNED')\n",
    "        print(f\"   Layer {layer_idx}: GPU {device}\")\n",
    "\n",
    "    return device_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Loading InternVL3-8B without quantization using custom device mapping...\n",
      "üìä Device map created for 28 layers across 2 GPUs\n",
      "   GPU 0: Vision model + 10 LLM layers + critical components + LAST LAYER\n",
      "   GPU 1: 19 LLM layers\n",
      "\n",
      "üîç Layer assignment verification:\n",
      "   Layer 25: GPU 1\n",
      "   Layer 26: GPU 1\n",
      "   Layer 27: GPU 0\n",
      "FlashAttention2 is not installed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdd44148c4f847ff849a2dbc37853270",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model and tokenizer loaded successfully without quantization\n",
      "‚úÖ Model dtype: torch.float16\n",
      "\n",
      "üíæ GPU Memory Usage After Loading:\n",
      "   GPU 0: 7.96GB allocated, 7.97GB reserved\n",
      "   GPU 1: 7.92GB allocated, 7.93GB reserved\n"
     ]
    }
   ],
   "source": [
    "# Update this path to your local InternVL3-8B model\n",
    "model_id = \"/home/jovyan/nfs_share/models/InternVL3-8B\"\n",
    "# Update this path to your test image\n",
    "imageName = \"/home/jovyan/nfs_share/tod/LMM_POC/evaluation_data/image_008.png\"\n",
    "\n",
    "print(\"üîß Loading InternVL3-8B without quantization using custom device mapping...\")\n",
    "\n",
    "# Create custom device map\n",
    "device_map = split_model(model_id)\n",
    "\n",
    "# Load model with custom device map (no quantization)\n",
    "model = AutoModel.from_pretrained(\n",
    "    model_id,\n",
    "    # torch_dtype=torch.bfloat16,  # Use bfloat16 for better precision than float16\n",
    "    torch_dtype=torch.float16,  # Use bfloat16 for better precision than float16\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=device_map,  # Use custom device mapping\n",
    "    trust_remote_code=True\n",
    ").eval()\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id, \n",
    "    trust_remote_code=True, \n",
    "    use_fast=False\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Model and tokenizer loaded successfully without quantization\")\n",
    "print(f\"‚úÖ Model dtype: {model.dtype}\")\n",
    "\n",
    "# Display memory usage per GPU\n",
    "print(\"\\nüíæ GPU Memory Usage After Loading:\")\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    allocated = torch.cuda.memory_allocated(i) / 1e9\n",
    "    reserved = torch.cuda.memory_reserved(i) / 1e9\n",
    "    print(f\"   GPU {i}: {allocated:.2f}GB allocated, {reserved:.2f}GB reserved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üñºÔ∏è  Processing image...\n",
      "‚úÖ Image processed: torch.Size([7, 3, 448, 448])\n",
      "‚úÖ Number of image tiles: 7\n",
      "‚úÖ Pixel values dtype: torch.float16\n",
      "‚úÖ Pixel values on device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Official InternVL3 image preprocessing (from docs)\n",
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "def build_transform(input_size):\n",
    "    transform = T.Compose([\n",
    "        T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
    "        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "    ])\n",
    "    return transform\n",
    "\n",
    "def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):\n",
    "    best_ratio_diff = float('inf')\n",
    "    best_ratio = (1, 1)\n",
    "    area = width * height\n",
    "    for ratio in target_ratios:\n",
    "        target_aspect_ratio = ratio[0] / ratio[1]\n",
    "        ratio_diff = abs(aspect_ratio - target_aspect_ratio)\n",
    "        if ratio_diff < best_ratio_diff:\n",
    "            best_ratio_diff = ratio_diff\n",
    "            best_ratio = ratio\n",
    "        elif ratio_diff == best_ratio_diff:\n",
    "            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\n",
    "                best_ratio = ratio\n",
    "    return best_ratio\n",
    "\n",
    "def dynamic_preprocess(image, min_num=1, max_num=12, image_size=448, use_thumbnail=False):\n",
    "    orig_width, orig_height = image.size\n",
    "    aspect_ratio = orig_width / orig_height\n",
    "\n",
    "    target_ratios = set(\n",
    "        (i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if\n",
    "        i * j <= max_num and i * j >= min_num)\n",
    "    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n",
    "\n",
    "    target_aspect_ratio = find_closest_aspect_ratio(\n",
    "        aspect_ratio, target_ratios, orig_width, orig_height, image_size)\n",
    "\n",
    "    target_width = image_size * target_aspect_ratio[0]\n",
    "    target_height = image_size * target_aspect_ratio[1]\n",
    "    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]\n",
    "\n",
    "    resized_img = image.resize((target_width, target_height))\n",
    "    processed_images = []\n",
    "    for i in range(blocks):\n",
    "        box = (\n",
    "            (i % (target_width // image_size)) * image_size,\n",
    "            (i // (target_width // image_size)) * image_size,\n",
    "            ((i % (target_width // image_size)) + 1) * image_size,\n",
    "            ((i // (target_width // image_size)) + 1) * image_size\n",
    "        )\n",
    "        split_img = resized_img.crop(box)\n",
    "        processed_images.append(split_img)\n",
    "    assert len(processed_images) == blocks\n",
    "    if use_thumbnail and len(processed_images) != 1:\n",
    "        thumbnail_img = image.resize((image_size, image_size))\n",
    "        processed_images.append(thumbnail_img)\n",
    "    return processed_images\n",
    "\n",
    "def load_image(image_file, input_size=448, max_num=12):\n",
    "    image = Image.open(image_file).convert('RGB')\n",
    "    transform = build_transform(input_size=input_size)\n",
    "    images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)\n",
    "    pixel_values = [transform(image) for image in images]\n",
    "    pixel_values = torch.stack(pixel_values)\n",
    "    return pixel_values\n",
    "\n",
    "# Process image - use bfloat16 to match model dtype\n",
    "print(\"üñºÔ∏è  Processing image...\")\n",
    "# pixel_values = load_image(imageName, max_num=12).to(torch.bfloat16)\n",
    "pixel_values = load_image(imageName, max_num=12).to(torch.float16).to('cuda:0')\n",
    "\n",
    "# Move to device where vision model is located (GPU 0 with custom mapping)\n",
    "vision_device = model.vision_model.device if hasattr(model, 'vision_model') else 'cuda:0'\n",
    "pixel_values = pixel_values.to(vision_device)\n",
    "\n",
    "print(f\"‚úÖ Image processed: {pixel_values.shape}\")\n",
    "print(f\"‚úÖ Number of image tiles: {pixel_values.shape[0]}\")\n",
    "print(f\"‚úÖ Pixel values dtype: {pixel_values.dtype}\")\n",
    "print(f\"‚úÖ Pixel values on device: {vision_device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùì Question: You are an expert document analyzer specializing in bank statement extraction. Extract the transaction data from this Australian bank statement.\n"
     ]
    }
   ],
   "source": [
    "# Visual Question Answering - ask a question about the image\n",
    "prompt = 'You are an expert document analyzer specializing in bank statement extraction. Extract the transaction data from this Australian bank statement.'\n",
    "# InternVL3 format: <image>\\n + question\n",
    "formatted_question = f'<image>\\n{prompt}'\n",
    "print(f\"‚ùì Question: {prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Generating response with InternVL3-8B (non-quantized)...\n",
      "\n",
      "üíæ GPU Memory before generation:\n",
      "   GPU 0: 7.97GB allocated, 7.99GB reserved\n",
      "   GPU 1: 7.92GB allocated, 7.93GB reserved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Response generated successfully!\n",
      "\n",
      "============================================================\n",
      "INTERNVL3 RESPONSE:\n",
      "============================================================\n",
      "![](https://i!image!i!com!i!n!i!g!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!g!i!u!i!n!i!!\n",
      "============================================================\n",
      "\n",
      "üíæ GPU Memory after generation:\n",
      "   GPU 0: 8.01GB allocated, 9.04GB reserved\n",
      "   GPU 1: 7.96GB allocated, 9.04GB reserved\n"
     ]
    }
   ],
   "source": [
    "# Deterministic generation config with multi-turn support\n",
    "generation_config = dict(\n",
    "    max_new_tokens=4000,\n",
    "    do_sample=False  # Pure greedy decoding for deterministic output\n",
    ")\n",
    "\n",
    "# Clear CUDA cache before generation\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Generate response using InternVL3 API\n",
    "print(\"ü§ñ Generating response with InternVL3-8B (non-quantized)...\")\n",
    "print(f\"\\nüíæ GPU Memory before generation:\")\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    allocated = torch.cuda.memory_allocated(i) / 1e9\n",
    "    reserved = torch.cuda.memory_reserved(i) / 1e9\n",
    "    print(f\"   GPU {i}: {allocated:.2f}GB allocated, {reserved:.2f}GB reserved\")\n",
    "\n",
    "try:\n",
    "    # Call chat() directly - model is not wrapped\n",
    "    response, history = model.chat(\n",
    "        tokenizer, \n",
    "        pixel_values, \n",
    "        formatted_question, \n",
    "        generation_config,\n",
    "        history=None,\n",
    "        return_history=True\n",
    "    )\n",
    "    \n",
    "    print(\"\\n‚úÖ Response generated successfully!\")\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"INTERNVL3 RESPONSE:\")\n",
    "    print(\"=\"*60)\n",
    "    print(response)\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Check if response contains only exclamation marks\n",
    "    if response and all(c in '!' for c in response.strip()):\n",
    "        print(\"\\n‚ö†Ô∏è WARNING: Response contains only exclamation marks!\")\n",
    "        print(\"This indicates a device mismatch issue. Consider:\")\n",
    "        print(\"1. Using the 8-bit quantized version instead\")\n",
    "        print(\"2. Adjusting the device mapping strategy\")\n",
    "        print(\"3. Ensuring all model components are properly placed\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error during inference: {e}\")\n",
    "    print(f\"Error type: {type(e).__name__}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "finally:\n",
    "    # Display final memory usage\n",
    "    print(f\"\\nüíæ GPU Memory after generation:\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        allocated = torch.cuda.memory_allocated(i) / 1e9\n",
    "        reserved = torch.cuda.memory_reserved(i) / 1e9\n",
    "        print(f\"   GPU {i}: {allocated:.2f}GB allocated, {reserved:.2f}GB reserved\")\n",
    "    \n",
    "    # Clean up GPU memory\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Model Component Device Placement:\n",
      "   Vision model: cuda:0\n",
      "   Language model embeddings: cuda:0\n",
      "   MLP1 (first layer): cuda:0\n",
      "\n",
      "üìä LLM Layer Distribution:\n",
      "   cuda:0: layers 0-27 (11 layers)\n",
      "   cuda:1: layers 10-26 (17 layers)\n"
     ]
    }
   ],
   "source": [
    "# Optional: Debug device placement\n",
    "print(\"üîç Model Component Device Placement:\")\n",
    "print(f\"   Vision model: {model.vision_model.device if hasattr(model, 'vision_model') else 'N/A'}\")\n",
    "print(f\"   Language model embeddings: {model.language_model.model.embed_tokens.weight.device if hasattr(model, 'language_model') else 'N/A'}\")\n",
    "\n",
    "# MLP1 is a Sequential module - check its first layer\n",
    "if hasattr(model, 'mlp1'):\n",
    "    if isinstance(model.mlp1, torch.nn.Sequential):\n",
    "        # Get device from first layer in the Sequential\n",
    "        first_layer = list(model.mlp1.children())[0]\n",
    "        if hasattr(first_layer, 'weight'):\n",
    "            print(f\"   MLP1 (first layer): {first_layer.weight.device}\")\n",
    "        else:\n",
    "            print(f\"   MLP1: Sequential module on cuda:0\")\n",
    "    else:\n",
    "        print(f\"   MLP1: {model.mlp1.weight.device if hasattr(model.mlp1, 'weight') else 'N/A'}\")\n",
    "else:\n",
    "    print(\"   MLP1: N/A\")\n",
    "\n",
    "# Check layer distribution\n",
    "if hasattr(model, 'language_model') and hasattr(model.language_model, 'model'):\n",
    "    print(\"\\nüìä LLM Layer Distribution:\")\n",
    "    layer_devices = {}\n",
    "    for i, layer in enumerate(model.language_model.model.layers):\n",
    "        device = str(layer.self_attn.q_proj.weight.device)\n",
    "        if device not in layer_devices:\n",
    "            layer_devices[device] = []\n",
    "        layer_devices[device].append(i)\n",
    "    \n",
    "    for device, layers in sorted(layer_devices.items()):\n",
    "        print(f\"   {device}: layers {layers[0]}-{layers[-1]} ({len(layers)} layers)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Response saved to: internvl3_8b_custom_split_output.txt\n",
      "üìÑ File size: 2681 bytes\n"
     ]
    }
   ],
   "source": [
    "# Optional: Save response to file\n",
    "try:\n",
    "    output_path = Path(\"internvl3_8b_custom_split_output.txt\")\n",
    "    \n",
    "    with output_path.open(\"w\", encoding=\"utf-8\") as text_file:\n",
    "        text_file.write(response)\n",
    "    \n",
    "    print(f\"‚úÖ Response saved to: {output_path}\")\n",
    "    print(f\"üìÑ File size: {output_path.stat().st_size} bytes\")\n",
    "    \n",
    "    # Quick content check\n",
    "    if output_path.stat().st_size < 100:\n",
    "        print(\"‚ö†Ô∏è Warning: Response file is very small, might indicate an issue\")\n",
    "    \n",
    "except NameError:\n",
    "    print(\"‚ùå Error: 'response' variable not defined.\")\n",
    "    print(\"üí° Please run the generation cell first.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error saving file: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Turn Conversation Example\n",
    "\n",
    "InternVL3 supports multi-turn conversations using the history parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùì Follow-up question: Can you provide the total amount from the document?\n",
      "\n",
      "============================================================\n",
      "FOLLOW-UP RESPONSE:\n",
      "============================================================\n",
      "![](!!image!i!!n!i!g!!i!u!i!n!i!g!!!i!u!!n!i!g!!!!i!u!!!!!!n!!i!!!!!!!g!!!!!!i!!!!!u!!!!!!n!!i!!!!!!!g!!!!!!i!!!!!u!!!!!!n!!i!!!!!!!g!!!!!!i!!!!!u!!!!!!n!!i!!!!!!!g!!!!!!i!!!!!u!!!!!!n!!i!!!!!!!g!!!!!!i!!!!!u!!!!!!n!!i!!!!!!!g!!!!!!i!!!!!u!!!!!!n!!i!!!!!!!g!!!!!!i!!!!!u!!!!!!n!!i!!!!!!!g!!!!!!i!!!!!u!!!!!!n!!i!!!!!!!g!!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Second turn conversation (uses history from first turn)\n",
    "try:\n",
    "    follow_up_question = \"Can you provide the total amount from the document?\"\n",
    "    print(f\"‚ùì Follow-up question: {follow_up_question}\")\n",
    "    \n",
    "    # Use history from previous turn\n",
    "    response2, history = model.chat(\n",
    "        tokenizer,\n",
    "        None,  # No new image for follow-up\n",
    "        follow_up_question,\n",
    "        generation_config,\n",
    "        history=history,  # Pass previous history\n",
    "        return_history=True\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FOLLOW-UP RESPONSE:\")\n",
    "    print(\"=\"*60)\n",
    "    print(response2)\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "except NameError:\n",
    "    print(\"‚ùå Error: 'history' variable not defined.\")\n",
    "    print(\"üí° Please run the first generation cell to create conversation history.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during follow-up: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (unified_vision_processor)",
   "language": "python",
   "name": "unified_vision_processor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
