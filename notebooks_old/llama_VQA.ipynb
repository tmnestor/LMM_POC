{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, MllamaForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Loading Llama-3.2-Vision model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb093322e020437abc153eb1d00a0795",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“· Loading image...\n",
      "âœ… Image loaded: (800, 1200)\n"
     ]
    }
   ],
   "source": [
    "# Update this path to your local Llama model\n",
    "model_id = \"/home/jovyan/nfs_share/models/Llama-3.2-11B-Vision-Instruct\"\n",
    "# Update this path to your test image\n",
    "imageName = \"/home/jovyan/nfs_share/tod/LMM_POC/evaluation_data/image_003.png\"\n",
    "\n",
    "print(\"ðŸ”§ Loading Llama-3.2-Vision model...\")\n",
    "model = MllamaForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "print(\"ðŸ“· Loading image...\")\n",
    "image = Image.open(imageName)\n",
    "print(f\"âœ… Image loaded: {image.size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â“ Question: Explain the structure of the transaction table\n",
      "ðŸ¤– Generating response with Llama-3.2-Vision...\n"
     ]
    }
   ],
   "source": [
    "# Visual Question Answering - ask a simple question about the image\n",
    "question = \"Explain the structure of the transaction table\"\n",
    "\n",
    "# Create message structure for Llama\n",
    "messageDataStructure = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\"},\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": question,\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"â“ Question: {question}\")\n",
    "print(\"ðŸ¤– Generating response with Llama-3.2-Vision...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Response generated successfully!\n",
      "\n",
      "============================================================\n",
      "LLAMA-3.2-VISION RESPONSE:\n",
      "============================================================\n",
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "<|image|>Explain the structure of the transaction table<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "The image presents a transaction table for the Commonwealth Bank of Australia, detailing financial transactions from August 7, 2025, to September 6, 2025. The table is structured into the following columns:\n",
      "\n",
      "*   **Date**: Displays the date of each transaction.\n",
      "*   **Transaction**: Lists the type of transaction, including debit or credit.\n",
      "*   **Debit**: Shows the amount deducted from the account balance.\n",
      "*   **Credit**: Shows the amount added to the account balance.\n",
      "*   **Balance**: Displays the updated account balance after each transaction.\n",
      "\n",
      "The table provides a clear and organized view of the account's activity during the specified period, allowing users to track their financial transactions and monitor their account balance.<|eot_id|>\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Process the input\n",
    "textInput = processor.apply_chat_template(\n",
    "    messageDataStructure, add_generation_prompt=True\n",
    ")\n",
    "inputs = processor(image, textInput, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Generate response\n",
    "output = model.generate(**inputs, max_new_tokens=2000)\n",
    "generatedOutput = processor.decode(output[0])\n",
    "\n",
    "print(\"âœ… Response generated successfully!\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"LLAMA-3.2-VISION RESPONSE:\")\n",
    "print(\"=\" * 60)\n",
    "print(generatedOutput)\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Response saved to: llama_vqa_output.txt\n",
      "ðŸ“„ File size: 935 bytes\n"
     ]
    }
   ],
   "source": [
    "# Optional: Save the response to a file\n",
    "output_path = Path(\"llama_vqa_output.txt\")\n",
    "\n",
    "with output_path.open(\"w\", encoding=\"utf-8\") as text_file:\n",
    "    text_file.write(generatedOutput)\n",
    "\n",
    "print(f\"âœ… Response saved to: {output_path}\")\n",
    "print(f\"ðŸ“„ File size: {output_path.stat().st_size} bytes\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (unified_vision_processor)",
   "language": "python",
   "name": "unified_vision_processor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
