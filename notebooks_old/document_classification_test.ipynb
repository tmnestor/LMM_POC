{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Document Type Classification Testing - Phase 1\n",
    "\n",
    "Interactive testing of the document type detection system.\n",
    "Test classification accuracy and tune parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "from IPython.display import display, HTML, Markdown\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path('..').absolute()\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"üìÇ Project root: {project_root}\")\n",
    "print(\"‚úÖ Environment configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "try:\n",
    "    from common.document_type_detector import DocumentTypeDetector\n",
    "    from common.extraction_parser import discover_images\n",
    "    print(\"‚úÖ Imports successful\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Import error: {e}\")\n",
    "    print(\"üí° Make sure all required files exist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MODEL_TYPE = \"llama\"  # Change to \"internvl3\" to test different model\n",
    "TEST_DIRECTORY = \"../evaluation_data\"\n",
    "CONFIDENCE_THRESHOLD = 0.85\n",
    "\n",
    "print(f\"üéØ Configuration:\")\n",
    "print(f\"   Model: {MODEL_TYPE}\")\n",
    "print(f\"   Test directory: {TEST_DIRECTORY}\")\n",
    "print(f\"   Confidence threshold: {CONFIDENCE_THRESHOLD}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Initialize Model and Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize processor\n",
    "print(f\"üöÄ Initializing {MODEL_TYPE.upper()} processor...\")\n",
    "\n",
    "try:\n",
    "    if MODEL_TYPE.lower() == \"llama\":\n",
    "        from models.llama_processor import LlamaProcessor\n",
    "        processor = LlamaProcessor()\n",
    "    elif MODEL_TYPE.lower() == \"internvl3\":\n",
    "        from models.internvl3_processor import InternVL3Processor\n",
    "        processor = InternVL3Processor()\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model type: {MODEL_TYPE}\")\n",
    "    \n",
    "    print(f\"‚úÖ {MODEL_TYPE.upper()} processor initialized successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to initialize processor: {e}\")\n",
    "    print(\"üí° Make sure you're running on a machine with GPU and model access\")\n",
    "    processor = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize document type detector\n",
    "if processor:\n",
    "    detector = DocumentTypeDetector(processor)\n",
    "    detector.confidence_threshold = CONFIDENCE_THRESHOLD\n",
    "    \n",
    "    print(f\"‚úÖ Document type detector initialized\")\n",
    "    print(f\"üéØ Supported types: {detector.supported_types}\")\n",
    "    print(f\"üéØ Confidence threshold: {detector.confidence_threshold}\")\n",
    "else:\n",
    "    detector = None\n",
    "    print(\"‚ùå Cannot initialize detector without processor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Single Image Classification Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test single image classification\n",
    "TEST_IMAGE = \"../evaluation_data/synthetic_invoice_001.png\"\n",
    "\n",
    "if detector and Path(TEST_IMAGE).exists():\n",
    "    display(HTML(\"<h3>üñºÔ∏è Test Image</h3>\"))\n",
    "    \n",
    "    # Display image\n",
    "    img = Image.open(TEST_IMAGE)\n",
    "    if img.width > 600:\n",
    "        ratio = 600 / img.width\n",
    "        new_height = int(img.height * ratio)\n",
    "        img_display = img.resize((600, new_height), Image.Resampling.LANCZOS)\n",
    "        display(img_display)\n",
    "    else:\n",
    "        display(img)\n",
    "    \n",
    "    print(f\"üìÅ Testing: {Path(TEST_IMAGE).name}\")\n",
    "    print(f\"üìê Image size: {img.width}x{img.height}\")\n",
    "    \n",
    "else:\n",
    "    print(f\"‚ùå Cannot test - detector: {detector is not None}, image exists: {Path(TEST_IMAGE).exists()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run classification\n",
    "if detector and Path(TEST_IMAGE).exists():\n",
    "    print(\"üîç Running document type classification...\")\n",
    "    \n",
    "    result = detector.detect_document_type(TEST_IMAGE)\n",
    "    \n",
    "    display(HTML(\"<h3>üìä Classification Result</h3>\"))\n",
    "    \n",
    "    # Display results in a nice format\n",
    "    confidence_color = \"green\" if result['confidence'] >= CONFIDENCE_THRESHOLD else \"orange\" if result['confidence'] >= 0.5 else \"red\"\n",
    "    confidence_icon = \"‚úÖ\" if result['confidence'] >= CONFIDENCE_THRESHOLD else \"‚ö†Ô∏è\" if result['confidence'] >= 0.5 else \"‚ùå\"\n",
    "    \n",
    "    result_html = f\"\"\"\n",
    "    <div style=\"border: 2px solid #ccc; padding: 20px; border-radius: 10px; background-color: #f9f9f9;\">\n",
    "        <h4>üè∑Ô∏è Document Type: <span style=\"color: {confidence_color}; font-weight: bold;\">{result['type'].upper()}</span></h4>\n",
    "        <p><strong>üéØ Confidence:</strong> <span style=\"color: {confidence_color};\">{result['confidence']:.3f}</span> {confidence_icon}</p>\n",
    "        <p><strong>‚è±Ô∏è Processing Time:</strong> {result['processing_time']:.2f} seconds</p>\n",
    "        <p><strong>üí≠ Reasoning:</strong> {result.get('reasoning', 'None provided')}</p>\n",
    "    \"\"\"\n",
    "    \n",
    "    if result.get('fallback_used'):\n",
    "        result_html += '<p><strong>‚ö†Ô∏è Note:</strong> Fallback classification was used</p>'\n",
    "    \n",
    "    if result.get('manual_review_needed'):\n",
    "        result_html += '<p><strong>üîç Note:</strong> Manual review recommended</p>'\n",
    "    \n",
    "    result_html += \"</div>\"\n",
    "    \n",
    "    display(HTML(result_html))\n",
    "    \n",
    "    # Show raw response for debugging\n",
    "    if 'raw_response' in result:\n",
    "        display(HTML(\"<h4>üîß Raw Model Response (for debugging):</h4>\"))\n",
    "        display(HTML(f'<div style=\"background-color: #f0f0f0; padding: 10px; border-radius: 5px; font-family: monospace; white-space: pre-wrap; max-height: 200px; overflow-y: auto;\">{result[\"raw_response\"]}</div>'))\n",
    "\nelse:\n",
    "    print(\"‚ùå Cannot run classification test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Batch Classification Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discover available test images\n",
    "if detector:\n",
    "    try:\n",
    "        test_images = discover_images(TEST_DIRECTORY)\n",
    "        print(f\"üìÇ Found {len(test_images)} test images in {TEST_DIRECTORY}\")\n",
    "        \n",
    "        # Show first few images\n",
    "        for i, img_path in enumerate(test_images[:5]):\n",
    "            print(f\"   {i+1}. {Path(img_path).name}\")\n",
    "        \n",
    "        if len(test_images) > 5:\n",
    "            print(f\"   ... and {len(test_images) - 5} more\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error discovering images: {e}\")\n",
    "        test_images = []\nelse:\n",
    "    test_images = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run batch classification\n",
    "if detector and test_images:\n",
    "    display(HTML(\"<h3>üî¨ Running Batch Classification</h3>\"))\n",
    "    \n",
    "    print(f\"Processing {len(test_images)} images...\")\n",
    "    \n",
    "    # Run batch classification\n",
    "    batch_results = detector.batch_classify_images(TEST_DIRECTORY)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Batch classification completed!\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Cannot run batch classification\")\n",
    "    batch_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display batch results\n",
    "if batch_results:\n",
    "    display(HTML(\"<h3>üìä Batch Classification Report</h3>\"))\n",
    "    \n",
    "    # Generate report\n",
    "    report = detector.generate_classification_report(batch_results)\n",
    "    \n",
    "    # Display as formatted text\n",
    "    display(HTML(f'<pre style=\"background-color: #f8f8f8; padding: 15px; border-radius: 5px; font-family: monospace;\">{report}</pre>'))\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No batch results to display\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed results table\n",
    "if batch_results:\n",
    "    display(HTML(\"<h3>üìã Detailed Results</h3>\"))\n",
    "    \n",
    "    # Create HTML table\n",
    "    table_html = \"\"\"\n",
    "    <table style=\"border-collapse: collapse; width: 100%;\">\n",
    "        <thead style=\"background-color: #f0f0f0;\">\n",
    "            <tr>\n",
    "                <th style=\"border: 1px solid #ccc; padding: 8px; text-align: left;\">Image</th>\n",
    "                <th style=\"border: 1px solid #ccc; padding: 8px; text-align: left;\">Type</th>\n",
    "                <th style=\"border: 1px solid #ccc; padding: 8px; text-align: left;\">Confidence</th>\n",
    "                <th style=\"border: 1px solid #ccc; padding: 8px; text-align: left;\">Time (s)</th>\n",
    "                <th style=\"border: 1px solid #ccc; padding: 8px; text-align: left;\">Notes</th>\n",
    "            </tr>\n",
    "        </thead>\n",
    "        <tbody>\n",
    "    \"\"\"\n",
    "    \n",
    "    for result in batch_results:\n",
    "        if result.get('error'):\n",
    "            table_html += f\"\"\"\n",
    "            <tr>\n",
    "                <td style=\"border: 1px solid #ccc; padding: 8px;\">{result['image_name']}</td>\n",
    "                <td style=\"border: 1px solid #ccc; padding: 8px; color: red;\">ERROR</td>\n",
    "                <td style=\"border: 1px solid #ccc; padding: 8px;\">-</td>\n",
    "                <td style=\"border: 1px solid #ccc; padding: 8px;\">-</td>\n",
    "                <td style=\"border: 1px solid #ccc; padding: 8px; color: red;\">{result.get('reasoning', 'Unknown error')}</td>\n",
    "            </tr>\n",
    "            \"\"\"\n",
    "        else:\n",
    "            confidence = result['confidence']\n",
    "            confidence_color = \"green\" if confidence >= CONFIDENCE_THRESHOLD else \"orange\" if confidence >= 0.5 else \"red\"\n",
    "            confidence_icon = \"‚úÖ\" if confidence >= CONFIDENCE_THRESHOLD else \"‚ö†Ô∏è\" if confidence >= 0.5 else \"‚ùå\"\n",
    "            \n",
    "            notes = []\n",
    "            if result.get('fallback_used'):\n",
    "                notes.append('Fallback')\n",
    "            if result.get('manual_review_needed'):\n",
    "                notes.append('Manual Review')\n",
    "            notes_text = ', '.join(notes) if notes else '-'\n",
    "            \n",
    "            table_html += f\"\"\"\n",
    "            <tr>\n",
    "                <td style=\"border: 1px solid #ccc; padding: 8px;\">{result['image_name']}</td>\n",
    "                <td style=\"border: 1px solid #ccc; padding: 8px; color: {confidence_color}; font-weight: bold;\">{result['type'].upper()}</td>\n",
    "                <td style=\"border: 1px solid #ccc; padding: 8px; color: {confidence_color};\">{confidence:.3f} {confidence_icon}</td>\n",
    "                <td style=\"border: 1px solid #ccc; padding: 8px;\">{result.get('processing_time', 0):.2f}</td>\n",
    "                <td style=\"border: 1px solid #ccc; padding: 8px;\">{notes_text}</td>\n",
    "            </tr>\n",
    "            \"\"\"\n",
    "    \n",
    "    table_html += \"\"\"\n",
    "        </tbody>\n",
    "    </table>\n",
    "    \"\"\"\n",
    "    \n",
    "    display(HTML(table_html))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Ground Truth Analysis (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple ground truth analysis based on filenames\n",
    "if batch_results:\n",
    "    display(HTML(\"<h3>üîç Ground Truth Analysis</h3>\"))\n",
    "    print(\"Note: This uses filename patterns to infer document types\")\n",
    "    \n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    accuracy_data = []\n",
    "    \n",
    "    for result in batch_results:\n",
    "        if result.get('error'):\n",
    "            continue\n",
    "        \n",
    "        filename = result['image_name'].lower()\n",
    "        predicted_type = result['type']\n",
    "        \n",
    "        # Infer actual type from filename\n",
    "        actual_type = None\n",
    "        if 'invoice' in filename:\n",
    "            actual_type = 'invoice'\n",
    "        elif 'statement' in filename or 'bank' in filename:\n",
    "            actual_type = 'bank_statement'\n",
    "        elif 'receipt' in filename:\n",
    "            actual_type = 'receipt'\n",
    "        \n",
    "        if actual_type:\n",
    "            total_predictions += 1\n",
    "            is_correct = (predicted_type == actual_type)\n",
    "            if is_correct:\n",
    "                correct_predictions += 1\n",
    "            \n",
    "            accuracy_data.append({\n",
    "                'filename': result['image_name'],\n",
    "                'predicted': predicted_type,\n",
    "                'actual': actual_type,\n",
    "                'correct': is_correct,\n",
    "                'confidence': result['confidence']\n",
    "            })\n",
    "    \n",
    "    if total_predictions > 0:\n",
    "        accuracy = (correct_predictions / total_predictions) * 100\n",
    "        \n",
    "        print(f\"\\nüìä ACCURACY METRICS:\")\n",
    "        print(f\"   Correct predictions: {correct_predictions}/{total_predictions}\")\n",
    "        print(f\"   Overall accuracy: {accuracy:.1f}%\")\n",
    "        \n",
    "        # Accuracy assessment\n",
    "        if accuracy >= 95:\n",
    "            print(\"üéâ Excellent accuracy! Ready for production.\")\n",
    "        elif accuracy >= 85:\n",
    "            print(\"üëç Good accuracy. Consider minor tuning.\")\n",
    "        elif accuracy >= 70:\n",
    "            print(\"‚ö†Ô∏è Moderate accuracy. Tuning recommended.\")\n",
    "        else:\n",
    "            print(\"üö® Low accuracy. Significant improvements needed.\")\n",
    "        \n",
    "        # Show misclassifications\n",
    "        misclassifications = [item for item in accuracy_data if not item['correct']]\n",
    "        if misclassifications:\n",
    "            print(f\"\\n‚ùå MISCLASSIFICATIONS ({len(misclassifications)}):\"))\n",
    "            for item in misclassifications:\n",
    "                print(f\"   {item['filename']}: {item['predicted']} != {item['actual']} (conf: {item['confidence']:.2f})\")\n",
    "    \n",
    "    else:\n",
    "        print(\"üí° No ground truth data available from filenames\")\n",
    "        print(\"üí° Consider using more descriptive filenames for accuracy testing\")\n",
    "\nelse:\n",
    "    print(\"‚ùå No results available for ground truth analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Performance Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance analysis and tuning recommendations\n",
    "if batch_results:\n",
    "    display(HTML(\"<h3>‚ö° Performance Analysis</h3>\"))\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    processing_times = [r.get('processing_time', 0) for r in batch_results if not r.get('error')]\n",
    "    confidences = [r['confidence'] for r in batch_results if not r.get('error')]\n",
    "    fallback_count = sum(1 for r in batch_results if r.get('fallback_used'))\n",
    "    \n",
    "    if processing_times:\n",
    "        avg_time = sum(processing_times) / len(processing_times)\n",
    "        max_time = max(processing_times)\n",
    "        min_time = min(processing_times)\n",
    "        \n",
    "        print(f\"‚è±Ô∏è PROCESSING TIMES:\")\n",
    "        print(f\"   Average: {avg_time:.2f}s\")\n",
    "        print(f\"   Range: {min_time:.2f}s - {max_time:.2f}s\")\n",
    "        \n",
    "        if avg_time < 2.0:\n",
    "            print(\"üöÄ Excellent speed performance\")\n",
    "        elif avg_time < 5.0:\n",
    "            print(\"üëç Good speed performance\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Consider optimizing for better speed\")\n",
    "    \n",
    "    if confidences:\n",
    "        avg_confidence = sum(confidences) / len(confidences)\n",
    "        high_conf_count = sum(1 for c in confidences if c >= CONFIDENCE_THRESHOLD)\n",
    "        high_conf_rate = (high_conf_count / len(confidences)) * 100\n",
    "        \n",
    "        print(f\"\\nüéØ CONFIDENCE METRICS:\")\n",
    "        print(f\"   Average confidence: {avg_confidence:.3f}\")\n",
    "        print(f\"   High confidence rate: {high_conf_rate:.1f}% ({high_conf_count}/{len(confidences)})\")\n",
    "        print(f\"   Fallback usage: {fallback_count} classifications\")\n",
    "    \n",
    "    # Tuning recommendations\n",
    "    print(f\"\\nüí° TUNING RECOMMENDATIONS:\")\n",
    "    \n",
    "    if avg_confidence < 0.8:\n",
    "        print(f\"   - Consider lowering confidence threshold to {avg_confidence:.1f}\")\n",
    "    \n",
    "    if fallback_count > len(batch_results) * 0.2:\n",
    "        print(f\"   - High fallback usage - refine classification prompts\")\n",
    "    \n",
    "    if avg_time > 3.0:\n",
    "        print(f\"   - Consider reducing max_tokens for faster classification\")\n",
    "    \n",
    "    if high_conf_rate < 80:\n",
    "        print(f\"   - Low high-confidence rate - improve prompt specificity\")\n",
    "\nelse:\n",
    "    print(\"‚ùå No results available for performance analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "display(HTML(\"<h3>üìã Phase 1 Summary</h3>\"))\n",
    "\n",
    "if batch_results:\n",
    "    successful = len([r for r in batch_results if not r.get('error')])\n",
    "    total = len(batch_results)\n",
    "    success_rate = (successful / total * 100) if total > 0 else 0\n",
    "    \n",
    "    print(f\"‚úÖ PHASE 1 COMPLETE: Document Type Detection\")\n",
    "    print(f\"\\nüìä RESULTS SUMMARY:\")\n",
    "    print(f\"   Documents processed: {total}\")\n",
    "    print(f\"   Successful classifications: {successful} ({success_rate:.1f}%)\")\n",
    "    print(f\"   Model used: {MODEL_TYPE.upper()}\")\n",
    "    print(f\"   Confidence threshold: {CONFIDENCE_THRESHOLD}\")\n",
    "    \n",
    "    print(f\"\\nüéØ NEXT STEPS FOR PHASE 2:\")\n",
    "    print(f\"   1. ‚úÖ Document classification working\")\n",
    "    print(f\"   2. üìù Create document-specific schemas\")\n",
    "    print(f\"   3. üîÑ Implement schema routing logic\")\n",
    "    print(f\"   4. üß™ Test type-specific extraction\")\n",
    "    \n",
    "    if success_rate >= 90:\n",
    "        print(f\"\\nüéâ Classification accuracy looks excellent! Ready for Phase 2.\")\n",
    "    elif success_rate >= 80:\n",
    "        print(f\"\\nüëç Good classification results. Minor tuning may help before Phase 2.\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è Classification needs improvement before proceeding to Phase 2.\")\n",
    "\nelse:\n",
    "    print(\"‚ùå Phase 1 testing incomplete - no results available\")\n",
    "    print(\"üí° Make sure to run on a machine with GPU and model access\")\n",
    "\n",
    "print(f\"\\nüìö DOCUMENTATION:\")\n",
    "print(f\"   Phase 1 implementation: common/document_type_detector.py\")\n",
    "print(f\"   Testing script: test_document_classification.py\")\n",
    "print(f\"   Full proposal: docs/document_type_specific_extraction_proposal.md\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Vision Notebooks)",
   "language": "python",
   "name": "vision_notebooks"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}