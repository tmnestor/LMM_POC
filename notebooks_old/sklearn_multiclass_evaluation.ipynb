{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-learn Multiclass Evaluation\n",
    "\n",
    "Comprehensive evaluation of multiclass predictions against ground truth annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    balanced_accuracy_score,\n",
    "    classification_report,\n",
    "    cohen_kappa_score,\n",
    "    confusion_matrix,\n",
    "    matthews_corrcoef,\n",
    "    precision_recall_fscore_support,\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Global plot configuration\n",
    "plt.rcParams[\"figure.facecolor\"] = \"white\"\n",
    "plt.rcParams[\"axes.facecolor\"] = \"white\"\n",
    "plt.rcParams[\"savefig.facecolor\"] = \"white\"\n",
    "plt.rcParams[\"savefig.bbox\"] = \"tight\"\n",
    "plt.rcParams[\"font.size\"] = 10\n",
    "plt.rcParams[\"axes.titlesize\"] = 12\n",
    "plt.rcParams[\"axes.labelsize\"] = 11\n",
    "plt.rcParams[\"xtick.labelsize\"] = 9\n",
    "plt.rcParams[\"ytick.labelsize\"] = 9\n",
    "plt.rcParams[\"legend.fontsize\"] = 10\n",
    "\n",
    "# Set seaborn style\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "# CONFIGURE BASE DIRECTORY - Change this to your project directory\n",
    "BASE_DIR = Path.cwd()  # Current directory\n",
    "# BASE_DIR = Path('/Users/tod/Desktop/LMM_POC')  # Or set specific path\n",
    "\n",
    "print(f\"üìÅ Base directory: {BASE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Load from CSV file\n",
    "# Uncomment and modify the path below to load your data\n",
    "# INPUT_FILE = DATA_DIR / 'your_predictions.csv'\n",
    "# df = pd.read_csv(INPUT_FILE)\n",
    "# print(f\"‚úÖ Loaded data from: {INPUT_FILE}\")\n",
    "\n",
    "# Option 2: Load from specific path\n",
    "# df = pd.read_csv(Path('/path/to/your/data.csv'))\n",
    "\n",
    "# Option 3: Use existing DataFrame from previous cell\n",
    "# df = your_existing_dataframe\n",
    "\n",
    "# Option 4: Create sample data for testing\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "classes = [\"class_A\", \"class_B\", \"class_C\", \"class_D\", \"class_E\"]\n",
    "\n",
    "# Generate ground truth\n",
    "annotator = np.random.choice(classes, n_samples, p=[0.3, 0.25, 0.2, 0.15, 0.1])\n",
    "\n",
    "# Generate predictions with some errors\n",
    "pred = annotator.copy()\n",
    "error_mask = np.random.random(n_samples) < 0.2  # 20% error rate\n",
    "pred[error_mask] = np.random.choice(classes, error_mask.sum())\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({PRED_COLUMN: pred, TRUTH_COLUMN: annotator})\n",
    "\n",
    "INPUT_FILE = OUTPUT_DIR / \"sample_data.csv\"\n",
    "df.to_csv(INPUT_FILE, index=False)\n",
    "print(f\"üíæ Sample data saved to: {INPUT_FILE}\")\n",
    "\n",
    "print(f\"\\nDataFrame shape: {df.shape}\")\n",
    "print(f\"Unique classes in ground truth: {df[TRUTH_COLUMN].nunique()}\")\n",
    "print(f\"Unique classes in predictions: {df[PRED_COLUMN].nunique()}\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Load from CSV file\n",
    "# df = pd.read_csv(BASE_DIR / 'your_predictions.csv')\n",
    "\n",
    "# Option 2: Use existing DataFrame\n",
    "# df = your_existing_dataframe\n",
    "\n",
    "# Option 3: Create sample data for testing\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "classes = [\"class_A\", \"class_B\", \"class_C\", \"class_D\", \"class_E\"]\n",
    "\n",
    "# Generate ground truth\n",
    "annotator = np.random.choice(classes, n_samples, p=[0.3, 0.25, 0.2, 0.15, 0.1])\n",
    "\n",
    "# Generate predictions with some errors\n",
    "pred = annotator.copy()\n",
    "error_mask = np.random.random(n_samples) < 0.2  # 20% error rate\n",
    "pred[error_mask] = np.random.choice(classes, error_mask.sum())\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\"pred\": pred, \"annotator\": annotator})\n",
    "\n",
    "print(f\"DataFrame shape: {df.shape}\")\n",
    "print(f\"Unique classes in ground truth: {df['annotator'].nunique()}\")\n",
    "print(f\"Unique classes in predictions: {df['pred'].nunique()}\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate All Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Ground truth distribution\n",
    "df[\"annotator\"].value_counts().plot(kind=\"bar\", ax=axes[0], color=\"skyblue\")\n",
    "axes[0].set_title(\"Ground Truth Distribution\", fontweight=\"bold\")\n",
    "axes[0].set_xlabel(\"Class\")\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "axes[0].tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "# Prediction distribution\n",
    "df[\"pred\"].value_counts().plot(kind=\"bar\", ax=axes[1], color=\"lightcoral\")\n",
    "axes[1].set_title(\"Prediction Distribution\", fontweight=\"bold\")\n",
    "axes[1].set_xlabel(\"Class\")\n",
    "axes[1].set_ylabel(\"Count\")\n",
    "axes[1].tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions and ground truth\n",
    "y_true = df[\"annotator\"].to_numpy()\n",
    "y_pred = df[\"pred\"].to_numpy()\n",
    "\n",
    "# Identify valid classes (only those in ground truth)\n",
    "valid_classes = np.unique(y_true)\n",
    "\n",
    "# Check for invalid predictions\n",
    "invalid_mask = ~np.isin(y_pred, valid_classes)\n",
    "n_invalid = invalid_mask.sum()\n",
    "\n",
    "if n_invalid > 0:\n",
    "    invalid_classes = set(np.unique(y_pred)) - set(valid_classes)\n",
    "    print(\n",
    "        f\"‚ö†Ô∏è WARNING: Found {n_invalid} predictions ({n_invalid / len(y_pred) * 100:.2f}%) with invalid classes\"\n",
    "    )\n",
    "    print(f\"   Invalid classes being predicted: {invalid_classes}\")\n",
    "    for cls in invalid_classes:\n",
    "        count = (y_pred == cls).sum()\n",
    "        print(f\"     '{cls}': {count} occurrences\")\n",
    "    print(\"   These will be counted as errors but excluded from visualizations\\n\")\n",
    "\n",
    "# Calculate all metrics (including invalid predictions as errors)\n",
    "metrics = {}\n",
    "\n",
    "# Basic accuracy (invalid predictions count as errors)\n",
    "metrics[\"accuracy\"] = accuracy_score(y_true, y_pred)\n",
    "metrics[\"balanced_accuracy\"] = balanced_accuracy_score(y_true, y_pred)\n",
    "\n",
    "# Agreement metrics\n",
    "metrics[\"cohen_kappa\"] = cohen_kappa_score(y_true, y_pred)\n",
    "metrics[\"matthews_corrcoef\"] = matthews_corrcoef(y_true, y_pred)\n",
    "\n",
    "# Per-class metrics - only for valid classes\n",
    "precision, recall, f1, support = precision_recall_fscore_support(\n",
    "    y_true, y_pred, labels=valid_classes, average=None, zero_division=0\n",
    ")\n",
    "\n",
    "# Macro averages (only on valid classes)\n",
    "metrics[\"precision_macro\"] = precision_recall_fscore_support(\n",
    "    y_true, y_pred, labels=valid_classes, average=\"macro\", zero_division=0\n",
    ")[0]\n",
    "metrics[\"recall_macro\"] = precision_recall_fscore_support(\n",
    "    y_true, y_pred, labels=valid_classes, average=\"macro\", zero_division=0\n",
    ")[1]\n",
    "metrics[\"f1_macro\"] = precision_recall_fscore_support(\n",
    "    y_true, y_pred, labels=valid_classes, average=\"macro\", zero_division=0\n",
    ")[2]\n",
    "\n",
    "# Weighted averages\n",
    "metrics[\"precision_weighted\"] = precision_recall_fscore_support(\n",
    "    y_true, y_pred, labels=valid_classes, average=\"weighted\", zero_division=0\n",
    ")[0]\n",
    "metrics[\"recall_weighted\"] = precision_recall_fscore_support(\n",
    "    y_true, y_pred, labels=valid_classes, average=\"weighted\", zero_division=0\n",
    ")[1]\n",
    "metrics[\"f1_weighted\"] = precision_recall_fscore_support(\n",
    "    y_true, y_pred, labels=valid_classes, average=\"weighted\", zero_division=0\n",
    ")[2]\n",
    "\n",
    "# Store valid classes for later use\n",
    "metrics[\"valid_classes\"] = valid_classes\n",
    "metrics[\"n_invalid_predictions\"] = n_invalid\n",
    "\n",
    "# Display results\n",
    "print(\"=\" * 60)\n",
    "print(\"OVERALL METRICS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Accuracy:                {metrics['accuracy']:.4f}\")\n",
    "print(f\"Balanced Accuracy:       {metrics['balanced_accuracy']:.4f}\")\n",
    "print(f\"Cohen's Kappa:           {metrics['cohen_kappa']:.4f}\")\n",
    "print(f\"Matthews Corr Coef:      {metrics['matthews_corrcoef']:.4f}\")\n",
    "if n_invalid > 0:\n",
    "    print(\n",
    "        f\"Invalid Predictions:     {n_invalid} ({n_invalid / len(y_pred) * 100:.2f}%)\"\n",
    "    )\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"AVERAGED METRICS (Valid Classes Only)\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nMacro Averages (unweighted):\")\n",
    "print(f\"  Precision: {metrics['precision_macro']:.4f}\")\n",
    "print(f\"  Recall:    {metrics['recall_macro']:.4f}\")\n",
    "print(f\"  F1-Score:  {metrics['f1_macro']:.4f}\")\n",
    "\n",
    "print(\"\\nWeighted Averages (by support):\")\n",
    "print(f\"  Precision: {metrics['precision_weighted']:.4f}\")\n",
    "print(f\"  Recall:    {metrics['recall_weighted']:.4f}\")\n",
    "print(f\"  F1-Score:  {metrics['f1_weighted']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create per-class metrics DataFrame (only for valid classes)\n",
    "class_metrics = []\n",
    "\n",
    "for i, cls in enumerate(valid_classes):\n",
    "    cls_mask = y_true == cls\n",
    "    cls_pred = y_pred[cls_mask]\n",
    "\n",
    "    # Calculate accuracy including invalid predictions as errors\n",
    "    accuracy = (cls_pred == cls).mean()\n",
    "\n",
    "    class_metrics.append(\n",
    "        {\n",
    "            \"class\": cls,\n",
    "            \"support\": cls_mask.sum(),\n",
    "            \"accuracy\": accuracy,\n",
    "            \"precision\": precision[i],\n",
    "            \"recall\": recall[i],\n",
    "            \"f1\": f1[i],\n",
    "        }\n",
    "    )\n",
    "\n",
    "class_df = pd.DataFrame(class_metrics)\n",
    "class_df = class_df.sort_values(\"f1\", ascending=False)\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"=\" * 70)\n",
    "print(\"CLASS PERFORMANCE SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Total number of valid classes: {len(class_df)}\")\n",
    "print(\"\\nF1-Score Statistics:\")\n",
    "print(class_df[\"f1\"].describe().to_string())\n",
    "\n",
    "if metrics[\"n_invalid_predictions\"] > 0:\n",
    "    print(\n",
    "        f\"\\n‚ö†Ô∏è Note: {metrics['n_invalid_predictions']} predictions with invalid classes\"\n",
    "    )\n",
    "    print(\"         are counted as errors but not shown as separate classes\")\n",
    "\n",
    "# Show top and bottom performers\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TOP 10 BEST PERFORMING CLASSES\")\n",
    "print(\"=\" * 70)\n",
    "top_10 = class_df.nlargest(10, \"f1\")\n",
    "print(top_10[[\"class\", \"f1\", \"precision\", \"recall\", \"support\"]].to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TOP 10 WORST PERFORMING CLASSES\")\n",
    "print(\"=\" * 70)\n",
    "bottom_10 = class_df.nsmallest(10, \"f1\")\n",
    "print(\n",
    "    bottom_10[[\"class\", \"f1\", \"precision\", \"recall\", \"support\"]].to_string(index=False)\n",
    ")\n",
    "\n",
    "# Create visualizations for many classes\n",
    "if len(class_df) > 30:\n",
    "    print(\n",
    "        f\"\\nüìä Detected {len(class_df)} classes - using optimized visualizations for clarity\"\n",
    "    )\n",
    "\n",
    "    # Figure 1: Distribution of metrics\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "    # Precision distribution\n",
    "    axes[0].hist(\n",
    "        class_df[\"precision\"], bins=30, color=\"skyblue\", edgecolor=\"black\", alpha=0.7\n",
    "    )\n",
    "    axes[0].axvline(\n",
    "        class_df[\"precision\"].mean(),\n",
    "        color=\"red\",\n",
    "        linestyle=\"--\",\n",
    "        label=f\"Mean: {class_df['precision'].mean():.3f}\",\n",
    "    )\n",
    "    axes[0].set_xlabel(\"Precision\")\n",
    "    axes[0].set_ylabel(\"Number of Classes\")\n",
    "    axes[0].set_title(\"Distribution of Precision Across Classes\")\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "    # Recall distribution\n",
    "    axes[1].hist(\n",
    "        class_df[\"recall\"], bins=30, color=\"lightcoral\", edgecolor=\"black\", alpha=0.7\n",
    "    )\n",
    "    axes[1].axvline(\n",
    "        class_df[\"recall\"].mean(),\n",
    "        color=\"red\",\n",
    "        linestyle=\"--\",\n",
    "        label=f\"Mean: {class_df['recall'].mean():.3f}\",\n",
    "    )\n",
    "    axes[1].set_xlabel(\"Recall\")\n",
    "    axes[1].set_ylabel(\"Number of Classes\")\n",
    "    axes[1].set_title(\"Distribution of Recall Across Classes\")\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "    # F1-Score distribution\n",
    "    axes[2].hist(\n",
    "        class_df[\"f1\"], bins=30, color=\"lightgreen\", edgecolor=\"black\", alpha=0.7\n",
    "    )\n",
    "    axes[2].axvline(\n",
    "        class_df[\"f1\"].mean(),\n",
    "        color=\"red\",\n",
    "        linestyle=\"--\",\n",
    "        label=f\"Mean: {class_df['f1'].mean():.3f}\",\n",
    "    )\n",
    "    axes[2].set_xlabel(\"F1-Score\")\n",
    "    axes[2].set_ylabel(\"Number of Classes\")\n",
    "    axes[2].set_title(\"Distribution of F1-Score Across Classes\")\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "    plt.suptitle(\"Performance Metrics Distribution\", fontweight=\"bold\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Figure 2: Top 20 performers using seaborn\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 8))\n",
    "    top_20 = class_df.nlargest(20, \"f1\")\n",
    "\n",
    "    # Use seaborn for cleaner horizontal bar plots\n",
    "    sns.barplot(data=top_20, y=\"class\", x=\"precision\", ax=axes[0], orient=\"h\")\n",
    "    axes[0].set_xlabel(\"Precision\")\n",
    "    axes[0].set_title(\"Top 20 Classes - Precision\", fontweight=\"bold\")\n",
    "    axes[0].set_ylabel(\"\")\n",
    "\n",
    "    sns.barplot(data=top_20, y=\"class\", x=\"recall\", ax=axes[1], orient=\"h\")\n",
    "    axes[1].set_xlabel(\"Recall\")\n",
    "    axes[1].set_title(\"Top 20 Classes - Recall\", fontweight=\"bold\")\n",
    "    axes[1].set_ylabel(\"\")\n",
    "\n",
    "    sns.barplot(data=top_20, y=\"class\", x=\"f1\", ax=axes[2], orient=\"h\")\n",
    "    axes[2].set_xlabel(\"F1-Score\")\n",
    "    axes[2].set_title(\"Top 20 Classes - F1-Score\", fontweight=\"bold\")\n",
    "    axes[2].set_ylabel(\"\")\n",
    "\n",
    "    plt.suptitle(\"Top 20 Best Performing Classes\", fontweight=\"bold\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Figure 3: Bottom 20 performers (if there are enough classes)\n",
    "    if len(class_df) > 20:\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 8))\n",
    "        bottom_20 = class_df.nsmallest(20, \"f1\")\n",
    "\n",
    "        sns.barplot(\n",
    "            data=bottom_20,\n",
    "            y=\"class\",\n",
    "            x=\"precision\",\n",
    "            ax=axes[0],\n",
    "            orient=\"h\",\n",
    "            color=\"#FF6B6B\",\n",
    "        )\n",
    "        axes[0].set_xlabel(\"Precision\")\n",
    "        axes[0].set_title(\"Bottom 20 Classes - Precision\", fontweight=\"bold\")\n",
    "        axes[0].set_ylabel(\"\")\n",
    "\n",
    "        sns.barplot(\n",
    "            data=bottom_20,\n",
    "            y=\"class\",\n",
    "            x=\"recall\",\n",
    "            ax=axes[1],\n",
    "            orient=\"h\",\n",
    "            color=\"#FF9999\",\n",
    "        )\n",
    "        axes[1].set_xlabel(\"Recall\")\n",
    "        axes[1].set_title(\"Bottom 20 Classes - Recall\", fontweight=\"bold\")\n",
    "        axes[1].set_ylabel(\"\")\n",
    "\n",
    "        sns.barplot(\n",
    "            data=bottom_20, y=\"class\", x=\"f1\", ax=axes[2], orient=\"h\", color=\"#FFB3B3\"\n",
    "        )\n",
    "        axes[2].set_xlabel(\"F1-Score\")\n",
    "        axes[2].set_title(\"Bottom 20 Classes - F1-Score\", fontweight=\"bold\")\n",
    "        axes[2].set_ylabel(\"\")\n",
    "\n",
    "        plt.suptitle(\"Bottom 20 Worst Performing Classes\", fontweight=\"bold\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "else:\n",
    "    # Original visualization for fewer classes using seaborn\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "    sns.barplot(data=class_df, x=\"class\", y=\"precision\", ax=axes[0])\n",
    "    axes[0].set_title(\"Precision by Class\", fontweight=\"bold\")\n",
    "    axes[0].set_xlabel(\"Class\")\n",
    "    axes[0].set_ylabel(\"Precision\")\n",
    "    axes[0].tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "    sns.barplot(data=class_df, x=\"class\", y=\"recall\", ax=axes[1])\n",
    "    axes[1].set_title(\"Recall by Class\", fontweight=\"bold\")\n",
    "    axes[1].set_xlabel(\"Class\")\n",
    "    axes[1].set_ylabel(\"Recall\")\n",
    "    axes[1].tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "    sns.barplot(data=class_df, x=\"class\", y=\"f1\", ax=axes[2])\n",
    "    axes[2].set_title(\"F1-Score by Class\", fontweight=\"bold\")\n",
    "    axes[2].set_xlabel(\"Class\")\n",
    "    axes[2].set_ylabel(\"F1-Score\")\n",
    "    axes[2].tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "    plt.suptitle(\"Per-Class Performance Metrics\", fontweight=\"bold\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate classification report (only for valid classes)\n",
    "print(classification_report(y_true, y_pred, labels=valid_classes, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per-Class Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrix (only for valid classes from ground truth)\n",
    "cm = confusion_matrix(y_true, y_pred, labels=valid_classes)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Blues\",\n",
    "    xticklabels=valid_classes,\n",
    "    yticklabels=valid_classes,\n",
    "    cbar_kws={\"label\": \"Count\"},\n",
    ")\n",
    "plt.title(\"Confusion Matrix (Valid Classes Only)\", fontweight=\"bold\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate and display accuracy for each class\n",
    "print(\"\\nPer-Class Accuracy:\")\n",
    "print(\"=\" * 40)\n",
    "class_accuracy = cm.diagonal() / cm.sum(axis=1)\n",
    "for i, cls in enumerate(valid_classes):\n",
    "    print(f\"{cls:<20}: {class_accuracy[i]:.4f}\")\n",
    "\n",
    "# Show impact of invalid predictions if any\n",
    "if metrics[\"n_invalid_predictions\"] > 0:\n",
    "    print(\n",
    "        f\"\\n‚ö†Ô∏è Note: {metrics['n_invalid_predictions']} invalid predictions are counted as errors\"\n",
    "    )\n",
    "    print(\"         but not shown in the confusion matrix above\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create per-class metrics DataFrame (only for valid classes)\n",
    "class_metrics = []\n",
    "\n",
    "for i, cls in enumerate(valid_classes):\n",
    "    cls_mask = y_true == cls\n",
    "    cls_pred = y_pred[cls_mask]\n",
    "\n",
    "    # Calculate accuracy including invalid predictions as errors\n",
    "    accuracy = (cls_pred == cls).mean()\n",
    "\n",
    "    class_metrics.append(\n",
    "        {\n",
    "            \"class\": cls,\n",
    "            \"support\": cls_mask.sum(),\n",
    "            \"accuracy\": accuracy,\n",
    "            \"precision\": precision[i],\n",
    "            \"recall\": recall[i],\n",
    "            \"f1\": f1[i],\n",
    "        }\n",
    "    )\n",
    "\n",
    "class_df = pd.DataFrame(class_metrics)\n",
    "class_df = class_df.sort_values(\"f1\", ascending=False)\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"=\" * 70)\n",
    "print(\"CLASS PERFORMANCE SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Total number of valid classes: {len(class_df)}\")\n",
    "print(\"\\nF1-Score Statistics:\")\n",
    "print(class_df[\"f1\"].describe().to_string())\n",
    "\n",
    "if metrics[\"n_invalid_predictions\"] > 0:\n",
    "    print(\n",
    "        f\"\\n‚ö†Ô∏è Note: {metrics['n_invalid_predictions']} predictions with invalid classes\"\n",
    "    )\n",
    "    print(\"         are counted as errors but not shown as separate classes\")\n",
    "\n",
    "# Show top and bottom performers\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TOP 10 BEST PERFORMING CLASSES\")\n",
    "print(\"=\" * 70)\n",
    "top_10 = class_df.nlargest(10, \"f1\")\n",
    "print(top_10[[\"class\", \"f1\", \"precision\", \"recall\", \"support\"]].to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TOP 10 WORST PERFORMING CLASSES\")\n",
    "print(\"=\" * 70)\n",
    "bottom_10 = class_df.nsmallest(10, \"f1\")\n",
    "print(\n",
    "    bottom_10[[\"class\", \"f1\", \"precision\", \"recall\", \"support\"]].to_string(index=False)\n",
    ")\n",
    "\n",
    "# SEABORN VISUALIZATIONS FOR MANY CLASSES\n",
    "print(f\"\\nüìä Creating visualizations for {len(class_df)} classes using Seaborn...\")\n",
    "\n",
    "# Prepare data in long format for seaborn\n",
    "class_df_melted = class_df.melt(\n",
    "    id_vars=[\"class\", \"support\", \"accuracy\"],\n",
    "    value_vars=[\"precision\", \"recall\", \"f1\"],\n",
    "    var_name=\"metric\",\n",
    "    value_name=\"score\",\n",
    ")\n",
    "\n",
    "# Figure 1: Distribution Analysis with Seaborn\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Violin plots for distributions\n",
    "ax1 = axes[0, 0]\n",
    "metrics_for_violin = class_df[[\"precision\", \"recall\", \"f1\"]]\n",
    "metrics_long = pd.melt(metrics_for_violin)\n",
    "sns.violinplot(data=metrics_long, x=\"variable\", y=\"value\", ax=ax1, palette=\"Set2\")\n",
    "ax1.set_xlabel(\"Metric\", fontweight=\"bold\")\n",
    "ax1.set_ylabel(\"Score\", fontweight=\"bold\")\n",
    "ax1.set_title(\"Score Distributions (Violin Plot)\", fontweight=\"bold\")\n",
    "ax1.set_xticklabels([\"Precision\", \"Recall\", \"F1-Score\"])\n",
    "\n",
    "# Box plots with outliers\n",
    "ax2 = axes[0, 1]\n",
    "sns.boxplot(data=metrics_long, x=\"variable\", y=\"value\", ax=ax2, palette=\"muted\")\n",
    "sns.swarmplot(\n",
    "    data=metrics_long, x=\"variable\", y=\"value\", ax=ax2, color=\"black\", alpha=0.3, size=2\n",
    ")\n",
    "ax2.set_xlabel(\"Metric\", fontweight=\"bold\")\n",
    "ax2.set_ylabel(\"Score\", fontweight=\"bold\")\n",
    "ax2.set_title(\"Score Distributions with Outliers\", fontweight=\"bold\")\n",
    "ax2.set_xticklabels([\"Precision\", \"Recall\", \"F1-Score\"])\n",
    "\n",
    "# KDE plots for smooth distributions\n",
    "ax3 = axes[1, 0]\n",
    "sns.kdeplot(data=class_df[\"precision\"], ax=ax3, label=\"Precision\", fill=True, alpha=0.6)\n",
    "sns.kdeplot(data=class_df[\"recall\"], ax=ax3, label=\"Recall\", fill=True, alpha=0.6)\n",
    "sns.kdeplot(data=class_df[\"f1\"], ax=ax3, label=\"F1-Score\", fill=True, alpha=0.6)\n",
    "ax3.set_xlabel(\"Score\", fontweight=\"bold\")\n",
    "ax3.set_ylabel(\"Density\", fontweight=\"bold\")\n",
    "ax3.set_title(\"Kernel Density Estimation\", fontweight=\"bold\")\n",
    "ax3.legend()\n",
    "ax3.set_xlim([0, 1])\n",
    "\n",
    "# Joint plot for precision vs recall\n",
    "ax4 = axes[1, 1]\n",
    "sns.scatterplot(\n",
    "    data=class_df,\n",
    "    x=\"precision\",\n",
    "    y=\"recall\",\n",
    "    size=\"support\",\n",
    "    hue=\"f1\",\n",
    "    palette=\"RdYlGn\",\n",
    "    ax=ax4,\n",
    "    sizes=(20, 200),\n",
    "    alpha=0.7,\n",
    ")\n",
    "ax4.set_xlabel(\"Precision\", fontweight=\"bold\")\n",
    "ax4.set_ylabel(\"Recall\", fontweight=\"bold\")\n",
    "ax4.set_title(\"Precision vs Recall (sized by support)\", fontweight=\"bold\")\n",
    "ax4.plot([0, 1], [0, 1], \"k--\", alpha=0.3)  # Diagonal reference line\n",
    "ax4.set_xlim([0, 1])\n",
    "ax4.set_ylim([0, 1])\n",
    "\n",
    "plt.suptitle(\"Performance Distribution Analysis\", fontweight=\"bold\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Figure 2: Top/Bottom Performers with Seaborn\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# Top 20 performers\n",
    "top_20 = class_df.nlargest(20, \"f1\")\n",
    "top_20_melted = top_20.melt(\n",
    "    id_vars=[\"class\"],\n",
    "    value_vars=[\"precision\", \"recall\", \"f1\"],\n",
    "    var_name=\"metric\",\n",
    "    value_name=\"score\",\n",
    ")\n",
    "\n",
    "sns.barplot(\n",
    "    data=top_20_melted,\n",
    "    y=\"class\",\n",
    "    x=\"score\",\n",
    "    hue=\"metric\",\n",
    "    ax=axes[0],\n",
    "    palette=\"viridis\",\n",
    "    orient=\"h\",\n",
    ")\n",
    "axes[0].set_xlabel(\"Score\", fontweight=\"bold\")\n",
    "axes[0].set_ylabel(\"\")\n",
    "axes[0].set_title(\n",
    "    \"Top 20 Best Performing Classes\", fontweight=\"bold\", color=\"darkgreen\"\n",
    ")\n",
    "axes[0].legend(title=\"Metric\", loc=\"lower right\")\n",
    "axes[0].set_xlim([0, 1])\n",
    "\n",
    "# Bottom 20 performers\n",
    "bottom_20 = class_df.nsmallest(20, \"f1\")\n",
    "bottom_20_melted = bottom_20.melt(\n",
    "    id_vars=[\"class\"],\n",
    "    value_vars=[\"precision\", \"recall\", \"f1\"],\n",
    "    var_name=\"metric\",\n",
    "    value_name=\"score\",\n",
    ")\n",
    "\n",
    "sns.barplot(\n",
    "    data=bottom_20_melted,\n",
    "    y=\"class\",\n",
    "    x=\"score\",\n",
    "    hue=\"metric\",\n",
    "    ax=axes[1],\n",
    "    palette=\"rocket_r\",\n",
    "    orient=\"h\",\n",
    ")\n",
    "axes[1].set_xlabel(\"Score\", fontweight=\"bold\")\n",
    "axes[1].set_ylabel(\"\")\n",
    "axes[1].set_title(\n",
    "    \"Bottom 20 Worst Performing Classes\", fontweight=\"bold\", color=\"darkred\"\n",
    ")\n",
    "axes[1].legend(title=\"Metric\", loc=\"upper right\")\n",
    "axes[1].set_xlim([0, 1])\n",
    "\n",
    "plt.suptitle(\"Best vs Worst Performers\", fontweight=\"bold\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Figure 3: Heatmap Analysis\n",
    "if len(class_df) > 20:\n",
    "    # Create two heatmaps - one for top performers, one for all classes\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "    # Top 30 classes heatmap\n",
    "    top_30 = class_df.nlargest(30, \"f1\")\n",
    "    heatmap_data_top = top_30.set_index(\"class\")[[\"precision\", \"recall\", \"f1\"]].T\n",
    "\n",
    "    sns.heatmap(\n",
    "        heatmap_data_top,\n",
    "        annot=False,\n",
    "        cmap=\"RdYlGn\",\n",
    "        vmin=0,\n",
    "        vmax=1,\n",
    "        ax=axes[0],\n",
    "        cbar_kws={\"label\": \"Score\"},\n",
    "        linewidths=0.5,\n",
    "    )\n",
    "    axes[0].set_xlabel(\"\")\n",
    "    axes[0].set_ylabel(\"Metric\", fontweight=\"bold\")\n",
    "    axes[0].set_title(\"Top 30 Classes - Performance Heatmap\", fontweight=\"bold\")\n",
    "    axes[0].set_xticklabels(axes[0].get_xticklabels(), rotation=45, ha=\"right\")\n",
    "\n",
    "    # All classes summary heatmap (binned)\n",
    "    # Create bins for F1 scores\n",
    "    n_bins = min(50, len(class_df))\n",
    "    class_df[\"f1_bin\"] = pd.cut(class_df[\"f1\"], bins=n_bins, labels=False)\n",
    "    binned_stats = class_df.groupby(\"f1_bin\")[[\"precision\", \"recall\", \"f1\"]].mean()\n",
    "\n",
    "    sns.heatmap(\n",
    "        binned_stats.T,\n",
    "        annot=False,\n",
    "        cmap=\"RdYlGn\",\n",
    "        vmin=0,\n",
    "        vmax=1,\n",
    "        ax=axes[1],\n",
    "        cbar_kws={\"label\": \"Score\"},\n",
    "        xticklabels=False,\n",
    "    )\n",
    "    axes[1].set_xlabel(\n",
    "        f\"Classes (binned by F1 score, {len(class_df)} total)\", fontweight=\"bold\"\n",
    "    )\n",
    "    axes[1].set_ylabel(\"Metric\", fontweight=\"bold\")\n",
    "    axes[1].set_title(\"All Classes - Binned Performance Heatmap\", fontweight=\"bold\")\n",
    "\n",
    "    plt.suptitle(\"Performance Heatmaps\", fontweight=\"bold\", y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Figure 4: Clustermap for Pattern Discovery\n",
    "if len(class_df) <= 100:  # Only for manageable number of classes\n",
    "    # Prepare data for clustering\n",
    "    cluster_data = class_df.set_index(\"class\")[[\"precision\", \"recall\", \"f1\"]]\n",
    "\n",
    "    # Create clustermap\n",
    "    plt.figure(figsize=(10, 12))\n",
    "    g = sns.clustermap(\n",
    "        cluster_data,\n",
    "        cmap=\"RdYlGn\",\n",
    "        vmin=0,\n",
    "        vmax=1,\n",
    "        col_cluster=False,\n",
    "        linewidths=0.5,\n",
    "        cbar_kws={\"label\": \"Score\"},\n",
    "        figsize=(10, 12),\n",
    "    )\n",
    "    g.ax_heatmap.set_xlabel(\"Metric\", fontweight=\"bold\")\n",
    "    g.ax_heatmap.set_title(\n",
    "        \"Hierarchical Clustering of Class Performance\", fontweight=\"bold\", pad=20\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "# Figure 5: Pairplot for Relationships\n",
    "if len(class_df) <= 50:  # Only for small number of classes\n",
    "    # Create pairplot\n",
    "    pair_vars = [\"precision\", \"recall\", \"f1\", \"support\"]\n",
    "    g = sns.pairplot(\n",
    "        class_df[pair_vars], diag_kind=\"kde\", plot_kws={\"alpha\": 0.6}, height=2.5\n",
    "    )\n",
    "    g.fig.suptitle(\"Metric Relationships\", fontweight=\"bold\", y=1.02)\n",
    "    plt.show()\n",
    "\n",
    "# Figure 6: Strip/Swarm Plot for All Classes\n",
    "if len(class_df) > 30:\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "    # Create strip plot with jitter\n",
    "    class_df_melted_subset = class_df_melted[\n",
    "        class_df_melted[\"metric\"].isin([\"precision\", \"recall\", \"f1\"])\n",
    "    ]\n",
    "    sns.stripplot(\n",
    "        data=class_df_melted_subset,\n",
    "        x=\"score\",\n",
    "        y=\"metric\",\n",
    "        hue=\"metric\",\n",
    "        palette=\"Set2\",\n",
    "        size=3,\n",
    "        alpha=0.6,\n",
    "        ax=ax,\n",
    "    )\n",
    "\n",
    "    # Add violin plot overlay\n",
    "    sns.violinplot(\n",
    "        data=class_df_melted_subset,\n",
    "        x=\"score\",\n",
    "        y=\"metric\",\n",
    "        palette=\"Set2\",\n",
    "        inner=None,\n",
    "        alpha=0.3,\n",
    "        ax=ax,\n",
    "    )\n",
    "\n",
    "    ax.set_xlabel(\"Score\", fontweight=\"bold\")\n",
    "    ax.set_ylabel(\"Metric\", fontweight=\"bold\")\n",
    "    ax.set_title(\n",
    "        f\"Score Distribution for All {len(class_df)} Classes\", fontweight=\"bold\"\n",
    "    )\n",
    "    ax.legend_.remove()\n",
    "    ax.set_xlim([0, 1])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Seaborn visualizations complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find misclassifications\n",
    "errors = df[df[PRED_COLUMN] != df[TRUTH_COLUMN]].copy()\n",
    "\n",
    "print(f\"Total errors: {len(errors):,} ({len(errors) / len(df) * 100:.2f}%)\")\n",
    "print(\n",
    "    f\"Total correct: {len(df) - len(errors):,} ({(len(df) - len(errors)) / len(df) * 100:.2f}%)\"\n",
    ")\n",
    "\n",
    "if len(errors) > 0:\n",
    "    # Create error pattern\n",
    "    errors[\"error_pattern\"] = (\n",
    "        errors[TRUTH_COLUMN].astype(str) + \" ‚Üí \" + errors[PRED_COLUMN].astype(str)\n",
    "    )\n",
    "\n",
    "    # Top error patterns\n",
    "    error_counts = errors[\"error_pattern\"].value_counts().head(10)\n",
    "\n",
    "    print(\"\\nTop 10 Most Common Misclassifications:\")\n",
    "    print(\"=\" * 50)\n",
    "    for pattern, count in error_counts.items():\n",
    "        percentage = count / len(errors) * 100\n",
    "        print(f\"{pattern:<30} : {count:>5} ({percentage:>5.1f}%)\")\n",
    "\n",
    "    # Plot error patterns\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    error_counts.plot(kind=\"barh\", color=\"salmon\")\n",
    "    plt.title(\"Top 10 Error Patterns\", fontsize=12, fontweight=\"bold\")\n",
    "    plt.xlabel(\"Count\")\n",
    "    plt.ylabel(\"Error Pattern (Actual ‚Üí Predicted)\")\n",
    "    plt.grid(axis=\"x\", alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if SAVE_FIGURES:\n",
    "        fig_path = OUTPUT_DIR / \"error_patterns.png\"\n",
    "        plt.savefig(fig_path, dpi=FIGURE_DPI, bbox_inches=\"tight\")\n",
    "        print(f\"\\nüíæ Figure saved to: {fig_path}\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metrics to CSV\n",
    "metrics_df = pd.DataFrame([metrics])\n",
    "metrics_df.to_csv(BASE_DIR / \"evaluation_metrics.csv\", index=False)\n",
    "print(f\"Metrics saved to: {BASE_DIR / 'evaluation_metrics.csv'}\")\n",
    "\n",
    "# Save per-class metrics\n",
    "class_df.to_csv(BASE_DIR / \"per_class_metrics.csv\", index=False)\n",
    "print(f\"Per-class metrics saved to: {BASE_DIR / 'per_class_metrics.csv'}\")\n",
    "\n",
    "# Save confusion matrix (with valid classes only)\n",
    "cm_df = pd.DataFrame(cm, index=valid_classes, columns=valid_classes)\n",
    "cm_df.to_csv(BASE_DIR / \"confusion_matrix.csv\")\n",
    "print(f\"Confusion matrix saved to: {BASE_DIR / 'confusion_matrix.csv'}\")\n",
    "\n",
    "print(\"\\n‚úÖ All results exported successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}