name: LMM_POC_IVL3.5

channels:
  - pytorch
  - nvidia
  - conda-forge
  - defaults

dependencies:
  - python=3.11
  - pandas
  - seaborn
  - matplotlib
  - rich
  - pip
  - pytorch::pytorch>=2.0.0
  - pytorch::torchvision>=0.15.0
  - pytorch::pytorch-cuda=12.4
  - numpy>=1.24.0

  # Pip dependencies
  - pip:
    - transformers>=4.52.1,<5.0.0  # InternVL3.5 custom code incompatible with transformers 5.0 meta device init
    - pillow>=10.0.0
    - timm>=0.9.0
    - einops>=0.6.0
    - accelerate>=0.27.0
    - pyyaml>=6.0.0
    - jupyter>=1.0.0
    - ipython>=8.0.0
    - ipykernel

# Installation:
# conda env create -f environment_ivl35.yml
# conda activate LMM_POC_IVL3.5
#
# IMPORTANT: Install flash-attn separately (requires torch to be installed first):
# pip install flash-attn --no-build-isolation
#
# If flash-attn import fails with "undefined symbol" ABI mismatch error,
# see docs/FLASH_ATTENTION_SOURCE_BUILD.md for patched source build instructions.
#
# H200 GPU Optimized:
# - 8B model fits easily on single H200 (80GB HBM3)
# - Use torch_dtype=torch.bfloat16
# - Enable use_flash_attn=True
#
# Jupyter kernel is registered automatically by LMM_POC_setup.sh
# Manual registration: python -m ipykernel install --user --name=LMM_POC_IVL3.5 --display-name "Python (LMM_POC_IVL3.5)"
