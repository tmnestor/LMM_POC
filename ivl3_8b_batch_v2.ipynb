{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Document Extraction with InternVL3-8B (V2 - Float32 for V100)\n",
    "\n",
    "**V100 PRODUCTION VERSION**: This notebook uses torch.float32 precision to solve the gibberish inference problem on V100 GPUs.\n",
    "\n",
    "**V2 Features:**\n",
    "- **Sophisticated bank statement extraction** using multi-turn UnifiedBankExtractor\n",
    "- Turn 0: Header detection (identifies actual column names)\n",
    "- Turn 1: Adaptive extraction with structure-dynamic prompts\n",
    "- Automatic strategy selection: BALANCE_DESCRIPTION, AMOUNT_DESCRIPTION, etc.\n",
    "- Float32 precision (solves bfloat16‚Üífloat16 gibberish issue on V100)\n",
    "- Document type detection\n",
    "- Structured field extraction  \n",
    "- Evaluation against ground truth\n",
    "- Comprehensive reporting\n",
    "\n",
    "**Bank Statement Processing Toggle:**\n",
    "- `USE_SOPHISTICATED_BANK_EXTRACTION`: True (default) uses multi-turn extraction\n",
    "- `ENABLE_BALANCE_CORRECTION`: Optional mathematical balance validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Current directory: /home/jovyan/nfs_share/tod/LMM_POC\n",
      "‚úÖ Added /home/jovyan/nfs_share/tod/LMM_POC to sys.path\n",
      "‚úÖ Common module found at: /home/jovyan/nfs_share/tod/LMM_POC/common/__init__.py\n",
      "‚úÖ Path setup complete - proceed to imports\n"
     ]
    }
   ],
   "source": [
    "#Cell 1\n",
    "# Path setup for V100 systems - ensures proper module resolution\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Get the notebook's directory\n",
    "notebook_path = Path().absolute()\n",
    "print(f\"üìÇ Current directory: {notebook_path}\")\n",
    "\n",
    "# Ensure the project root is in the Python path\n",
    "if str(notebook_path) not in sys.path:\n",
    "    sys.path.insert(0, str(notebook_path))\n",
    "    print(f\"‚úÖ Added {notebook_path} to sys.path\")\n",
    "\n",
    "# Verify common module can be found\n",
    "try:\n",
    "    import common\n",
    "    print(f\"‚úÖ Common module found at: {common.__file__ if hasattr(common, '__file__') else 'built-in'}\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Common module not found: {e}\")\n",
    "    print(\"üìã Current sys.path:\")\n",
    "    for p in sys.path[:5]:  # Show first 5 paths\n",
    "        print(f\"   - {p}\")\n",
    "\n",
    "print(\"‚úÖ Path setup complete - proceed to imports\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Path Setup (V100 Compatibility)\n",
    "\n",
    "**IMPORTANT**: If you encounter import errors on V100 systems, this cell ensures proper module resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: field_types_from_yaml = {'monetary': ['GST_AMOUNT', 'TOTAL_AMOUNT', 'LINE_ITEM_PRICES', 'LINE_ITEM_TOTAL_PRICES', 'TRANSACTION_AMOUNTS_PAID', 'TRANSACTION_AMOUNTS_RECEIVED', 'ACCOUNT_BALANCE'], 'boolean': ['IS_GST_INCLUDED'], 'list': ['LINE_ITEM_DESCRIPTIONS', 'LINE_ITEM_QUANTITIES', 'LINE_ITEM_PRICES', 'LINE_ITEM_TOTAL_PRICES', 'TRANSACTION_DATES', 'TRANSACTION_AMOUNTS_PAID', 'TRANSACTION_AMOUNTS_RECEIVED', 'ACCOUNT_BALANCE'], 'date': ['INVOICE_DATE', 'STATEMENT_DATE_RANGE', 'TRANSACTION_DATES'], 'transaction_list': ['TRANSACTION_DATES', 'TRANSACTION_AMOUNTS_PAID', 'TRANSACTION_AMOUNTS_RECEIVED', 'ACCOUNT_BALANCE']}\n",
      "DEBUG: boolean fields from YAML = ['IS_GST_INCLUDED']\n",
      "DEBUG: self.boolean_fields = ['IS_GST_INCLUDED']\n",
      "DEBUG _ensure_fields_loaded: BOOLEAN_FIELDS = ['IS_GST_INCLUDED']\n",
      "‚úÖ All imports loaded successfully\n",
      "‚úÖ InternVL3 Hybrid Processor imported successfully\n",
      "‚úÖ Proven batch processing modules imported successfully\n",
      "‚úÖ V2: BankStatementAdapter imported for sophisticated bank extraction\n",
      "üìÇ Working directory: /home/jovyan/nfs_share/tod/LMM_POC\n"
     ]
    }
   ],
   "source": [
    "#Cell 2\n",
    "# Enable autoreload for module changes\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Standard library imports\n",
    "import sys\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Add current directory to path to ensure proper module resolution\n",
    "# This fixes import issues on multi-GPU V100 setups\n",
    "notebook_dir = Path.cwd()\n",
    "if str(notebook_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(notebook_dir))\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from rich import print as rprint\n",
    "from rich.console import Console\n",
    "\n",
    "# Project-specific imports - using absolute imports to avoid conflicts\n",
    "from common.batch_analytics import BatchAnalytics\n",
    "from common.batch_processor import BatchDocumentProcessor\n",
    "from common.batch_reporting import BatchReporter\n",
    "from common.batch_visualizations import BatchVisualizer\n",
    "from common.evaluation_metrics import load_ground_truth\n",
    "from common.extraction_parser import discover_images\n",
    "from common.gpu_optimization import emergency_cleanup\n",
    "from common.internvl3_model_loader import load_internvl3_model\n",
    "from models.document_aware_internvl3_processor import (\n",
    "    DocumentAwareInternVL3HybridProcessor,\n",
    ")\n",
    "\n",
    "# V2: Sophisticated bank statement processing\n",
    "from common.bank_statement_adapter import BankStatementAdapter\n",
    "\n",
    "print(\"‚úÖ All imports loaded successfully\")\n",
    "print(\"‚úÖ InternVL3 Hybrid Processor imported successfully\") \n",
    "print(\"‚úÖ Proven batch processing modules imported successfully\")\n",
    "print(\"‚úÖ V2: BankStatementAdapter imported for sophisticated bank extraction\")\n",
    "print(f\"üìÇ Working directory: {notebook_dir}\")\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pre-emptive Memory Cleanup\n",
    "\n",
    "**CRITICAL for V100**: Run this cell first to prevent OOM errors when switching between models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">üßπ PRE-EMPTIVE V100 MEMORY CLEANUP</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31müßπ PRE-EMPTIVE V100 MEMORY CLEANUP\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">Clearing any existing model caches before loading...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33mClearing any existing model caches before loading\u001b[0m\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">üí° This prevents OOM errors when switching between models on V100</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36müí° This prevents OOM errors when switching between models on V100\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üö® Running V100 emergency GPU cleanup...\n",
      "üßπ Starting V100-optimized GPU memory cleanup...\n",
      "   üìä Initial GPU memory: 0.00GB allocated, 0.00GB reserved\n",
      "   ‚úÖ Final GPU memory: 0.00GB allocated, 0.00GB reserved\n",
      "   üíæ Memory freed: 0.00GB\n",
      "‚úÖ V100-optimized memory cleanup complete\n",
      "‚úÖ V100 emergency cleanup complete\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">‚úÖ Memory cleanup complete - ready for model loading</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m‚úÖ Memory cleanup complete - ready for model loading\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">üìã Next: Import modules and configure settings</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2müìã Next: Import modules and configure settings\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Cell 3\n",
    "# Pre-emptive V100 Memory Cleanup - Run FIRST to prevent OOM errors\n",
    "rprint(\"[bold red]üßπ PRE-EMPTIVE V100 MEMORY CLEANUP[/bold red]\")\n",
    "rprint(\"[yellow]Clearing any existing model caches before loading...[/yellow]\")\n",
    "rprint(\"[cyan]üí° This prevents OOM errors when switching between models on V100[/cyan]\")\n",
    "\n",
    "# Emergency cleanup to ensure clean slate\n",
    "emergency_cleanup(verbose=True)\n",
    "\n",
    "rprint(\"[green]‚úÖ Memory cleanup complete - ready for model loading[/green]\")\n",
    "rprint(\"[dim]üìã Next: Import modules and configure settings[/dim]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration set up successfully\n",
      "üìÇ Evaluation data: /home/jovyan/nfs_share/tod/LMM_POC/LMM_POC/evaluation_data/synthetic\n",
      "üìä Ground truth: /home/jovyan/nfs_share/tod/LMM_POC/LMM_POC/evaluation_data/synthetic/ground_truth_synthetic.csv\n",
      "ü§ñ Model path: /home/jovyan/nfs_share/models/InternVL3-8B\n",
      "üìÅ Output base: /home/jovyan/nfs_share/tod/LMM_POC/LMM_POC/output\n",
      "üìã Universal fields: 17 (validation-only fields excluded)\n",
      "üéØ Mode: Evaluation mode\n",
      "‚öôÔ∏è  Precision: FLOAT32 (V100 production-ready - solves gibberish issue)\n",
      "‚ö° Flash Attention: DISABLED (V100 compatible)\n",
      "üî≤ Max Tiles: 6 (V100 optimized)\n",
      "üè¶ V2 Bank Extraction: Enabled (multi-turn)\n",
      "üìê Balance Correction: Enabled\n"
     ]
    }
   ],
   "source": [
    "#Cell 4\n",
    "# Initialize console and environment configuration\n",
    "console = Console()\n",
    "\n",
    "# Environment-specific base paths\n",
    "ENVIRONMENT_BASES = {\n",
    "    'sandbox': '/home/jovyan/nfs_share/tod/LMM_POC',\n",
    "    'efs': '/efs/shared/PoC_data'\n",
    "}\n",
    "base_data_path = ENVIRONMENT_BASES['sandbox']\n",
    "\n",
    "CONFIG = {\n",
    "    # Model settings\n",
    "    'MODEL_PATH': '/home/jovyan/nfs_share/models/InternVL3-8B',\n",
    "    # 'MODEL_PATH': '/efs/shared/PTM/InternVL3-8B',\n",
    "    \n",
    "    # Batch settings - Using base path for consistency\n",
    "    'DATA_DIR': f'{base_data_path}/LMM_POC/evaluation_data/synthetic',\n",
    "    'GROUND_TRUTH': f'{base_data_path}/LMM_POC/evaluation_data/synthetic/ground_truth_synthetic.csv',\n",
    "    'OUTPUT_BASE': f'{base_data_path}/LMM_POC/output',\n",
    "    'MAX_IMAGES': None,  # None for all, or set limit\n",
    "    'DOCUMENT_TYPES': None,  # None for all, or ['invoice', 'receipt']\n",
    "    'ENABLE_MATH_ENHANCEMENT': False,  # Disable mathematical correction for bank statements\n",
    "    \n",
    "    # Inference and evaluation mode\n",
    "    'INFERENCE_ONLY': False,  # Default: False (evaluation mode)\n",
    "    \n",
    "    # Verbosity control\n",
    "    'VERBOSE': True,\n",
    "    'SHOW_PROMPTS': True,\n",
    "    \n",
    "    # ============================================================================\n",
    "    # V100 FLOAT32 CONFIGURATION - SOLVES GIBBERISH INFERENCE PROBLEM\n",
    "    # ============================================================================\n",
    "    # CRITICAL: On V100 GPUs, bfloat16‚Üífloat16 conversion produces gibberish output\n",
    "    # Solution: Use torch.float32 (full precision) instead of quantization\n",
    "    # \n",
    "    # Research findings:\n",
    "    # - V100 lacks native bfloat16 hardware support (requires compute capability ‚â• 8.0)\n",
    "    # - Converting bfloat16 models to float16 causes numerical instability\n",
    "    # - torch.float32 provides stable, correct inference on V100\n",
    "    #\n",
    "    # Trade-offs:\n",
    "    # - Memory: 2x usage vs quantized (but still fits on V100 16GB)\n",
    "    # - Speed: Slower than quantized, but produces CORRECT outputs\n",
    "    # - Accuracy: Maximum stability, no gibberish\n",
    "    # ============================================================================\n",
    "    'USE_QUANTIZATION': False,  # DISABLED - Use full precision for V100\n",
    "    'DEVICE_MAP': 'auto',\n",
    "    'MAX_NEW_TOKENS': 1000,\n",
    "    'TORCH_DTYPE': 'float32',  # Full precision - solves gibberish issue\n",
    "    'LOW_CPU_MEM_USAGE': True,\n",
    "    # Flash Attention: NOT supported on V100, only enable for modern GPUs\n",
    "    'USE_FLASH_ATTN': False,  # V100 compatible default\n",
    "    \n",
    "    # V100 TILE CONFIGURATION\n",
    "    'MAX_TILES': 6,  # V100 optimized - InternVL3-8B config default\n",
    "    \n",
    "    # ============================================================================\n",
    "    # V2: SOPHISTICATED BANK STATEMENT EXTRACTION\n",
    "    # ============================================================================\n",
    "    # Use multi-turn UnifiedBankExtractor for bank statements instead of\n",
    "    # single-turn extraction. This provides:\n",
    "    # - Turn 0: Header detection (identifies actual column names)\n",
    "    # - Turn 1: Adaptive extraction with structure-dynamic prompts\n",
    "    # - Automatic strategy selection based on detected columns\n",
    "    # - Higher accuracy for bank statements\n",
    "    #\n",
    "    # Set to False to use original single-turn extraction behavior\n",
    "    # ============================================================================\n",
    "    'USE_SOPHISTICATED_BANK_EXTRACTION': True,\n",
    "    \n",
    "    # Optional: Enable balance-based mathematical correction\n",
    "    # Uses balance column deltas to validate debit/credit values\n",
    "    'ENABLE_BALANCE_CORRECTION': True,\n",
    "}\n",
    "\n",
    "# Make GROUND_TRUTH conditional based on INFERENCE_ONLY mode\n",
    "if CONFIG['INFERENCE_ONLY']:\n",
    "    CONFIG['GROUND_TRUTH'] = None\n",
    "\n",
    "# ============================================================================\n",
    "# PROMPT CONFIGURATION - Explicit file and key mapping\n",
    "# ============================================================================\n",
    "# This configuration controls which prompt files and keys are used for each\n",
    "# document type. You can explicitly override both the file and the key.\n",
    "#\n",
    "# Structure:\n",
    "#   'extraction_files': Maps document types to YAML prompt files\n",
    "#   'extraction_keys': (Optional) Maps document types to specific keys in those files\n",
    "#\n",
    "# If 'extraction_keys' is not specified for a document type, the key will be\n",
    "# derived from the document type name (e.g., 'INVOICE' -> 'invoice')\n",
    "#\n",
    "# For bank statements, structure classification (_flat or _date_grouped) is \n",
    "# automatically appended UNLESS you provide a full key in 'extraction_keys'\n",
    "#\n",
    "# NOTE: When USE_SOPHISTICATED_BANK_EXTRACTION is True, bank statements bypass\n",
    "# this configuration and use UnifiedBankExtractor with config/bank_prompts.yaml\n",
    "# ============================================================================\n",
    "\n",
    "PROMPT_CONFIG = {\n",
    "    # Document type detection configuration\n",
    "    'detection_file': 'prompts/document_type_detection.yaml',\n",
    "    'detection_key': 'detection',\n",
    "    \n",
    "    # Extraction prompt file mapping (REQUIRED)\n",
    "    'extraction_files': {\n",
    "        'INVOICE': 'prompts/internvl3_prompts.yaml',\n",
    "        'RECEIPT': 'prompts/internvl3_prompts.yaml', \n",
    "        'BANK_STATEMENT': 'prompts/internvl3_prompts.yaml'\n",
    "    },\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# UNIVERSAL FIELDS - Explicit field list (no common/config.py dependency)\n",
    "# ============================================================================\n",
    "# CRITICAL: TRANSACTION_AMOUNTS_RECEIVED and ACCOUNT_BALANCE are excluded\n",
    "# These fields are only for mathematical validation, not extraction/evaluation\n",
    "# ============================================================================\n",
    "UNIVERSAL_FIELDS = [\n",
    "    'DOCUMENT_TYPE',\n",
    "    'BUSINESS_ABN',\n",
    "    'SUPPLIER_NAME',\n",
    "    'BUSINESS_ADDRESS',\n",
    "    'PAYER_NAME',\n",
    "    'PAYER_ADDRESS',\n",
    "    'INVOICE_DATE',\n",
    "    'STATEMENT_DATE_RANGE',\n",
    "    'LINE_ITEM_DESCRIPTIONS',\n",
    "    'LINE_ITEM_QUANTITIES',\n",
    "    'LINE_ITEM_PRICES',\n",
    "    'LINE_ITEM_TOTAL_PRICES',\n",
    "    'IS_GST_INCLUDED',\n",
    "    'GST_AMOUNT',\n",
    "    'TOTAL_AMOUNT',\n",
    "    'TRANSACTION_DATES',\n",
    "    'TRANSACTION_AMOUNTS_PAID',\n",
    "]\n",
    "\n",
    "print(\"‚úÖ Configuration set up successfully\")\n",
    "print(f\"üìÇ Evaluation data: {CONFIG['DATA_DIR']}\")\n",
    "print(f\"üìä Ground truth: {CONFIG['GROUND_TRUTH']}\")\n",
    "print(f\"ü§ñ Model path: {CONFIG['MODEL_PATH']}\")\n",
    "print(f\"üìÅ Output base: {CONFIG['OUTPUT_BASE']}\")\n",
    "print(f\"üìã Universal fields: {len(UNIVERSAL_FIELDS)} (validation-only fields excluded)\")\n",
    "print(f\"üéØ Mode: {'Inference-only' if CONFIG['INFERENCE_ONLY'] else 'Evaluation mode'}\")\n",
    "print(f\"‚öôÔ∏è  Precision: FLOAT32 (V100 production-ready - solves gibberish issue)\")\n",
    "print(f\"‚ö° Flash Attention: {'ENABLED' if CONFIG['USE_FLASH_ATTN'] else 'DISABLED (V100 compatible)'}\")\n",
    "print(f\"üî≤ Max Tiles: {CONFIG['MAX_TILES']} (V100 optimized)\")\n",
    "print(f\"üè¶ V2 Bank Extraction: {'Enabled (multi-turn)' if CONFIG['USE_SOPHISTICATED_BANK_EXTRACTION'] else 'Disabled (single-turn)'}\")\n",
    "print(f\"üìê Balance Correction: {'Enabled' if CONFIG['ENABLE_BALANCE_CORRECTION'] else 'Disabled'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Output Directory Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 5\n",
    "# Setup output directories - Handle both absolute and relative paths\n",
    "\n",
    "# Convert OUTPUT_BASE to Path and handle absolute/relative paths\n",
    "OUTPUT_BASE = Path(CONFIG['OUTPUT_BASE'])\n",
    "if not OUTPUT_BASE.is_absolute():\n",
    "    # If relative, make it relative to current working directory\n",
    "    OUTPUT_BASE = Path.cwd() / OUTPUT_BASE\n",
    "\n",
    "BATCH_TIMESTAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "OUTPUT_DIRS = {\n",
    "    'base': OUTPUT_BASE,\n",
    "    'batch': OUTPUT_BASE / 'batch_results',\n",
    "    'csv': OUTPUT_BASE / 'csv',\n",
    "    'visualizations': OUTPUT_BASE / 'visualizations',\n",
    "    'reports': OUTPUT_BASE / 'reports'\n",
    "}\n",
    "\n",
    "for dir_path in OUTPUT_DIRS.values():\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Loading InternVL3-8B model with Float32 precision (V100 production)...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mLoading InternVL3-8B model with Float32 precision \u001b[0m\u001b[1;32m(\u001b[0m\u001b[1;32mV100 production\u001b[0m\u001b[1;32m)\u001b[0m\u001b[1;32m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">üí° Using torch.float32 to solve bfloat16‚Üífloat16 gibberish issue on V100</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36müí° Using torch.float32 to solve bfloat16‚Üífloat16 gibberish issue on V100\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">üöÄ Loading InternVL3 model with official optimizations...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34müöÄ Loading InternVL3 model with official optimizations\u001b[0m\u001b[1;34m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">üîß Configuring CUDA memory for InternVL3...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34müîß Configuring CUDA memory for InternVL3\u001b[0m\u001b[34m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">üìä Initial CUDA state: <span style=\"color: #808000; text-decoration-color: #808000\">Allocated</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.</span>00GB, <span style=\"color: #808000; text-decoration-color: #808000\">Reserved</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.</span>00GB\n",
       "</pre>\n"
      ],
      "text/plain": [
       "üìä Initial CUDA state: \u001b[33mAllocated\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.\u001b[0m00GB, \u001b[33mReserved\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.\u001b[0m00GB\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">üîç Performing robust GPU memory detection...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34müîç Performing robust GPU memory detection\u001b[0m\u001b[34m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Starting robust GPU memory detection...\n",
      "üìä Detected 1 GPU(s), analyzing each device...\n",
      "   GPU 0 (NVIDIA L4): 22.0GB total, 22.0GB available\n",
      "\n",
      "======================================================================\n",
      "üîç ROBUST GPU MEMORY DETECTION REPORT\n",
      "======================================================================\n",
      "‚úÖ Success: 1/1 GPUs detected\n",
      "üìä Total Memory: 21.95GB\n",
      "üíæ Available Memory: 21.95GB\n",
      "‚ö° Allocated Memory: 0.00GB\n",
      "üîÑ Reserved Memory: 0.00GB\n",
      "üì¶ Fragmentation: 0.00GB\n",
      "üñ•Ô∏è  Multi-GPU: No\n",
      "‚öñÔ∏è  Balanced Distribution: Yes\n",
      "\n",
      "üìã Per-GPU Breakdown:\n",
      "   GPU 0 (NVIDIA L4): 22.0GB total, 22.0GB available (0.0% used)\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">üìä GPU Hardware: NVIDIA L4 </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">(</span><span style=\"color: #000080; text-decoration-color: #000080\">1x 22GB = 22GB total</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34müìä GPU Hardware: NVIDIA L4 \u001b[0m\u001b[1;34m(\u001b[0m\u001b[34m1x 22GB = 22GB total\u001b[0m\u001b[1;34m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">üèóÔ∏è Architecture: cloud_inference </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">(</span><span style=\"color: #000080; text-decoration-color: #000080\">dynamic detection</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34müèóÔ∏è Architecture: cloud_inference \u001b[0m\u001b[1;34m(\u001b[0m\u001b[34mdynamic detection\u001b[0m\u001b[1;34m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">üéØ Model variant: InternVL3-8B </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">(</span><span style=\"color: #000080; text-decoration-color: #000080\">estimated need: 16GB + </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">3.</span><span style=\"color: #000080; text-decoration-color: #000080\">5GB buffer</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34müéØ Model variant: InternVL3-8B \u001b[0m\u001b[1;34m(\u001b[0m\u001b[34mestimated need: 16GB + \u001b[0m\u001b[1;34m3.\u001b[0m\u001b[34m5GB buffer\u001b[0m\u001b[1;34m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">üíæ Available Memory: </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">22.</span><span style=\"color: #000080; text-decoration-color: #000080\">0GB across </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">1</span><span style=\"color: #000080; text-decoration-color: #000080\"> </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">GPU(</span><span style=\"color: #000080; text-decoration-color: #000080\">s</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34müíæ Available Memory: \u001b[0m\u001b[1;34m22.\u001b[0m\u001b[34m0GB across \u001b[0m\u001b[1;34m1\u001b[0m\u001b[34m \u001b[0m\u001b[1;34mGPU\u001b[0m\u001b[1;34m(\u001b[0m\u001b[34ms\u001b[0m\u001b[1;34m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">üí° Memory sufficient: ‚úÖ Yes</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34müí° Memory sufficient: ‚úÖ Yes\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">‚úÖ Cloud inference GPU with 22GB - running InternVL3-8B in full precision</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m‚úÖ Cloud inference GPU with 22GB - running InternVL3-8B in full precision\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">üìä FINAL QUANTIZATION DECISION: DISABLED (full precision)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36müìä FINAL QUANTIZATION DECISION: DISABLED \u001b[0m\u001b[1;36m(\u001b[0m\u001b[1;36mfull precision\u001b[0m\u001b[1;36m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">   Total GPU Memory: 22GB</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36m   Total GPU Memory: 22GB\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">   Available Memory: 22GB</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36m   Available Memory: 22GB\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   Model needs: ~16GB + <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.</span>5GB buffer for InternVL3-8B\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   Model needs: ~16GB + \u001b[1;36m3.\u001b[0m5GB buffer for InternVL3-8B\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">   Working GPUs: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #008080; text-decoration-color: #008080\">/</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36m   Working GPUs: \u001b[0m\u001b[1;36m1\u001b[0m\u001b[36m/\u001b[0m\u001b[1;36m1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">üöÄ Using full precision mode</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32müöÄ Using full precision mode\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Loading InternVL3 model...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mLoading InternVL3 model\u001b[0m\u001b[36m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">üñ•Ô∏è  Detected </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #008080; text-decoration-color: #008080\"> </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">GPU(</span><span style=\"color: #008080; text-decoration-color: #008080\">s</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36müñ•Ô∏è  Detected \u001b[0m\u001b[1;36m1\u001b[0m\u001b[36m \u001b[0m\u001b[1;36mGPU\u001b[0m\u001b[1;36m(\u001b[0m\u001b[36ms\u001b[0m\u001b[1;36m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">üì• Loading model on single GPU...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36müì• Loading model on single GPU\u001b[0m\u001b[36m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">‚ö†Ô∏è Flash Attention disabled </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">V100 compatible</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33m‚ö†Ô∏è Flash Attention disabled \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mV100 compatible\u001b[0m\u001b[1;33m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FlashAttention2 is not installed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55a2b0cdefd742f681f2b567483b4fa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">‚ùå Failed to load model: CUDA out of memory. Tried to allocate </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">260.00</span><span style=\"color: #800000; text-decoration-color: #800000\"> MiB. GPU </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0</span><span style=\"color: #800000; text-decoration-color: #800000\"> has a total capacity of </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">21.95</span><span style=\"color: #800000; text-decoration-color: #800000\"> GiB </span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">of which </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">1.05</span><span style=\"color: #800000; text-decoration-color: #800000\"> GiB is free. Process </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">109074</span><span style=\"color: #800000; text-decoration-color: #800000\"> has </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">20.89</span><span style=\"color: #800000; text-decoration-color: #800000\"> GiB memory in use. </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">20.85</span><span style=\"color: #800000; text-decoration-color: #800000\"> GiB allowed; Of the allocated memory </span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">20.62</span><span style=\"color: #800000; text-decoration-color: #800000\"> GiB is allocated by PyTorch, and </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">91.37</span><span style=\"color: #800000; text-decoration-color: #800000\"> MiB is reserved by PyTorch but unallocated. If reserved but </span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">unallocated memory is large try setting </span><span style=\"color: #800000; text-decoration-color: #800000\">PYTORCH_CUDA_ALLOC_CONF</span><span style=\"color: #800000; text-decoration-color: #800000\">=</span><span style=\"color: #800000; text-decoration-color: #800000\">expandable_segments</span><span style=\"color: #800000; text-decoration-color: #800000\">:</span><span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">True</span><span style=\"color: #800000; text-decoration-color: #800000\"> to avoid fragmentation.  </span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">See documentation for Memory Management  </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">(</span><span style=\"color: #800000; text-decoration-color: #800000; text-decoration: underline\">https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m‚ùå Failed to load model: CUDA out of memory. Tried to allocate \u001b[0m\u001b[1;31m260.00\u001b[0m\u001b[31m MiB. GPU \u001b[0m\u001b[1;31m0\u001b[0m\u001b[31m has a total capacity of \u001b[0m\u001b[1;31m21.95\u001b[0m\u001b[31m GiB \u001b[0m\n",
       "\u001b[31mof which \u001b[0m\u001b[1;31m1.05\u001b[0m\u001b[31m GiB is free. Process \u001b[0m\u001b[1;31m109074\u001b[0m\u001b[31m has \u001b[0m\u001b[1;31m20.89\u001b[0m\u001b[31m GiB memory in use. \u001b[0m\u001b[1;31m20.85\u001b[0m\u001b[31m GiB allowed; Of the allocated memory \u001b[0m\n",
       "\u001b[1;31m20.62\u001b[0m\u001b[31m GiB is allocated by PyTorch, and \u001b[0m\u001b[1;31m91.37\u001b[0m\u001b[31m MiB is reserved by PyTorch but unallocated. If reserved but \u001b[0m\n",
       "\u001b[31munallocated memory is large try setting \u001b[0m\u001b[31mPYTORCH_CUDA_ALLOC_CONF\u001b[0m\u001b[31m=\u001b[0m\u001b[31mexpandable_segments\u001b[0m\u001b[31m:\u001b[0m\u001b[3;31mTrue\u001b[0m\u001b[31m to avoid fragmentation.  \u001b[0m\n",
       "\u001b[31mSee documentation for Memory Management  \u001b[0m\u001b[1;31m(\u001b[0m\u001b[4;31mhttps://pytorch.org/docs/stable/notes/cuda.html#environment-variables\u001b[0m\u001b[4;31m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 260.00 MiB. GPU 0 has a total capacity of 21.95 GiB of which 1.05 GiB is free. Process 109074 has 20.89 GiB memory in use. 20.85 GiB allowed; Of the allocated memory 20.62 GiB is allocated by PyTorch, and 91.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m rprint(\u001b[33m\"\u001b[39m\u001b[33m[cyan]üí° Using torch.float32 to solve bfloat16‚Üífloat16 gibberish issue on V100[/cyan]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Load InternVL3 model using robust infrastructure with float32\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m model, tokenizer = \u001b[43mload_internvl3_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mMODEL_PATH\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_quantization\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mUSE_QUANTIZATION\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# False - no quantization\u001b[39;49;00m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mDEVICE_MAP\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mMAX_NEW_TOKENS\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mTORCH_DTYPE\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 'float32' - full precision\u001b[39;49;00m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mLOW_CPU_MEM_USAGE\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_flash_attn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mUSE_FLASH_ATTN\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# V100 compatible setting\u001b[39;49;00m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mVERBOSE\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Initialize the hybrid processor with loaded model components AND prompt_config\u001b[39;00m\n\u001b[32m     19\u001b[39m hybrid_processor = DocumentAwareInternVL3HybridProcessor(\n\u001b[32m     20\u001b[39m     field_list=UNIVERSAL_FIELDS,\n\u001b[32m     21\u001b[39m     model_path=CONFIG[\u001b[33m'\u001b[39m\u001b[33mMODEL_PATH\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m     26\u001b[39m     max_tiles=CONFIG[\u001b[33m'\u001b[39m\u001b[33mMAX_TILES\u001b[39m\u001b[33m'\u001b[39m]  \u001b[38;5;66;03m# V100 optimized tile configuration\u001b[39;00m\n\u001b[32m     27\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/nfs_share/tod/LMM_POC/common/internvl3_model_loader.py:505\u001b[39m, in \u001b[36mload_internvl3_model\u001b[39m\u001b[34m(model_path, use_quantization, device_map, max_new_tokens, torch_dtype, low_cpu_mem_usage, use_flash_attn, verbose)\u001b[39m\n\u001b[32m    502\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m verbose:\n\u001b[32m    503\u001b[39m     rprint(\u001b[33m\"\u001b[39m\u001b[33m[yellow]‚ö†Ô∏è Flash Attention disabled (V100 compatible)[/yellow]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m505\u001b[39m model = \u001b[43mAutoModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m.eval()\n\u001b[32m    507\u001b[39m \u001b[38;5;66;03m# Load tokenizer\u001b[39;00m\n\u001b[32m    508\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/unified_vision_processor/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:559\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    557\u001b[39m     \u001b[38;5;28mcls\u001b[39m.register(config.\u001b[34m__class__\u001b[39m, model_class, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    558\u001b[39m     model_class = add_generation_mixin_to_remote_model(model_class)\n\u001b[32m--> \u001b[39m\u001b[32m559\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    560\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    561\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._model_mapping.keys():\n\u001b[32m    563\u001b[39m     model_class = _get_model_class(config, \u001b[38;5;28mcls\u001b[39m._model_mapping)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/unified_vision_processor/lib/python3.11/site-packages/transformers/modeling_utils.py:4014\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4004\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   4005\u001b[39m         torch.set_default_dtype(dtype_orig)\n\u001b[32m   4007\u001b[39m     (\n\u001b[32m   4008\u001b[39m         model,\n\u001b[32m   4009\u001b[39m         missing_keys,\n\u001b[32m   4010\u001b[39m         unexpected_keys,\n\u001b[32m   4011\u001b[39m         mismatched_keys,\n\u001b[32m   4012\u001b[39m         offload_index,\n\u001b[32m   4013\u001b[39m         error_msgs,\n\u001b[32m-> \u001b[39m\u001b[32m4014\u001b[39m     ) = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4015\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4016\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4017\u001b[39m \u001b[43m        \u001b[49m\u001b[43mloaded_state_dict_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# XXX: rename?\u001b[39;49;00m\n\u001b[32m   4018\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4019\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4020\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4021\u001b[39m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4022\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_fast_init\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_fast_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4023\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4024\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4025\u001b[39m \u001b[43m        \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4026\u001b[39m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4027\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4028\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4029\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4030\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgguf_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgguf_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4031\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4033\u001b[39m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[32m   4034\u001b[39m model.tie_weights()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/unified_vision_processor/lib/python3.11/site-packages/transformers/modeling_utils.py:4502\u001b[39m, in \u001b[36mPreTrainedModel._load_pretrained_model\u001b[39m\u001b[34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules, gguf_path)\u001b[39m\n\u001b[32m   4498\u001b[39m                 set_module_tensor_to_device(\n\u001b[32m   4499\u001b[39m                     model_to_load, key, \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m, torch.empty(*param.size(), dtype=dtype)\n\u001b[32m   4500\u001b[39m                 )\n\u001b[32m   4501\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4502\u001b[39m         new_error_msgs, offload_index, state_dict_index = \u001b[43m_load_state_dict_into_meta_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4503\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4504\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4505\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstart_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4506\u001b[39m \u001b[43m            \u001b[49m\u001b[43mexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4507\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4508\u001b[39m \u001b[43m            \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4509\u001b[39m \u001b[43m            \u001b[49m\u001b[43moffload_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4510\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstate_dict_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstate_dict_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4511\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstate_dict_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstate_dict_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4512\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4513\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4514\u001b[39m \u001b[43m            \u001b[49m\u001b[43mis_safetensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4515\u001b[39m \u001b[43m            \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4516\u001b[39m \u001b[43m            \u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4517\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4518\u001b[39m         error_msgs += new_error_msgs\n\u001b[32m   4519\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   4520\u001b[39m     \u001b[38;5;66;03m# Sharded checkpoint or whole but low_cpu_mem_usage==True\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/unified_vision_processor/lib/python3.11/site-packages/transformers/modeling_utils.py:973\u001b[39m, in \u001b[36m_load_state_dict_into_meta_model\u001b[39m\u001b[34m(model, state_dict, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, hf_quantizer, is_safetensors, keep_in_fp32_modules, unexpected_keys, pretrained_model_name_or_path)\u001b[39m\n\u001b[32m    970\u001b[39m         param_device = \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_local_dist_rank_0() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mmeta\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    972\u001b[39m     \u001b[38;5;66;03m# For backward compatibility with older versions of `accelerate` and for non-quantized params\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m973\u001b[39m     \u001b[43mset_module_tensor_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mset_module_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    974\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    975\u001b[39m     hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/unified_vision_processor/lib/python3.11/site-packages/accelerate/utils/modeling.py:343\u001b[39m, in \u001b[36mset_module_tensor_to_device\u001b[39m\u001b[34m(module, tensor_name, device, value, dtype, fp16_statistics, tied_params_map, non_blocking, clear_cache)\u001b[39m\n\u001b[32m    341\u001b[39m             module._parameters[tensor_name] = param_cls(new_value, requires_grad=old_value.requires_grad)\n\u001b[32m    342\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, torch.Tensor):\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m     new_value = \u001b[43mvalue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    344\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    345\u001b[39m     new_value = torch.tensor(value, device=device)\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 260.00 MiB. GPU 0 has a total capacity of 21.95 GiB of which 1.05 GiB is free. Process 109074 has 20.89 GiB memory in use. 20.85 GiB allowed; Of the allocated memory 20.62 GiB is allocated by PyTorch, and 91.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "#Cell 6\n",
    "# Load InternVL3-8B model with FLOAT32 precision for V100 compatibility\n",
    "rprint(\"[bold green]Loading InternVL3-8B model with Float32 precision (V100 production)...[/bold green]\")\n",
    "rprint(\"[cyan]üí° Using torch.float32 to solve bfloat16‚Üífloat16 gibberish issue on V100[/cyan]\")\n",
    "\n",
    "# Load InternVL3 model using robust infrastructure with float32\n",
    "model, tokenizer = load_internvl3_model(\n",
    "    model_path=CONFIG['MODEL_PATH'],\n",
    "    use_quantization=CONFIG['USE_QUANTIZATION'],  # False - no quantization\n",
    "    device_map=CONFIG['DEVICE_MAP'],\n",
    "    max_new_tokens=CONFIG['MAX_NEW_TOKENS'],\n",
    "    torch_dtype=CONFIG['TORCH_DTYPE'],  # 'float32' - full precision\n",
    "    low_cpu_mem_usage=CONFIG['LOW_CPU_MEM_USAGE'],\n",
    "    use_flash_attn=CONFIG['USE_FLASH_ATTN'],  # V100 compatible setting\n",
    "    verbose=CONFIG['VERBOSE']\n",
    ")\n",
    "\n",
    "# Initialize the hybrid processor with loaded model components AND prompt_config\n",
    "hybrid_processor = DocumentAwareInternVL3HybridProcessor(\n",
    "    field_list=UNIVERSAL_FIELDS,\n",
    "    model_path=CONFIG['MODEL_PATH'],\n",
    "    debug=CONFIG['VERBOSE'],\n",
    "    pre_loaded_model=model,\n",
    "    pre_loaded_tokenizer=tokenizer,\n",
    "    prompt_config=PROMPT_CONFIG,  # Single source of truth for configuration!\n",
    "    max_tiles=CONFIG['MAX_TILES']  # V100 optimized tile configuration\n",
    ")\n",
    "\n",
    "# Model and processor will be used by BatchDocumentProcessor\n",
    "rprint(\"[bold green]‚úÖ InternVL3-8B model ready for V100 production inference (float32)[/bold green]\")\n",
    "rprint(f\"[cyan]üî≤ Using {CONFIG['MAX_TILES']} tiles (V100 optimized)[/cyan]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Image Discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 7\n",
    "# Discover and filter images - Handle both absolute and relative paths\n",
    "\n",
    "# Convert DATA_DIR to Path and handle absolute/relative paths\n",
    "data_dir = Path(CONFIG['DATA_DIR'])\n",
    "if not data_dir.is_absolute():\n",
    "    # If relative, make it relative to current working directory\n",
    "    data_dir = Path.cwd() / data_dir\n",
    "\n",
    "# Discover images from the resolved data directory\n",
    "all_images = discover_images(str(data_dir))\n",
    "\n",
    "# Conditionally load ground truth only when not in inference-only mode\n",
    "ground_truth = {}\n",
    "if not CONFIG['INFERENCE_ONLY'] and CONFIG['GROUND_TRUTH']:\n",
    "    # Convert GROUND_TRUTH to Path and handle absolute/relative paths\n",
    "    ground_truth_path = Path(CONFIG['GROUND_TRUTH'])\n",
    "    if not ground_truth_path.is_absolute():\n",
    "        # If relative, make it relative to current working directory\n",
    "        ground_truth_path = Path.cwd() / ground_truth_path\n",
    "    \n",
    "    # Load ground truth from the resolved path\n",
    "    ground_truth = load_ground_truth(str(ground_truth_path), verbose=CONFIG['VERBOSE'])\n",
    "    \n",
    "    rprint(f\"[green]‚úÖ Ground truth loaded for {len(ground_truth)} images[/green]\")\n",
    "else:\n",
    "    rprint(\"[cyan]üìã Running in inference-only mode (no ground truth required)[/cyan]\")\n",
    "\n",
    "# Apply filters (only if ground truth is available)\n",
    "if CONFIG['DOCUMENT_TYPES'] and ground_truth:\n",
    "    filtered = []\n",
    "    for img in all_images:\n",
    "        img_name = Path(img).name\n",
    "        if img_name in ground_truth:\n",
    "            doc_type = ground_truth[img_name].get('DOCUMENT_TYPE', '').lower()\n",
    "            if any(dt.lower() in doc_type for dt in CONFIG['DOCUMENT_TYPES']):\n",
    "                filtered.append(img)\n",
    "    all_images = filtered\n",
    "\n",
    "if CONFIG['MAX_IMAGES']:\n",
    "    all_images = all_images[:CONFIG['MAX_IMAGES']]\n",
    "\n",
    "rprint(f\"[bold green]Ready to process {len(all_images)} images[/bold green]\")\n",
    "rprint(f\"[cyan]Data directory: {data_dir}[/cyan]\")\n",
    "if not CONFIG['INFERENCE_ONLY'] and CONFIG['GROUND_TRUTH']:\n",
    "    rprint(f\"[cyan]Ground truth: {ground_truth_path}[/cyan]\")\n",
    "rprint(f\"[cyan]Mode: {'Inference-only' if CONFIG['INFERENCE_ONLY'] else 'Evaluation mode'}[/cyan]\")\n",
    "for i, img in enumerate(all_images[:5], 1):\n",
    "    print(f\"  {i}. {Path(img).name}\")\n",
    "if len(all_images) > 5:\n",
    "    print(f\"  ... and {len(all_images) - 5} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Batch Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 8\n",
    "# ============================================================================\n",
    "# V2: BATCH PROCESSING WITH SOPHISTICATED BANK STATEMENT EXTRACTION\n",
    "# ============================================================================\n",
    "# This cell initializes the batch processor and optionally sets up the\n",
    "# sophisticated multi-turn bank statement extraction using UnifiedBankExtractor.\n",
    "#\n",
    "# When USE_SOPHISTICATED_BANK_EXTRACTION is True:\n",
    "# - Bank statements use Turn 0 (header detection) + Turn 1 (adaptive extraction)\n",
    "# - Automatic strategy selection: BALANCE_DESCRIPTION, AMOUNT_DESCRIPTION, etc.\n",
    "# - Expected accuracy improvement for bank statements\n",
    "#\n",
    "# When USE_SOPHISTICATED_BANK_EXTRACTION is False:\n",
    "# - Uses original single-turn extraction (backward compatible)\n",
    "# ============================================================================\n",
    "\n",
    "# Initialize batch processor with proven infrastructure (same pattern as llama_batch.ipynb)\n",
    "batch_processor = BatchDocumentProcessor(\n",
    "    model=hybrid_processor,  # InternVL3 hybrid processor (handler)\n",
    "    processor=None,          # Not needed for InternVL3\n",
    "    prompt_config=PROMPT_CONFIG,\n",
    "    ground_truth_csv=CONFIG['GROUND_TRUTH'],  # None in inference-only mode\n",
    "    console=console,\n",
    "    enable_math_enhancement=CONFIG['ENABLE_MATH_ENHANCEMENT']\n",
    ")\n",
    "\n",
    "# V2: Set up sophisticated bank statement extraction if enabled\n",
    "if CONFIG.get('USE_SOPHISTICATED_BANK_EXTRACTION', False):\n",
    "    import torch\n",
    "    rprint(\"[bold cyan]üè¶ V2: Setting up sophisticated bank statement extraction...[/bold cyan]\")\n",
    "    \n",
    "    # Detect model dtype from the loaded model\n",
    "    try:\n",
    "        model_dtype = next(model.parameters()).dtype\n",
    "    except (StopIteration, AttributeError):\n",
    "        model_dtype = torch.float32  # Default for V100\n",
    "    \n",
    "    # Create bank adapter for multi-turn extraction (InternVL3 mode)\n",
    "    bank_adapter = BankStatementAdapter(\n",
    "        model=hybrid_processor,  # Pass hybrid processor - adapter extracts model/tokenizer\n",
    "        processor=None,\n",
    "        verbose=CONFIG['VERBOSE'],\n",
    "        use_balance_correction=CONFIG.get('ENABLE_BALANCE_CORRECTION', False),\n",
    "        model_type=\"internvl3\",\n",
    "        model_dtype=model_dtype,\n",
    "    )\n",
    "    \n",
    "    # Store original processing method\n",
    "    original_process = batch_processor._process_internvl3_image\n",
    "    \n",
    "    def enhanced_process_internvl3(image_path, verbose):\n",
    "        \"\"\"Enhanced processing that routes bank statements through sophisticated adapter.\"\"\"\n",
    "        from pathlib import Path\n",
    "        import sys\n",
    "        \n",
    "        def _safe_print(msg: str) -> None:\n",
    "            \"\"\"Print without triggering Rich console recursion in Jupyter.\"\"\"\n",
    "            try:\n",
    "                sys.__stdout__.write(msg + \"\\n\")\n",
    "                sys.__stdout__.flush()\n",
    "            except Exception:\n",
    "                pass\n",
    "        \n",
    "        # First, do a quick document type detection using original method\n",
    "        try:\n",
    "            doc_type, original_result, original_prompt = original_process(image_path, verbose=False)\n",
    "        except Exception as e:\n",
    "            rprint(f\"[red]Error in document type detection: {e}[/red]\")\n",
    "            raise\n",
    "        \n",
    "        # Check if this is a bank statement\n",
    "        if doc_type.upper() == \"BANK_STATEMENT\":\n",
    "            if verbose:\n",
    "                _safe_print(f\"üè¶ Routing {Path(image_path).name} through sophisticated multi-turn extraction\")\n",
    "            \n",
    "            try:\n",
    "                # Use sophisticated multi-turn extraction\n",
    "                schema_fields, metadata = bank_adapter.extract_bank_statement(image_path)\n",
    "                \n",
    "                # Build result structure compatible with BatchDocumentProcessor\n",
    "                extraction_result = {\n",
    "                    \"extracted_data\": schema_fields,\n",
    "                    \"raw_response\": metadata.get(\"raw_responses\", {}).get(\"turn1\", \"\"),\n",
    "                    \"field_list\": list(schema_fields.keys()),\n",
    "                    \"metadata\": metadata,  # Include full metadata for debugging\n",
    "                }\n",
    "                \n",
    "                # Create prompt name that indicates which strategy was used\n",
    "                strategy = metadata.get(\"strategy_used\", \"unknown\")\n",
    "                prompt_name = f\"unified_bank_{strategy}\"\n",
    "                \n",
    "                if verbose:\n",
    "                    _safe_print(f\"  ‚úÖ Strategy: {strategy}\")\n",
    "                    tx_count = len(schema_fields.get('TRANSACTION_DATES', '').split('|')) if schema_fields.get('TRANSACTION_DATES') != 'NOT_FOUND' else 0\n",
    "                    _safe_print(f\"  ‚úÖ Transactions extracted: {tx_count}\")\n",
    "                \n",
    "                return doc_type, extraction_result, prompt_name\n",
    "                \n",
    "            except Exception as e:\n",
    "                rprint(f\"[yellow]‚ö†Ô∏è  Sophisticated extraction failed for {Path(image_path).name}: {e}[/yellow]\")\n",
    "                rprint(f\"[yellow]   Falling back to original extraction...[/yellow]\")\n",
    "                # Fall back to original extraction result\n",
    "                return doc_type, original_result, original_prompt\n",
    "        \n",
    "        else:\n",
    "            # Non-bank documents: use original processing with verbose output\n",
    "            if verbose:\n",
    "                rprint(f\"[dim]üìÑ Processing {Path(image_path).name} ({doc_type}) with standard extraction[/dim]\")\n",
    "            return original_process(image_path, verbose)\n",
    "    \n",
    "    # Replace the processing method\n",
    "    batch_processor._process_internvl3_image = enhanced_process_internvl3\n",
    "    \n",
    "    rprint(\"[green]‚úÖ V2: Sophisticated bank statement extraction enabled[/green]\")\n",
    "    rprint(\"[cyan]   Turn 0: Header detection ‚Üí Turn 1: Adaptive extraction[/cyan]\")\n",
    "    rprint(f\"[cyan]   Balance correction: {'Enabled' if CONFIG.get('ENABLE_BALANCE_CORRECTION', False) else 'Disabled'}[/cyan]\")\n",
    "    rprint(f\"[cyan]   Model dtype: {model_dtype}[/cyan]\")\n",
    "else:\n",
    "    rprint(\"[dim]‚è≠Ô∏è  V2: Sophisticated bank extraction disabled - using original single-turn[/dim]\")\n",
    "\n",
    "# Process batch using proven evaluation infrastructure\n",
    "batch_results, processing_times, document_types_found = batch_processor.process_batch(\n",
    "    all_images, verbose=CONFIG['VERBOSE']\n",
    ")\n",
    "\n",
    "# Brief summary\n",
    "rprint(f\"[bold green]‚úÖ Processed {len(batch_results)} images[/bold green]\")\n",
    "rprint(f\"[cyan]Average time: {np.mean(processing_times):.2f}s[/cyan]\")\n",
    "if CONFIG['INFERENCE_ONLY']:\n",
    "    rprint(\"[cyan]üìã Inference-only mode: No accuracy evaluation performed[/cyan]\")\n",
    "else:\n",
    "    avg_accuracy = np.mean([r.get('evaluation', {}).get('overall_accuracy', 0) * 100 for r in batch_results if 'evaluation' in r])\n",
    "    rprint(f\"[cyan]Average accuracy: {avg_accuracy:.1f}%[/cyan]\")\n",
    "    \n",
    "    # V2: Show bank statement specific results if sophisticated extraction was used\n",
    "    if CONFIG.get('USE_SOPHISTICATED_BANK_EXTRACTION', False):\n",
    "        bank_results = [r for r in batch_results if r.get('document_type', '').upper() == 'BANK_STATEMENT']\n",
    "        if bank_results:\n",
    "            bank_accuracy = np.mean([r.get('evaluation', {}).get('overall_accuracy', 0) * 100 for r in bank_results if 'evaluation' in r])\n",
    "            rprint(f\"[cyan]üè¶ Bank statement accuracy (V2): {bank_accuracy:.1f}%[/cyan]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Generate Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 9\n",
    "# Create model-specific CSV file to match Llama structure\n",
    "# Use UNIVERSAL_FIELDS (already filtered to exclude validation-only fields)\n",
    "# CRITICAL: TRANSACTION_AMOUNTS_RECEIVED and ACCOUNT_BALANCE are excluded\n",
    "FIELD_COLUMNS = UNIVERSAL_FIELDS\n",
    "\n",
    "# Create comprehensive results data matching Llama structure\n",
    "internvl3_csv_data = []\n",
    "\n",
    "for i, result in enumerate(batch_results):\n",
    "    # Basic metadata\n",
    "    image_name = Path(result['image_path']).name\n",
    "    doc_type = result.get('document_type', '').lower()\n",
    "    processing_time = processing_times[i] if i < len(processing_times) else 0\n",
    "    \n",
    "    # Extract fields from result\n",
    "    extraction_result = result.get('extraction_result', {})\n",
    "    extracted_fields = extraction_result.get('extracted_data', {})\n",
    "    accuracy_data = result.get('evaluation', {})\n",
    "    \n",
    "    # Count fields (using filtered field list)\n",
    "    total_fields = len(FIELD_COLUMNS)\n",
    "    found_fields = sum(1 for field in FIELD_COLUMNS if extracted_fields.get(field, 'NOT_FOUND') != 'NOT_FOUND')\n",
    "    field_coverage = (found_fields / total_fields * 100) if total_fields > 0 else 0\n",
    "    \n",
    "    # Handle both inference-only and evaluation modes\n",
    "    if CONFIG['INFERENCE_ONLY'] or accuracy_data.get('inference_only', False):\n",
    "        # Inference-only mode\n",
    "        overall_accuracy = None\n",
    "        fields_extracted = found_fields\n",
    "        fields_matched = 0  # No matching in inference mode\n",
    "        eval_total_fields = total_fields\n",
    "    else:\n",
    "        # Evaluation mode\n",
    "        overall_accuracy = accuracy_data.get('overall_accuracy', 0) * 100 if accuracy_data else 0\n",
    "        fields_extracted = accuracy_data.get('fields_extracted', 0) if accuracy_data else 0\n",
    "        fields_matched = accuracy_data.get('fields_matched', 0) if accuracy_data else 0\n",
    "        eval_total_fields = accuracy_data.get('total_fields', total_fields) if accuracy_data else total_fields\n",
    "    \n",
    "    # Create prompt identifier\n",
    "    prompt_used = f\"internvl3_{doc_type}\" if doc_type else \"internvl3_unknown\"\n",
    "    \n",
    "    # Create row data\n",
    "    row_data = {\n",
    "        'image_file': image_name,\n",
    "        'image_name': image_name,\n",
    "        'document_type': doc_type,\n",
    "        'processing_time': processing_time,\n",
    "        'field_count': eval_total_fields,\n",
    "        'found_fields': fields_extracted,\n",
    "        'field_coverage': field_coverage,\n",
    "        'prompt_used': prompt_used,\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'overall_accuracy': overall_accuracy,\n",
    "        'fields_extracted': fields_extracted,\n",
    "        'fields_matched': fields_matched,\n",
    "        'total_fields': eval_total_fields,\n",
    "        'inference_only': CONFIG['INFERENCE_ONLY']\n",
    "    }\n",
    "    \n",
    "    # Add all field values (only for fields in filtered list)\n",
    "    for field in FIELD_COLUMNS:\n",
    "        row_data[field] = extracted_fields.get(field, 'NOT_FOUND')\n",
    "    \n",
    "    internvl3_csv_data.append(row_data)\n",
    "\n",
    "# Create DataFrame and save\n",
    "internvl3_df = pd.DataFrame(internvl3_csv_data)\n",
    "internvl3_csv_path = OUTPUT_DIRS['csv'] / f\"internvl3_batch_results_{BATCH_TIMESTAMP}.csv\"\n",
    "internvl3_df.to_csv(internvl3_csv_path, index=False)\n",
    "\n",
    "rprint(\"[bold green]‚úÖ InternVL3 model-specific CSV exported:[/bold green]\")\n",
    "rprint(f\"[cyan]üìÑ File: {internvl3_csv_path}[/cyan]\")\n",
    "rprint(f\"[cyan]üìä Structure: {len(internvl3_df)} rows √ó {len(internvl3_df.columns)} columns[/cyan]\")\n",
    "rprint(f\"[cyan]üìã Fields: {len(FIELD_COLUMNS)} (validation-only fields excluded)[/cyan]\")\n",
    "rprint(\"[cyan]üîó Compatible with model_comparison.ipynb pattern: *internvl3*batch*results*.csv[/cyan]\")\n",
    "\n",
    "# Display sample of the exported data (conditional based on mode)\n",
    "if CONFIG['INFERENCE_ONLY']:\n",
    "    rprint(\"\\n[bold blue]üìã Sample exported data (inference-only mode):[/bold blue]\")\n",
    "    sample_cols = ['image_file', 'document_type', 'processing_time', 'found_fields', 'field_coverage', 'inference_only']\n",
    "    if len(internvl3_df) > 0:\n",
    "        display(internvl3_df[sample_cols].head(3))\n",
    "    else:\n",
    "        rprint(\"[yellow]‚ö†Ô∏è No data to display[/yellow]\")\n",
    "else:\n",
    "    rprint(\"\\n[bold blue]üìã Sample exported data (first 3 rows, key columns):[/bold blue]\")\n",
    "    sample_cols = ['image_file', 'document_type', 'overall_accuracy', 'processing_time', 'found_fields', 'field_coverage']\n",
    "    if len(internvl3_df) > 0:\n",
    "        display(internvl3_df[sample_cols].head(3))\n",
    "    else:\n",
    "        rprint(\"[yellow]‚ö†Ô∏è No data to display[/yellow]\")\n",
    "\n",
    "    # Verification: Show accuracy values to confirm they're correct (evaluation mode only)\n",
    "    rprint(\"\\n[bold blue]üîç Accuracy verification:[/bold blue]\")\n",
    "    for i, result in enumerate(batch_results[:3]):  # Show first 3\n",
    "        evaluation = result.get('evaluation', {})\n",
    "        original_accuracy = evaluation.get('overall_accuracy', 0)\n",
    "        percentage_accuracy = original_accuracy * 100\n",
    "        rprint(f\"  {result['image_name']}: {original_accuracy:.4f} ‚Üí {percentage_accuracy:.2f}%\")\n",
    "\n",
    "# Debug: Show extraction structure to verify extraction works\n",
    "rprint(\"\\n[bold blue]üîç Data structure verification:[/bold blue]\")\n",
    "for i, result in enumerate(batch_results[:2]):  # Show first 2\n",
    "    image_name = result['image_name']\n",
    "    extraction_result = result.get('extraction_result', {})\n",
    "    extracted_data = extraction_result.get('extracted_data', {})\n",
    "    found_count = sum(1 for v in extracted_data.values() if v != 'NOT_FOUND')\n",
    "    rprint(f\"  {image_name}: Found {found_count} fields in extracted_data\")\n",
    "    if found_count > 0:\n",
    "        # Show first few found fields\n",
    "        found_fields = [(k, v) for k, v in extracted_data.items() if v != 'NOT_FOUND'][:3]\n",
    "        for field, value in found_fields:\n",
    "            rprint(f\"    {field}: {value}\")\n",
    "\n",
    "# Create analytics using proven infrastructure (same pattern as llama_batch.ipynb)\n",
    "analytics = BatchAnalytics(batch_results, processing_times)\n",
    "\n",
    "# Generate and save DataFrames using established patterns\n",
    "saved_files, df_results, df_summary, df_doctype_stats, df_field_stats = analytics.save_all_dataframes(\n",
    "    OUTPUT_DIRS['csv'], BATCH_TIMESTAMP, verbose=CONFIG['VERBOSE']\n",
    ")\n",
    "\n",
    "# Display key results based on mode\n",
    "rprint(\"\\n[bold blue]üìä InternVL3 Results Summary[/bold blue]\")\n",
    "if CONFIG['INFERENCE_ONLY']:\n",
    "    rprint(\"[cyan]üìã Running in inference-only mode - no accuracy metrics available[/cyan]\")\n",
    "    # Show extraction statistics instead\n",
    "    rprint(f\"[cyan]‚úÖ Total images processed: {len(batch_results)}[/cyan]\")\n",
    "    rprint(f\"[cyan]‚úÖ Average fields found: {internvl3_df['found_fields'].mean():.1f}[/cyan]\")\n",
    "    rprint(f\"[cyan]‚úÖ Average field coverage: {internvl3_df['field_coverage'].mean():.1f}%[/cyan]\")\n",
    "else:\n",
    "    display(df_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Export Model-Specific CSV for Comparison\n",
    "\n",
    "Create InternVL3-specific CSV file that matches Llama structure for model comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 10\n",
    "# Create visualizations using proven infrastructure (same pattern as llama_batch.ipynb)\n",
    "visualizer = BatchVisualizer()\n",
    "\n",
    "viz_files = visualizer.create_all_visualizations(\n",
    "    df_results, \n",
    "    df_doctype_stats,\n",
    "    OUTPUT_DIRS['visualizations'],\n",
    "    BATCH_TIMESTAMP,\n",
    "    show=False  # Disable display to reduce output\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Generate Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 11\n",
    "# Generate reports using proven infrastructure (same pattern as llama_batch.ipynb)\n",
    "reporter = BatchReporter(\n",
    "    batch_results, \n",
    "    processing_times,\n",
    "    document_types_found,\n",
    "    BATCH_TIMESTAMP\n",
    ")\n",
    "\n",
    "# Save all reports using CONFIG verbose setting\n",
    "report_files = reporter.save_all_reports(\n",
    "    OUTPUT_DIRS,\n",
    "    df_results,\n",
    "    df_summary,\n",
    "    df_doctype_stats,\n",
    "    CONFIG['MODEL_PATH'],\n",
    "    {\n",
    "        'data_dir': CONFIG['DATA_DIR'],\n",
    "        'ground_truth': CONFIG['GROUND_TRUTH'],\n",
    "        'max_images': CONFIG['MAX_IMAGES'],\n",
    "        'document_types': CONFIG['DOCUMENT_TYPES']\n",
    "    },\n",
    "    {\n",
    "        'use_quantization': CONFIG['USE_QUANTIZATION'],\n",
    "        'device_map': CONFIG['DEVICE_MAP'],\n",
    "        'max_new_tokens': CONFIG['MAX_NEW_TOKENS'],\n",
    "        'torch_dtype': CONFIG['TORCH_DTYPE'],\n",
    "        'low_cpu_mem_usage': CONFIG['LOW_CPU_MEM_USAGE']\n",
    "    },\n",
    "    verbose=CONFIG['VERBOSE']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Display Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 12\n",
    "# Display final summary\n",
    "console.rule(\"[bold green]InternVL3 Batch Processing Complete[/bold green]\")\n",
    "\n",
    "total_images = len(batch_results)\n",
    "successful = len([r for r in batch_results if 'error' not in r])\n",
    "avg_accuracy = df_results['overall_accuracy'].mean() if len(df_results) > 0 else 0\n",
    "\n",
    "rprint(f\"[bold green]‚úÖ Processed: {total_images} images[/bold green]\")\n",
    "rprint(f\"[cyan]Success Rate: {(successful/total_images*100):.1f}%[/cyan]\")\n",
    "rprint(f\"[cyan]Average Accuracy: {avg_accuracy:.2f}%[/cyan]\")\n",
    "rprint(f\"[cyan]Output: {OUTPUT_BASE}[/cyan]\")\n",
    "\n",
    "# Document type distribution\n",
    "if document_types_found:\n",
    "    rprint(\"\\n[bold blue]üìã Document Type Distribution:[/bold blue]\")\n",
    "    for doc_type, count in document_types_found.items():\n",
    "        percentage = (count / total_images * 100) if total_images > 0 else 0\n",
    "        rprint(f\"[cyan]  {doc_type}: {count} documents ({percentage:.1f}%)[/cyan]\")\n",
    "\n",
    "# Display dashboard if available\n",
    "dashboard_files = list(OUTPUT_DIRS['visualizations'].glob(f\"dashboard_{BATCH_TIMESTAMP}.png\"))\n",
    "if dashboard_files:\n",
    "    from IPython.display import Image, display\n",
    "    dashboard_path = dashboard_files[0]\n",
    "    rprint(\"\\n[bold blue]üìä Visual Dashboard:[/bold blue]\")\n",
    "    display(Image(str(dashboard_path)))\n",
    "else:\n",
    "    rprint(f\"\\n[yellow]‚ö†Ô∏è Dashboard not found in {OUTPUT_DIRS['visualizations']}[/yellow]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Failed Extractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 13\n",
    "# Calculate zero accuracy extractions\n",
    "zero_accuracy_count = 0\n",
    "zero_accuracy_images = []\n",
    "total_evaluated = 0\n",
    "\n",
    "for result in batch_results:\n",
    "    # Check if evaluation data exists (not inference-only mode)\n",
    "    evaluation = result.get(\"evaluation\", {})\n",
    "\n",
    "    if evaluation and not evaluation.get(\"inference_only\", False):\n",
    "        total_evaluated += 1\n",
    "        accuracy = evaluation.get(\"overall_accuracy\", 0)\n",
    "\n",
    "        if accuracy == 0.0:\n",
    "            zero_accuracy_count += 1\n",
    "            zero_accuracy_images.append(\n",
    "                {\n",
    "                    \"image_name\": result.get(\"image_name\", \"unknown\"),\n",
    "                    \"document_type\": result.get(\"document_type\", \"unknown\"),\n",
    "                    \"fields_extracted\": evaluation.get(\"fields_extracted\", 0),\n",
    "                    \"total_fields\": evaluation.get(\"total_fields\", 0),\n",
    "                }\n",
    "            )\n",
    "\n",
    "# Display results\n",
    "if total_evaluated > 0:\n",
    "    console.rule(\"[bold red]Zero Accuracy Analysis[/bold red]\")\n",
    "\n",
    "    rprint(f\"[cyan]Total documents evaluated: {total_evaluated}[/cyan]\")\n",
    "    rprint(f\"[red]Documents with 0% accuracy: {zero_accuracy_count}[/red]\")\n",
    "\n",
    "    if zero_accuracy_count > 0:\n",
    "        percentage = (zero_accuracy_count / total_evaluated) * 100\n",
    "        rprint(f\"[red]Zero accuracy rate: {percentage:.1f}%[/red]\")\n",
    "\n",
    "        rprint(\"\\n[bold red]Documents with 0% Accuracy:[/bold red]\")\n",
    "        for i, img_info in enumerate(zero_accuracy_images, 1):\n",
    "            rprint(f\"  {i}. {img_info['image_name']} ({img_info['document_type']})\")\n",
    "            rprint(\n",
    "                f\"     Fields extracted: {img_info['fields_extracted']}/{img_info['total_fields']}\"\n",
    "            )\n",
    "    else:\n",
    "        rprint(\n",
    "            \"[green]‚úÖ No documents with 0% accuracy - all extractions had some success![/green]\"\n",
    "        )\n",
    "else:\n",
    "    rprint(\n",
    "        \"[yellow]‚ö†Ô∏è Running in inference-only mode - no accuracy metrics available[/yellow]\"\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (LMM_POC)",
   "language": "python",
   "name": "unified_vision_processor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
