{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Llama Two Independent Prompts Test\n",
    "\n",
    "This tests what llama_batch.ipynb does: running TWO SEPARATE prompts on the SAME image.\n",
    "This is NOT multi-turn - each prompt is independent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Random seed set\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, MllamaForConditionalGeneration\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "print(\"‚úÖ Random seed set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Loading model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5686847a0aa47759c835e33ff5ed237",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "model_id = \"/home/jovyan/nfs_share/models/Llama-3.2-11B-Vision-Instruct\"\n",
    "print(\"üîß Loading model...\")\n",
    "model = MllamaForConditionalGeneration.from_pretrained(\n",
    "    model_id, torch_dtype=torch.bfloat16, device_map=\"auto\"\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "print(\"‚úÖ Model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Image loaded: (900, 1320)\n"
     ]
    }
   ],
   "source": [
    "# Load image\n",
    "imageName = \"/home/jovyan/nfs_share/tod/LMM_POC/evaluation_data/image_008.png\"\n",
    "image = Image.open(imageName)\n",
    "print(f\"‚úÖ Image loaded: {image.size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_llama_response(response: str) -> str:\n",
    "    start_marker = \"<|start_header_id|>assistant<|end_header_id|>\"\n",
    "    end_marker = \"<|eot_id|>\"\n",
    "    start_idx = response.find(start_marker)\n",
    "    if start_idx != -1:\n",
    "        start_idx += len(start_marker)\n",
    "        end_idx = response.find(end_marker, start_idx)\n",
    "        if end_idx != -1:\n",
    "            return response[start_idx:end_idx].strip()\n",
    "    return response.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Prompt 1: Document Type (Independent Query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí¨ Prompt 1: What type of document is this?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.conda/envs/unified_vision_processor/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/jovyan/.conda/envs/unified_vision_processor/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "RESPONSE 1:\n",
      "============================================================\n",
      "This document is a bank statement, specifically a Commonwealth Bank statement, detailing the account holder's transactions from August 8, 2025, to September 7, 2025.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# First independent prompt\n",
    "prompt1 = \"What type of document is this?\"\n",
    "\n",
    "messages1 = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [\n",
    "        {\"type\": \"image\"},\n",
    "        {\"type\": \"text\", \"text\": prompt1}\n",
    "    ]\n",
    "}]\n",
    "\n",
    "print(f\"üí¨ Prompt 1: {prompt1}\")\n",
    "textInput1 = processor.apply_chat_template(messages1, add_generation_prompt=True)\n",
    "inputs1 = processor(image, textInput1, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "output1 = model.generate(**inputs1, max_new_tokens=500, do_sample=False)\n",
    "response1 = clean_llama_response(processor.decode(output1[0]))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RESPONSE 1:\")\n",
    "print(\"=\"*60)\n",
    "print(response1)\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Prompt 2: Transaction Count (Independent Query - Same Image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí¨ Prompt 2: How many transaction entries are visible on this bank statement?\n",
      "\n",
      "============================================================\n",
      "RESPONSE 2:\n",
      "============================================================\n",
      "There are 40 transaction entries visible on this bank statement.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Second independent prompt (NOT continuing conversation from prompt 1)\n",
    "prompt2 = \"How many transaction entries are visible on this bank statement?\"\n",
    "\n",
    "messages2 = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [\n",
    "        {\"type\": \"image\"},\n",
    "        {\"type\": \"text\", \"text\": prompt2}\n",
    "    ]\n",
    "}]\n",
    "\n",
    "print(f\"üí¨ Prompt 2: {prompt2}\")\n",
    "textInput2 = processor.apply_chat_template(messages2, add_generation_prompt=True)\n",
    "inputs2 = processor(image, textInput2, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "output2 = model.generate(**inputs2, max_new_tokens=500, do_sample=False)\n",
    "response2 = clean_llama_response(processor.decode(output2[0]))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RESPONSE 2:\")\n",
    "print(\"=\"*60)\n",
    "print(response2)\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "If BOTH prompts get different, relevant responses:\n",
    "- ‚úÖ The model CAN process the same image multiple times\n",
    "- ‚úÖ Different prompts produce different outputs\n",
    "- ‚ùå But multi-turn conversation (with history) doesn't work\n",
    "\n",
    "This would confirm multi-turn is broken, not the basic vision capability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (unified_vision_processor)",
   "language": "python",
   "name": "unified_vision_processor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
