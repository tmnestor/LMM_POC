print("ðŸ”§ Loading InternVL3-8B in pure float16...")
model = AutoModel.from_pretrained(
    model_path,
    torch_dtype=torch.float16,           # Target dtype
    device_map="auto",                    # Auto GPU distribution
    low_cpu_mem_usage=True,               # Optional: reduces CPU RAM during loading
    trust_remote_code=True                # Required for InternVL3
).eval()

tokenizer = AutoTokenizer.from_pretrained(
    model_path,
    trust_remote_code=True,
    use_fast=False
)

print("âœ… Model loaded in float16 (no quantization)")