{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 0\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "print(\"✅ Imports loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 1\n",
    "# Configuration\n",
    "GROUND_TRUTH_PATH = 'evaluation_data/ground_truth.csv'\n",
    "\n",
    "# Find latest Llama results\n",
    "llama_files = sorted(glob.glob('output/csv/llama_batch_results_*.csv'))\n",
    "LLAMA_RESULTS_PATH = llama_files[-1] if llama_files else None\n",
    "\n",
    "print(f\"Ground truth: {GROUND_TRUTH_PATH}\")\n",
    "print(f\"Llama results: {LLAMA_RESULTS_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 2\n",
    "# Load data\n",
    "gt = pd.read_csv(GROUND_TRUTH_PATH)\n",
    "llama = pd.read_csv(LLAMA_RESULTS_PATH)\n",
    "\n",
    "print(f\"Ground truth: {len(gt)} images\")\n",
    "print(f\"Llama results: {len(llama)} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 3\n",
    "# Merge and compare\n",
    "merged = gt.merge(\n",
    "    llama[['image_file', 'document_type']],\n",
    "    on='image_file',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "merged['match'] = (\n",
    "    merged['DOCUMENT_TYPE'].str.upper() == \n",
    "    merged['document_type'].str.upper()\n",
    ")\n",
    "\n",
    "merged['ground_truth'] = merged['DOCUMENT_TYPE']\n",
    "merged['detected'] = merged['document_type']\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DOCUMENT TYPE CLASSIFICATION COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(merged[['image_file', 'ground_truth', 'detected', 'match']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 4\n",
    "# Summary statistics\n",
    "total = len(merged)\n",
    "correct = merged['match'].sum()\n",
    "incorrect = total - correct\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total images: {total}\")\n",
    "print(f\"Correct classifications: {correct} ({correct/total*100:.1f}%)\")\n",
    "print(f\"Incorrect classifications: {incorrect} ({incorrect/total*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 5\n",
    "# Show misclassifications (if any)\n",
    "misclassified = merged[~merged['match']]\n",
    "\n",
    "if len(misclassified) > 0:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"⚠️  MISCLASSIFIED DOCUMENTS\")\n",
    "    print(\"=\"*80)\n",
    "    for _, row in misclassified.iterrows():\n",
    "        print(f\"\\n❌ {row['image_file']}\")\n",
    "        print(f\"   Ground Truth: {row['ground_truth']}\")\n",
    "        print(f\"   Detected:     {row['detected']}\")\n",
    "else:\n",
    "    print(\"\\n✅ All documents correctly classified!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 6\n",
    "# Document type distribution\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DOCUMENT TYPE DISTRIBUTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nGround Truth:\")\n",
    "print(gt['DOCUMENT_TYPE'].value_counts())\n",
    "\n",
    "print(\"\\nDetected:\")\n",
    "print(llama['document_type'].str.upper().value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 7\n",
    "# Per-type accuracy\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ACCURACY BY DOCUMENT TYPE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for doc_type in merged['ground_truth'].unique():\n",
    "    subset = merged[merged['ground_truth'] == doc_type]\n",
    "    accuracy = subset['match'].sum() / len(subset) * 100\n",
    "    print(f\"{doc_type:20s}: {subset['match'].sum()}/{len(subset)} ({accuracy:.1f}%)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
