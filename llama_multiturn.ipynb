{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, MllamaForConditionalGeneration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Random Seed for Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.reproducibility import set_seed\n",
    "set_seed(42)\n",
    "print(\"✅ Random seed set to 42 for reproducibility\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update this path to your local Llama model\n",
    "model_id = \"/home/jovyan/shared_PTM/Llama-3.2-11B-Vision-Instruct\"\n",
    "\n",
    "print(\"🔧 Loading Llama-3.2-Vision model...\")\n",
    "# model = MllamaForConditionalGeneration.from_pretrained(\n",
    "#     model_id,\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     device_map=\"auto\",\n",
    "# )\n",
    "# processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "from common.llama_model_loader_robust import load_llama_model_robust\n",
    "\n",
    "model, processor = load_llama_model_robust(\n",
    "    model_path=model_id,\n",
    "    use_quantization=False,\n",
    "    device_map='auto',\n",
    "    max_new_tokens=2000,\n",
    "    torch_dtype='bfloat16',\n",
    "    low_cpu_mem_usage=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Add tie_weights() call\n",
    "try:\n",
    "    model.tie_weights()\n",
    "    print(\"✅ Model weights tied successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ tie_weights() warning: {e}\")\n",
    "\n",
    "# processor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update this path to your test image\n",
    "# imageName = \"/home/jovyan/shared_PoC_data/evaluation_data/image_009.png\"\n",
    "imageName = \"/home/jovyan/nfs_share/tod/LMM_POC/evaluation_data/image_008.png\"\n",
    "print(\"📁 Loading image...\")\n",
    "image = Image.open(imageName)\n",
    "\n",
    "# CRITICAL: Store as list for multi-turn compatibility\n",
    "images = [image]\n",
    "\n",
    "print(f\"✅ Image loaded: {image.size}\")\n",
    "print(f\"✅ Images list created with {len(images)} image(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual Question Answering - ask a simple question about the image\n",
    "prompt = \"\"\"\n",
    "You are an expert document analyser specializing in Date Grouped Australian Bank Statement extraction.\n",
    "Date Grouped Bank Statements are date ordered, with one or more transactions for each date header.\n",
    "Every transaction for a given date heading has a description, a debit/credit amount and finally a balance amount with a ' CR' suffix.\n",
    "Extract all balance amounts along with their ' CR' suffix, the transaction dates (from the date heading) and transaction descriptions,\n",
    "maintaining the same date ordering as the image, with every transaction appearing on its own row and remembering that some date headings have more than one balance.\n",
    "\"\"\"\n",
    "\n",
    "# Create message structure for Llama\n",
    "messageDataStructure = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\"},\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": prompt,\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"💬 Prompt: {prompt}\")\n",
    "print(\"🤖 Generating response with Llama-3.2-Vision...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Response Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_llama_response(response: str) -> str:\n",
    "    \"\"\"Remove chat template artifacts and extract only the assistant's response.\n",
    "    \n",
    "    Note: This function is kept for backwards compatibility, but when using\n",
    "    the proper multi-turn pattern (trimming generate_ids), it's not needed.\n",
    "    \"\"\"\n",
    "    start_marker = \"<|start_header_id|>assistant<|end_header_id|>\"\n",
    "    end_marker = \"<|eot_id|>\"\n",
    "    \n",
    "    start_idx = response.find(start_marker)\n",
    "    if start_idx != -1:\n",
    "        start_idx += len(start_marker)\n",
    "        end_idx = response.find(end_marker, start_idx)\n",
    "        if end_idx != -1:\n",
    "            return response[start_idx:end_idx].strip()\n",
    "    \n",
    "    return response.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the input using the CORRECT multi-turn pattern\n",
    "# Based on: https://medium.com/data-science/chat-with-your-images-using-multimodal-llms-60af003e8bfa\n",
    "\n",
    "textInput = processor.apply_chat_template(\n",
    "    messageDataStructure, add_generation_prompt=True\n",
    ")\n",
    "\n",
    "# CRITICAL: Use named parameter 'images=' with list\n",
    "inputs = processor(images=images, text=textInput, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Generate response with deterministic parameters\n",
    "output = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=2000,\n",
    "    do_sample=False,\n",
    "    temperature=None,\n",
    "    top_p=None,\n",
    ")\n",
    "\n",
    "# CRITICAL: Trim input tokens from output (this is the key to clean responses!)\n",
    "generate_ids = output[:, inputs['input_ids'].shape[1]:-1]\n",
    "cleanedOutput = processor.decode(generate_ids[0], clean_up_tokenization_spaces=False)\n",
    "\n",
    "print(\"✅ Response generated successfully!\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CLEANED EXTRACTION:\")\n",
    "print(\"=\" * 60)\n",
    "print(cleanedOutput)\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Save the cleaned response to a file\n",
    "output_path = Path(\"llama_grouped_bank_statement_output.txt\")\n",
    "\n",
    "with output_path.open(\"w\", encoding=\"utf-8\") as text_file:\n",
    "    text_file.write(cleanedOutput)\n",
    "\n",
    "print(f\"✅ Response saved to: {output_path}\")\n",
    "print(f\"📊 File size: {output_path.stat().st_size} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Turn Conversation Support\n",
    "\n",
    "Llama supports multi-turn conversations by maintaining a conversation history list:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔑 Key Multi-Turn Pattern for Llama 3.2 Vision\n",
    "\n",
    "This notebook uses the **correct multi-turn conversation pattern** discovered from the Medium article:\n",
    "[Chat with Your Images Using Llama 3.2-Vision Multimodal LLMs](https://medium.com/data-science/chat-with-your-images-using-multimodal-llms-60af003e8bfa)\n",
    "\n",
    "### Critical Requirements:\n",
    "\n",
    "1. **Images as List**: `images = [image]` (not just `image`)\n",
    "2. **Named Parameter**: `processor(images=images, text=text, ...)` (not positional args)\n",
    "3. **Trim Generated Tokens**: `generate_ids[:, inputs['input_ids'].shape[1]:-1]`\n",
    "4. **Same Images Every Turn**: Pass the same `images` list for all turns\n",
    "\n",
    "### Message Structure:\n",
    "\n",
    "- **Turn 1**: `{\"role\": \"user\", \"content\": [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": \"...\"}]}`\n",
    "- **Turn 2+**: `{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"...\"}]}` (no image in content)\n",
    "- **Assistant**: `{\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": \"...\"}]}`\n",
    "\n",
    "The model attends to the image only in the first turn, but the processor needs the images list for all turns because the chat template contains the `<|image|>` token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store conversation history for multi-turn support\n",
    "# Initialize with first exchange\n",
    "conversation_history = messageDataStructure.copy()\n",
    "\n",
    "# Add assistant's response to history\n",
    "conversation_history.append({\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": [{\"type\": \"text\", \"text\": cleanedOutput}]\n",
    "})\n",
    "\n",
    "print(\"✅ Conversation history initialized\")\n",
    "print(f\"📊 Current conversation has {len(conversation_history)} messages (1 user + 1 assistant)\")\n",
    "print(f\"💡 Pattern: Using working multi-turn approach from Medium article\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug: View Conversation Context\n",
    "\n",
    "This cell helps you see what's being sent to the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Debug conversation structure\n",
    "print(\"🔍 Current conversation structure:\")\n",
    "print(\"=\" * 60)\n",
    "for i, msg in enumerate(conversation_history, 1):\n",
    "    print(f\"\\nMessage {i} ({msg['role']}):\")\n",
    "    for content in msg['content']:\n",
    "        if content['type'] == 'text':\n",
    "            preview = content['text'][:100] + \"...\" if len(content['text']) > 100 else content['text']\n",
    "            print(f\"  [text]: {preview}\")\n",
    "        else:\n",
    "            print(f\"  [{content['type']}]\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Follow-up Question (Turn 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Follow-up question (Turn 2)\n",
    "# Using the WORKING pattern from: https://medium.com/data-science/chat-with-your-images-using-multimodal-llms-60af003e8bfa\n",
    "\n",
    "follow_up_prompt = \"How many transactions are shown in this bank statement?\"\n",
    "\n",
    "# Append user's follow-up to conversation history (text only - NO image in content)\n",
    "conversation_history.append({\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [{\"type\": \"text\", \"text\": follow_up_prompt}]\n",
    "})\n",
    "\n",
    "print(f\"💬 Follow-up question: {follow_up_prompt}\")\n",
    "print(\"🤖 Generating follow-up response with Llama-3.2-Vision...\")\n",
    "\n",
    "# Process with updated conversation history\n",
    "textInput = processor.apply_chat_template(\n",
    "    conversation_history, add_generation_prompt=True\n",
    ")\n",
    "\n",
    "# CRITICAL: Use named parameter 'images=' and pass the SAME images list\n",
    "inputs = processor(images=images, text=textInput, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Generate response\n",
    "output = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=2000,\n",
    "    do_sample=False,\n",
    "    temperature=None,\n",
    "    top_p=None,\n",
    ")\n",
    "\n",
    "# CRITICAL: Trim input tokens from output\n",
    "generate_ids = output[:, inputs['input_ids'].shape[1]:-1]\n",
    "cleanedOutput2 = processor.decode(generate_ids[0], clean_up_tokenization_spaces=False)\n",
    "\n",
    "print(\"\\n✅ Follow-up response generated successfully!\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FOLLOW-UP RESPONSE:\")\n",
    "print(\"=\" * 60)\n",
    "print(cleanedOutput2)\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Update conversation history with assistant's response\n",
    "conversation_history.append({\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": [{\"type\": \"text\", \"text\": cleanedOutput2}]\n",
    "})\n",
    "\n",
    "print(f\"\\n📊 Conversation now has {len(conversation_history)} messages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Follow-up (Turn 3 - Optional)\n",
    "\n",
    "You can continue the conversation by running this cell with different questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third turn - another follow-up (uncomment to use)\n",
    "follow_up_prompt_3 = \"What is the date range covered by this bank statement?\"\n",
    "\n",
    "# Append user's follow-up to conversation history\n",
    "conversation_history.append({\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [{\"type\": \"text\", \"text\": follow_up_prompt_3}]\n",
    "})\n",
    "\n",
    "print(f\"💬 Follow-up question: {follow_up_prompt_3}\")\n",
    "print(\"🤖 Generating follow-up response with Llama-3.2-Vision...\")\n",
    "\n",
    "# Process with updated conversation history\n",
    "textInput = processor.apply_chat_template(\n",
    "    conversation_history, add_generation_prompt=True\n",
    ")\n",
    "\n",
    "# Use named parameter 'images=' and pass the SAME images list\n",
    "inputs = processor(images=images, text=textInput, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Generate response\n",
    "output = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=2000,\n",
    "    do_sample=False,\n",
    "    temperature=None,\n",
    "    top_p=None,\n",
    ")\n",
    "\n",
    "# Trim input tokens from output\n",
    "generate_ids = output[:, inputs['input_ids'].shape[1]:-1]\n",
    "cleanedOutput3 = processor.decode(generate_ids[0], clean_up_tokenization_spaces=False)\n",
    "\n",
    "print(\"\\n✅ Follow-up response generated successfully!\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FOLLOW-UP RESPONSE:\")\n",
    "print(\"=\" * 60)\n",
    "print(cleanedOutput3)\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Update conversation history with assistant's response\n",
    "conversation_history.append({\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": [{\"type\": \"text\", \"text\": cleanedOutput3}]\n",
    "})\n",
    "\n",
    "print(f\"\\n📊 Conversation now has {len(conversation_history)} messages\")\n",
    "print(\"\\n💡 To ask more questions, copy this cell and modify the 'follow_up_prompt_3' variable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Multi-Turn Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the entire conversation to a file\n",
    "output_path = Path(\"llama_multiturn_conversation.txt\")\n",
    "\n",
    "with output_path.open(\"w\", encoding=\"utf-8\") as text_file:\n",
    "    text_file.write(\"=\" * 60 + \"\\n\")\n",
    "    text_file.write(\"MULTI-TURN CONVERSATION WITH LLAMA-3.2-VISION\\n\")\n",
    "    text_file.write(\"=\" * 60 + \"\\n\\n\")\n",
    "    \n",
    "    for i, msg in enumerate(conversation_history, 1):\n",
    "        role = msg[\"role\"].upper()\n",
    "        text_file.write(f\"\\n{'-' * 60}\\n\")\n",
    "        text_file.write(f\"MESSAGE {i} - {role}\\n\")\n",
    "        text_file.write(f\"{'-' * 60}\\n\\n\")\n",
    "        \n",
    "        for content in msg[\"content\"]:\n",
    "            if content[\"type\"] == \"text\":\n",
    "                text_file.write(content[\"text\"] + \"\\n\")\n",
    "            elif content[\"type\"] == \"image\":\n",
    "                text_file.write(\"[IMAGE]\\n\")\n",
    "    \n",
    "    text_file.write(\"\\n\" + \"=\" * 60 + \"\\n\")\n",
    "    text_file.write(f\"Total messages: {len(conversation_history)}\\n\")\n",
    "    text_file.write(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "print(f\"✅ Full conversation saved to: {output_path}\")\n",
    "print(f\"📊 File size: {output_path.stat().st_size} bytes\")\n",
    "print(f\"💬 Total messages in conversation: {len(conversation_history)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (unified_vision_processor)",
   "language": "python",
   "name": "unified_vision_processor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
