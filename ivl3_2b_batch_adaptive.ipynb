{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0523269d",
   "metadata": {},
   "source": [
    "# InternVL3-2B Document-Type-Aware Adaptive Extraction\n",
    "\n",
    "**Llama-style explicit multi-stage processing** for transparency and debugging:\n",
    "\n",
    "1. **Stage 0**: Document Type Classification (INVOICE/RECEIPT/BANK_STATEMENT)\n",
    "2. **Stage 1**: Structure Classification (if BANK_STATEMENT: FLAT/GROUPED)\n",
    "3. **Stage 2**: Document-Type-Aware Extraction (using appropriate prompt)\n",
    "\n",
    "**Key Features**:\n",
    "- Saves intermediate VLM responses (`doctype_classification`, `structure_classification`, `extraction_raw`)\n",
    "- Multi-turn chat capability for conversation history\n",
    "- Llama-compatible CSV output for model comparison\n",
    "- Explicit stage-by-stage progress display\n",
    "\n",
    "**Pattern**: Follows llama_batch_adaptive.ipynb for consistency\n",
    "\n",
    "Outputs compatible with model_comparison.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffdd5bd",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed6114bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ Current directory: /home/jovyan/nfs_share/tod/LMM_POC\n",
      "âœ… Added /home/jovyan/nfs_share/tod/LMM_POC to sys.path\n",
      "âœ… Common module found at: /home/jovyan/nfs_share/tod/LMM_POC/common/__init__.py\n",
      "âœ… Path setup complete - proceed to imports\n"
     ]
    }
   ],
   "source": [
    "# Path setup for V100 systems - ensures proper module resolution\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "os.environ['EVALUATION_METHOD'] = 'order_aware_f1'  # or 'f1', 'kieval', 'order_aware_f1', 'correlation'\n",
    "\n",
    "\n",
    "# Get the notebook's directory\n",
    "notebook_path = Path().absolute()\n",
    "print(f\"ğŸ“‚ Current directory: {notebook_path}\")\n",
    "\n",
    "# Ensure the project root is in the Python path\n",
    "if str(notebook_path) not in sys.path:\n",
    "    sys.path.insert(0, str(notebook_path))\n",
    "    print(f\"âœ… Added {notebook_path} to sys.path\")\n",
    "\n",
    "# Verify common module can be found\n",
    "try:\n",
    "    import common\n",
    "    print(f\"âœ… Common module found at: {common.__file__ if hasattr(common, '__file__') else 'built-in'}\")\n",
    "except ImportError as e:\n",
    "    print(f\"âŒ Common module not found: {e}\")\n",
    "    print(\"ğŸ“‹ Current sys.path:\")\n",
    "    for p in sys.path[:5]:  # Show first 5 paths\n",
    "        print(f\"   - {p}\")\n",
    "\n",
    "print(\"âœ… Path setup complete - proceed to imports\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c483572a",
   "metadata": {},
   "source": [
    "## 1a. Path Setup (V100 Compatibility)\n",
    "\n",
    "**IMPORTANT**: If you encounter import errors on V100 systems, this cell ensures proper module resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44805fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All imports loaded successfully\n",
      "âœ… InternVL3 Hybrid Processor imported\n",
      "ğŸ“‚ Working directory: /home/jovyan/nfs_share/tod/LMM_POC\n",
      "ğŸ”¬ ADAPTIVE MODE: Explicit multi-stage processing for transparency\n"
     ]
    }
   ],
   "source": [
    "# Enable autoreload for module changes\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Standard library imports\n",
    "import gc\n",
    "import json\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Add current directory to path to ensure proper module resolution\n",
    "notebook_dir = Path.cwd()\n",
    "if str(notebook_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(notebook_dir))\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from IPython.display import display\n",
    "from rich import print as rprint\n",
    "from rich.console import Console\n",
    "from rich.progress import track\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# Project-specific imports - only what we actually use\n",
    "from models.document_aware_internvl3_processor import (\n",
    "    DocumentAwareInternVL3HybridProcessor,\n",
    ")\n",
    "from common.gpu_optimization import emergency_cleanup\n",
    "from common.extraction_parser import discover_images\n",
    "from common.evaluation_metrics import load_ground_truth\n",
    "\n",
    "print(\"âœ… All imports loaded successfully\")\n",
    "print(\"âœ… InternVL3 Hybrid Processor imported\")\n",
    "print(f\"ğŸ“‚ Working directory: {notebook_dir}\")\n",
    "print(\"ğŸ”¬ ADAPTIVE MODE: Explicit multi-stage processing for transparency\")\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dea10a3",
   "metadata": {},
   "source": [
    "## 2. Pre-emptive Memory Cleanup\n",
    "\n",
    "**CRITICAL for V100**: Run this cell first to prevent OOM errors when switching between models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02bd6274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">ğŸ§¹ PRE-EMPTIVE V100 MEMORY CLEANUP</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mğŸ§¹ PRE-EMPTIVE V100 MEMORY CLEANUP\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">Clearing any existing model caches before loading...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33mClearing any existing model caches before loading\u001b[0m\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">ğŸ’¡ This prevents OOM errors when switching between models on V100</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mğŸ’¡ This prevents OOM errors when switching between models on V100\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš¨ Running V100 emergency GPU cleanup...\n",
      "ğŸ§¹ Starting V100-optimized GPU memory cleanup...\n",
      "   ğŸ“Š Initial GPU memory: 0.00GB allocated, 0.00GB reserved\n",
      "   âœ… Final GPU memory: 0.00GB allocated, 0.00GB reserved\n",
      "   ğŸ’¾ Memory freed: 0.00GB\n",
      "âœ… V100-optimized memory cleanup complete\n",
      "âœ… V100 emergency cleanup complete\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">âœ… Memory cleanup complete - ready for model loading</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mâœ… Memory cleanup complete - ready for model loading\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">ğŸ“‹ Next: Import modules and configure settings</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mğŸ“‹ Next: Import modules and configure settings\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Pre-emptive V100 Memory Cleanup - Run FIRST to prevent OOM errors\n",
    "rprint(\"[bold red]ğŸ§¹ PRE-EMPTIVE V100 MEMORY CLEANUP[/bold red]\")\n",
    "rprint(\"[yellow]Clearing any existing model caches before loading...[/yellow]\")\n",
    "rprint(\"[cyan]ğŸ’¡ This prevents OOM errors when switching between models on V100[/cyan]\")\n",
    "\n",
    "# Emergency cleanup to ensure clean slate\n",
    "emergency_cleanup(verbose=True)\n",
    "\n",
    "rprint(\"[green]âœ… Memory cleanup complete - ready for model loading[/green]\")\n",
    "rprint(\"[dim]ğŸ“‹ Next: Import modules and configure settings[/dim]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc3bb36",
   "metadata": {},
   "source": [
    "## 3. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d3ec88",
   "metadata": {},
   "outputs": [],
   "source": "# Initialize console and environment configuration\nconsole = Console()\n\n# Environment-specific base paths\nENVIRONMENT_BASES = {\n    'sandbox': '/home/jovyan/nfs_share/tod',\n    'efs': '/efs/shared/PoC_data'\n}\nbase_data_path = ENVIRONMENT_BASES['sandbox']\n\nCONFIG = {\n    # Model settings\n    'MODEL_PATH': '/home/jovyan/nfs_share/models/InternVL3-2B', #DANGER WILL ROBINSON\n    # 'MODEL_PATH': '/home/jovyan/nfs_share/models/InternVL3-8B',\n    # 'MODEL_PATH': '/efs/shared/PTM/InternVL3-2B',\n    \n    # Batch settings\n    'DATA_DIR': f'{base_data_path}/evaluation_data',\n    'GROUND_TRUTH': f'{base_data_path}/evaluation_data/ground_truth.csv',\n    # 'OUTPUT_BASE': f'{base_data_path}/output',\n    'OUTPUT_BASE': f'{base_data_path}/LMM_POC/output',\n    'MAX_IMAGES': None,  # None for all, or set limit\n    'DOCUMENT_TYPES': None,  # None for all, or ['invoice', 'receipt']\n    'ENABLE_MATH_ENHANCEMENT': False,  # Disable mathematical correction for bank statements\n    \n    # Inference and evaluation mode\n    'INFERENCE_ONLY': False,  # Default: True (inference-only mode)\n    \n    # Verbosity control\n    'VERBOSE': True,\n    'SHOW_PROMPTS': True,\n    \n    # InternVL3 optimization settings - NON-QUANTIZED TESTING\n    # TESTING: Non-quantized performance after bug fixes (Rich recursion, prompt repetition)\n    # This follows the official InternVL3 documentation pattern exactly\n    'USE_QUANTIZATION': False,  # TESTING: Disabled to test non-quantized performance\n    'DEVICE_MAP': 'auto',\n    'MAX_NEW_TOKENS': 600,\n    'TORCH_DTYPE': 'bfloat16',\n    'LOW_CPU_MEM_USAGE': True,\n    # Flash Attention: NOT supported on V100, only enable for modern GPUs\n    'USE_FLASH_ATTN': False,  # V100 compatible default\n    \n    # InternVL3-2B TILE CONFIGURATION\n    'MAX_TILES': 18,  # InternVL3-2B config default\n}\n\n# Make GROUND_TRUTH conditional based on INFERENCE_ONLY mode\nif CONFIG['INFERENCE_ONLY']:\n    CONFIG['GROUND_TRUTH'] = None\n\n# ============================================================================\n# PROMPT CONFIGURATION - Explicit file and key mapping\n# ============================================================================\n# This configuration controls which prompt files and keys are used for each\n# document type. You can explicitly override both the file and the key.\n#\n# Structure:\n#   'extraction_files': Maps document types to YAML prompt files\n#   'extraction_keys': (Optional) Maps document types to specific keys in those files\n#\n# If 'extraction_keys' is not specified for a document type, the key will be\n# derived from the document type name (e.g., 'INVOICE' -> 'invoice')\n#\n# For bank statements, structure classification (_flat or _date_grouped) is \n# automatically appended UNLESS you provide a full key in 'extraction_keys'\n# ============================================================================\n\nPROMPT_CONFIG = {\n    # Document type detection configuration\n    'detection_file': 'prompts/document_type_detection.yaml',\n    'detection_key': 'detection',\n    \n    # Extraction prompt file mapping (REQUIRED)\n    'extraction_files': {\n        'INVOICE': 'prompts/internvl3_prompts.yaml',\n        'RECEIPT': 'prompts/internvl3_prompts.yaml', \n        'BANK_STATEMENT': 'prompts/internvl3_prompts.yaml'\n    },\n}\n\n# Import field filtering from config to exclude validation-only fields\nfrom common.config import get_v4_field_list, filter_evaluation_fields\n\n# Get universal field list and filter out validation-only fields\n# CRITICAL: TRANSACTION_AMOUNTS_RECEIVED and ACCOUNT_BALANCE are excluded\n# These fields are only for mathematical validation, not extraction/evaluation\nUNIVERSAL_FIELDS = filter_evaluation_fields(get_v4_field_list())\n\nprint(\"âœ… Configuration set up successfully\")\nprint(f\"ğŸ“‚ Evaluation data: {CONFIG['DATA_DIR']}\")\nprint(f\"ğŸ“Š Ground truth: {CONFIG['GROUND_TRUTH']}\")\nprint(f\"ğŸ¤– Model path: {CONFIG['MODEL_PATH']}\")\nprint(f\"ğŸ“ Output base: {CONFIG['OUTPUT_BASE']}\")\nprint(f\"ğŸ“‹ Universal fields: {len(UNIVERSAL_FIELDS)} (validation-only fields excluded)\")\nprint(f\"ğŸ¯ Mode: {'Inference-only' if CONFIG['INFERENCE_ONLY'] else 'Evaluation mode'}\")\nprint(f\"âš™ï¸  Quantization: {'ENABLED (8-bit)' if CONFIG['USE_QUANTIZATION'] else 'DISABLED (full precision)'}\")\nprint(f\"âš¡ Flash Attention: {'ENABLED' if CONFIG['USE_FLASH_ATTN'] else 'DISABLED (V100 compatible)'}\")\nprint(f\"ğŸ”² Max Tiles: {CONFIG['MAX_TILES']} (InternVL3-2B default)\")\nprint(\"ğŸ”¬ TESTING: Non-quantized InternVL3 performance after bug fixes\")"
  },
  {
   "cell_type": "markdown",
   "id": "c19ec25a",
   "metadata": {},
   "source": [
    "# 4. Output Directory Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9659dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup output directories - Handle both absolute and relative paths\n",
    "\n",
    "# Convert OUTPUT_BASE to Path and handle absolute/relative paths\n",
    "OUTPUT_BASE = Path(CONFIG['OUTPUT_BASE'])\n",
    "if not OUTPUT_BASE.is_absolute():\n",
    "    # If relative, make it relative to current working directory\n",
    "    OUTPUT_BASE = Path.cwd() / OUTPUT_BASE\n",
    "\n",
    "BATCH_TIMESTAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "OUTPUT_DIRS = {\n",
    "    'base': OUTPUT_BASE,\n",
    "    'batch': OUTPUT_BASE / 'batch_results',\n",
    "    'csv': OUTPUT_BASE / 'csv',\n",
    "    'visualizations': OUTPUT_BASE / 'visualizations',\n",
    "    'reports': OUTPUT_BASE / 'reports'\n",
    "}\n",
    "\n",
    "for dir_path in OUTPUT_DIRS.values():\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e75bfa",
   "metadata": {},
   "source": [
    "# 5. Model Loading (Direct Official Pattern)\n",
    "\n",
    "**NON-QUANTIZED TESTING**: Loading InternVL3 without quantization using the official documentation pattern to test whether the recent bug fixes (Rich recursion, prompt repetition) resolved the underlying issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411f53b2",
   "metadata": {},
   "outputs": [],
   "source": "# Load InternVL3 model using DIRECT official pattern (bypassing wrapper)\n# https://internvl.readthedocs.io/en/latest/internvl3.0/quick_start.html\nrprint(\"[bold green]Loading InternVL3 model with official NON-QUANTIZED pattern...[/bold green]\")\nrprint(\"[cyan]ğŸ”¬ Testing: Non-quantized performance after bug fixes[/cyan]\")\nrprint(\"[cyan]ğŸ“– Following: https://internvl.readthedocs.io/en/latest/internvl3.0/quick_start.html[/cyan]\")\n\ntry:\n    # Clear any existing CUDA cache\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        rprint(\"[blue]ğŸ§¹ CUDA cache cleared[/blue]\")\n    \n    # Load model using exact official pattern\n    rprint(\"[cyan]ğŸ“¥ Loading model with official parameters...[/cyan]\")\n    model = AutoModel.from_pretrained(\n        CONFIG['MODEL_PATH'],\n        torch_dtype=torch.bfloat16,\n        low_cpu_mem_usage=True,\n        use_flash_attn=False,  # V100 compatible\n        trust_remote_code=True,\n        device_map=\"auto\"  # Distribute across available GPUs\n    ).eval()\n    \n    # Load tokenizer\n    rprint(\"[cyan]ğŸ“¥ Loading tokenizer...[/cyan]\")\n    tokenizer = AutoTokenizer.from_pretrained(\n        CONFIG['MODEL_PATH'],\n        trust_remote_code=True,\n        use_fast=False\n    )\n    \n    # Set generation parameters\n    model.config.max_new_tokens = CONFIG['MAX_NEW_TOKENS']\n    \n    # Display model information\n    rprint(\"[green]âœ… Model and tokenizer loaded successfully![/green]\")\n    \n    # GPU memory check\n    if torch.cuda.is_available():\n        allocated = torch.cuda.memory_allocated() / 1e9\n        reserved = torch.cuda.memory_reserved() / 1e9\n        total = torch.cuda.get_device_properties(0).total_memory / 1e9\n        rprint(f\"[blue]ğŸ“Š GPU Memory: {allocated:.2f}GB allocated, {reserved:.2f}GB reserved, {total:.0f}GB total[/blue]\")\n        rprint(f\"[blue]ğŸ” Memory usage: {(allocated/total*100):.1f}%[/blue]\")\n    \n    # Model parameters\n    param_count = sum(p.numel() for p in model.parameters())\n    rprint(f\"[blue]ğŸ”¢ Model parameters: {param_count:,}[/blue]\")\n    rprint(f\"[blue]ğŸ¯ Data type: {model.dtype}[/blue]\")\n    rprint(f\"[blue]ğŸ–¥ï¸  Device: {next(model.parameters()).device}[/blue]\")\n    \n    # Add device map diagnostic\n    if hasattr(model, 'hf_device_map'):\n        from collections import Counter\n        device_distribution = Counter(model.hf_device_map.values())\n        rprint(f\"[blue]ğŸ”„ Model distribution: {dict(device_distribution)}[/blue]\")\n    else:\n        rprint(\"[blue]ğŸ“ No device map found - model placed manually[/blue]\")\n    \n    # Initialize the hybrid processor with loaded model components\n    rprint(\"[cyan]ğŸ”§ Initializing document-aware processor...[/cyan]\")\n    hybrid_processor = DocumentAwareInternVL3HybridProcessor(\n        field_list=UNIVERSAL_FIELDS,\n        model_path=CONFIG['MODEL_PATH'],\n        debug=CONFIG['VERBOSE'],\n        pre_loaded_model=model,\n        pre_loaded_tokenizer=tokenizer,\n        prompt_config=PROMPT_CONFIG,  # Single source of truth for configuration!\n        max_tiles=CONFIG['MAX_TILES']  # InternVL3-2B optimized tile configuration\n    )\n    \n    rprint(\"[bold green]âœ… InternVL3 NON-QUANTIZED model ready for document-aware processing[/bold green]\")\n    rprint(f\"[cyan]ğŸ”² Using {CONFIG['MAX_TILES']} tiles (InternVL3-2B default)[/cyan]\")\n    rprint(\"[yellow]ğŸ”¬ If you see gibberish responses, it confirms quantization is still needed for V100[/yellow]\")\n    rprint(\"[yellow]ğŸ‰ If responses are clean, it proves the bug fixes resolved the core issues![/yellow]\")\n    \nexcept Exception as e:\n    rprint(f\"[red]âŒ Error loading model: {e}[/red]\")\n    rprint(\"[yellow]ğŸ’¡ This may indicate that quantization is still required for V100 GPUs[/yellow]\")\n    raise"
  },
  {
   "cell_type": "markdown",
   "id": "b54a67da",
   "metadata": {},
   "source": [
    "## 5a. Multi-Turn Chat Function\n",
    "\n",
    "InternVL3 equivalent of Llama's `chat_with_mllm` for maintaining conversation history across multiple stages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "897a985c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">âœ… Multi-turn chat function defined</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mâœ… Multi-turn chat function defined\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def chat_with_internvl(model, tokenizer, prompt, pixel_values, messages=None, max_new_tokens=2000, do_sample=False):\n",
    "    \"\"\"\n",
    "    Multi-turn chat with InternVL3 using conversation history.\n",
    "\n",
    "    Similar to Llama's chat_with_mllm but adapted for InternVL3's chat method.\n",
    "    Maintains conversation history across multiple queries on the same image.\n",
    "\n",
    "    Args:\n",
    "        model: InternVL3 model\n",
    "        tokenizer: InternVL3 tokenizer\n",
    "        prompt: Text prompt for this turn\n",
    "        pixel_values: Preprocessed image tensor\n",
    "        messages: Conversation history (list of [role, content] pairs) or None\n",
    "        max_new_tokens: Maximum tokens to generate\n",
    "        do_sample: Whether to use sampling\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (response, updated_messages)\n",
    "    \"\"\"\n",
    "    # Initialize or extend conversation history\n",
    "    if messages is None:\n",
    "        messages = []\n",
    "\n",
    "    # Add current prompt to history\n",
    "    messages.append(['user', f'<image>\\n{prompt}'])\n",
    "\n",
    "    # Generate response using InternVL3 chat method\n",
    "    generation_config = {\n",
    "        \"max_new_tokens\": max_new_tokens,\n",
    "        \"temperature\": None if not do_sample else 0.6,\n",
    "        \"do_sample\": do_sample,\n",
    "        \"top_p\": 0.9 if do_sample else None,\n",
    "        \"pad_token_id\": tokenizer.eos_token_id,\n",
    "    }\n",
    "\n",
    "    # Use InternVL3 chat with history\n",
    "    response = model.chat(\n",
    "        tokenizer,\n",
    "        pixel_values,\n",
    "        prompt,\n",
    "        generation_config=generation_config,\n",
    "        history=messages[:-1] if len(messages) > 1 else None,  # Exclude current prompt from history\n",
    "        return_history=False\n",
    "    )\n",
    "\n",
    "    # Add response to history\n",
    "    messages.append(['assistant', response])\n",
    "\n",
    "    return response, messages\n",
    "\n",
    "rprint(\"[green]âœ… Multi-turn chat function defined[/green]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfd4f4d",
   "metadata": {},
   "source": [
    "# 6. Image Discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65c25216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Ground truth CSV loaded with 9 rows and 20 columns\n",
      "ğŸ“‹ Available columns: ['image_file', 'DOCUMENT_TYPE', 'BUSINESS_ABN', 'BUSINESS_ADDRESS', 'GST_AMOUNT', 'INVOICE_DATE', 'IS_GST_INCLUDED', 'LINE_ITEM_DESCRIPTIONS', 'LINE_ITEM_QUANTITIES', 'LINE_ITEM_PRICES', 'LINE_ITEM_TOTAL_PRICES', 'PAYER_ADDRESS', 'PAYER_NAME', 'STATEMENT_DATE_RANGE', 'SUPPLIER_NAME', 'TOTAL_AMOUNT', 'TRANSACTION_AMOUNTS_PAID', 'TRANSACTION_DATES', 'TRANSACTION_AMOUNTS_RECEIVED', 'ACCOUNT_BALANCE']\n",
      "âœ… Using 'image_file' as image identifier column\n",
      "âœ… Ground truth mapping created for 9 images\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">âœ… Ground truth loaded for </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">9</span><span style=\"color: #008000; text-decoration-color: #008000\"> images</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mâœ… Ground truth loaded for \u001b[0m\u001b[1;32m9\u001b[0m\u001b[32m images\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Ready to process </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">9</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> images</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mReady to process \u001b[0m\u001b[1;32m9\u001b[0m\u001b[1;32m images\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Data directory: /home/jovyan/nfs_share/tod/evaluation_data</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mData directory: \u001b[0m\u001b[36m/home/jovyan/nfs_share/tod/\u001b[0m\u001b[36mevaluation_data\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Ground truth: /home/jovyan/nfs_share/tod/evaluation_data/ground_truth.csv</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mGround truth: \u001b[0m\u001b[36m/home/jovyan/nfs_share/tod/evaluation_data/\u001b[0m\u001b[36mground_truth.csv\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Mode: Evaluation mode</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mMode: Evaluation mode\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1. image_001.png\n",
      "  2. image_002.png\n",
      "  3. image_003.png\n",
      "  4. image_004.png\n",
      "  5. image_005.png\n",
      "  ... and 4 more\n"
     ]
    }
   ],
   "source": [
    "# Discover and filter images - Handle both absolute and relative paths\n",
    "\n",
    "# Convert DATA_DIR to Path and handle absolute/relative paths\n",
    "data_dir = Path(CONFIG['DATA_DIR'])\n",
    "if not data_dir.is_absolute():\n",
    "    # If relative, make it relative to current working directory\n",
    "    data_dir = Path.cwd() / data_dir\n",
    "\n",
    "# Discover images from the resolved data directory\n",
    "all_images = discover_images(str(data_dir))\n",
    "\n",
    "# Conditionally load ground truth only when not in inference-only mode\n",
    "ground_truth = {}\n",
    "if not CONFIG['INFERENCE_ONLY'] and CONFIG['GROUND_TRUTH']:\n",
    "    # Convert GROUND_TRUTH to Path and handle absolute/relative paths\n",
    "    ground_truth_path = Path(CONFIG['GROUND_TRUTH'])\n",
    "    if not ground_truth_path.is_absolute():\n",
    "        # If relative, make it relative to current working directory\n",
    "        ground_truth_path = Path.cwd() / ground_truth_path\n",
    "    \n",
    "    # Load ground truth from the resolved path\n",
    "    ground_truth = load_ground_truth(str(ground_truth_path), verbose=CONFIG['VERBOSE'])\n",
    "    \n",
    "    rprint(f\"[green]âœ… Ground truth loaded for {len(ground_truth)} images[/green]\")\n",
    "else:\n",
    "    rprint(\"[cyan]ğŸ“‹ Running in inference-only mode (no ground truth required)[/cyan]\")\n",
    "\n",
    "# Apply filters (only if ground truth is available)\n",
    "if CONFIG['DOCUMENT_TYPES'] and ground_truth:\n",
    "    filtered = []\n",
    "    for img in all_images:\n",
    "        img_name = Path(img).name\n",
    "        if img_name in ground_truth:\n",
    "            doc_type = ground_truth[img_name].get('DOCUMENT_TYPE', '').lower()\n",
    "            if any(dt.lower() in doc_type for dt in CONFIG['DOCUMENT_TYPES']):\n",
    "                filtered.append(img)\n",
    "    all_images = filtered\n",
    "\n",
    "if CONFIG['MAX_IMAGES']:\n",
    "    all_images = all_images[:CONFIG['MAX_IMAGES']]\n",
    "\n",
    "rprint(f\"[bold green]Ready to process {len(all_images)} images[/bold green]\")\n",
    "rprint(f\"[cyan]Data directory: {data_dir}[/cyan]\")\n",
    "if not CONFIG['INFERENCE_ONLY'] and CONFIG['GROUND_TRUTH']:\n",
    "    rprint(f\"[cyan]Ground truth: {ground_truth_path}[/cyan]\")\n",
    "rprint(f\"[cyan]Mode: {'Inference-only' if CONFIG['INFERENCE_ONLY'] else 'Evaluation mode'}[/cyan]\")\n",
    "for i, img in enumerate(all_images[:5], 1):\n",
    "    print(f\"  {i}. {Path(img).name}\")\n",
    "if len(all_images) > 5:\n",
    "    print(f\"  ... and {len(all_images) - 5} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e700afb",
   "metadata": {},
   "source": [
    "## 7. Multi-Stage Batch Processing\n",
    "\n",
    "**Explicit multi-stage processing** (Llama-style transparency):\n",
    "- **Stage 0**: Document Type Classification (INVOICE/RECEIPT/BANK_STATEMENT)\n",
    "- **Stage 1**: Structure Classification (for BANK_STATEMENT only: FLAT/GROUPED)\n",
    "- **Stage 2**: Document-Type-Aware Extraction (using appropriate prompt)\n",
    "\n",
    "**Saves intermediate responses**:\n",
    "- `doctype_classification`: Raw VLM response from document type detection\n",
    "- `structure_classification`: Raw VLM response from structure classification\n",
    "- `extraction_raw`: Raw VLM response from field extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff746da",
   "metadata": {},
   "outputs": [],
   "source": "# Multi-stage adaptive extraction with explicit stages (Llama-style)\nresults = []\nprocessing_times = []\ndoctype_counts = {'INVOICE': 0, 'RECEIPT': 0, 'BANK_STATEMENT': 0}\nstructure_counts = {'flat': 0, 'date_grouped': 0}\n\nrprint(\"\\n[bold green]ğŸš€ Starting multi-stage adaptive extraction...[/bold green]\\n\")\n\nfor idx, image_path in enumerate(track(all_images, description=\"Processing images\"), 1):\n    image_name = Path(image_path).name\n\n    try:\n        start_time = time.time()\n\n        # Initialize conversation history and load image once\n        messages = []\n        pixel_values = hybrid_processor.load_image(str(image_path))\n\n        # ===================================================================\n        # STAGE 0: Document Type Classification\n        # ===================================================================\n        if CONFIG['VERBOSE']:\n            rprint(f\"\\n[bold blue]Processing [{idx}/{len(all_images)}]: {image_name}[/bold blue]\")\n            rprint(\"[dim]Stage 0: Document type detection...[/dim]\")\n\n        classification_result = hybrid_processor.detect_and_classify_document(\n            str(image_path), verbose=False\n        )\n\n        document_type = classification_result['document_type']\n        doctype_answer = classification_result.get('raw_response', document_type)\n        doctype_counts[document_type] = doctype_counts.get(document_type, 0) + 1\n\n        # ===================================================================\n        # STAGE 1: Structure Classification (for BANK_STATEMENT only)\n        # ===================================================================\n        structure_type = \"N/A\"\n        structure_answer = \"N/A\"\n\n        if document_type == \"BANK_STATEMENT\":\n            if CONFIG['VERBOSE']:\n                rprint(\"[dim]Stage 1: Bank statement structure classification...[/dim]\")\n\n            # Use vision-based structure classifier\n            from common.vision_bank_statement_classifier import classify_bank_statement_structure_vision\n\n            structure_type = classify_bank_statement_structure_vision(\n                str(image_path),\n                model=hybrid_processor,  # Pass the processor (has load_image method)\n                processor=None,  # InternVL3 doesn't use separate processor\n                verbose=False\n            )\n\n            structure_answer = structure_type  # For InternVL3, we have the parsed result directly\n            structure_counts[structure_type] = structure_counts.get(structure_type, 0) + 1\n            prompt_key = f\"internvl3_bank_statement_{structure_type}\"\n        elif document_type == \"INVOICE\":\n            prompt_key = \"internvl3_invoice\"\n        elif document_type == \"RECEIPT\":\n            prompt_key = \"internvl3_receipt\"\n\n        # ===================================================================\n        # STAGE 2: Document-Type-Aware Extraction\n        # ===================================================================\n        if CONFIG['VERBOSE']:\n            rprint(f\"[dim]Stage 2: Extraction using {prompt_key}...[/dim]\")\n\n        extraction_result = hybrid_processor.process_document_aware(\n            str(image_path), classification_result, verbose=False\n        )\n\n        # Extract data and raw response\n        extracted_fields = extraction_result.get('extracted_data', {})\n        extraction_raw = extraction_result.get('raw_response', '')\n\n        # Store comprehensive results (Llama-style structure)\n        result = {\n            'image_file': image_name,\n            'document_type': document_type,\n            'structure_type': structure_type,\n            'prompt_used': prompt_key,\n            'doctype_classification': doctype_answer.strip() if isinstance(doctype_answer, str) else str(doctype_answer),\n            'structure_classification': structure_answer.strip() if isinstance(structure_answer, str) else str(structure_answer),\n            'extraction_raw': extraction_raw,\n            **extracted_fields  # Add all individual field columns\n        }\n        results.append(result)\n\n        processing_time = time.time() - start_time\n        processing_times.append(processing_time)\n\n        structure_display = structure_type if structure_type != 'N/A' else 'direct'\n        rprint(f\"[green]âœ… {image_name}: {document_type} ({structure_display}) - {processing_time:.2f}s[/green]\")\n\n    except Exception as e:\n        rprint(f\"[red]âŒ {image_name}: Error - {e}[/red]\")\n        results.append({\n            'image_file': image_name,\n            'document_type': 'ERROR',\n            'structure_type': 'ERROR',\n            'error': str(e)\n        })\n        processing_times.append(0)\n\n    finally:\n        # Memory cleanup after each image\n        if 'pixel_values' in locals():\n            del pixel_values\n\n        # Clear GPU cache\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n\n        # Periodic garbage collection every 3 images\n        if idx % 3 == 0:\n            gc.collect()\n\nconsole.rule(\"[bold green]Batch Processing Complete[/bold green]\")\n\n# Display summary statistics\nrprint(f\"\\n[bold blue]ğŸ“Š Document Type Classification Summary:[/bold blue]\")\nrprint(f\"[cyan]  Invoices: {doctype_counts.get('INVOICE', 0)}[/cyan]\")\nrprint(f\"[cyan]  Receipts: {doctype_counts.get('RECEIPT', 0)}[/cyan]\")\nrprint(f\"[cyan]  Bank Statements: {doctype_counts.get('BANK_STATEMENT', 0)}[/cyan]\")\n\nif doctype_counts.get('BANK_STATEMENT', 0) > 0:\n    rprint(f\"\\n[bold blue]ğŸ“Š Bank Statement Structure Summary:[/bold blue]\")\n    rprint(f\"[cyan]  Flat table: {structure_counts.get('flat', 0)}[/cyan]\")\n    rprint(f\"[cyan]  Date-grouped: {structure_counts.get('date_grouped', 0)}[/cyan]\")"
  },
  {
   "cell_type": "markdown",
   "id": "6ae41782",
   "metadata": {},
   "source": [
    "## 8. Save Results (Llama-Compatible Format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec129450",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">âœ… CSV saved to: /home/jovyan/nfs_share/tod/LMM_POC/output/csv/internvl3_adaptive_results_20251021_221057.csv</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mâœ… CSV saved to: \u001b[0m\u001b[32m/home/jovyan/nfs_share/tod/LMM_POC/output/csv/\u001b[0m\u001b[32minternvl3_adaptive_results_20251021_221057.csv\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">  Rows: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36m  Rows: \u001b[0m\u001b[1;36m9\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">  Columns: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36m  Columns: \u001b[0m\u001b[1;36m26\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">ğŸ“‹ CSV Columns (Llama-compatible):</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;34mğŸ“‹ CSV Columns \u001b[0m\u001b[1;34m(\u001b[0m\u001b[1;34mLlama-compatible\u001b[0m\u001b[1;34m)\u001b[0m\u001b[1;34m:\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Core columns: image_file, document_type, structure_type, prompt_used, doctype_classification, </span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">structure_classification, extraction_raw</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mCore columns: image_file, document_type, structure_type, prompt_used, doctype_classification, \u001b[0m\n",
       "\u001b[36mstructure_classification, extraction_raw\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Field columns </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">)</span><span style=\"color: #008080; text-decoration-color: #008080\">: DOCUMENT_TYPE, BUSINESS_ABN, SUPPLIER_NAME, BUSINESS_ADDRESS, PAYER_NAME...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mField columns \u001b[0m\u001b[1;36m(\u001b[0m\u001b[1;36m19\u001b[0m\u001b[1;36m)\u001b[0m\u001b[36m: DOCUMENT_TYPE, BUSINESS_ABN, SUPPLIER_NAME, BUSINESS_ADDRESS, PAYER_NAME\u001b[0m\u001b[36m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">âœ… JSON saved to: /home/jovyan/nfs_share/tod/LMM_POC/output/csv/internvl3_adaptive_results_20251021_221057.json</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mâœ… JSON saved to: \u001b[0m\u001b[32m/home/jovyan/nfs_share/tod/LMM_POC/output/csv/\u001b[0m\u001b[32minternvl3_adaptive_results_20251021_221057.json\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert results to DataFrame (Llama-compatible structure)\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Save to CSV (compatible with model_comparison.ipynb pattern)\n",
    "csv_output = OUTPUT_DIRS['csv'] / f\"internvl3_adaptive_results_{BATCH_TIMESTAMP}.csv\"\n",
    "df.to_csv(csv_output, index=False)\n",
    "\n",
    "rprint(f\"[green]âœ… CSV saved to: {csv_output}[/green]\")\n",
    "rprint(f\"[cyan]  Rows: {len(df)}[/cyan]\")\n",
    "rprint(f\"[cyan]  Columns: {len(df.columns)}[/cyan]\")\n",
    "\n",
    "# Show column names to verify Llama-compatible structure\n",
    "rprint(\"\\n[bold blue]ğŸ“‹ CSV Columns (Llama-compatible):[/bold blue]\")\n",
    "core_cols = ['image_file', 'document_type', 'structure_type', 'prompt_used',\n",
    "             'doctype_classification', 'structure_classification', 'extraction_raw']\n",
    "rprint(f\"[cyan]Core columns: {', '.join(core_cols)}[/cyan]\")\n",
    "field_cols = [col for col in df.columns if col not in core_cols and col != 'error']\n",
    "rprint(f\"[cyan]Field columns ({len(field_cols)}): {', '.join(field_cols[:5])}{'...' if len(field_cols) > 5 else ''}[/cyan]\")\n",
    "\n",
    "# Save detailed JSON results\n",
    "json_output = OUTPUT_DIRS['csv'] / f\"internvl3_adaptive_results_{BATCH_TIMESTAMP}.json\"\n",
    "with open(json_output, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "rprint(f\"[green]âœ… JSON saved to: {json_output}[/green]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb956f3",
   "metadata": {},
   "source": [
    "## 9. Display Sample Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a1cfb20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">Sample Results</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ \u001b[0m\u001b[1;34mSample Results\u001b[0m\u001b[92m â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   image_file  document_type structure_type                           prompt_used\n",
       "image_001.png        RECEIPT            N/A                     internvl3_receipt\n",
       "image_002.png        RECEIPT            N/A                     internvl3_receipt\n",
       "image_003.png BANK_STATEMENT   date_grouped internvl3_bank_statement_date_grouped\n",
       "image_004.png        RECEIPT            N/A                     internvl3_receipt\n",
       "image_005.png        INVOICE            N/A                     internvl3_invoice\n",
       "image_006.png        INVOICE            N/A                     internvl3_invoice\n",
       "image_007.png        INVOICE            N/A                     internvl3_invoice\n",
       "image_008.png BANK_STATEMENT   date_grouped internvl3_bank_statement_date_grouped\n",
       "image_009.png BANK_STATEMENT   date_grouped internvl3_bank_statement_date_grouped\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   image_file  document_type structure_type                           prompt_used\n",
       "image_001.png        RECEIPT            N/A                     internvl3_receipt\n",
       "image_002.png        RECEIPT            N/A                     internvl3_receipt\n",
       "image_003.png BANK_STATEMENT   date_grouped internvl3_bank_statement_date_grouped\n",
       "image_004.png        RECEIPT            N/A                     internvl3_receipt\n",
       "image_005.png        INVOICE            N/A                     internvl3_invoice\n",
       "image_006.png        INVOICE            N/A                     internvl3_invoice\n",
       "image_007.png        INVOICE            N/A                     internvl3_invoice\n",
       "image_008.png BANK_STATEMENT   date_grouped internvl3_bank_statement_date_grouped\n",
       "image_009.png BANK_STATEMENT   date_grouped internvl3_bank_statement_date_grouped\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display sample results\n",
    "console.rule(\"[bold blue]Sample Results[/bold blue]\")\n",
    "\n",
    "display_cols = ['image_file', 'document_type', 'structure_type', 'prompt_used']\n",
    "rprint(df[display_cols].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90763354",
   "metadata": {},
   "source": [
    "## 10. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0593878f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š DOCUMENT-TYPE-AWARE ADAPTIVE EXTRACTION SUMMARY\n",
      "================================================================================\n",
      "Total images processed: 9\n",
      "Successful extractions: 9\n",
      "Errors: 0\n",
      "\n",
      "Document Type Classification:\n",
      "  Invoices: 3\n",
      "  Receipts: 3\n",
      "  Bank Statements: 3\n",
      "\n",
      "Bank Statement Structure Classification:\n",
      "  Flat table format: 0\n",
      "  Date-grouped format: 3\n",
      "\n",
      "Prompts Used:\n",
      "  internvl3_bank_statement_date_grouped: 3\n",
      "  internvl3_invoice: 3\n",
      "  internvl3_receipt: 3\n",
      "================================================================================\n",
      "\n",
      "ğŸ“ˆ Field Extraction Coverage:\n",
      "  DOCUMENT_TYPE: 9/9 (100.0%)\n",
      "  BUSINESS_ABN: 6/9 (66.7%)\n",
      "  SUPPLIER_NAME: 6/9 (66.7%)\n",
      "  BUSINESS_ADDRESS: 6/9 (66.7%)\n",
      "  PAYER_NAME: 6/9 (66.7%)\n",
      "  PAYER_ADDRESS: 6/9 (66.7%)\n",
      "  INVOICE_DATE: 6/9 (66.7%)\n",
      "  LINE_ITEM_DESCRIPTIONS: 9/9 (100.0%)\n",
      "  LINE_ITEM_QUANTITIES: 6/9 (66.7%)\n",
      "  LINE_ITEM_PRICES: 6/9 (66.7%)\n",
      "  LINE_ITEM_TOTAL_PRICES: 6/9 (66.7%)\n",
      "  IS_GST_INCLUDED: 6/9 (66.7%)\n",
      "  GST_AMOUNT: 6/9 (66.7%)\n",
      "  TOTAL_AMOUNT: 6/9 (66.7%)\n",
      "  STATEMENT_DATE_RANGE: 3/9 (33.3%)\n",
      "  TRANSACTION_DATES: 3/9 (33.3%)\n",
      "  TRANSACTION_AMOUNTS_PAID: 3/9 (33.3%)\n",
      "  TRANSACTION_AMOUNTS_RECEIVED: 3/9 (33.3%)\n",
      "  ACCOUNT_BALANCE: 3/9 (33.3%)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nğŸ“Š DOCUMENT-TYPE-AWARE ADAPTIVE EXTRACTION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total images processed: {len(results)}\")\n",
    "print(f\"Successful extractions: {len([r for r in results if 'error' not in r])}\")\n",
    "print(f\"Errors: {len([r for r in results if 'error' in r])}\")\n",
    "\n",
    "print(\"\\nDocument Type Classification:\")\n",
    "print(f\"  Invoices: {doctype_counts.get('INVOICE', 0)}\")\n",
    "print(f\"  Receipts: {doctype_counts.get('RECEIPT', 0)}\")\n",
    "print(f\"  Bank Statements: {doctype_counts.get('BANK_STATEMENT', 0)}\")\n",
    "\n",
    "if doctype_counts.get('BANK_STATEMENT', 0) > 0:\n",
    "    print(\"\\nBank Statement Structure Classification:\")\n",
    "    print(f\"  Flat table format: {structure_counts.get('flat', 0)}\")\n",
    "    print(f\"  Date-grouped format: {structure_counts.get('date_grouped', 0)}\")\n",
    "\n",
    "print(\"\\nPrompts Used:\")\n",
    "prompt_usage = {}\n",
    "for result in results:\n",
    "    if 'prompt_used' in result:\n",
    "        prompt = result['prompt_used']\n",
    "        prompt_usage[prompt] = prompt_usage.get(prompt, 0) + 1\n",
    "\n",
    "for prompt, count in sorted(prompt_usage.items()):\n",
    "    print(f\"  {prompt}: {count}\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Field extraction statistics\n",
    "if len(df) > 0:\n",
    "    field_cols = [col for col in df.columns if col not in [\n",
    "        'image_file', 'document_type', 'structure_type', 'prompt_used',\n",
    "        'doctype_classification', 'structure_classification', 'extraction_raw', 'error'\n",
    "    ]]\n",
    "\n",
    "    if field_cols:\n",
    "        print(\"\\nğŸ“ˆ Field Extraction Coverage:\")\n",
    "        for field in field_cols:\n",
    "            if field in df.columns:\n",
    "                found_count = df[field].notna().sum()\n",
    "                coverage = (found_count / len(df)) * 100\n",
    "                print(f\"  {field}: {found_count}/{len(df)} ({coverage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26247ed",
   "metadata": {},
   "source": [
    "## 11. View Individual Extraction\n",
    "\n",
    "Change `image_to_view` to view detailed extraction for a specific image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "226a3b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” Detailed Extraction: image_003.png\n",
      "================================================================================\n",
      "Document Type: BANK_STATEMENT\n",
      "Structure Type: date_grouped\n",
      "Prompt Used: internvl3_bank_statement_date_grouped\n",
      "\n",
      "Document Type Classification Response:\n",
      "BANK_STATEMENT\n",
      "\n",
      "Structure Classification Response:\n",
      "date_grouped\n",
      "\n",
      "Extraction Result:\n",
      "```json\n",
      "{\n",
      "  \"DOCUMENT_TYPE\": \"BANK_STATEMENT\",\n",
      "  \"STATEMENT_DATE_RANGE\": \"03/05/2025 to 10/05/2025\",\n",
      "  \"LINE_ITEM_DESCRIPTIONS\": \"EFTPOS PURCHASE WOOLWORTHS | INTEREST PAYMENT | REFUND PROCESSED | DIRECT CREDIT SALARY | ATM WITHDRAWAL ANZ ATM\",\n",
      "  \"TRANSACTION_DATES\": \"03/05/2025 | 04/05/2025 | 05/05/2025 | 06/05/2025 | 07/05/2025 | 08/05/2025 | 09/05/2025 | 10/05/2025\",\n",
      "  \"TRANSACTION_AMOUNTS_PAID\": \"288.03 | 22.50 | 114.66 | 3497.47 | 187.59 | 112.50 | 5.16 | 146.72\"\n",
      "}\n",
      "```\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# View detailed extraction for specific image\n",
    "image_to_view = \"image_003.png\"  # Change this\n",
    "\n",
    "result = next((r for r in results if r['image_file'] == image_to_view), None)\n",
    "\n",
    "if result:\n",
    "    print(f\"\\nğŸ” Detailed Extraction: {image_to_view}\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Document Type: {result['document_type']}\")\n",
    "    print(f\"Structure Type: {result['structure_type']}\")\n",
    "    print(f\"Prompt Used: {result['prompt_used']}\")\n",
    "    print(f\"\\nDocument Type Classification Response:\")\n",
    "    print(result.get('doctype_classification', 'N/A'))\n",
    "    print(f\"\\nStructure Classification Response:\")\n",
    "    print(result.get('structure_classification', 'N/A'))\n",
    "    print(f\"\\nExtraction Result:\")\n",
    "    extraction_display = result.get('extraction_raw', 'N/A')\n",
    "    if len(extraction_display) > 1000:\n",
    "        extraction_display = extraction_display[:1000] + \"\\n...[truncated]...\"\n",
    "    print(extraction_display)\n",
    "    print(\"=\"*80)\n",
    "else:\n",
    "    print(f\"Image {image_to_view} not found in results\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (unified_vision_processor)",
   "language": "python",
   "name": "unified_vision_processor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}