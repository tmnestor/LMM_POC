{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0523269d",
   "metadata": {},
   "source": [
    "# InternVL3-2B Document-Type-Aware Adaptive Extraction\n",
    "\n",
    "**Llama-style explicit multi-stage processing** for transparency and debugging:\n",
    "\n",
    "1. **Stage 0**: Document Type Classification (INVOICE/RECEIPT/BANK_STATEMENT)\n",
    "2. **Stage 1**: Structure Classification (if BANK_STATEMENT: FLAT/GROUPED)\n",
    "3. **Stage 2**: Document-Type-Aware Extraction (using appropriate prompt)\n",
    "\n",
    "**Key Features**:\n",
    "- Saves intermediate VLM responses (`doctype_classification`, `structure_classification`, `extraction_raw`)\n",
    "- Multi-turn chat capability for conversation history\n",
    "- Llama-compatible CSV output for model comparison\n",
    "- Explicit stage-by-stage progress display\n",
    "\n",
    "**Pattern**: Follows llama_batch_adaptive.ipynb for consistency\n",
    "\n",
    "Outputs compatible with model_comparison.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffdd5bd",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed6114bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Current directory: /home/jovyan/nfs_share/tod/LMM_POC\n",
      "‚úÖ Added /home/jovyan/nfs_share/tod/LMM_POC to sys.path\n",
      "‚úÖ Common module found at: /home/jovyan/nfs_share/tod/LMM_POC/common/__init__.py\n",
      "‚úÖ Path setup complete - proceed to imports\n"
     ]
    }
   ],
   "source": [
    "# Path setup for V100 systems - ensures proper module resolution\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "os.environ['EVALUATION_METHOD'] = 'order_aware_f1'  # or 'f1', 'kieval', 'order_aware_f1', 'correlation'\n",
    "\n",
    "\n",
    "# Get the notebook's directory\n",
    "notebook_path = Path().absolute()\n",
    "print(f\"üìÇ Current directory: {notebook_path}\")\n",
    "\n",
    "# Ensure the project root is in the Python path\n",
    "if str(notebook_path) not in sys.path:\n",
    "    sys.path.insert(0, str(notebook_path))\n",
    "    print(f\"‚úÖ Added {notebook_path} to sys.path\")\n",
    "\n",
    "# Verify common module can be found\n",
    "try:\n",
    "    import common\n",
    "    print(f\"‚úÖ Common module found at: {common.__file__ if hasattr(common, '__file__') else 'built-in'}\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Common module not found: {e}\")\n",
    "    print(\"üìã Current sys.path:\")\n",
    "    for p in sys.path[:5]:  # Show first 5 paths\n",
    "        print(f\"   - {p}\")\n",
    "\n",
    "print(\"‚úÖ Path setup complete - proceed to imports\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c483572a",
   "metadata": {},
   "source": [
    "## 1a. Path Setup (V100 Compatibility)\n",
    "\n",
    "**IMPORTANT**: If you encounter import errors on V100 systems, this cell ensures proper module resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44805fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All imports loaded successfully\n",
      "‚úÖ InternVL3 Hybrid Processor imported\n",
      "üìÇ Working directory: /home/jovyan/nfs_share/tod/LMM_POC\n",
      "üî¨ ADAPTIVE MODE: Explicit multi-stage processing for transparency\n"
     ]
    }
   ],
   "source": [
    "# Enable autoreload for module changes\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Standard library imports\n",
    "import gc\n",
    "import json\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Add current directory to path to ensure proper module resolution\n",
    "notebook_dir = Path.cwd()\n",
    "if str(notebook_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(notebook_dir))\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from IPython.display import display\n",
    "from rich import print as rprint\n",
    "from rich.console import Console\n",
    "from rich.progress import track\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# Project-specific imports - only what we actually use\n",
    "from models.document_aware_internvl3_processor import (\n",
    "    DocumentAwareInternVL3HybridProcessor,\n",
    ")\n",
    "from common.gpu_optimization import emergency_cleanup\n",
    "from common.extraction_parser import discover_images\n",
    "from common.evaluation_metrics import load_ground_truth\n",
    "\n",
    "print(\"‚úÖ All imports loaded successfully\")\n",
    "print(\"‚úÖ InternVL3 Hybrid Processor imported\")\n",
    "print(f\"üìÇ Working directory: {notebook_dir}\")\n",
    "print(\"üî¨ ADAPTIVE MODE: Explicit multi-stage processing for transparency\")\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dea10a3",
   "metadata": {},
   "source": [
    "## 2. Pre-emptive Memory Cleanup\n",
    "\n",
    "**CRITICAL for V100**: Run this cell first to prevent OOM errors when switching between models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02bd6274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">üßπ PRE-EMPTIVE V100 MEMORY CLEANUP</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31müßπ PRE-EMPTIVE V100 MEMORY CLEANUP\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">Clearing any existing model caches before loading...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33mClearing any existing model caches before loading\u001b[0m\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">üí° This prevents OOM errors when switching between models on V100</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36müí° This prevents OOM errors when switching between models on V100\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üö® Running V100 emergency GPU cleanup...\n",
      "üßπ Starting V100-optimized GPU memory cleanup...\n",
      "   üìä Initial GPU memory: 0.00GB allocated, 0.00GB reserved\n",
      "   ‚úÖ Final GPU memory: 0.00GB allocated, 0.00GB reserved\n",
      "   üíæ Memory freed: 0.00GB\n",
      "‚úÖ V100-optimized memory cleanup complete\n",
      "‚úÖ V100 emergency cleanup complete\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">‚úÖ Memory cleanup complete - ready for model loading</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m‚úÖ Memory cleanup complete - ready for model loading\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">üìã Next: Import modules and configure settings</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2müìã Next: Import modules and configure settings\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Pre-emptive V100 Memory Cleanup - Run FIRST to prevent OOM errors\n",
    "rprint(\"[bold red]üßπ PRE-EMPTIVE V100 MEMORY CLEANUP[/bold red]\")\n",
    "rprint(\"[yellow]Clearing any existing model caches before loading...[/yellow]\")\n",
    "rprint(\"[cyan]üí° This prevents OOM errors when switching between models on V100[/cyan]\")\n",
    "\n",
    "# Emergency cleanup to ensure clean slate\n",
    "emergency_cleanup(verbose=True)\n",
    "\n",
    "rprint(\"[green]‚úÖ Memory cleanup complete - ready for model loading[/green]\")\n",
    "rprint(\"[dim]üìã Next: Import modules and configure settings[/dim]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc3bb36",
   "metadata": {},
   "source": [
    "## 3. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0d3ec88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration set up successfully\n",
      "üìÇ Evaluation data: /home/jovyan/nfs_share/tod/evaluation_data\n",
      "üìä Ground truth: /home/jovyan/nfs_share/tod/evaluation_data/ground_truth.csv\n",
      "ü§ñ Model path: /home/jovyan/nfs_share/models/InternVL3-2B\n",
      "üìÅ Output base: /home/jovyan/nfs_share/tod/LMM_POC/output\n",
      "üìã Universal fields: 19\n",
      "üéØ Mode: Evaluation mode\n",
      "‚öôÔ∏è  Quantization: DISABLED (full precision)\n",
      "‚ö° Flash Attention: DISABLED (V100 compatible)\n",
      "üî¨ TESTING: Non-quantized InternVL3 performance after bug fixes\n"
     ]
    }
   ],
   "source": [
    "# Initialize console and environment configuration\n",
    "console = Console()\n",
    "\n",
    "# Environment-specific base paths\n",
    "ENVIRONMENT_BASES = {\n",
    "    'sandbox': '/home/jovyan/nfs_share/tod',\n",
    "    'efs': '/efs/shared/PoC_data'\n",
    "}\n",
    "base_data_path = ENVIRONMENT_BASES['sandbox']\n",
    "\n",
    "CONFIG = {\n",
    "    # Model settings\n",
    "    'MODEL_PATH': '/home/jovyan/nfs_share/models/InternVL3-2B', #DANGER WILL ROBINSON\n",
    "    # 'MODEL_PATH': '/home/jovyan/nfs_share/models/InternVL3-8B',\n",
    "    # 'MODEL_PATH': '/efs/shared/PTM/InternVL3-2B',\n",
    "    \n",
    "    # Batch settings\n",
    "    'DATA_DIR': f'{base_data_path}/evaluation_data',\n",
    "    'GROUND_TRUTH': f'{base_data_path}/evaluation_data/ground_truth.csv',\n",
    "    # 'OUTPUT_BASE': f'{base_data_path}/output',\n",
    "    'OUTPUT_BASE': f'{base_data_path}/LMM_POC/output',\n",
    "    'MAX_IMAGES': None,  # None for all, or set limit\n",
    "    'DOCUMENT_TYPES': None,  # None for all, or ['invoice', 'receipt']\n",
    "    'ENABLE_MATH_ENHANCEMENT': False,  # Disable mathematical correction for bank statements\n",
    "    \n",
    "    # Inference and evaluation mode\n",
    "    'INFERENCE_ONLY': False,  # Default: True (inference-only mode)\n",
    "    \n",
    "    # Verbosity control\n",
    "    'VERBOSE': True,\n",
    "    'SHOW_PROMPTS': True,\n",
    "    \n",
    "    # InternVL3 optimization settings - NON-QUANTIZED TESTING\n",
    "    # TESTING: Non-quantized performance after bug fixes (Rich recursion, prompt repetition)\n",
    "    # This follows the official InternVL3 documentation pattern exactly\n",
    "    'USE_QUANTIZATION': False,  # TESTING: Disabled to test non-quantized performance\n",
    "    'DEVICE_MAP': 'auto',\n",
    "    'MAX_NEW_TOKENS': 600,\n",
    "    'TORCH_DTYPE': 'bfloat16',\n",
    "    'LOW_CPU_MEM_USAGE': True,\n",
    "    # Flash Attention: NOT supported on V100, only enable for modern GPUs\n",
    "    'USE_FLASH_ATTN': False  # V100 compatible default\n",
    "}\n",
    "\n",
    "# Make GROUND_TRUTH conditional based on INFERENCE_ONLY mode\n",
    "if CONFIG['INFERENCE_ONLY']:\n",
    "    CONFIG['GROUND_TRUTH'] = None\n",
    "\n",
    "# ============================================================================\n",
    "# PROMPT CONFIGURATION - Explicit file and key mapping\n",
    "# ============================================================================\n",
    "# This configuration controls which prompt files and keys are used for each\n",
    "# document type. You can explicitly override both the file and the key.\n",
    "#\n",
    "# Structure:\n",
    "#   'extraction_files': Maps document types to YAML prompt files\n",
    "#   'extraction_keys': (Optional) Maps document types to specific keys in those files\n",
    "#\n",
    "# If 'extraction_keys' is not specified for a document type, the key will be\n",
    "# derived from the document type name (e.g., 'INVOICE' -> 'invoice')\n",
    "#\n",
    "# For bank statements, structure classification (_flat or _date_grouped) is \n",
    "# automatically appended UNLESS you provide a full key in 'extraction_keys'\n",
    "# ============================================================================\n",
    "\n",
    "PROMPT_CONFIG = {\n",
    "    # Document type detection configuration\n",
    "    'detection_file': 'prompts/document_type_detection.yaml',\n",
    "    'detection_key': 'detection',\n",
    "    \n",
    "    # Extraction prompt file mapping (REQUIRED)\n",
    "    'extraction_files': {\n",
    "        'INVOICE': 'prompts/internvl3_prompts.yaml',\n",
    "        'RECEIPT': 'prompts/internvl3_prompts.yaml', \n",
    "        'BANK_STATEMENT': 'prompts/internvl3_prompts.yaml'\n",
    "    },\n",
    "    \n",
    "    # Extraction prompt key mapping (OPTIONAL - for explicit control)\n",
    "    # Uncomment and configure to override automatic key derivation\n",
    "    # 'extraction_keys': {\n",
    "    #     'INVOICE': 'invoice',\n",
    "    #     'RECEIPT': 'receipt',\n",
    "    #     'BANK_STATEMENT': 'bank_statement',  # Will auto-append _flat or _date_grouped\n",
    "    #     # Or specify full key to skip automatic structure suffix:\n",
    "    #     # 'BANK_STATEMENT': 'bank_statement_flat',  # Forces flat table prompt\n",
    "    # }\n",
    "}\n",
    "\n",
    "# Example configurations:\n",
    "# ----------------------\n",
    "# Use generated prompts (if you create InternVL3 generated versions):\n",
    "#   'extraction_files': {\n",
    "#       'INVOICE': 'prompts/generated/internvl3_invoice_prompt.yaml',\n",
    "#       'RECEIPT': 'prompts/generated/internvl3_receipt_prompt.yaml',\n",
    "#       'BANK_STATEMENT': 'prompts/generated/internvl3_bank_statement_prompt.yaml'\n",
    "#   }\n",
    "#\n",
    "# Mix standard and custom prompts:\n",
    "#   'extraction_files': {\n",
    "#       'INVOICE': 'prompts/internvl3_prompts.yaml',\n",
    "#       'RECEIPT': 'prompts/custom_receipt_prompt.yaml',\n",
    "#       'BANK_STATEMENT': 'prompts/internvl3_prompts.yaml'\n",
    "#   }\n",
    "#\n",
    "# Force specific bank statement structure:\n",
    "#   'extraction_keys': {\n",
    "#       'BANK_STATEMENT': 'bank_statement_flat'  # Ignores vision classification\n",
    "#   }\n",
    "\n",
    "# Field list required for DocumentAwareInternVL3HybridProcessor\n",
    "UNIVERSAL_FIELDS = [\n",
    "    \"DOCUMENT_TYPE\", \"BUSINESS_ABN\", \"SUPPLIER_NAME\", \"BUSINESS_ADDRESS\",\n",
    "    \"PAYER_NAME\", \"PAYER_ADDRESS\", \"INVOICE_DATE\", \"STATEMENT_DATE_RANGE\",\n",
    "    \"LINE_ITEM_DESCRIPTIONS\", \"LINE_ITEM_QUANTITIES\", \"LINE_ITEM_PRICES\",\n",
    "    \"LINE_ITEM_TOTAL_PRICES\", \"IS_GST_INCLUDED\", \"GST_AMOUNT\", \"TOTAL_AMOUNT\",\n",
    "    \"TRANSACTION_DATES\", \"TRANSACTION_AMOUNTS_PAID\", \"TRANSACTION_AMOUNTS_RECEIVED\",\n",
    "    \"ACCOUNT_BALANCE\"\n",
    "]\n",
    "\n",
    "print(\"‚úÖ Configuration set up successfully\")\n",
    "print(f\"üìÇ Evaluation data: {CONFIG['DATA_DIR']}\")\n",
    "print(f\"üìä Ground truth: {CONFIG['GROUND_TRUTH']}\")\n",
    "print(f\"ü§ñ Model path: {CONFIG['MODEL_PATH']}\")\n",
    "print(f\"üìÅ Output base: {CONFIG['OUTPUT_BASE']}\")\n",
    "print(f\"üìã Universal fields: {len(UNIVERSAL_FIELDS)}\")\n",
    "print(f\"üéØ Mode: {'Inference-only' if CONFIG['INFERENCE_ONLY'] else 'Evaluation mode'}\")\n",
    "print(f\"‚öôÔ∏è  Quantization: {'ENABLED (8-bit)' if CONFIG['USE_QUANTIZATION'] else 'DISABLED (full precision)'}\")\n",
    "print(f\"‚ö° Flash Attention: {'ENABLED' if CONFIG['USE_FLASH_ATTN'] else 'DISABLED (V100 compatible)'}\")\n",
    "print(\"üî¨ TESTING: Non-quantized InternVL3 performance after bug fixes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19ec25a",
   "metadata": {},
   "source": [
    "# 4. Output Directory Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9659dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup output directories - Handle both absolute and relative paths\n",
    "\n",
    "# Convert OUTPUT_BASE to Path and handle absolute/relative paths\n",
    "OUTPUT_BASE = Path(CONFIG['OUTPUT_BASE'])\n",
    "if not OUTPUT_BASE.is_absolute():\n",
    "    # If relative, make it relative to current working directory\n",
    "    OUTPUT_BASE = Path.cwd() / OUTPUT_BASE\n",
    "\n",
    "BATCH_TIMESTAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "OUTPUT_DIRS = {\n",
    "    'base': OUTPUT_BASE,\n",
    "    'batch': OUTPUT_BASE / 'batch_results',\n",
    "    'csv': OUTPUT_BASE / 'csv',\n",
    "    'visualizations': OUTPUT_BASE / 'visualizations',\n",
    "    'reports': OUTPUT_BASE / 'reports'\n",
    "}\n",
    "\n",
    "for dir_path in OUTPUT_DIRS.values():\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e75bfa",
   "metadata": {},
   "source": [
    "# 5. Model Loading (Direct Official Pattern)\n",
    "\n",
    "**NON-QUANTIZED TESTING**: Loading InternVL3 without quantization using the official documentation pattern to test whether the recent bug fixes (Rich recursion, prompt repetition) resolved the underlying issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "411f53b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Loading InternVL3 model with official NON-QUANTIZED pattern...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mLoading InternVL3 model with official NON-QUANTIZED pattern\u001b[0m\u001b[1;32m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">üî¨ Testing: Non-quantized performance after bug fixes</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36müî¨ Testing: Non-quantized performance after bug fixes\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">üìñ Following: </span><span style=\"color: #008080; text-decoration-color: #008080; text-decoration: underline\">https://internvl.readthedocs.io/en/latest/internvl3.0/quick_start.html</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36müìñ Following: \u001b[0m\u001b[4;36mhttps://internvl.readthedocs.io/en/latest/internvl3.0/quick_start.html\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">üßπ CUDA cache cleared</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34müßπ CUDA cache cleared\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">üì• Loading model with official parameters...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36müì• Loading model with official parameters\u001b[0m\u001b[36m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FlashAttention2 is not installed.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">üì• Loading tokenizer...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36müì• Loading tokenizer\u001b[0m\u001b[36m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">‚úÖ Model and tokenizer loaded successfully!</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m‚úÖ Model and tokenizer loaded successfully!\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">üìä GPU Memory: </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">2.</span><span style=\"color: #000080; text-decoration-color: #000080\">29GB allocated, </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">2.</span><span style=\"color: #000080; text-decoration-color: #000080\">32GB reserved, 150GB total</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34müìä GPU Memory: \u001b[0m\u001b[1;34m2.\u001b[0m\u001b[34m29GB allocated, \u001b[0m\u001b[1;34m2.\u001b[0m\u001b[34m32GB reserved, 150GB total\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">üîç Memory usage: </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">1.5</span><span style=\"color: #000080; text-decoration-color: #000080\">%</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34müîç Memory usage: \u001b[0m\u001b[1;34m1.5\u001b[0m\u001b[34m%\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">üî¢ Model parameters: </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">2</span><span style=\"color: #000080; text-decoration-color: #000080\">,</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">088</span><span style=\"color: #000080; text-decoration-color: #000080\">,</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">957</span><span style=\"color: #000080; text-decoration-color: #000080\">,</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">440</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34müî¢ Model parameters: \u001b[0m\u001b[1;34m2\u001b[0m\u001b[34m,\u001b[0m\u001b[1;34m088\u001b[0m\u001b[34m,\u001b[0m\u001b[1;34m957\u001b[0m\u001b[34m,\u001b[0m\u001b[1;34m440\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">üéØ Data type: torch.bfloat16</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34müéØ Data type: torch.bfloat16\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">üñ•Ô∏è  Device: cu</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">da:0</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34müñ•Ô∏è  Device: cu\u001b[0m\u001b[1;34mda:0\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">üîÑ Model distribution: </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">{</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">0</span><span style=\"color: #000080; text-decoration-color: #000080\">: </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">15</span><span style=\"color: #000080; text-decoration-color: #000080\">, </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">1</span><span style=\"color: #000080; text-decoration-color: #000080\">: </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">19</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34müîÑ Model distribution: \u001b[0m\u001b[1;34m{\u001b[0m\u001b[1;34m0\u001b[0m\u001b[34m: \u001b[0m\u001b[1;34m15\u001b[0m\u001b[34m, \u001b[0m\u001b[1;34m1\u001b[0m\u001b[34m: \u001b[0m\u001b[1;34m19\u001b[0m\u001b[1;34m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">üîß Initializing document-aware processor...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36müîß Initializing document-aware processor\u001b[0m\u001b[36m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ InternVL3 Hybrid processor initialized for 19 fields: DOCUMENT_TYPE ‚Üí ACCOUNT_BALANCE\n",
      "üîß CUDA memory allocation configured: max_split_size_mb:64\n",
      "üí° Using 64MB memory blocks to reduce fragmentation\n",
      "üìä Initial CUDA state (Multi-GPU Total): Allocated=3.89GB, Reserved=3.95GB\n",
      "ü§ñ Auto-detected batch size: 8 (GPU Memory: 275.5GB)\n",
      "üéØ DOCUMENT AWARE REDUCTION: 19 fields (~34% fewer than original 29)\n",
      "üéØ Generation config: max_new_tokens=2000, temperature=0.0, do_sample=False\n",
      "‚úÖ Using pre-loaded InternVL3 model and tokenizer\n",
      "üîß Device: cuda:0\n",
      "üíæ Model parameters: 2,088,957,440\n",
      "üöÄ V100 optimizations applied\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">‚úÖ InternVL3 NON-QUANTIZED model ready for document-aware processing</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m‚úÖ InternVL3 NON-QUANTIZED model ready for document-aware processing\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">üî¨ If you see gibberish responses, it confirms quantization is still needed for V100</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33müî¨ If you see gibberish responses, it confirms quantization is still needed for V100\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">üéâ If responses are clean, it proves the bug fixes resolved the core issues!</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33müéâ If responses are clean, it proves the bug fixes resolved the core issues!\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load InternVL3 model using DIRECT official pattern (bypassing wrapper)\n",
    "# https://internvl.readthedocs.io/en/latest/internvl3.0/quick_start.html\n",
    "rprint(\"[bold green]Loading InternVL3 model with official NON-QUANTIZED pattern...[/bold green]\")\n",
    "rprint(\"[cyan]üî¨ Testing: Non-quantized performance after bug fixes[/cyan]\")\n",
    "rprint(\"[cyan]üìñ Following: https://internvl.readthedocs.io/en/latest/internvl3.0/quick_start.html[/cyan]\")\n",
    "\n",
    "try:\n",
    "    # Clear any existing CUDA cache\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        rprint(\"[blue]üßπ CUDA cache cleared[/blue]\")\n",
    "    \n",
    "    # Load model using exact official pattern\n",
    "    rprint(\"[cyan]üì• Loading model with official parameters...[/cyan]\")\n",
    "    model = AutoModel.from_pretrained(\n",
    "        CONFIG['MODEL_PATH'],\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        low_cpu_mem_usage=True,\n",
    "        use_flash_attn=False,  # V100 compatible\n",
    "        trust_remote_code=True,\n",
    "        device_map=\"auto\"  # Distribute across available GPUs\n",
    "    ).eval()\n",
    "    \n",
    "    # Load tokenizer\n",
    "    rprint(\"[cyan]üì• Loading tokenizer...[/cyan]\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        CONFIG['MODEL_PATH'],\n",
    "        trust_remote_code=True,\n",
    "        use_fast=False\n",
    "    )\n",
    "    \n",
    "    # Set generation parameters\n",
    "    model.config.max_new_tokens = CONFIG['MAX_NEW_TOKENS']\n",
    "    \n",
    "    # Display model information\n",
    "    rprint(\"[green]‚úÖ Model and tokenizer loaded successfully![/green]\")\n",
    "    \n",
    "    # GPU memory check\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1e9\n",
    "        reserved = torch.cuda.memory_reserved() / 1e9\n",
    "        total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "        rprint(f\"[blue]üìä GPU Memory: {allocated:.2f}GB allocated, {reserved:.2f}GB reserved, {total:.0f}GB total[/blue]\")\n",
    "        rprint(f\"[blue]üîç Memory usage: {(allocated/total*100):.1f}%[/blue]\")\n",
    "    \n",
    "    # Model parameters\n",
    "    param_count = sum(p.numel() for p in model.parameters())\n",
    "    rprint(f\"[blue]üî¢ Model parameters: {param_count:,}[/blue]\")\n",
    "    rprint(f\"[blue]üéØ Data type: {model.dtype}[/blue]\")\n",
    "    rprint(f\"[blue]üñ•Ô∏è  Device: {next(model.parameters()).device}[/blue]\")\n",
    "    \n",
    "    # Add device map diagnostic\n",
    "    if hasattr(model, 'hf_device_map'):\n",
    "        from collections import Counter\n",
    "        device_distribution = Counter(model.hf_device_map.values())\n",
    "        rprint(f\"[blue]üîÑ Model distribution: {dict(device_distribution)}[/blue]\")\n",
    "    else:\n",
    "        rprint(\"[blue]üìç No device map found - model placed manually[/blue]\")\n",
    "    \n",
    "    # Initialize the hybrid processor with loaded model components\n",
    "    rprint(\"[cyan]üîß Initializing document-aware processor...[/cyan]\")\n",
    "    hybrid_processor = DocumentAwareInternVL3HybridProcessor(\n",
    "        field_list=UNIVERSAL_FIELDS,\n",
    "        model_path=CONFIG['MODEL_PATH'],\n",
    "        debug=CONFIG['VERBOSE'],\n",
    "        pre_loaded_model=model,\n",
    "        pre_loaded_tokenizer=tokenizer,\n",
    "        prompt_config=PROMPT_CONFIG  # Single source of truth for configuration!\n",
    "    )\n",
    "    \n",
    "    rprint(\"[bold green]‚úÖ InternVL3 NON-QUANTIZED model ready for document-aware processing[/bold green]\")\n",
    "    rprint(\"[yellow]üî¨ If you see gibberish responses, it confirms quantization is still needed for V100[/yellow]\")\n",
    "    rprint(\"[yellow]üéâ If responses are clean, it proves the bug fixes resolved the core issues![/yellow]\")\n",
    "    \n",
    "except Exception as e:\n",
    "    rprint(f\"[red]‚ùå Error loading model: {e}[/red]\")\n",
    "    rprint(\"[yellow]üí° This may indicate that quantization is still required for V100 GPUs[/yellow]\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54a67da",
   "metadata": {},
   "source": [
    "## 5a. Multi-Turn Chat Function\n",
    "\n",
    "InternVL3 equivalent of Llama's `chat_with_mllm` for maintaining conversation history across multiple stages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "897a985c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">‚úÖ Multi-turn chat function defined</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m‚úÖ Multi-turn chat function defined\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def chat_with_internvl(model, tokenizer, prompt, pixel_values, messages=None, max_new_tokens=2000, do_sample=False):\n",
    "    \"\"\"\n",
    "    Multi-turn chat with InternVL3 using conversation history.\n",
    "\n",
    "    Similar to Llama's chat_with_mllm but adapted for InternVL3's chat method.\n",
    "    Maintains conversation history across multiple queries on the same image.\n",
    "\n",
    "    Args:\n",
    "        model: InternVL3 model\n",
    "        tokenizer: InternVL3 tokenizer\n",
    "        prompt: Text prompt for this turn\n",
    "        pixel_values: Preprocessed image tensor\n",
    "        messages: Conversation history (list of [role, content] pairs) or None\n",
    "        max_new_tokens: Maximum tokens to generate\n",
    "        do_sample: Whether to use sampling\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (response, updated_messages)\n",
    "    \"\"\"\n",
    "    # Initialize or extend conversation history\n",
    "    if messages is None:\n",
    "        messages = []\n",
    "\n",
    "    # Add current prompt to history\n",
    "    messages.append(['user', f'<image>\\n{prompt}'])\n",
    "\n",
    "    # Generate response using InternVL3 chat method\n",
    "    generation_config = {\n",
    "        \"max_new_tokens\": max_new_tokens,\n",
    "        \"temperature\": None if not do_sample else 0.6,\n",
    "        \"do_sample\": do_sample,\n",
    "        \"top_p\": 0.9 if do_sample else None,\n",
    "        \"pad_token_id\": tokenizer.eos_token_id,\n",
    "    }\n",
    "\n",
    "    # Use InternVL3 chat with history\n",
    "    response = model.chat(\n",
    "        tokenizer,\n",
    "        pixel_values,\n",
    "        prompt,\n",
    "        generation_config=generation_config,\n",
    "        history=messages[:-1] if len(messages) > 1 else None,  # Exclude current prompt from history\n",
    "        return_history=False\n",
    "    )\n",
    "\n",
    "    # Add response to history\n",
    "    messages.append(['assistant', response])\n",
    "\n",
    "    return response, messages\n",
    "\n",
    "rprint(\"[green]‚úÖ Multi-turn chat function defined[/green]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfd4f4d",
   "metadata": {},
   "source": [
    "# 6. Image Discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65c25216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Ground truth CSV loaded with 9 rows and 20 columns\n",
      "üìã Available columns: ['image_file', 'DOCUMENT_TYPE', 'BUSINESS_ABN', 'BUSINESS_ADDRESS', 'GST_AMOUNT', 'INVOICE_DATE', 'IS_GST_INCLUDED', 'LINE_ITEM_DESCRIPTIONS', 'LINE_ITEM_QUANTITIES', 'LINE_ITEM_PRICES', 'LINE_ITEM_TOTAL_PRICES', 'PAYER_ADDRESS', 'PAYER_NAME', 'STATEMENT_DATE_RANGE', 'SUPPLIER_NAME', 'TOTAL_AMOUNT', 'TRANSACTION_AMOUNTS_PAID', 'TRANSACTION_DATES', 'TRANSACTION_AMOUNTS_RECEIVED', 'ACCOUNT_BALANCE']\n",
      "‚úÖ Using 'image_file' as image identifier column\n",
      "‚úÖ Ground truth mapping created for 9 images\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">‚úÖ Ground truth loaded for </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">9</span><span style=\"color: #008000; text-decoration-color: #008000\"> images</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m‚úÖ Ground truth loaded for \u001b[0m\u001b[1;32m9\u001b[0m\u001b[32m images\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Ready to process </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">9</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> images</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mReady to process \u001b[0m\u001b[1;32m9\u001b[0m\u001b[1;32m images\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Data directory: /home/jovyan/nfs_share/tod/evaluation_data</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mData directory: \u001b[0m\u001b[36m/home/jovyan/nfs_share/tod/\u001b[0m\u001b[36mevaluation_data\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Ground truth: /home/jovyan/nfs_share/tod/evaluation_data/ground_truth.csv</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mGround truth: \u001b[0m\u001b[36m/home/jovyan/nfs_share/tod/evaluation_data/\u001b[0m\u001b[36mground_truth.csv\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Mode: Evaluation mode</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mMode: Evaluation mode\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1. image_001.png\n",
      "  2. image_002.png\n",
      "  3. image_003.png\n",
      "  4. image_004.png\n",
      "  5. image_005.png\n",
      "  ... and 4 more\n"
     ]
    }
   ],
   "source": [
    "# Discover and filter images - Handle both absolute and relative paths\n",
    "\n",
    "# Convert DATA_DIR to Path and handle absolute/relative paths\n",
    "data_dir = Path(CONFIG['DATA_DIR'])\n",
    "if not data_dir.is_absolute():\n",
    "    # If relative, make it relative to current working directory\n",
    "    data_dir = Path.cwd() / data_dir\n",
    "\n",
    "# Discover images from the resolved data directory\n",
    "all_images = discover_images(str(data_dir))\n",
    "\n",
    "# Conditionally load ground truth only when not in inference-only mode\n",
    "ground_truth = {}\n",
    "if not CONFIG['INFERENCE_ONLY'] and CONFIG['GROUND_TRUTH']:\n",
    "    # Convert GROUND_TRUTH to Path and handle absolute/relative paths\n",
    "    ground_truth_path = Path(CONFIG['GROUND_TRUTH'])\n",
    "    if not ground_truth_path.is_absolute():\n",
    "        # If relative, make it relative to current working directory\n",
    "        ground_truth_path = Path.cwd() / ground_truth_path\n",
    "    \n",
    "    # Load ground truth from the resolved path\n",
    "    ground_truth = load_ground_truth(str(ground_truth_path), verbose=CONFIG['VERBOSE'])\n",
    "    \n",
    "    rprint(f\"[green]‚úÖ Ground truth loaded for {len(ground_truth)} images[/green]\")\n",
    "else:\n",
    "    rprint(\"[cyan]üìã Running in inference-only mode (no ground truth required)[/cyan]\")\n",
    "\n",
    "# Apply filters (only if ground truth is available)\n",
    "if CONFIG['DOCUMENT_TYPES'] and ground_truth:\n",
    "    filtered = []\n",
    "    for img in all_images:\n",
    "        img_name = Path(img).name\n",
    "        if img_name in ground_truth:\n",
    "            doc_type = ground_truth[img_name].get('DOCUMENT_TYPE', '').lower()\n",
    "            if any(dt.lower() in doc_type for dt in CONFIG['DOCUMENT_TYPES']):\n",
    "                filtered.append(img)\n",
    "    all_images = filtered\n",
    "\n",
    "if CONFIG['MAX_IMAGES']:\n",
    "    all_images = all_images[:CONFIG['MAX_IMAGES']]\n",
    "\n",
    "rprint(f\"[bold green]Ready to process {len(all_images)} images[/bold green]\")\n",
    "rprint(f\"[cyan]Data directory: {data_dir}[/cyan]\")\n",
    "if not CONFIG['INFERENCE_ONLY'] and CONFIG['GROUND_TRUTH']:\n",
    "    rprint(f\"[cyan]Ground truth: {ground_truth_path}[/cyan]\")\n",
    "rprint(f\"[cyan]Mode: {'Inference-only' if CONFIG['INFERENCE_ONLY'] else 'Evaluation mode'}[/cyan]\")\n",
    "for i, img in enumerate(all_images[:5], 1):\n",
    "    print(f\"  {i}. {Path(img).name}\")\n",
    "if len(all_images) > 5:\n",
    "    print(f\"  ... and {len(all_images) - 5} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e700afb",
   "metadata": {},
   "source": [
    "## 7. Multi-Stage Batch Processing\n",
    "\n",
    "**Explicit multi-stage processing** (Llama-style transparency):\n",
    "- **Stage 0**: Document Type Classification (INVOICE/RECEIPT/BANK_STATEMENT)\n",
    "- **Stage 1**: Structure Classification (for BANK_STATEMENT only: FLAT/GROUPED)\n",
    "- **Stage 2**: Document-Type-Aware Extraction (using appropriate prompt)\n",
    "\n",
    "**Saves intermediate responses**:\n",
    "- `doctype_classification`: Raw VLM response from document type detection\n",
    "- `structure_classification`: Raw VLM response from structure classification\n",
    "- `extraction_raw`: Raw VLM response from field extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff746da",
   "metadata": {},
   "outputs": [],
   "source": "# Multi-stage adaptive extraction with explicit stages (Llama-style)\nresults = []\nprocessing_times = []\ndoctype_counts = {'INVOICE': 0, 'RECEIPT': 0, 'BANK_STATEMENT': 0}\nstructure_counts = {'flat': 0, 'date_grouped': 0}\n\nrprint(\"\\n[bold green]üöÄ Starting multi-stage adaptive extraction...[/bold green]\\n\")\n\nfor idx, image_path in enumerate(track(all_images, description=\"Processing images\"), 1):\n    image_name = Path(image_path).name\n\n    try:\n        start_time = time.time()\n\n        # Initialize conversation history and load image once\n        messages = []\n        pixel_values = hybrid_processor.load_image(str(image_path))\n\n        # ===================================================================\n        # STAGE 0: Document Type Classification\n        # ===================================================================\n        if CONFIG['VERBOSE']:\n            rprint(f\"\\n[bold blue]Processing [{idx}/{len(all_images)}]: {image_name}[/bold blue]\")\n            rprint(\"[dim]Stage 0: Document type detection...[/dim]\")\n\n        classification_result = hybrid_processor.detect_and_classify_document(\n            str(image_path), verbose=False\n        )\n\n        document_type = classification_result['document_type']\n        doctype_answer = classification_result.get('raw_response', document_type)\n        doctype_counts[document_type] = doctype_counts.get(document_type, 0) + 1\n\n        # ===================================================================\n        # STAGE 1: Structure Classification (for BANK_STATEMENT only)\n        # ===================================================================\n        structure_type = \"N/A\"\n        structure_answer = \"N/A\"\n\n        if document_type == \"BANK_STATEMENT\":\n            if CONFIG['VERBOSE']:\n                rprint(\"[dim]Stage 1: Bank statement structure classification...[/dim]\")\n\n            # Use vision-based structure classifier\n            from common.vision_bank_statement_classifier import classify_bank_statement_structure_vision\n\n            structure_type = classify_bank_statement_structure_vision(\n                str(image_path),\n                model=hybrid_processor,  # Pass the processor (has load_image method)\n                processor=None,  # InternVL3 doesn't use separate processor\n                verbose=False\n            )\n\n            structure_answer = structure_type  # For InternVL3, we have the parsed result directly\n            structure_counts[structure_type] = structure_counts.get(structure_type, 0) + 1\n            prompt_key = f\"internvl3_bank_statement_{structure_type}\"\n        elif document_type == \"INVOICE\":\n            prompt_key = \"internvl3_invoice\"\n        elif document_type == \"RECEIPT\":\n            prompt_key = \"internvl3_receipt\"\n\n        # ===================================================================\n        # STAGE 2: Document-Type-Aware Extraction\n        # ===================================================================\n        if CONFIG['VERBOSE']:\n            rprint(f\"[dim]Stage 2: Extraction using {prompt_key}...[/dim]\")\n\n        extraction_result = hybrid_processor.process_document_aware(\n            str(image_path), classification_result, verbose=False\n        )\n\n        # Extract data and raw response\n        extracted_fields = extraction_result.get('extracted_data', {})\n        extraction_raw = extraction_result.get('raw_response', '')\n\n        # Store comprehensive results (Llama-style structure)\n        result = {\n            'image_file': image_name,\n            'document_type': document_type,\n            'structure_type': structure_type,\n            'prompt_used': prompt_key,\n            'doctype_classification': doctype_answer.strip() if isinstance(doctype_answer, str) else str(doctype_answer),\n            'structure_classification': structure_answer.strip() if isinstance(structure_answer, str) else str(structure_answer),\n            'extraction_raw': extraction_raw,\n            **extracted_fields  # Add all individual field columns\n        }\n        results.append(result)\n\n        processing_time = time.time() - start_time\n        processing_times.append(processing_time)\n\n        structure_display = structure_type if structure_type != 'N/A' else 'direct'\n        rprint(f\"[green]‚úÖ {image_name}: {document_type} ({structure_display}) - {processing_time:.2f}s[/green]\")\n\n    except Exception as e:\n        rprint(f\"[red]‚ùå {image_name}: Error - {e}[/red]\")\n        results.append({\n            'image_file': image_name,\n            'document_type': 'ERROR',\n            'structure_type': 'ERROR',\n            'error': str(e)\n        })\n        processing_times.append(0)\n\n    finally:\n        # Memory cleanup after each image\n        if 'pixel_values' in locals():\n            del pixel_values\n\n        # Clear GPU cache\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n\n        # Periodic garbage collection every 3 images\n        if idx % 3 == 0:\n            gc.collect()\n\nconsole.rule(\"[bold green]Batch Processing Complete[/bold green]\")\n\n# Display summary statistics\nrprint(f\"\\n[bold blue]üìä Document Type Classification Summary:[/bold blue]\")\nrprint(f\"[cyan]  Invoices: {doctype_counts.get('INVOICE', 0)}[/cyan]\")\nrprint(f\"[cyan]  Receipts: {doctype_counts.get('RECEIPT', 0)}[/cyan]\")\nrprint(f\"[cyan]  Bank Statements: {doctype_counts.get('BANK_STATEMENT', 0)}[/cyan]\")\n\nif doctype_counts.get('BANK_STATEMENT', 0) > 0:\n    rprint(f\"\\n[bold blue]üìä Bank Statement Structure Summary:[/bold blue]\")\n    rprint(f\"[cyan]  Flat table: {structure_counts.get('flat', 0)}[/cyan]\")\n    rprint(f\"[cyan]  Date-grouped: {structure_counts.get('date_grouped', 0)}[/cyan]\")"
  },
  {
   "cell_type": "markdown",
   "id": "6ae41782",
   "metadata": {},
   "source": [
    "## 8. Save Results (Llama-Compatible Format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec129450",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">‚úÖ CSV saved to: /home/jovyan/nfs_share/tod/LMM_POC/output/csv/internvl3_adaptive_results_20251021_221057.csv</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m‚úÖ CSV saved to: \u001b[0m\u001b[32m/home/jovyan/nfs_share/tod/LMM_POC/output/csv/\u001b[0m\u001b[32minternvl3_adaptive_results_20251021_221057.csv\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">  Rows: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36m  Rows: \u001b[0m\u001b[1;36m9\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">  Columns: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36m  Columns: \u001b[0m\u001b[1;36m26\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">üìã CSV Columns (Llama-compatible):</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;34müìã CSV Columns \u001b[0m\u001b[1;34m(\u001b[0m\u001b[1;34mLlama-compatible\u001b[0m\u001b[1;34m)\u001b[0m\u001b[1;34m:\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Core columns: image_file, document_type, structure_type, prompt_used, doctype_classification, </span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">structure_classification, extraction_raw</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mCore columns: image_file, document_type, structure_type, prompt_used, doctype_classification, \u001b[0m\n",
       "\u001b[36mstructure_classification, extraction_raw\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Field columns </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">)</span><span style=\"color: #008080; text-decoration-color: #008080\">: DOCUMENT_TYPE, BUSINESS_ABN, SUPPLIER_NAME, BUSINESS_ADDRESS, PAYER_NAME...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mField columns \u001b[0m\u001b[1;36m(\u001b[0m\u001b[1;36m19\u001b[0m\u001b[1;36m)\u001b[0m\u001b[36m: DOCUMENT_TYPE, BUSINESS_ABN, SUPPLIER_NAME, BUSINESS_ADDRESS, PAYER_NAME\u001b[0m\u001b[36m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">‚úÖ JSON saved to: /home/jovyan/nfs_share/tod/LMM_POC/output/csv/internvl3_adaptive_results_20251021_221057.json</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m‚úÖ JSON saved to: \u001b[0m\u001b[32m/home/jovyan/nfs_share/tod/LMM_POC/output/csv/\u001b[0m\u001b[32minternvl3_adaptive_results_20251021_221057.json\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert results to DataFrame (Llama-compatible structure)\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Save to CSV (compatible with model_comparison.ipynb pattern)\n",
    "csv_output = OUTPUT_DIRS['csv'] / f\"internvl3_adaptive_results_{BATCH_TIMESTAMP}.csv\"\n",
    "df.to_csv(csv_output, index=False)\n",
    "\n",
    "rprint(f\"[green]‚úÖ CSV saved to: {csv_output}[/green]\")\n",
    "rprint(f\"[cyan]  Rows: {len(df)}[/cyan]\")\n",
    "rprint(f\"[cyan]  Columns: {len(df.columns)}[/cyan]\")\n",
    "\n",
    "# Show column names to verify Llama-compatible structure\n",
    "rprint(\"\\n[bold blue]üìã CSV Columns (Llama-compatible):[/bold blue]\")\n",
    "core_cols = ['image_file', 'document_type', 'structure_type', 'prompt_used',\n",
    "             'doctype_classification', 'structure_classification', 'extraction_raw']\n",
    "rprint(f\"[cyan]Core columns: {', '.join(core_cols)}[/cyan]\")\n",
    "field_cols = [col for col in df.columns if col not in core_cols and col != 'error']\n",
    "rprint(f\"[cyan]Field columns ({len(field_cols)}): {', '.join(field_cols[:5])}{'...' if len(field_cols) > 5 else ''}[/cyan]\")\n",
    "\n",
    "# Save detailed JSON results\n",
    "json_output = OUTPUT_DIRS['csv'] / f\"internvl3_adaptive_results_{BATCH_TIMESTAMP}.json\"\n",
    "with open(json_output, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "rprint(f\"[green]‚úÖ JSON saved to: {json_output}[/green]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb956f3",
   "metadata": {},
   "source": [
    "## 9. Display Sample Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a1cfb20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">Sample Results</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ \u001b[0m\u001b[1;34mSample Results\u001b[0m\u001b[92m ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   image_file  document_type structure_type                           prompt_used\n",
       "image_001.png        RECEIPT            N/A                     internvl3_receipt\n",
       "image_002.png        RECEIPT            N/A                     internvl3_receipt\n",
       "image_003.png BANK_STATEMENT   date_grouped internvl3_bank_statement_date_grouped\n",
       "image_004.png        RECEIPT            N/A                     internvl3_receipt\n",
       "image_005.png        INVOICE            N/A                     internvl3_invoice\n",
       "image_006.png        INVOICE            N/A                     internvl3_invoice\n",
       "image_007.png        INVOICE            N/A                     internvl3_invoice\n",
       "image_008.png BANK_STATEMENT   date_grouped internvl3_bank_statement_date_grouped\n",
       "image_009.png BANK_STATEMENT   date_grouped internvl3_bank_statement_date_grouped\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   image_file  document_type structure_type                           prompt_used\n",
       "image_001.png        RECEIPT            N/A                     internvl3_receipt\n",
       "image_002.png        RECEIPT            N/A                     internvl3_receipt\n",
       "image_003.png BANK_STATEMENT   date_grouped internvl3_bank_statement_date_grouped\n",
       "image_004.png        RECEIPT            N/A                     internvl3_receipt\n",
       "image_005.png        INVOICE            N/A                     internvl3_invoice\n",
       "image_006.png        INVOICE            N/A                     internvl3_invoice\n",
       "image_007.png        INVOICE            N/A                     internvl3_invoice\n",
       "image_008.png BANK_STATEMENT   date_grouped internvl3_bank_statement_date_grouped\n",
       "image_009.png BANK_STATEMENT   date_grouped internvl3_bank_statement_date_grouped\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display sample results\n",
    "console.rule(\"[bold blue]Sample Results[/bold blue]\")\n",
    "\n",
    "display_cols = ['image_file', 'document_type', 'structure_type', 'prompt_used']\n",
    "rprint(df[display_cols].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90763354",
   "metadata": {},
   "source": [
    "## 10. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0593878f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä DOCUMENT-TYPE-AWARE ADAPTIVE EXTRACTION SUMMARY\n",
      "================================================================================\n",
      "Total images processed: 9\n",
      "Successful extractions: 9\n",
      "Errors: 0\n",
      "\n",
      "Document Type Classification:\n",
      "  Invoices: 3\n",
      "  Receipts: 3\n",
      "  Bank Statements: 3\n",
      "\n",
      "Bank Statement Structure Classification:\n",
      "  Flat table format: 0\n",
      "  Date-grouped format: 3\n",
      "\n",
      "Prompts Used:\n",
      "  internvl3_bank_statement_date_grouped: 3\n",
      "  internvl3_invoice: 3\n",
      "  internvl3_receipt: 3\n",
      "================================================================================\n",
      "\n",
      "üìà Field Extraction Coverage:\n",
      "  DOCUMENT_TYPE: 9/9 (100.0%)\n",
      "  BUSINESS_ABN: 6/9 (66.7%)\n",
      "  SUPPLIER_NAME: 6/9 (66.7%)\n",
      "  BUSINESS_ADDRESS: 6/9 (66.7%)\n",
      "  PAYER_NAME: 6/9 (66.7%)\n",
      "  PAYER_ADDRESS: 6/9 (66.7%)\n",
      "  INVOICE_DATE: 6/9 (66.7%)\n",
      "  LINE_ITEM_DESCRIPTIONS: 9/9 (100.0%)\n",
      "  LINE_ITEM_QUANTITIES: 6/9 (66.7%)\n",
      "  LINE_ITEM_PRICES: 6/9 (66.7%)\n",
      "  LINE_ITEM_TOTAL_PRICES: 6/9 (66.7%)\n",
      "  IS_GST_INCLUDED: 6/9 (66.7%)\n",
      "  GST_AMOUNT: 6/9 (66.7%)\n",
      "  TOTAL_AMOUNT: 6/9 (66.7%)\n",
      "  STATEMENT_DATE_RANGE: 3/9 (33.3%)\n",
      "  TRANSACTION_DATES: 3/9 (33.3%)\n",
      "  TRANSACTION_AMOUNTS_PAID: 3/9 (33.3%)\n",
      "  TRANSACTION_AMOUNTS_RECEIVED: 3/9 (33.3%)\n",
      "  ACCOUNT_BALANCE: 3/9 (33.3%)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüìä DOCUMENT-TYPE-AWARE ADAPTIVE EXTRACTION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total images processed: {len(results)}\")\n",
    "print(f\"Successful extractions: {len([r for r in results if 'error' not in r])}\")\n",
    "print(f\"Errors: {len([r for r in results if 'error' in r])}\")\n",
    "\n",
    "print(\"\\nDocument Type Classification:\")\n",
    "print(f\"  Invoices: {doctype_counts.get('INVOICE', 0)}\")\n",
    "print(f\"  Receipts: {doctype_counts.get('RECEIPT', 0)}\")\n",
    "print(f\"  Bank Statements: {doctype_counts.get('BANK_STATEMENT', 0)}\")\n",
    "\n",
    "if doctype_counts.get('BANK_STATEMENT', 0) > 0:\n",
    "    print(\"\\nBank Statement Structure Classification:\")\n",
    "    print(f\"  Flat table format: {structure_counts.get('flat', 0)}\")\n",
    "    print(f\"  Date-grouped format: {structure_counts.get('date_grouped', 0)}\")\n",
    "\n",
    "print(\"\\nPrompts Used:\")\n",
    "prompt_usage = {}\n",
    "for result in results:\n",
    "    if 'prompt_used' in result:\n",
    "        prompt = result['prompt_used']\n",
    "        prompt_usage[prompt] = prompt_usage.get(prompt, 0) + 1\n",
    "\n",
    "for prompt, count in sorted(prompt_usage.items()):\n",
    "    print(f\"  {prompt}: {count}\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Field extraction statistics\n",
    "if len(df) > 0:\n",
    "    field_cols = [col for col in df.columns if col not in [\n",
    "        'image_file', 'document_type', 'structure_type', 'prompt_used',\n",
    "        'doctype_classification', 'structure_classification', 'extraction_raw', 'error'\n",
    "    ]]\n",
    "\n",
    "    if field_cols:\n",
    "        print(\"\\nüìà Field Extraction Coverage:\")\n",
    "        for field in field_cols:\n",
    "            if field in df.columns:\n",
    "                found_count = df[field].notna().sum()\n",
    "                coverage = (found_count / len(df)) * 100\n",
    "                print(f\"  {field}: {found_count}/{len(df)} ({coverage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26247ed",
   "metadata": {},
   "source": [
    "## 11. View Individual Extraction\n",
    "\n",
    "Change `image_to_view` to view detailed extraction for a specific image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "226a3b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Detailed Extraction: image_003.png\n",
      "================================================================================\n",
      "Document Type: BANK_STATEMENT\n",
      "Structure Type: date_grouped\n",
      "Prompt Used: internvl3_bank_statement_date_grouped\n",
      "\n",
      "Document Type Classification Response:\n",
      "BANK_STATEMENT\n",
      "\n",
      "Structure Classification Response:\n",
      "date_grouped\n",
      "\n",
      "Extraction Result:\n",
      "```json\n",
      "{\n",
      "  \"DOCUMENT_TYPE\": \"BANK_STATEMENT\",\n",
      "  \"STATEMENT_DATE_RANGE\": \"03/05/2025 to 10/05/2025\",\n",
      "  \"LINE_ITEM_DESCRIPTIONS\": \"EFTPOS PURCHASE WOOLWORTHS | INTEREST PAYMENT | REFUND PROCESSED | DIRECT CREDIT SALARY | ATM WITHDRAWAL ANZ ATM\",\n",
      "  \"TRANSACTION_DATES\": \"03/05/2025 | 04/05/2025 | 05/05/2025 | 06/05/2025 | 07/05/2025 | 08/05/2025 | 09/05/2025 | 10/05/2025\",\n",
      "  \"TRANSACTION_AMOUNTS_PAID\": \"288.03 | 22.50 | 114.66 | 3497.47 | 187.59 | 112.50 | 5.16 | 146.72\"\n",
      "}\n",
      "```\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# View detailed extraction for specific image\n",
    "image_to_view = \"image_003.png\"  # Change this\n",
    "\n",
    "result = next((r for r in results if r['image_file'] == image_to_view), None)\n",
    "\n",
    "if result:\n",
    "    print(f\"\\nüîç Detailed Extraction: {image_to_view}\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Document Type: {result['document_type']}\")\n",
    "    print(f\"Structure Type: {result['structure_type']}\")\n",
    "    print(f\"Prompt Used: {result['prompt_used']}\")\n",
    "    print(f\"\\nDocument Type Classification Response:\")\n",
    "    print(result.get('doctype_classification', 'N/A'))\n",
    "    print(f\"\\nStructure Classification Response:\")\n",
    "    print(result.get('structure_classification', 'N/A'))\n",
    "    print(f\"\\nExtraction Result:\")\n",
    "    extraction_display = result.get('extraction_raw', 'N/A')\n",
    "    if len(extraction_display) > 1000:\n",
    "        extraction_display = extraction_display[:1000] + \"\\n...[truncated]...\"\n",
    "    print(extraction_display)\n",
    "    print(\"=\"*80)\n",
    "else:\n",
    "    print(f\"Image {image_to_view} not found in results\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (unified_vision_processor)",
   "language": "python",
   "name": "unified_vision_processor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}