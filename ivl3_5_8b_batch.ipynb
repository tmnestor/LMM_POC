{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Document Extraction with InternVL3.5-8B\n",
    "\n",
    "This notebook uses InternVL3.5-8B for document extraction on GPU hardware.\n",
    "\n",
    "**Model**: InternVL3.5-8B (8.5B parameters: 0.3B vision + 8.2B language)\n",
    "\n",
    "**Requirements**: \n",
    "- `transformers>=4.52.1` (critical for InternVL3.5 support)\n",
    "- PyTorch with CUDA support\n",
    "- Single A100/H100/H200 GPU recommended\n",
    "\n",
    "**Key Features**:\n",
    "- Cascade Reinforcement Learning (Cascade RL) for enhanced reasoning\n",
    "- Visual Resolution Router (ViR) for dynamic resolution adjustment\n",
    "- Improved performance over InternVL3 series\n",
    "\n",
    "This notebook includes steps for:\n",
    "* loading documents\n",
    "* detecting document types  \n",
    "* extracting relevant information\n",
    "* generating reports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Current directory: /home/jovyan/nfs_share/tod/LMM_POC\n",
      "‚úÖ Added /home/jovyan/nfs_share/tod/LMM_POC to sys.path\n",
      "‚úÖ Common module found at: /home/jovyan/nfs_share/tod/LMM_POC/common/__init__.py\n",
      "‚úÖ Path setup complete - proceed to imports\n"
     ]
    }
   ],
   "source": [
    "#Cell 1\n",
    "# Path setup for V100 systems - ensures proper module resolution\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "os.environ['EVALUATION_METHOD'] = 'order_aware_f1'  # or 'f1', 'kieval', 'order_aware_f1', 'correlation'\n",
    "\n",
    "\n",
    "# Get the notebook's directory\n",
    "notebook_path = Path().absolute()\n",
    "print(f\"üìÇ Current directory: {notebook_path}\")\n",
    "\n",
    "# Ensure the project root is in the Python path\n",
    "if str(notebook_path) not in sys.path:\n",
    "    sys.path.insert(0, str(notebook_path))\n",
    "    print(f\"‚úÖ Added {notebook_path} to sys.path\")\n",
    "\n",
    "# Verify common module can be found\n",
    "try:\n",
    "    import common\n",
    "    print(f\"‚úÖ Common module found at: {common.__file__ if hasattr(common, '__file__') else 'built-in'}\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Common module not found: {e}\")\n",
    "    print(\"üìã Current sys.path:\")\n",
    "    for p in sys.path[:5]:  # Show first 5 paths\n",
    "        print(f\"   - {p}\")\n",
    "\n",
    "print(\"‚úÖ Path setup complete - proceed to imports\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Path Setup (V100 Compatibility)\n",
    "\n",
    "**IMPORTANT**: If you encounter import errors on V100 systems, this cell ensures proper module resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'TransformGetItemToIndex' from 'torch._dynamo._trace_wrapped_higher_order_op' (/home/jovyan/.conda/envs/unified_vision_processor/lib/python3.11/site-packages/torch/_dynamo/_trace_wrapped_higher_order_op.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrich\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;28mprint\u001b[39m \u001b[38;5;28;01mas\u001b[39;00m rprint\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrich\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconsole\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Console\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoModel, AutoTokenizer\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Project-specific imports - using absolute imports to avoid conflicts\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcommon\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbatch_analytics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BatchAnalytics\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/unified_vision_processor/lib/python3.11/site-packages/transformers/utils/import_utils.py:2317\u001b[39m, in \u001b[36m_LazyModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   2315\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._class_to_module:\n\u001b[32m   2316\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2317\u001b[39m         module = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2318\u001b[39m         value = \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[32m   2319\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/unified_vision_processor/lib/python3.11/site-packages/transformers/utils/import_utils.py:2347\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   2345\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib.import_module(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m + module_name, \u001b[38;5;28mself\u001b[39m.\u001b[34m__name__\u001b[39m)\n\u001b[32m   2346\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m2347\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/unified_vision_processor/lib/python3.11/site-packages/transformers/utils/import_utils.py:2345\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   2343\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m   2344\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2345\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2346\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   2347\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/unified_vision_processor/lib/python3.11/importlib/__init__.py:126\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m    124\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    125\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/unified_vision_processor/lib/python3.11/site-packages/transformers/models/auto/modeling_auto.py:23\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING, Union\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m logging\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mauto_factory\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     24\u001b[39m     _BaseAutoBackboneClass,\n\u001b[32m     25\u001b[39m     _BaseAutoModelClass,\n\u001b[32m     26\u001b[39m     _LazyAutoMapping,\n\u001b[32m     27\u001b[39m     auto_class_update,\n\u001b[32m     28\u001b[39m )\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfiguration_auto\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CONFIG_MAPPING_NAMES\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/unified_vision_processor/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:43\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfiguration_auto\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoConfig, model_type_to_module_name, replace_list_option_in_docstrings\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_torch_available():\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgeneration\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GenerationMixin\n\u001b[32m     46\u001b[39m logger = logging.get_logger(\u001b[34m__name__\u001b[39m)\n\u001b[32m     48\u001b[39m _T = TypeVar(\u001b[33m\"\u001b[39m\u001b[33m_T\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/unified_vision_processor/lib/python3.11/site-packages/transformers/utils/import_utils.py:2317\u001b[39m, in \u001b[36m_LazyModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   2315\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._class_to_module:\n\u001b[32m   2316\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2317\u001b[39m         module = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2318\u001b[39m         value = \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[32m   2319\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/unified_vision_processor/lib/python3.11/site-packages/transformers/utils/import_utils.py:2347\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   2345\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib.import_module(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m + module_name, \u001b[38;5;28mself\u001b[39m.\u001b[34m__name__\u001b[39m)\n\u001b[32m   2346\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m2347\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/unified_vision_processor/lib/python3.11/site-packages/transformers/utils/import_utils.py:2345\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   2343\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m   2344\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2345\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2346\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   2347\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/unified_vision_processor/lib/python3.11/importlib/__init__.py:126\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m    124\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    125\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/unified_vision_processor/lib/python3.11/site-packages/transformers/generation/utils.py:43\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mintegrations\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdeepspeed\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_deepspeed_zero3_enabled\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mintegrations\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfsdp\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_fsdp_managed_module\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmasking_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_masks_for_generate\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpytorch_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m isin_mps_friendly\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtokenization_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ExtensionsTrie\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/unified_vision_processor/lib/python3.11/site-packages/transformers/masking_utils.py:40\u001b[39m\n\u001b[32m     37\u001b[39m _is_torch_xpu_available = is_torch_xpu_available()\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _is_torch_greater_or_equal_than_2_6:\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_dynamo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_trace_wrapped_higher_order_op\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TransformGetItemToIndex\n\u001b[32m     43\u001b[39m logger = logging.get_logger(\u001b[34m__name__\u001b[39m)\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mand_masks\u001b[39m(*mask_functions: Callable) -> Callable:\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'TransformGetItemToIndex' from 'torch._dynamo._trace_wrapped_higher_order_op' (/home/jovyan/.conda/envs/unified_vision_processor/lib/python3.11/site-packages/torch/_dynamo/_trace_wrapped_higher_order_op.py)"
     ]
    }
   ],
   "source": [
    "#Cell 2\n",
    "# Enable autoreload for module changes\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Standard library imports\n",
    "import sys\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Add current directory to path to ensure proper module resolution\n",
    "notebook_dir = Path.cwd()\n",
    "if str(notebook_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(notebook_dir))\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from IPython.display import display\n",
    "from rich import print as rprint\n",
    "from rich.console import Console\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# Project-specific imports - using absolute imports to avoid conflicts\n",
    "from common.batch_analytics import BatchAnalytics\n",
    "from common.batch_processor import BatchDocumentProcessor\n",
    "from common.batch_reporting import BatchReporter\n",
    "from common.batch_visualizations import BatchVisualizer\n",
    "from common.evaluation_metrics import load_ground_truth\n",
    "from common.extraction_parser import discover_images\n",
    "from common.gpu_optimization import emergency_cleanup\n",
    "from models.document_aware_internvl3_processor import (\n",
    "    DocumentAwareInternVL3HybridProcessor,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ All imports loaded successfully\")\n",
    "print(\"‚úÖ InternVL3 Hybrid Processor imported successfully\") \n",
    "print(\"‚úÖ Batch processing modules imported successfully\")\n",
    "print(f\"üìÇ Working directory: {notebook_dir}\")\n",
    "print(\"üöÄ InternVL3.5-8B: Cascade RL + Visual Resolution Router\")\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pre-emptive Memory Cleanup\n",
    "\n",
    "**CRITICAL for V100**: Run this cell first to prevent OOM errors when switching between models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 3\n",
    "# Pre-emptive V100 Memory Cleanup - Run FIRST to prevent OOM errors\n",
    "rprint(\"[bold red]üßπ PRE-EMPTIVE V100 MEMORY CLEANUP[/bold red]\")\n",
    "rprint(\"[yellow]Clearing any existing model caches before loading...[/yellow]\")\n",
    "rprint(\"[cyan]üí° This prevents OOM errors when switching between models on V100[/cyan]\")\n",
    "\n",
    "# Emergency cleanup to ensure clean slate\n",
    "emergency_cleanup(verbose=True)\n",
    "\n",
    "rprint(\"[green]‚úÖ Memory cleanup complete - ready for model loading[/green]\")\n",
    "rprint(\"[dim]üìã Next: Import modules and configure settings[/dim]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 4\n",
    "# Initialize console and environment configuration\n",
    "console = Console()\n",
    "\n",
    "# Environment-specific base paths\n",
    "ENVIRONMENT_BASES = {\n",
    "    'sandbox': '/home/jovyan/nfs_share/tod',\n",
    "    'efs': '/efs/shared/PoC_data'\n",
    "}\n",
    "base_data_path = ENVIRONMENT_BASES['sandbox']\n",
    "\n",
    "CONFIG = {\n",
    "    # Model settings - InternVL3.5-8B\n",
    "    'MODEL_PATH': '/home/jovyan/nfs_share/models/InternVL3_5-8B',\n",
    "    # Alternative paths:\n",
    "    # 'MODEL_PATH': '/efs/shared/PTM/InternVL3_5-8B',\n",
    "    # 'MODEL_PATH': 'OpenGVLab/InternVL3_5-8B',  # Auto-download from HuggingFace\n",
    "    \n",
    "    # Batch settings\n",
    "    'DATA_DIR': f'{base_data_path}/evaluation_data',\n",
    "    'GROUND_TRUTH': f'{base_data_path}/evaluation_data/ground_truth.csv',\n",
    "    'OUTPUT_BASE': f'{base_data_path}/LMM_POC/output',\n",
    "    'MAX_IMAGES': None,  # None for all, or set limit\n",
    "    'DOCUMENT_TYPES': None,  # None for all, or ['invoice', 'receipt']\n",
    "    'ENABLE_MATH_ENHANCEMENT': False,  # Disable mathematical correction for bank statements\n",
    "    \n",
    "    # Inference and evaluation mode\n",
    "    'INFERENCE_ONLY': False,  # Default: False (evaluation mode)\n",
    "    \n",
    "    # Verbosity control\n",
    "    'VERBOSE': True,\n",
    "    'SHOW_PROMPTS': True,\n",
    "    \n",
    "    # InternVL3.5 optimization settings\n",
    "    'USE_QUANTIZATION': False,  # InternVL3.5-8B runs well without quantization\n",
    "    'DEVICE_MAP': 'auto',\n",
    "    'MAX_NEW_TOKENS': 2000,\n",
    "    'TORCH_DTYPE': 'bfloat16',\n",
    "    'LOW_CPU_MEM_USAGE': True,\n",
    "    # Flash Attention: Supported on A100/H100/H200, not on V100\n",
    "    'USE_FLASH_ATTN': False  # Set to True if using modern GPUs with Flash Attention support\n",
    "}\n",
    "\n",
    "# Make GROUND_TRUTH conditional based on INFERENCE_ONLY mode\n",
    "if CONFIG['INFERENCE_ONLY']:\n",
    "    CONFIG['GROUND_TRUTH'] = None\n",
    "\n",
    "# ============================================================================\n",
    "# PROMPT CONFIGURATION - Explicit file and key mapping\n",
    "# ============================================================================\n",
    "# This configuration controls which prompt files and keys are used for each\n",
    "# document type. You can explicitly override both the file and the key.\n",
    "#\n",
    "# Structure:\n",
    "#   'extraction_files': Maps document types to YAML prompt files\n",
    "#   'extraction_keys': (Optional) Maps document types to specific keys in those files\n",
    "#\n",
    "# If 'extraction_keys' is not specified for a document type, the key will be\n",
    "# derived from the document type name (e.g., 'INVOICE' -> 'invoice')\n",
    "#\n",
    "# For bank statements, structure classification (_flat or _date_grouped) is \n",
    "# automatically appended UNLESS you provide a full key in 'extraction_keys'\n",
    "# ============================================================================\n",
    "\n",
    "PROMPT_CONFIG = {\n",
    "    # Document type detection configuration\n",
    "    'detection_file': 'prompts/document_type_detection.yaml',\n",
    "    'detection_key': 'detection',\n",
    "    \n",
    "    # Extraction prompt file mapping (REQUIRED)\n",
    "    'extraction_files': {\n",
    "        'INVOICE': 'prompts/internvl3_prompts.yaml',\n",
    "        'RECEIPT': 'prompts/internvl3_prompts.yaml', \n",
    "        'BANK_STATEMENT': 'prompts/internvl3_prompts.yaml'\n",
    "    },\n",
    "}\n",
    "\n",
    "# Field list required for DocumentAwareInternVL3HybridProcessor\n",
    "UNIVERSAL_FIELDS = [\n",
    "    \"DOCUMENT_TYPE\", \"BUSINESS_ABN\", \"SUPPLIER_NAME\", \"BUSINESS_ADDRESS\",\n",
    "    \"PAYER_NAME\", \"PAYER_ADDRESS\", \"INVOICE_DATE\", \"STATEMENT_DATE_RANGE\",\n",
    "    \"LINE_ITEM_DESCRIPTIONS\", \"LINE_ITEM_QUANTITIES\", \"LINE_ITEM_PRICES\",\n",
    "    \"LINE_ITEM_TOTAL_PRICES\", \"IS_GST_INCLUDED\", \"GST_AMOUNT\", \"TOTAL_AMOUNT\",\n",
    "    \"TRANSACTION_DATES\", \"TRANSACTION_AMOUNTS_PAID\", \"TRANSACTION_AMOUNTS_RECEIVED\",\n",
    "    \"ACCOUNT_BALANCE\"\n",
    "]\n",
    "\n",
    "print(\"‚úÖ Configuration set up successfully\")\n",
    "print(f\"üìÇ Evaluation data: {CONFIG['DATA_DIR']}\")\n",
    "print(f\"üìä Ground truth: {CONFIG['GROUND_TRUTH']}\")\n",
    "print(f\"ü§ñ Model: InternVL3.5-8B\")\n",
    "print(f\"üìÅ Model path: {CONFIG['MODEL_PATH']}\")\n",
    "print(f\"üìÅ Output base: {CONFIG['OUTPUT_BASE']}\")\n",
    "print(f\"üìã Universal fields: {len(UNIVERSAL_FIELDS)}\")\n",
    "print(f\"üéØ Mode: {'Inference-only' if CONFIG['INFERENCE_ONLY'] else 'Evaluation mode'}\")\n",
    "print(f\"‚öôÔ∏è  Quantization: {'ENABLED (8-bit)' if CONFIG['USE_QUANTIZATION'] else 'DISABLED (full precision)'}\")\n",
    "print(f\"‚ö° Flash Attention: {'ENABLED' if CONFIG['USE_FLASH_ATTN'] else 'DISABLED'}\")\n",
    "print(\"üöÄ InternVL3.5-8B: Enhanced reasoning and dynamic resolution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Output Directory Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 5\n",
    "# Setup output directories - Handle both absolute and relative paths\n",
    "\n",
    "# Convert OUTPUT_BASE to Path and handle absolute/relative paths\n",
    "OUTPUT_BASE = Path(CONFIG['OUTPUT_BASE'])\n",
    "if not OUTPUT_BASE.is_absolute():\n",
    "    # If relative, make it relative to current working directory\n",
    "    OUTPUT_BASE = Path.cwd() / OUTPUT_BASE\n",
    "\n",
    "BATCH_TIMESTAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "OUTPUT_DIRS = {\n",
    "    'base': OUTPUT_BASE,\n",
    "    'batch': OUTPUT_BASE / 'batch_results',\n",
    "    'csv': OUTPUT_BASE / 'csv',\n",
    "    'visualizations': OUTPUT_BASE / 'visualizations',\n",
    "    'reports': OUTPUT_BASE / 'reports'\n",
    "}\n",
    "\n",
    "for dir_path in OUTPUT_DIRS.values():\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Model Loading\n",
    "\n",
    "**InternVL3.5-8B**: Loading the latest InternVL3.5 model with Cascade RL and Visual Resolution Router.\n",
    "\n",
    "**Requirements**: Ensure `transformers>=4.52.1` is installed for InternVL3.5 compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 6\n",
    "# Load InternVL3.5 model using official pattern\n",
    "# Model page: https://huggingface.co/OpenGVLab/InternVL3_5-8B\n",
    "rprint(\"[bold green]Loading InternVL3.5-8B model...[/bold green]\")\n",
    "rprint(\"[cyan]üöÄ Model: InternVL3.5-8B (8.5B params: 0.3B vision + 8.2B language)[/cyan]\")\n",
    "rprint(\"[cyan]üìñ Features: Cascade RL, Visual Resolution Router (ViR)[/cyan]\")\n",
    "\n",
    "try:\n",
    "    # Clear any existing CUDA cache\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        rprint(\"[blue]üßπ CUDA cache cleared[/blue]\")\n",
    "    \n",
    "    # Load model using official InternVL3.5 pattern\n",
    "    rprint(\"[cyan]üì• Loading model with official parameters...[/cyan]\")\n",
    "    model = AutoModel.from_pretrained(\n",
    "        CONFIG['MODEL_PATH'],\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        low_cpu_mem_usage=True,\n",
    "        use_flash_attn=CONFIG['USE_FLASH_ATTN'],\n",
    "        trust_remote_code=True,\n",
    "        device_map=\"auto\"  # Distribute across available GPUs\n",
    "    ).eval()\n",
    "    \n",
    "    # Load tokenizer\n",
    "    rprint(\"[cyan]üì• Loading tokenizer...[/cyan]\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        CONFIG['MODEL_PATH'],\n",
    "        trust_remote_code=True,\n",
    "        use_fast=False\n",
    "    )\n",
    "    \n",
    "    # Set generation parameters\n",
    "    model.config.max_new_tokens = CONFIG['MAX_NEW_TOKENS']\n",
    "    \n",
    "    # Display model information\n",
    "    rprint(\"[green]‚úÖ Model and tokenizer loaded successfully![/green]\")\n",
    "    \n",
    "    # GPU memory check\n",
    "    if torch.cuda.is_available():\n",
    "        device_count = torch.cuda.device_count()\n",
    "        if device_count > 1:\n",
    "            total_allocated = sum(torch.cuda.memory_allocated(i) / 1e9 for i in range(device_count))\n",
    "            total_reserved = sum(torch.cuda.memory_reserved(i) / 1e9 for i in range(device_count))\n",
    "            total_capacity = sum(torch.cuda.get_device_properties(i).total_memory / 1e9 for i in range(device_count))\n",
    "            rprint(f\"[blue]üìä Multi-GPU Memory: {total_allocated:.2f}GB allocated, {total_reserved:.2f}GB reserved, {total_capacity:.0f}GB total[/blue]\")\n",
    "            rprint(f\"[blue]üîç Memory usage: {(total_allocated/total_capacity*100):.1f}%[/blue]\")\n",
    "        else:\n",
    "            allocated = torch.cuda.memory_allocated() / 1e9\n",
    "            reserved = torch.cuda.memory_reserved() / 1e9\n",
    "            total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "            rprint(f\"[blue]üìä GPU Memory: {allocated:.2f}GB allocated, {reserved:.2f}GB reserved, {total:.0f}GB total[/blue]\")\n",
    "            rprint(f\"[blue]üîç Memory usage: {(allocated/total*100):.1f}%[/blue]\")\n",
    "    \n",
    "    # Model parameters\n",
    "    param_count = sum(p.numel() for p in model.parameters())\n",
    "    rprint(f\"[blue]üî¢ Model parameters: {param_count:,}[/blue]\")\n",
    "    rprint(f\"[blue]üéØ Data type: {model.dtype}[/blue]\")\n",
    "    rprint(f\"[blue]üñ•Ô∏è  Device: {next(model.parameters()).device}[/blue]\")\n",
    "    \n",
    "    # Add device map diagnostic\n",
    "    if hasattr(model, 'hf_device_map'):\n",
    "        from collections import Counter\n",
    "        device_distribution = Counter(model.hf_device_map.values())\n",
    "        rprint(f\"[blue]üîÑ Model distribution: {dict(device_distribution)}[/blue]\")\n",
    "    else:\n",
    "        rprint(\"[blue]üìç Single device placement[/blue]\")\n",
    "    \n",
    "    # Initialize the hybrid processor with loaded model components\n",
    "    rprint(\"[cyan]üîß Initializing document-aware processor...[/cyan]\")\n",
    "    hybrid_processor = DocumentAwareInternVL3HybridProcessor(\n",
    "        field_list=UNIVERSAL_FIELDS,\n",
    "        model_path=CONFIG['MODEL_PATH'],\n",
    "        debug=CONFIG['VERBOSE'],\n",
    "        pre_loaded_model=model,\n",
    "        pre_loaded_tokenizer=tokenizer,\n",
    "        prompt_config=PROMPT_CONFIG\n",
    "    )\n",
    "    \n",
    "    rprint(\"[bold green]‚úÖ InternVL3.5-8B ready for document-aware processing[/bold green]\")\n",
    "    rprint(\"[cyan]üéØ Enhanced with Cascade RL for improved reasoning[/cyan]\")\n",
    "    \n",
    "except Exception as e:\n",
    "    rprint(f\"[red]‚ùå Error loading model: {e}[/red]\")\n",
    "    rprint(\"[yellow]üí° Ensure transformers>=4.52.1 is installed[/yellow]\")\n",
    "    rprint(\"[yellow]üí° Check model path is correct[/yellow]\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Image Discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 7\n",
    "# Discover and filter images - Handle both absolute and relative paths\n",
    "\n",
    "# Convert DATA_DIR to Path and handle absolute/relative paths\n",
    "data_dir = Path(CONFIG['DATA_DIR'])\n",
    "if not data_dir.is_absolute():\n",
    "    # If relative, make it relative to current working directory\n",
    "    data_dir = Path.cwd() / data_dir\n",
    "\n",
    "# Discover images from the resolved data directory\n",
    "all_images = discover_images(str(data_dir))\n",
    "\n",
    "# Conditionally load ground truth only when not in inference-only mode\n",
    "ground_truth = {}\n",
    "if not CONFIG['INFERENCE_ONLY'] and CONFIG['GROUND_TRUTH']:\n",
    "    # Convert GROUND_TRUTH to Path and handle absolute/relative paths\n",
    "    ground_truth_path = Path(CONFIG['GROUND_TRUTH'])\n",
    "    if not ground_truth_path.is_absolute():\n",
    "        # If relative, make it relative to current working directory\n",
    "        ground_truth_path = Path.cwd() / ground_truth_path\n",
    "    \n",
    "    # Load ground truth from the resolved path\n",
    "    ground_truth = load_ground_truth(str(ground_truth_path), verbose=CONFIG['VERBOSE'])\n",
    "    \n",
    "    rprint(f\"[green]‚úÖ Ground truth loaded for {len(ground_truth)} images[/green]\")\n",
    "else:\n",
    "    rprint(\"[cyan]üìã Running in inference-only mode (no ground truth required)[/cyan]\")\n",
    "\n",
    "# Apply filters (only if ground truth is available)\n",
    "if CONFIG['DOCUMENT_TYPES'] and ground_truth:\n",
    "    filtered = []\n",
    "    for img in all_images:\n",
    "        img_name = Path(img).name\n",
    "        if img_name in ground_truth:\n",
    "            doc_type = ground_truth[img_name].get('DOCUMENT_TYPE', '').lower()\n",
    "            if any(dt.lower() in doc_type for dt in CONFIG['DOCUMENT_TYPES']):\n",
    "                filtered.append(img)\n",
    "    all_images = filtered\n",
    "\n",
    "if CONFIG['MAX_IMAGES']:\n",
    "    all_images = all_images[:CONFIG['MAX_IMAGES']]\n",
    "\n",
    "rprint(f\"[bold green]Ready to process {len(all_images)} images[/bold green]\")\n",
    "rprint(f\"[cyan]Data directory: {data_dir}[/cyan]\")\n",
    "if not CONFIG['INFERENCE_ONLY'] and CONFIG['GROUND_TRUTH']:\n",
    "    rprint(f\"[cyan]Ground truth: {ground_truth_path}[/cyan]\")\n",
    "rprint(f\"[cyan]Mode: {'Inference-only' if CONFIG['INFERENCE_ONLY'] else 'Evaluation mode'}[/cyan]\")\n",
    "for i, img in enumerate(all_images[:5], 1):\n",
    "    print(f\"  {i}. {Path(img).name}\")\n",
    "if len(all_images) > 5:\n",
    "    print(f\"  ... and {len(all_images) - 5} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Batch Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 8\n",
    "# Initialize batch processor with proven infrastructure (same pattern as llama_batch.ipynb)\n",
    "processor = BatchDocumentProcessor(\n",
    "    model=hybrid_processor,  # InternVL3 hybrid processor (handler)\n",
    "    processor=None,          # Not needed for InternVL3\n",
    "    prompt_config=PROMPT_CONFIG,\n",
    "    ground_truth_csv=CONFIG['GROUND_TRUTH'],  # None in inference-only mode\n",
    "    console=console,\n",
    "    enable_math_enhancement=CONFIG['ENABLE_MATH_ENHANCEMENT']\n",
    ")\n",
    "\n",
    "# Process batch using proven evaluation infrastructure\n",
    "batch_results, processing_times, document_types_found = processor.process_batch(\n",
    "    all_images, verbose=CONFIG['VERBOSE']\n",
    ")\n",
    "\n",
    "# Brief summary\n",
    "rprint(f\"[bold green]‚úÖ Processed {len(batch_results)} images[/bold green]\")\n",
    "rprint(f\"[cyan]Average time: {np.mean(processing_times):.2f}s[/cyan]\")\n",
    "if CONFIG['INFERENCE_ONLY']:\n",
    "    rprint(\"[cyan]üìã Inference-only mode: No accuracy evaluation performed[/cyan]\")\n",
    "else:\n",
    "    avg_accuracy = np.mean([r.get('evaluation', {}).get('overall_accuracy', 0) * 100 for r in batch_results if 'evaluation' in r])\n",
    "    rprint(f\"[cyan]Average accuracy: {avg_accuracy:.1f}%[/cyan]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Generate Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 9\n",
    "# Create model-specific CSV file to match Llama structure\n",
    "# Define all field columns that should be in the CSV (matching Llama)\n",
    "FIELD_COLUMNS = [\n",
    "    'DOCUMENT_TYPE', 'BUSINESS_ABN', 'SUPPLIER_NAME', 'BUSINESS_ADDRESS', \n",
    "    'PAYER_NAME', 'PAYER_ADDRESS', 'INVOICE_DATE', 'LINE_ITEM_DESCRIPTIONS',\n",
    "    'LINE_ITEM_QUANTITIES', 'LINE_ITEM_PRICES', 'LINE_ITEM_TOTAL_PRICES',\n",
    "    'IS_GST_INCLUDED', 'GST_AMOUNT', 'TOTAL_AMOUNT', 'STATEMENT_DATE_RANGE',\n",
    "    'TRANSACTION_DATES', 'TRANSACTION_AMOUNTS_PAID', 'TRANSACTION_AMOUNTS_RECEIVED',\n",
    "    'ACCOUNT_BALANCE'\n",
    "]\n",
    "\n",
    "# Create comprehensive results data matching Llama structure\n",
    "internvl3_5_csv_data = []\n",
    "\n",
    "for i, result in enumerate(batch_results):\n",
    "    # Basic metadata\n",
    "    image_name = Path(result['image_path']).name\n",
    "    doc_type = result.get('document_type', '').lower()\n",
    "    processing_time = processing_times[i] if i < len(processing_times) else 0\n",
    "    \n",
    "    # Extract fields from result\n",
    "    extraction_result = result.get('extraction_result', {})\n",
    "    extracted_fields = extraction_result.get('extracted_data', {})\n",
    "    accuracy_data = result.get('evaluation', {})\n",
    "    \n",
    "    # Count fields\n",
    "    total_fields = len(FIELD_COLUMNS)\n",
    "    found_fields = sum(1 for field in FIELD_COLUMNS if extracted_fields.get(field, 'NOT_FOUND') != 'NOT_FOUND')\n",
    "    field_coverage = (found_fields / total_fields * 100) if total_fields > 0 else 0\n",
    "    \n",
    "    # Handle both inference-only and evaluation modes\n",
    "    if CONFIG['INFERENCE_ONLY'] or accuracy_data.get('inference_only', False):\n",
    "        # Inference-only mode\n",
    "        overall_accuracy = None\n",
    "        fields_extracted = found_fields\n",
    "        fields_matched = 0  # No matching in inference mode\n",
    "        eval_total_fields = total_fields\n",
    "    else:\n",
    "        # Evaluation mode\n",
    "        overall_accuracy = accuracy_data.get('overall_accuracy', 0) * 100 if accuracy_data else 0\n",
    "        fields_extracted = accuracy_data.get('fields_extracted', 0) if accuracy_data else 0\n",
    "        fields_matched = accuracy_data.get('fields_matched', 0) if accuracy_data else 0\n",
    "        eval_total_fields = accuracy_data.get('total_fields', total_fields) if accuracy_data else total_fields\n",
    "    \n",
    "    # Create prompt identifier for InternVL3.5-8B\n",
    "    prompt_used = f\"internvl3_5_8b_{doc_type}\" if doc_type else \"internvl3_5_8b_unknown\"\n",
    "    \n",
    "    # Create row data\n",
    "    row_data = {\n",
    "        'image_file': image_name,\n",
    "        'image_name': image_name,\n",
    "        'document_type': doc_type,\n",
    "        'processing_time': processing_time,\n",
    "        'field_count': eval_total_fields,\n",
    "        'found_fields': fields_extracted,\n",
    "        'field_coverage': field_coverage,\n",
    "        'prompt_used': prompt_used,\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'overall_accuracy': overall_accuracy,\n",
    "        'fields_extracted': fields_extracted,\n",
    "        'fields_matched': fields_matched,\n",
    "        'total_fields': eval_total_fields,\n",
    "        'inference_only': CONFIG['INFERENCE_ONLY'],\n",
    "        'model_version': 'InternVL3.5-8B'\n",
    "    }\n",
    "    \n",
    "    # Add all field values\n",
    "    for field in FIELD_COLUMNS:\n",
    "        row_data[field] = extracted_fields.get(field, 'NOT_FOUND')\n",
    "    \n",
    "    internvl3_5_csv_data.append(row_data)\n",
    "\n",
    "# Create DataFrame and save\n",
    "internvl3_5_df = pd.DataFrame(internvl3_5_csv_data)\n",
    "internvl3_5_csv_path = OUTPUT_DIRS['csv'] / f\"internvl3_5_8b_batch_results_{BATCH_TIMESTAMP}.csv\"\n",
    "internvl3_5_df.to_csv(internvl3_5_csv_path, index=False)\n",
    "\n",
    "rprint(\"[bold green]‚úÖ InternVL3.5-8B model-specific CSV exported:[/bold green]\")\n",
    "rprint(f\"[cyan]üìÑ File: {internvl3_5_csv_path}[/cyan]\")\n",
    "rprint(f\"[cyan]üìä Structure: {len(internvl3_5_df)} rows √ó {len(internvl3_5_df.columns)} columns[/cyan]\")\n",
    "rprint(\"[cyan]üîó Compatible with model_comparison.ipynb pattern: *internvl3_5_8b*batch*results*.csv[/cyan]\")\n",
    "\n",
    "# Display sample of the exported data (conditional based on mode)\n",
    "if CONFIG['INFERENCE_ONLY']:\n",
    "    rprint(\"\\n[bold blue]üìã Sample exported data (inference-only mode):[/bold blue]\")\n",
    "    sample_cols = ['image_file', 'document_type', 'processing_time', 'found_fields', 'field_coverage', 'model_version']\n",
    "    if len(internvl3_5_df) > 0:\n",
    "        display(internvl3_5_df[sample_cols].head(3))\n",
    "    else:\n",
    "        rprint(\"[yellow]‚ö†Ô∏è No data to display[/yellow]\")\n",
    "else:\n",
    "    rprint(\"\\n[bold blue]üìã Sample exported data (first 3 rows, key columns):[/bold blue]\")\n",
    "    sample_cols = ['image_file', 'document_type', 'overall_accuracy', 'processing_time', 'found_fields', 'model_version']\n",
    "    if len(internvl3_5_df) > 0:\n",
    "        display(internvl3_5_df[sample_cols].head(3))\n",
    "    else:\n",
    "        rprint(\"[yellow]‚ö†Ô∏è No data to display[/yellow]\")\n",
    "\n",
    "    # Verification: Show accuracy values to confirm they're correct (evaluation mode only)\n",
    "    rprint(\"\\n[bold blue]üîç Accuracy verification:[/bold blue]\")\n",
    "    for i, result in enumerate(batch_results[:3]):  # Show first 3\n",
    "        evaluation = result.get('evaluation', {})\n",
    "        original_accuracy = evaluation.get('overall_accuracy', 0)\n",
    "        percentage_accuracy = original_accuracy * 100\n",
    "        rprint(f\"  {result['image_name']}: {original_accuracy:.4f} ‚Üí {percentage_accuracy:.2f}%\")\n",
    "\n",
    "# Create analytics using proven infrastructure (same pattern as llama_batch.ipynb)\n",
    "analytics = BatchAnalytics(batch_results, processing_times)\n",
    "\n",
    "# Generate and save DataFrames using established patterns\n",
    "saved_files, df_results, df_summary, df_doctype_stats, df_field_stats = analytics.save_all_dataframes(\n",
    "    OUTPUT_DIRS['csv'], BATCH_TIMESTAMP, verbose=CONFIG['VERBOSE']\n",
    ")\n",
    "\n",
    "# Display key results based on mode\n",
    "rprint(\"\\n[bold blue]üìä InternVL3.5-8B Results Summary[/bold blue]\")\n",
    "if CONFIG['INFERENCE_ONLY']:\n",
    "    rprint(\"[cyan]üìã Running in inference-only mode - no accuracy metrics available[/cyan]\")\n",
    "    # Show extraction statistics instead\n",
    "    rprint(f\"[cyan]‚úÖ Total images processed: {len(batch_results)}[/cyan]\")\n",
    "    rprint(f\"[cyan]‚úÖ Average fields found: {internvl3_5_df['found_fields'].mean():.1f}[/cyan]\")\n",
    "    rprint(f\"[cyan]‚úÖ Average field coverage: {internvl3_5_df['field_coverage'].mean():.1f}%[/cyan]\")\n",
    "else:\n",
    "    display(df_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Export Model-Specific CSV for Comparison\n",
    "\n",
    "Create InternVL3 NON-QUANTIZED specific CSV file that matches Llama structure for model comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 10\n",
    "# Create visualizations using proven infrastructure (same pattern as llama_batch.ipynb)\n",
    "visualizer = BatchVisualizer()\n",
    "\n",
    "viz_files = visualizer.create_all_visualizations(\n",
    "    df_results, \n",
    "    df_doctype_stats,\n",
    "    OUTPUT_DIRS['visualizations'],\n",
    "    BATCH_TIMESTAMP,\n",
    "    show=False  # Disable display to reduce output\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Generate Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 11\n",
    "# Generate reports using proven infrastructure (same pattern as llama_batch.ipynb)\n",
    "reporter = BatchReporter(\n",
    "    batch_results, \n",
    "    processing_times,\n",
    "    document_types_found,\n",
    "    BATCH_TIMESTAMP\n",
    ")\n",
    "\n",
    "# Save all reports using CONFIG verbose setting\n",
    "report_files = reporter.save_all_reports(\n",
    "    OUTPUT_DIRS,\n",
    "    df_results,\n",
    "    df_summary,\n",
    "    df_doctype_stats,\n",
    "    CONFIG['MODEL_PATH'],\n",
    "    {\n",
    "        'data_dir': CONFIG['DATA_DIR'],\n",
    "        'ground_truth': CONFIG['GROUND_TRUTH'],\n",
    "        'max_images': CONFIG['MAX_IMAGES'],\n",
    "        'document_types': CONFIG['DOCUMENT_TYPES']\n",
    "    },\n",
    "    {\n",
    "        'use_quantization': CONFIG['USE_QUANTIZATION'],\n",
    "        'device_map': CONFIG['DEVICE_MAP'],\n",
    "        'max_new_tokens': CONFIG['MAX_NEW_TOKENS'],\n",
    "        'torch_dtype': CONFIG['TORCH_DTYPE'],\n",
    "        'low_cpu_mem_usage': CONFIG['LOW_CPU_MEM_USAGE']\n",
    "    },\n",
    "    verbose=CONFIG['VERBOSE']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Display Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 12\n",
    "# Display final summary\n",
    "console.rule(\"[bold green]InternVL3.5-8B Batch Processing Complete[/bold green]\")\n",
    "\n",
    "total_images = len(batch_results)\n",
    "successful = len([r for r in batch_results if 'error' not in r])\n",
    "avg_accuracy = df_results['overall_accuracy'].mean() if len(df_results) > 0 else 0\n",
    "\n",
    "rprint(f\"[bold green]‚úÖ Processed: {total_images} images[/bold green]\")\n",
    "rprint(f\"[cyan]Success Rate: {(successful/total_images*100):.1f}%[/cyan]\")\n",
    "rprint(f\"[cyan]Average Accuracy: {avg_accuracy:.2f}%[/cyan]\")\n",
    "rprint(f\"[cyan]Output: {OUTPUT_BASE}[/cyan]\")\n",
    "rprint(\"[blue]üöÄ Model: InternVL3.5-8B with Cascade RL and ViR[/blue]\")\n",
    "\n",
    "# Performance assessment\n",
    "if successful == total_images and avg_accuracy > 50:\n",
    "    rprint(\"\\n[bold green]üéâ SUCCESS: InternVL3.5-8B processing completed successfully![/bold green]\")\n",
    "    rprint(\"[green]‚úÖ Enhanced reasoning with Cascade RL is working[/green]\")\n",
    "    rprint(\"[green]‚úÖ Dynamic resolution adjustment with ViR is active[/green]\")\n",
    "elif successful < total_images:\n",
    "    rprint(\"\\n[bold red]‚ùå FAILURE: Processing errors occurred[/bold red]\")\n",
    "    rprint(\"[red]üîç Review error logs for diagnostic information[/red]\")\n",
    "elif avg_accuracy < 30:\n",
    "    rprint(\"\\n[bold yellow]‚ö†Ô∏è POOR PERFORMANCE: Low accuracy detected[/bold yellow]\")\n",
    "    rprint(\"[yellow]üîç Review extraction results for quality issues[/yellow]\")\n",
    "else:\n",
    "    rprint(\"\\n[bold blue]üìä MIXED RESULTS: Partially working[/bold blue]\")\n",
    "    rprint(\"[blue]üîç Review individual results to assess performance[/blue]\")\n",
    "\n",
    "# Document type distribution\n",
    "if document_types_found:\n",
    "    rprint(\"\\n[bold blue]üìã Document Type Distribution:[/bold blue]\")\n",
    "    for doc_type, count in document_types_found.items():\n",
    "        percentage = (count / total_images * 100) if total_images > 0 else 0\n",
    "        rprint(f\"[cyan]  {doc_type}: {count} documents ({percentage:.1f}%)[/cyan]\")\n",
    "\n",
    "# Display dashboard if available\n",
    "dashboard_files = list(OUTPUT_DIRS['visualizations'].glob(f\"dashboard_{BATCH_TIMESTAMP}.png\"))\n",
    "if dashboard_files:\n",
    "    from IPython.display import Image, display\n",
    "    dashboard_path = dashboard_files[0]\n",
    "    rprint(\"\\n[bold blue]üìä Visual Dashboard:[/bold blue]\")\n",
    "    display(Image(str(dashboard_path)))\n",
    "else:\n",
    "    rprint(f\"\\n[yellow]‚ö†Ô∏è Dashboard not found in {OUTPUT_DIRS['visualizations']}[/yellow]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Failed Extractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 13\n",
    "# Calculate zero accuracy extractions\n",
    "zero_accuracy_count = 0\n",
    "zero_accuracy_images = []\n",
    "total_evaluated = 0\n",
    "\n",
    "for result in batch_results:\n",
    "    # Check if evaluation data exists (not inference-only mode)\n",
    "    evaluation = result.get(\"evaluation\", {})\n",
    "\n",
    "    if evaluation and not evaluation.get(\"inference_only\", False):\n",
    "        total_evaluated += 1\n",
    "        accuracy = evaluation.get(\"overall_accuracy\", 0)\n",
    "\n",
    "        if accuracy == 0.0:\n",
    "            zero_accuracy_count += 1\n",
    "            zero_accuracy_images.append(\n",
    "                {\n",
    "                    \"image_name\": result.get(\"image_name\", \"unknown\"),\n",
    "                    \"document_type\": result.get(\"document_type\", \"unknown\"),\n",
    "                    \"fields_extracted\": evaluation.get(\"fields_extracted\", 0),\n",
    "                    \"total_fields\": evaluation.get(\"total_fields\", 0),\n",
    "                }\n",
    "            )\n",
    "\n",
    "# Display results\n",
    "if total_evaluated > 0:\n",
    "    console.rule(\"[bold red]Zero Accuracy Analysis[/bold red]\")\n",
    "\n",
    "    rprint(f\"[cyan]Total documents evaluated: {total_evaluated}[/cyan]\")\n",
    "    rprint(f\"[red]Documents with 0% accuracy: {zero_accuracy_count}[/red]\")\n",
    "\n",
    "    if zero_accuracy_count > 0:\n",
    "        percentage = (zero_accuracy_count / total_evaluated) * 100\n",
    "        rprint(f\"[red]Zero accuracy rate: {percentage:.1f}%[/red]\")\n",
    "\n",
    "        rprint(\"\\n[bold red]Documents with 0% Accuracy:[/bold red]\")\n",
    "        for i, img_info in enumerate(zero_accuracy_images, 1):\n",
    "            rprint(f\"  {i}. {img_info['image_name']} ({img_info['document_type']})\")\n",
    "            rprint(\n",
    "                f\"     Fields extracted: {img_info['fields_extracted']}/{img_info['total_fields']}\"\n",
    "            )\n",
    "    else:\n",
    "        rprint(\n",
    "            \"[green]‚úÖ No documents with 0% accuracy - all extractions had some success![/green]\"\n",
    "        )\n",
    "else:\n",
    "    rprint(\n",
    "        \"[yellow]‚ö†Ô∏è Running in inference-only mode - no accuracy metrics available[/yellow]\"\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (unified_vision_processor)",
   "language": "python",
   "name": "unified_vision_processor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
