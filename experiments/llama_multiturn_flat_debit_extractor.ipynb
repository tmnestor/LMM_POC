{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama 3.2 Vision Multi-Turn Flat Debit Extractor\n",
    "\n",
    "- This notebook demonstrates multi-turn conversational extraction using Llama 3.2 Vision.\n",
    "- Uses the `chat_with_mllm` pattern to handle multiple turns of conversation with Flat Bank Statement images.\n",
    "- Each turn focuses on a specific extraction task, such as initial data extraction, selecting specific columns, extracting debit amounts, and filtering by date range.\n",
    "\n",
    "**Reference**: [Chat with Your Images Using Multimodal LLMs](https://medium.com/data-science/chat-with-your-images-using-multimodal-llms-60af003e8bfa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add project root to path for common/ imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, MllamaForConditionalGeneration\n",
    "from transformers.image_utils import load_image\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-emptive Memory Cleanup\n",
    "\n",
    "Optional GPU memory cleanup to prevent OOM errors when switching between models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Pre-emptive memory cleanup (useful when switching models)\n",
    "try:\n",
    "    from common.gpu_optimization import emergency_cleanup\n",
    "    print(\"üßπ Clearing GPU memory...\")\n",
    "    emergency_cleanup(verbose=False)\n",
    "    print(\"‚úÖ Memory cleanup complete\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è GPU optimization module not available - skipping cleanup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Random Seed for Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.reproducibility import set_seed\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"/home/jovyan/shared_PTM/Llama-3.2-11B-Vision-Instruct\"\n",
    "\n",
    "print(\"üîß Loading Llama-3.2-Vision model...\")\n",
    "\n",
    "from common.llama_model_loader_robust import load_llama_model_robust\n",
    "\n",
    "model, processor = load_llama_model_robust(\n",
    "    model_path=model_id,\n",
    "    use_quantization=False,\n",
    "    device_map='auto',\n",
    "    max_new_tokens=2000,\n",
    "    torch_dtype='bfloat16',\n",
    "    low_cpu_mem_usage=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Add tie_weights() call\n",
    "try:\n",
    "    model.tie_weights()\n",
    "    print(\"‚úÖ Model weights tied successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è tie_weights() warning: {e}\")\n",
    "\n",
    "print(\"‚úÖ Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Manual Memory Cleanup\n",
    "\n",
    "Run this cell if you experience memory issues during the conversation. Not needed for normal operation on H200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Run this cell if you experience memory issues during conversation\n",
    "# import gc\n",
    "\n",
    "# print(\"üßπ Manual memory cleanup...\")\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# # Show memory status\n",
    "# if torch.cuda.is_available():\n",
    "#     for i in range(torch.cuda.device_count()):\n",
    "#         allocated = torch.cuda.memory_allocated(i) / 1e9\n",
    "#         reserved = torch.cuda.memory_reserved(i) / 1e9\n",
    "#         print(f\"   GPU {i}: {allocated:.2f}GB allocated, {reserved:.2f}GB reserved\")\n",
    "# print(\"‚úÖ Cleanup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define chat_with_mllm Function\n",
    "\n",
    "This function encapsulates the multi-turn conversation pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.llama_multiturn_chat import chat_with_mllm\n",
    "\n",
    "print(\"‚úÖ chat_with_mllm function imported from common.llama_multiturn_chat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Conversation Prompts\n",
    "\n",
    "All prompts are defined upfront for easy modification and review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversation prompts dictionary\n",
    "CONVERSATION_PROMPTS = {\n",
    "    \"turn_0_identify_headers\": \"\"\"Look at the bank statement image.\n",
    "\n",
    "Find the transaction table where transactions are listed.\n",
    "\n",
    "At the top of this table, there is a row of column headers.\n",
    "\n",
    "Your task: List ALL the column headers from this table, starting from the left edge and moving to the right edge.\n",
    "\n",
    "IMPORTANT: The leftmost column is usually \"Date\" or \"Date of Transaction\". Start there and read every header moving right.\n",
    "\n",
    "Include EVERY header you see. Do not skip any.\n",
    "\n",
    "Output Format:\n",
    "- TRANSACTION TABLE HEADERS (reading left to right):\n",
    "- [Write ONLY the headers you actually see in the image, separated by commas]\n",
    "- If no table headers visible, write: NO_HEADERS\"\"\",\n",
    "    \n",
    "    \"turn_1_initial_extraction\": \"\"\"Step 1: Extract ONLY the table of transaction data from this Australian bank statement in markdown format.\"\"\",\n",
    "    \n",
    "    \"turn_2_select_columns\": \"\"\"STEP 2: From the table you extracted in STEP 1, extract only the \"Date | Description | Withdrawal\" columns\"\"\",\n",
    "    \n",
    "    \"turn_3_extract_debits\": \"\"\"STEP 3: From the table you extracted in STEP 2, remove any row NOT showing an amount (i.e. having an empty cell) in the \"Withdrawal\" column.\"\"\",\n",
    "    \n",
    "    \"turn_4_date_range\": \"\"\"Extract earliest date and the latest date from the \"Date\" colum. Express your answer in the format \"STATEMENT_DATE_RANGE: dd/mm/yyyy - dd/mm/yyyy\" \"\"\"\n",
    "}\n",
    "\n",
    "def display_prompts(prompts_dict):\n",
    "    \"\"\"Display all conversation prompts in a readable format.\"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"CONVERSATION PROMPTS\")\n",
    "    print(\"=\" * 70)\n",
    "    for i, (key, prompt) in enumerate(prompts_dict.items(), 1):\n",
    "        # Format the key for display\n",
    "        turn_name = key.replace(\"_\", \" \").title()\n",
    "        print(f\"\\n{i}. {turn_name}\")\n",
    "        print(\"-\" * 70)\n",
    "        # Truncate long prompts for preview\n",
    "        preview = prompt if len(prompt) <= 200 else prompt[:197] + \"...\"\n",
    "        print(f\"{preview}\")\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(f\"Total prompts defined: {len(prompts_dict)}\")\n",
    "\n",
    "# Display the prompts\n",
    "display_prompts(CONVERSATION_PROMPTS)\n",
    "print(\"\\n‚úÖ Conversation prompts defined and ready to use\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image path\n",
    "imageName = \"/home/jovyan/_LMM_POC/evaluation_data/image_003.png\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Uncomment to enable preprocessing\n",
    "# from common.image_preprocessing import enhance_for_llama, preprocess_statement_for_llama, enhance_statement_quality\n",
    "# from PIL import Image\n",
    "# import tempfile\n",
    "\n",
    "# # Choose ONE preprocessing approach:\n",
    "\n",
    "# # Option 1: Light enhancement (recommended for high-quality scans)\n",
    "# # preprocessed_img = enhance_statement_quality(imageName)\n",
    "\n",
    "# # Option 2: Moderate enhancement (upscaling + sharpness + contrast)\n",
    "# # preprocessed_img = enhance_for_llama(imageName)\n",
    "\n",
    "# # Option 3: Aggressive preprocessing (denoise + binarize + remove lines)\n",
    "# # preprocessed_img = preprocess_statement_for_llama(imageName)\n",
    "\n",
    "# # Save preprocessed image to temporary file and update imageName\n",
    "# # with tempfile.NamedTemporaryFile(suffix='.png', delete=False) as tmp:\n",
    "# #     preprocessed_img.save(tmp.name)\n",
    "# #     imageName = tmp.name\n",
    "# #     print(f\"‚úÖ Using preprocessed image: {imageName}\")\n",
    "# #     display(preprocessed_img)\n",
    "\n",
    "print(\"‚è≠Ô∏è  Skipping preprocessing - using original image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPTIONAL: Image Preprocessing\n",
    "\n",
    "**Experimental:** Test whether preprocessing improves extraction accuracy.\n",
    "\n",
    "Available preprocessing functions:\n",
    "- `enhance_for_llama()` - Upscale, sharpen, increase contrast\n",
    "- `preprocess_statement_for_llama()` - Denoise, binarize, remove table lines\n",
    "- `enhance_statement_quality()` - Moderate enhancement for bank statements\n",
    "\n",
    "**Note:** Modern VLMs are trained on natural images. Preprocessing may help with low-quality scans but could hurt performance on high-quality images. Test both approaches to see what works best for your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turn 0: Identify Table Headers\n",
    "\n",
    "First, identify all column headers in the transaction table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize conversation\n",
    "messages = []\n",
    "images = []\n",
    "\n",
    "print(\"üì∏ Processing bank statement image...\")\n",
    "print(f\"üìù Using prompt: turn_0_identify_headers\")\n",
    "\n",
    "response0, messages, images = chat_with_mllm(\n",
    "    model, \n",
    "    processor, \n",
    "    CONVERSATION_PROMPTS[\"turn_0_identify_headers\"],\n",
    "    images_path=[imageName],\n",
    "    do_sample=False,\n",
    "    max_new_tokens=500,\n",
    "    show_image=True,\n",
    "    messages=messages,\n",
    "    images=images\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TURN 0 - IDENTIFY TABLE HEADERS:\")\n",
    "print(\"=\" * 60)\n",
    "print(response0)\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turn 1: Extract Transaction Table\n",
    "\n",
    "Extract the complete transaction table in markdown format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"üìù Using prompt: turn_1_initial_extraction\")\n",
    "\n",
    "response1, messages, images = chat_with_mllm(\n",
    "    model, \n",
    "    processor, \n",
    "    CONVERSATION_PROMPTS[\"turn_1_initial_extraction\"],\n",
    "    messages=messages,\n",
    "    images=images,\n",
    "    do_sample=False,\n",
    "    max_new_tokens=3000\n",
    ")\n",
    "\n",
    "display(Markdown(response1))\n",
    "\n",
    "# Save initial extraction\n",
    "Path(\"llama_debit_extractor_initial.txt\").write_text(response1)\n",
    "print(\"\\n‚úÖ Initial extraction saved to llama_debit_extractor_initial.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turn 2: Select Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"üìù Using prompt: turn_2_select_columns\")\n",
    "\n",
    "response2, messages, images = chat_with_mllm(\n",
    "    model, processor,\n",
    "    CONVERSATION_PROMPTS[\"turn_2_select_columns\"],\n",
    "    messages=messages, \n",
    "    images=images,\n",
    "    max_new_tokens=2000\n",
    ")\n",
    "\n",
    "display(Markdown(response2))\n",
    "\n",
    "# print(\"\\n\" + \"=\" * 60)\n",
    "# print(\"TURN 2 - SELECT COLUMNS:\")\n",
    "# print(\"=\" * 60)\n",
    "# print(response2)\n",
    "# print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turn 3: Extract Debit/Withdrawal Amounts Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"üìù Using prompt: turn_3_extract_debits\")\n",
    "\n",
    "response3, messages, images = chat_with_mllm(\n",
    "    model, processor,\n",
    "    CONVERSATION_PROMPTS[\"turn_3_extract_debits\"],\n",
    "    messages=messages,\n",
    "    images=images,\n",
    "    max_new_tokens=2000\n",
    ")\n",
    "\n",
    "display(Markdown(response3))\n",
    "\n",
    "# print(\"\\n\" + \"=\" * 60)\n",
    "# print(\"TURN 3 - DEBIT AMOUNTS ONLY:\")\n",
    "# print(\"=\" * 60)\n",
    "# print(response3)\n",
    "# print(\"=\" * 60)\n",
    "\n",
    "# Save debit amounts\n",
    "Path(\"llama_debit_amounts.txt\").write_text(response3)\n",
    "print(\"\\n‚úÖ Debit amounts saved to llama_debit_amounts.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turn 4: Date Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"üìù Using prompt: turn_4_date_range\")\n",
    "\n",
    "response4, messages, images = chat_with_mllm(\n",
    "    model, processor,\n",
    "    CONVERSATION_PROMPTS[\"turn_4_date_range\"],\n",
    "    messages=messages,\n",
    "    images=images,\n",
    "    max_new_tokens=50\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TURN 4 - DATE RANGE:\")\n",
    "print(\"=\" * 60)\n",
    "print(response4)\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug: View Conversation Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç Current conversation structure:\")\n",
    "print(\"=\" * 60)\n",
    "for i, msg in enumerate(messages, 1):\n",
    "    print(f\"\\nMessage {i} ({msg['role']}):\")\n",
    "    for content in msg['content']:\n",
    "        if content['type'] == 'text':\n",
    "            preview = content['text'][:100] + \"...\" if len(content['text']) > 100 else content['text']\n",
    "            print(f\"  [text]: {preview}\")\n",
    "        else:\n",
    "            print(f\"  [{content['type']}]\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nüìä Total messages: {len(messages)}\")\n",
    "print(f\"üìä User messages: {sum(1 for m in messages if m['role'] == 'user')}\")\n",
    "print(f\"üìä Assistant messages: {sum(1 for m in messages if m['role'] == 'assistant')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Full Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the entire conversation to a file\n",
    "output_path = Path(\"llama_multiturn_debit_conversation.txt\")\n",
    "\n",
    "with output_path.open(\"w\", encoding=\"utf-8\") as text_file:\n",
    "    text_file.write(\"=\" * 60 + \"\\n\")\n",
    "    text_file.write(\"MULTI-TURN DEBIT EXTRACTION CONVERSATION\\n\")\n",
    "    text_file.write(\"Llama-3.2-Vision-11B\\n\")\n",
    "    text_file.write(\"=\" * 60 + \"\\n\\n\")\n",
    "    \n",
    "    for i, msg in enumerate(messages, 1):\n",
    "        role = msg[\"role\"].upper()\n",
    "        text_file.write(f\"\\n{'-' * 60}\\n\")\n",
    "        text_file.write(f\"MESSAGE {i} - {role}\\n\")\n",
    "        text_file.write(f\"{'-' * 60}\\n\\n\")\n",
    "        \n",
    "        for content in msg[\"content\"]:\n",
    "            if content[\"type\"] == \"text\":\n",
    "                text_file.write(content[\"text\"] + \"\\n\")\n",
    "            elif content[\"type\"] == \"image\":\n",
    "                text_file.write(\"[IMAGE]\\n\")\n",
    "    \n",
    "    text_file.write(\"\\n\" + \"=\" * 60 + \"\\n\")\n",
    "    text_file.write(f\"Total messages: {len(messages)}\\n\")\n",
    "    text_file.write(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "print(f\"‚úÖ Full conversation saved to: {output_path}\")\n",
    "print(f\"üìä File size: {output_path.stat().st_size} bytes\")\n",
    "print(f\"üí¨ Total messages in conversation: {len(messages)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
