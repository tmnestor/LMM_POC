{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Llama 3.2 Vision Single-Column Multi-Turn Extraction\n",
        "\n",
        "This notebook implements a **single-column extraction strategy** to avoid column merging issues observed with standard table extraction.\n",
        "\n",
        "**Strategy:**\n",
        "- 5 separate conversation turns, each extracting ONE column only\n",
        "- Turn 1: Date column\n",
        "- Turn 2: Transaction/Description column\n",
        "- Turn 3: Debit column\n",
        "- Turn 4: Credit column\n",
        "- Turn 5: Balance column\n",
        "\n",
        "**Reference:** Based on OCRBench v2 findings that VLMs struggle with spatial column reasoning [arXiv:2412.20662v2]"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Add project root to path for common/ imports\n",
        "import sys\n",
        "from pathlib import Path\n",
        "sys.path.insert(0, str(Path.cwd().parent))\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "from PIL import Image\n",
        "from transformers import AutoProcessor, MllamaForConditionalGeneration\n",
        "from transformers.image_utils import load_image\n",
        "from tqdm.notebook import tqdm\n",
        "from IPython.display import display, Markdown"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pre-emptive Memory Cleanup\n",
        "\n",
        "Optional GPU memory cleanup to prevent OOM errors when switching between models."
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Optional: Pre-emptive memory cleanup (useful when switching models)\n",
        "try:\n",
        "    from common.gpu_optimization import emergency_cleanup\n",
        "    print(\"🧹 Clearing GPU memory...\")\n",
        "    emergency_cleanup(verbose=False)\n",
        "    print(\"✅ Memory cleanup complete\")\n",
        "except ImportError:\n",
        "    print(\"⚠️ GPU optimization module not available - skipping cleanup\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Set Random Seed for Reproducibility"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from common.reproducibility import set_seed\n",
        "set_seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Model"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model_id = \"/home/jovyan/shared_PTM/Llama-3.2-11B-Vision-Instruct\"\n",
        "\n",
        "print(\"🔧 Loading Llama-3.2-Vision model...\")\n",
        "\n",
        "from common.llama_model_loader_robust import load_llama_model_robust\n",
        "\n",
        "model, processor = load_llama_model_robust(\n",
        "    model_path=model_id,\n",
        "    use_quantization=False,\n",
        "    device_map='auto',\n",
        "    max_new_tokens=2000,\n",
        "    torch_dtype='bfloat16',\n",
        "    low_cpu_mem_usage=True,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Add tie_weights() call\n",
        "try:\n",
        "    model.tie_weights()\n",
        "    print(\"✅ Model weights tied successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"⚠️ tie_weights() warning: {e}\")\n",
        "\n",
        "print(\"✅ Model loaded successfully!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define chat_with_mllm Function\n",
        "\n",
        "This function encapsulates the multi-turn conversation pattern."
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def chat_with_mllm(model, processor, prompt, images_path=[], do_sample=False, \n",
        "                   temperature=0.1, show_image=False, max_new_tokens=2000, \n",
        "                   messages=[], images=[]):\n",
        "    \"\"\"Chat with Llama vision model in multi-turn conversation mode.\n",
        "    \n",
        "    Args:\n",
        "        model: Loaded Llama vision model\n",
        "        processor: AutoProcessor for the model\n",
        "        prompt: User's text prompt\n",
        "        images_path: Path(s) to image files (string or list)\n",
        "        do_sample: Enable sampling (if True, uses temperature)\n",
        "        temperature: Sampling temperature (default 0.1)\n",
        "        show_image: Display image in notebook (default False)\n",
        "        max_new_tokens: Maximum tokens to generate (default 2000)\n",
        "        messages: Conversation history (empty list for new conversation)\n",
        "        images: Loaded image objects (empty list to load from paths)\n",
        "    \n",
        "    Returns:\n",
        "        tuple: (generated_text, updated_messages, images)\n",
        "    \"\"\"\n",
        "    # Ensure list\n",
        "    if not isinstance(images_path, list):\n",
        "        images_path = [images_path]\n",
        "\n",
        "    # Load images\n",
        "    if len(images) == 0 and len(images_path) > 0:\n",
        "        for image_path in tqdm(images_path, desc=\"Loading images\"):\n",
        "            image = load_image(image_path)\n",
        "            images.append(image)\n",
        "            if show_image:\n",
        "                display(image)\n",
        "\n",
        "    # If starting a new conversation about an image\n",
        "    if len(messages) == 0:\n",
        "        messages = [\n",
        "            {\n",
        "                \"role\": \"user\", \n",
        "                \"content\": [\n",
        "                    {\"type\": \"image\"}, \n",
        "                    {\"type\": \"text\", \"text\": prompt}\n",
        "                ]\n",
        "            }\n",
        "        ]\n",
        "\n",
        "    # If continuing conversation on the image\n",
        "    else:\n",
        "        messages.append({\n",
        "            \"role\": \"user\", \n",
        "            \"content\": [{\"type\": \"text\", \"text\": prompt}]\n",
        "        })\n",
        "\n",
        "    # Process input data\n",
        "    text = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
        "    inputs = processor(images=images, text=text, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    # Generate response\n",
        "    generation_args = {\"max_new_tokens\": max_new_tokens, \"do_sample\": do_sample}\n",
        "    if do_sample:\n",
        "        generation_args[\"temperature\"] = temperature\n",
        "    else:\n",
        "        generation_args[\"temperature\"] = None\n",
        "        generation_args[\"top_p\"] = None\n",
        "    \n",
        "    generate_ids = model.generate(**inputs, **generation_args)\n",
        "    \n",
        "    # Trim input tokens from output\n",
        "    generate_ids = generate_ids[:, inputs['input_ids'].shape[1]:-1]\n",
        "    generated_texts = processor.decode(generate_ids[0], clean_up_tokenization_spaces=False)\n",
        "\n",
        "    # Append the model's response to the conversation history\n",
        "    messages.append({\n",
        "        \"role\": \"assistant\", \n",
        "        \"content\": [{\"type\": \"text\", \"text\": generated_texts}]\n",
        "    })\n",
        "\n",
        "    return generated_texts, messages, images\n",
        "\n",
        "print(\"✅ chat_with_mllm function defined\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define Conversation Prompts\n",
        "\n",
        "Each prompt extracts a single column to avoid spatial reasoning failures."
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Single-column extraction prompts\n",
        "CONVERSATION_PROMPTS = {\n",
        "    \"turn_1_date_column\": \"\"\"Look at the transaction table in this bank statement.\n",
        "\n",
        "Find the leftmost column with the header \"Date\" or \"Date of Transaction\".\n",
        "\n",
        "Extract ONLY the date values from this column, ignoring all other columns.\n",
        "\n",
        "Important:\n",
        "- Extract ONLY dates (e.g., \"15 Mar 2024\" or \"15 Mar\")\n",
        "- Do NOT include transaction descriptions\n",
        "- Do NOT include amounts\n",
        "- List one date per line\n",
        "\n",
        "Output format:\n",
        "[First date]\n",
        "[Second date]\n",
        "[Third date]\n",
        "...\"\"\",\n",
        "    \n",
        "    \"turn_2_transaction_column\": \"\"\"Look at the transaction table in this bank statement.\n",
        "\n",
        "Find the column with the header \"Transaction\" or \"Description\".\n",
        "\n",
        "This column is located IMMEDIATELY TO THE RIGHT of the Date column.\n",
        "\n",
        "Extract ONLY the transaction descriptions from this column, ignoring all other columns.\n",
        "\n",
        "Important:\n",
        "- Extract ONLY transaction descriptions\n",
        "- If a description spans multiple lines, combine them with spaces\n",
        "- Do NOT include dates\n",
        "- Do NOT include amounts\n",
        "- List one description per line\n",
        "\n",
        "Output format:\n",
        "[First description]\n",
        "[Second description]\n",
        "[Third description]\n",
        "...\"\"\",\n",
        "    \n",
        "    \"turn_3_debit_column\": \"\"\"Look at the transaction table in this bank statement.\n",
        "\n",
        "Find the column with the header \"Debit\" or \"Withdrawal\".\n",
        "\n",
        "Extract ONLY the debit amounts from this column, ignoring all other columns.\n",
        "\n",
        "Important:\n",
        "- Extract ONLY debit amounts with currency symbols\n",
        "- If a cell is empty, write \"EMPTY\"\n",
        "- Do NOT extract credit amounts or balance amounts\n",
        "- List one amount per line\n",
        "\n",
        "Output format:\n",
        "[First amount or EMPTY]\n",
        "[Second amount or EMPTY]\n",
        "[Third amount or EMPTY]\n",
        "...\"\"\",\n",
        "    \n",
        "    \"turn_4_credit_column\": \"\"\"Look at the transaction table in this bank statement.\n",
        "\n",
        "Find the column with the header \"Credit\" or \"Deposit\".\n",
        "\n",
        "Extract ONLY the credit amounts from this column, ignoring all other columns.\n",
        "\n",
        "Important:\n",
        "- Extract ONLY credit amounts with currency symbols\n",
        "- If a cell is empty, write \"EMPTY\"\n",
        "- NEVER add \"CR\" suffix to these amounts\n",
        "- Do NOT extract debit amounts or balance amounts\n",
        "- List one amount per line\n",
        "\n",
        "Output format:\n",
        "[First amount or EMPTY]\n",
        "[Second amount or EMPTY]\n",
        "[Third amount or EMPTY]\n",
        "...\"\"\",\n",
        "    \n",
        "    \"turn_5_balance_column\": \"\"\"Look at the transaction table in this bank statement.\n",
        "\n",
        "Find the rightmost column with the header \"Balance\".\n",
        "\n",
        "Extract ONLY the balance amounts from this column, ignoring all other columns.\n",
        "\n",
        "Important:\n",
        "- Extract ONLY balance amounts\n",
        "- Preserve \"CR\" notation exactly as shown\n",
        "- Do NOT extract debit or credit amounts\n",
        "- List one balance per line\n",
        "\n",
        "Output format:\n",
        "[First balance]\n",
        "[Second balance]\n",
        "[Third balance]\n",
        "...\"\"\"\n",
        "}\n",
        "\n",
        "def display_prompts(prompts_dict):\n",
        "    \"\"\"Display all conversation prompts in a readable format.\"\"\"\n",
        "    print(\"=\" * 70)\n",
        "    print(\"SINGLE-COLUMN EXTRACTION PROMPTS\")\n",
        "    print(\"=\" * 70)\n",
        "    for i, (key, prompt) in enumerate(prompts_dict.items(), 1):\n",
        "        turn_name = key.replace(\"_\", \" \").title()\n",
        "        print(f\"\\n{i}. {turn_name}\")\n",
        "        print(\"-\" * 70)\n",
        "        preview = prompt if len(prompt) <= 200 else prompt[:197] + \"...\"\n",
        "        print(f\"{preview}\")\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(f\"Total prompts defined: {len(prompts_dict)}\")\n",
        "\n",
        "display_prompts(CONVERSATION_PROMPTS)\n",
        "print(\"\\n✅ Conversation prompts defined and ready to use\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Image and Initialize Conversation"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Image path\n",
        "imageName = \"/home/jovyan/_LMM_POC/evaluation_data/image_003.png\"\n",
        "\n",
        "# Initialize conversation\n",
        "messages = []\n",
        "images = []\n",
        "\n",
        "print(\"📸 Processing bank statement image...\")\n",
        "print(f\"📁 Image: {imageName}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Turn 1: Extract Date Column Only"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(f\"📝 Using prompt: turn_1_date_column\")\n",
        "\n",
        "response1, messages, images = chat_with_mllm(\n",
        "    model, \n",
        "    processor, \n",
        "    CONVERSATION_PROMPTS[\"turn_1_date_column\"],\n",
        "    images_path=[imageName],\n",
        "    do_sample=False,\n",
        "    max_new_tokens=1000,\n",
        "    show_image=True,\n",
        "    messages=messages,\n",
        "    images=images\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"TURN 1 - DATE COLUMN:\")\n",
        "print(\"=\" * 60)\n",
        "print(response1)\n",
        "print(\"=\" * 60)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Turn 2: Extract Transaction Column Only"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(f\"📝 Using prompt: turn_2_transaction_column\")\n",
        "\n",
        "response2, messages, images = chat_with_mllm(\n",
        "    model, processor,\n",
        "    CONVERSATION_PROMPTS[\"turn_2_transaction_column\"],\n",
        "    messages=messages, \n",
        "    images=images,\n",
        "    max_new_tokens=2000\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"TURN 2 - TRANSACTION COLUMN:\")\n",
        "print(\"=\" * 60)\n",
        "print(response2)\n",
        "print(\"=\" * 60)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Turn 3: Extract Debit Column Only"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(f\"📝 Using prompt: turn_3_debit_column\")\n",
        "\n",
        "response3, messages, images = chat_with_mllm(\n",
        "    model, processor,\n",
        "    CONVERSATION_PROMPTS[\"turn_3_debit_column\"],\n",
        "    messages=messages,\n",
        "    images=images,\n",
        "    max_new_tokens=1000\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"TURN 3 - DEBIT COLUMN:\")\n",
        "print(\"=\" * 60)\n",
        "print(response3)\n",
        "print(\"=\" * 60)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Turn 4: Extract Credit Column Only"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(f\"📝 Using prompt: turn_4_credit_column\")\n",
        "\n",
        "response4, messages, images = chat_with_mllm(\n",
        "    model, processor,\n",
        "    CONVERSATION_PROMPTS[\"turn_4_credit_column\"],\n",
        "    messages=messages,\n",
        "    images=images,\n",
        "    max_new_tokens=1000\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"TURN 4 - CREDIT COLUMN:\")\n",
        "print(\"=\" * 60)\n",
        "print(response4)\n",
        "print(\"=\" * 60)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Turn 5: Extract Balance Column Only"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(f\"📝 Using prompt: turn_5_balance_column\")\n",
        "\n",
        "response5, messages, images = chat_with_mllm(\n",
        "    model, processor,\n",
        "    CONVERSATION_PROMPTS[\"turn_5_balance_column\"],\n",
        "    messages=messages,\n",
        "    images=images,\n",
        "    max_new_tokens=1000\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"TURN 5 - BALANCE COLUMN:\")\n",
        "print(\"=\" * 60)\n",
        "print(response5)\n",
        "print(\"=\" * 60)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Validation: Check Row Counts"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Parse responses to count rows\n",
        "def count_lines(response_text):\n",
        "    \"\"\"Count non-empty lines in response.\"\"\"\n",
        "    lines = [line.strip() for line in response_text.strip().split('\\n') if line.strip()]\n",
        "    return len(lines)\n",
        "\n",
        "date_count = count_lines(response1)\n",
        "transaction_count = count_lines(response2)\n",
        "debit_count = count_lines(response3)\n",
        "credit_count = count_lines(response4)\n",
        "balance_count = count_lines(response5)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"VALIDATION - ROW COUNTS:\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Date column:        {date_count} rows\")\n",
        "print(f\"Transaction column: {transaction_count} rows\")\n",
        "print(f\"Debit column:       {debit_count} rows\")\n",
        "print(f\"Credit column:      {credit_count} rows\")\n",
        "print(f\"Balance column:     {balance_count} rows\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "if len(set([date_count, transaction_count, debit_count, credit_count, balance_count])) == 1:\n",
        "    print(\"✅ All columns have matching row counts\")\n",
        "else:\n",
        "    print(\"⚠️ WARNING: Row count mismatch detected!\")\n",
        "    print(\"   Manual review required\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save Extraction Results"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Save all column extractions to a file\n",
        "output_path = Path(\"llama_single_column_extraction.txt\")\n",
        "\n",
        "with output_path.open(\"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"=\" * 60 + \"\\n\")\n",
        "    f.write(\"SINGLE-COLUMN EXTRACTION RESULTS\\n\")\n",
        "    f.write(\"Llama-3.2-Vision-11B\\n\")\n",
        "    f.write(\"=\" * 60 + \"\\n\\n\")\n",
        "    \n",
        "    f.write(\"DATE COLUMN:\\n\")\n",
        "    f.write(\"-\" * 60 + \"\\n\")\n",
        "    f.write(response1 + \"\\n\\n\")\n",
        "    \n",
        "    f.write(\"TRANSACTION COLUMN:\\n\")\n",
        "    f.write(\"-\" * 60 + \"\\n\")\n",
        "    f.write(response2 + \"\\n\\n\")\n",
        "    \n",
        "    f.write(\"DEBIT COLUMN:\\n\")\n",
        "    f.write(\"-\" * 60 + \"\\n\")\n",
        "    f.write(response3 + \"\\n\\n\")\n",
        "    \n",
        "    f.write(\"CREDIT COLUMN:\\n\")\n",
        "    f.write(\"-\" * 60 + \"\\n\")\n",
        "    f.write(response4 + \"\\n\\n\")\n",
        "    \n",
        "    f.write(\"BALANCE COLUMN:\\n\")\n",
        "    f.write(\"-\" * 60 + \"\\n\")\n",
        "    f.write(response5 + \"\\n\\n\")\n",
        "    \n",
        "    f.write(\"=\" * 60 + \"\\n\")\n",
        "    f.write(f\"Total conversations: {len(messages)}\\n\")\n",
        "    f.write(\"=\" * 60 + \"\\n\")\n",
        "\n",
        "print(f\"✅ Extraction results saved to: {output_path}\")\n",
        "print(f\"📊 File size: {output_path.stat().st_size} bytes\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
