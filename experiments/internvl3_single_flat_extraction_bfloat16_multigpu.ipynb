{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# InternVL3 Single Flat Extraction - BFloat16 Multi-GPU\n",
        "\n",
        "**Model Loading**: bfloat16 precision with multi-GPU device mapping (no quantization)  \n",
        "**Reference**: https://internvl.readthedocs.io/en/latest/internvl3.0/quick_start.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import random\n",
        "import math\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torchvision.transforms as T\n",
        "from PIL import Image\n",
        "from torchvision.transforms.functional import InterpolationMode\n",
        "from transformers import AutoConfig, AutoModel, AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def set_seed(seed=42):\n",
        "    \"\"\"Set random seeds for reproducibility\"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)\n",
        "print(\"‚úÖ Random seed set to 42 for reproducibility\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def split_model(model_name):\n",
        "    \"\"\"Create device map for multi-GPU distribution.\n",
        "    \n",
        "    Strategy:\n",
        "    - GPU 0: Vision encoder, MLP, embeddings, first/last LLM layers\n",
        "    - Remaining GPUs: Distributed transformer layers\n",
        "    \n",
        "    Reference: https://internvl.readthedocs.io/en/latest/internvl3.0/quick_start.html\n",
        "    \"\"\"\n",
        "    device_map = {}\n",
        "    world_size = torch.cuda.device_count()\n",
        "    config = AutoConfig.from_pretrained(model_name, trust_remote_code=True)\n",
        "    num_layers = config.llm_config.num_hidden_layers\n",
        "    \n",
        "    # Since the first GPU will be used for ViT, treat it as half a GPU.\n",
        "    num_layers_per_gpu = math.ceil(num_layers / (world_size - 0.5))\n",
        "    num_layers_per_gpu = [num_layers_per_gpu] * world_size\n",
        "    num_layers_per_gpu[0] = math.ceil(num_layers_per_gpu[0] * 0.5)\n",
        "    \n",
        "    layer_cnt = 0\n",
        "    for i, num_layer in enumerate(num_layers_per_gpu):\n",
        "        for j in range(num_layer):\n",
        "            device_map[f'language_model.model.layers.{layer_cnt}'] = i\n",
        "            layer_cnt += 1\n",
        "    \n",
        "    # Assign vision and embedding components to GPU 0\n",
        "    device_map['vision_model'] = 0\n",
        "    device_map['mlp1'] = 0\n",
        "    device_map['language_model.model.tok_embeddings'] = 0\n",
        "    device_map['language_model.model.embed_tokens'] = 0\n",
        "    device_map['language_model.output'] = 0\n",
        "    device_map['language_model.model.norm'] = 0\n",
        "    device_map['language_model.model.rotary_emb'] = 0\n",
        "    device_map['language_model.lm_head'] = 0\n",
        "    device_map[f'language_model.model.layers.{num_layers - 1}'] = 0\n",
        "    \n",
        "    return device_map\n",
        "\n",
        "print(\"‚úÖ Multi-GPU device mapping function defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_path = \"/home/jovyan/shared_PTM/InternVL3-8B\"\n",
        "\n",
        "print(f\"üîß Loading InternVL3-8B model in bfloat16...\")\n",
        "print(f\"üìä Available GPUs: {torch.cuda.device_count()}\")\n",
        "\n",
        "# Create device map for multi-GPU distribution\n",
        "device_map = split_model(model_path)\n",
        "\n",
        "print(\"\\nüìç Device Map Summary:\")\n",
        "# Count components per GPU\n",
        "gpu_counts = {}\n",
        "for component, gpu_id in device_map.items():\n",
        "    gpu_counts[gpu_id] = gpu_counts.get(gpu_id, 0) + 1\n",
        "\n",
        "for gpu_id in sorted(gpu_counts.keys()):\n",
        "    print(f\"   GPU {gpu_id}: {gpu_counts[gpu_id]} components\")\n",
        "\n",
        "# Load model with bfloat16 and device_map\n",
        "model = AutoModel.from_pretrained(\n",
        "    model_path,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    low_cpu_mem_usage=True,\n",
        "    use_flash_attn=True,\n",
        "    trust_remote_code=True,\n",
        "    device_map=device_map\n",
        ").eval()\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True, use_fast=False)\n",
        "\n",
        "print(\"\\n‚úÖ Model loaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inspect model parameter datatypes\n",
        "print(\"üîç Model Parameter Datatypes:\")\n",
        "\n",
        "dtype_counts = {}\n",
        "param_count = 0\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "    dtype = str(param.dtype)\n",
        "    dtype_counts[dtype] = dtype_counts.get(dtype, 0) + 1\n",
        "    param_count += 1\n",
        "    \n",
        "    # Show first few parameters as examples\n",
        "    if param_count <= 5:\n",
        "        print(f\"   {name}: {param.dtype} on {param.device}\")\n",
        "\n",
        "print(f\"\\nüìä Dtype Distribution (Total {param_count} parameters):\")\n",
        "for dtype, count in sorted(dtype_counts.items()):\n",
        "    print(f\"   {dtype}: {count} parameters\")\n",
        "\n",
        "print(\"\\nüíæ GPU Memory Usage:\")\n",
        "for i in range(torch.cuda.device_count()):\n",
        "    allocated = torch.cuda.memory_allocated(i) / 1e9\n",
        "    reserved = torch.cuda.memory_reserved(i) / 1e9\n",
        "    print(f\"   GPU [{i}]: {allocated:.2f}GB allocated, {reserved:.2f}GB reserved\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_transform(input_size):\n",
        "    \"\"\"Build image transformation pipeline for InternVL3\"\"\"\n",
        "    IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
        "    IMAGENET_STD = (0.229, 0.224, 0.225)\n",
        "    \n",
        "    transform = T.Compose([\n",
        "        T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
        "        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
        "    ])\n",
        "    return transform\n",
        "\n",
        "\n",
        "def load_image(image_file, input_size=448, max_num=12):\n",
        "    \"\"\"Load and preprocess image for InternVL3\"\"\"\n",
        "    image = Image.open(image_file).convert('RGB')\n",
        "    transform = build_transform(input_size=input_size)\n",
        "    pixel_values = transform(image).unsqueeze(0).to(torch.bfloat16).cuda()\n",
        "    return pixel_values\n",
        "\n",
        "print(\"‚úÖ Image preprocessing functions defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "imageName = \"/home/jovyan/_LMM_POC/evaluation_data/image_008.png\"\n",
        "\n",
        "print(\"üìÇ Loading image...\")\n",
        "image = Image.open(imageName)\n",
        "print(f\"‚úÖ Image loaded: {image.size}\")\n",
        "\n",
        "# Preprocess for InternVL3\n",
        "pixel_values = load_image(imageName, input_size=448)\n",
        "\n",
        "print(f\"\\nüîç Pixel Values Tensor Info:\")\n",
        "print(f\"   Shape: {pixel_values.shape}\")\n",
        "print(f\"   Dtype: {pixel_values.dtype}\")\n",
        "print(f\"   Device: {pixel_values.device}\")\n",
        "print(f\"   Memory: {pixel_values.element_size() * pixel_values.nelement() / 1e6:.2f}MB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# basic flat 5 column [\"Date\", \"Description\", \"Withdrawal\", \"Credit\", \"Balance\"] transaction table prompt\n",
        "prompt_text = \"\"\"\n",
        "You are an expert document analyzer specializing in bank statement extraction.\n",
        "Extract structured data from this flat table bank statement for taxpayer expense claims.\n",
        "\n",
        "CONVERSATION PROTOCOL:\n",
        "- Start your response immediately with \"DOCUMENT_TYPE: BANK_STATEMENT\"\n",
        "- Do NOT include conversational text like \"I'll extract...\" or \"Based on the document...\"\n",
        "- Do NOT use bullet points, numbered lists, asterisks, or markdown formatting (no **, no ##, no 1., no -)\n",
        "- Output ONLY the structured extraction data below\n",
        "- End immediately after \"TRANSACTION_AMOUNTS_PAID:\" with no additional text\n",
        "- NO explanations, NO comments, NO additional text\n",
        "\n",
        "CRITICAL:\n",
        "- The transaction table in the image has a \"Date\", a \"Description\", a \"Withdrawal\", a \"Deposit\" and a \"Balance\" column\n",
        "- Specifically, it has a \"Date\" column, a \"Description\" column, a \"Withdrawal\" column, a \"Deposit\" column and a \"Balance\" column\n",
        "\n",
        "ANTI-HALLUCINATION RULES:\n",
        "- YOU MUST NOT GUESS values you are unsure of\n",
        "- Rows may have missing values\n",
        "- Rows NEVER HAVE REPEATED AMOUNTS, SO YOU MUST NOT REPEAT VALUES THAT YOU ARE UNSURE OF\n",
        "- If a value is unclear or missing, use \"NOT_FOUND\" instead of guessing\n",
        "\n",
        "STEP 1:\n",
        "- Extract the Transaction Table formatted as markdown.\n",
        "\n",
        "STEP 2:\n",
        "- Extract the earliest and latest date in the \"Date\" column from the extracted Transaction Table in STEP 1\n",
        "- Format as STATEMENT_DATE_RANGE: [ First date in \"Date\" column - Last date in \"Date\" column ]\n",
        "\n",
        "STEP 3:\n",
        "- Extract the \"Date\" column from the extracted Transaction Table in STEP 1\n",
        "- Format as TRANSACTION_DATES: [ All \"Date\" column dates, each separated by \" | \" ] on a single line\n",
        "\n",
        "STEP 4:\n",
        "- Extract the \"Description\" column from the extracted Transaction Table in STEP 1\n",
        "- Format as LINE_ITEM_DESCRIPTIONS: [ All \"Description\" column descriptions, each separated by \" | \" ] on a single line\n",
        "\n",
        "STEP 5:\n",
        "- Extract the \"Withdrawal\" column from the extracted Transaction Table in STEP 1, replacing missing values with \"NOT_FOUND\".\n",
        "- Format as TRANSACTION_AMOUNTS_PAID: [ All \"Withdrawal\" column amounts each separated by \" | \" ] on a single line\n",
        "\"\"\"\n",
        "\n",
        "print(f\"üìù Prompt length: {len(prompt_text)} characters\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "generation_config = dict(max_new_tokens=4000, do_sample=False)\n",
        "\n",
        "print(\"ü§ñ Generating response with InternVL3-8B...\")\n",
        "print(\"\\nüíæ GPU Memory Before Generation:\")\n",
        "for i in range(torch.cuda.device_count()):\n",
        "    allocated = torch.cuda.memory_allocated(i) / 1e9\n",
        "    print(f\"   GPU [{i}]: {allocated:.2f}GB\")\n",
        "\n",
        "# Generate response\n",
        "response = model.chat(\n",
        "    tokenizer=tokenizer,\n",
        "    pixel_values=pixel_values,\n",
        "    question=prompt_text,\n",
        "    generation_config=generation_config\n",
        ")\n",
        "\n",
        "print(\"\\nüíæ GPU Memory After Generation:\")\n",
        "for i in range(torch.cuda.device_count()):\n",
        "    allocated = torch.cuda.memory_allocated(i) / 1e9\n",
        "    print(f\"   GPU [{i}]: {allocated:.2f}GB\")\n",
        "\n",
        "print(\"\\n‚úÖ Response generated successfully!\")\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"EXTRACTION RESULT:\")\n",
        "print(\"=\" * 60)\n",
        "print(response)\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the response to a file\n",
        "output_path = Path(\"internvl3_grouped_bank_statement_output.txt\")\n",
        "\n",
        "with output_path.open(\"w\", encoding=\"utf-8\") as text_file:\n",
        "    text_file.write(response)\n",
        "\n",
        "print(f\"‚úÖ Response saved to: {output_path}\")\n",
        "print(f\"üìÅ File size: {output_path.stat().st_size} bytes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final dtype and memory summary\n",
        "print(\"üìä FINAL SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Model Dtype: torch.bfloat16\")\n",
        "print(f\"Input Tensor Dtype: {pixel_values.dtype}\")\n",
        "print(f\"GPUs Used: {torch.cuda.device_count()}\")\n",
        "print(\"\\nüíæ Final GPU Memory:\")\n",
        "total_allocated = 0\n",
        "for i in range(torch.cuda.device_count()):\n",
        "    allocated = torch.cuda.memory_allocated(i) / 1e9\n",
        "    reserved = torch.cuda.memory_reserved(i) / 1e9\n",
        "    total_allocated += allocated\n",
        "    print(f\"   GPU [{i}]: {allocated:.2f}GB allocated, {reserved:.2f}GB reserved\")\n",
        "print(f\"\\n   Total Allocated: {total_allocated:.2f}GB\")\n",
        "print(\"=\" * 60)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
