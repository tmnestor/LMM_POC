{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama 3.2 Vision Multi-Turn Debit Extractor\n",
    "\n",
    "This notebook demonstrates multi-turn conversational extraction using Llama 3.2 Vision.\n",
    "Uses the `chat_with_mllm` pattern for clean, maintainable multi-turn conversations.\n",
    "\n",
    "**Reference**: [Chat with Your Images Using Multimodal LLMs](https://medium.com/data-science/chat-with-your-images-using-multimodal-llms-60af003e8bfa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add project root to path for common/ imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, MllamaForConditionalGeneration\n",
    "from transformers.image_utils import load_image\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-emptive Memory Cleanup\n",
    "\n",
    "Optional GPU memory cleanup to prevent OOM errors when switching between models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Pre-emptive memory cleanup (useful when switching models)\n",
    "try:\n",
    "    from common.gpu_optimization import emergency_cleanup\n",
    "    print(\"ðŸ§¹ Clearing GPU memory...\")\n",
    "    emergency_cleanup(verbose=False)\n",
    "    print(\"âœ… Memory cleanup complete\")\n",
    "except ImportError:\n",
    "    print(\"âš ï¸ GPU optimization module not available - skipping cleanup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Random Seed for Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.reproducibility import set_seed\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"/home/jovyan/shared_PTM/Llama-3.2-11B-Vision-Instruct\"\n",
    "\n",
    "print(\"ðŸ”§ Loading Llama-3.2-Vision model...\")\n",
    "\n",
    "from common.llama_model_loader_robust import load_llama_model_robust\n",
    "\n",
    "model, processor = load_llama_model_robust(\n",
    "    model_path=model_id,\n",
    "    use_quantization=False,\n",
    "    device_map='auto',\n",
    "    max_new_tokens=2000,\n",
    "    torch_dtype='bfloat16',\n",
    "    low_cpu_mem_usage=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Add tie_weights() call\n",
    "try:\n",
    "    model.tie_weights()\n",
    "    print(\"âœ… Model weights tied successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ tie_weights() warning: {e}\")\n",
    "\n",
    "print(\"âœ… Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Manual Memory Cleanup\n",
    "\n",
    "Run this cell if you experience memory issues during the conversation. Not needed for normal operation on H200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Run this cell if you experience memory issues during conversation\n",
    "import gc\n",
    "\n",
    "print(\"ðŸ§¹ Manual memory cleanup...\")\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Show memory status\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        allocated = torch.cuda.memory_allocated(i) / 1e9\n",
    "        reserved = torch.cuda.memory_reserved(i) / 1e9\n",
    "        print(f\"   GPU {i}: {allocated:.2f}GB allocated, {reserved:.2f}GB reserved\")\n",
    "print(\"âœ… Cleanup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define chat_with_mllm Function\n",
    "\n",
    "This function encapsulates the multi-turn conversation pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_mllm(model, processor, prompt, images_path=[], do_sample=False, \n",
    "                   temperature=0.1, show_image=False, max_new_tokens=2000, \n",
    "                   messages=[], images=[]):\n",
    "    \"\"\"Chat with Llama vision model in multi-turn conversation mode.\n",
    "    \n",
    "    Args:\n",
    "        model: Loaded Llama vision model\n",
    "        processor: AutoProcessor for the model\n",
    "        prompt: User's text prompt\n",
    "        images_path: Path(s) to image files (string or list)\n",
    "        do_sample: Enable sampling (if True, uses temperature)\n",
    "        temperature: Sampling temperature (default 0.1)\n",
    "        show_image: Display image in notebook (default False)\n",
    "        max_new_tokens: Maximum tokens to generate (default 2000)\n",
    "        messages: Conversation history (empty list for new conversation)\n",
    "        images: Loaded image objects (empty list to load from paths)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (generated_text, updated_messages, images)\n",
    "    \"\"\"\n",
    "    # Ensure list\n",
    "    if not isinstance(images_path, list):\n",
    "        images_path = [images_path]\n",
    "\n",
    "    # Load images\n",
    "    if len(images) == 0 and len(images_path) > 0:\n",
    "        for image_path in tqdm(images_path, desc=\"Loading images\"):\n",
    "            image = load_image(image_path)\n",
    "            images.append(image)\n",
    "            if show_image:\n",
    "                display(image)\n",
    "\n",
    "    # If starting a new conversation about an image\n",
    "    if len(messages) == 0:\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": [\n",
    "                    {\"type\": \"image\"}, \n",
    "                    {\"type\": \"text\", \"text\": prompt}\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "\n",
    "    # If continuing conversation on the image\n",
    "    else:\n",
    "        messages.append({\n",
    "            \"role\": \"user\", \n",
    "            \"content\": [{\"type\": \"text\", \"text\": prompt}]\n",
    "        })\n",
    "\n",
    "    # Process input data\n",
    "    text = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "    inputs = processor(images=images, text=text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Generate response\n",
    "    generation_args = {\"max_new_tokens\": max_new_tokens, \"do_sample\": do_sample}\n",
    "    if do_sample:\n",
    "        generation_args[\"temperature\"] = temperature\n",
    "    else:\n",
    "        generation_args[\"temperature\"] = None\n",
    "        generation_args[\"top_p\"] = None\n",
    "    \n",
    "    generate_ids = model.generate(**inputs, **generation_args)\n",
    "    \n",
    "    # Trim input tokens from output\n",
    "    generate_ids = generate_ids[:, inputs['input_ids'].shape[1]:-1]\n",
    "    generated_texts = processor.decode(generate_ids[0], clean_up_tokenization_spaces=False)\n",
    "\n",
    "    # Append the model's response to the conversation history\n",
    "    messages.append({\n",
    "        \"role\": \"assistant\", \n",
    "        \"content\": [{\"type\": \"text\", \"text\": generated_texts}]\n",
    "    })\n",
    "\n",
    "    return generated_texts, messages, images\n",
    "\n",
    "print(\"âœ… chat_with_mllm function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Conversation Prompts\n",
    "\n",
    "All prompts are defined upfront for easy modification and review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversation prompts dictionary\n",
    "CONVERSATION_PROMPTS = {\n",
    "    \"turn_1_initial_extraction\": \"\"\"You are an expert document analyser specializing in Date Grouped Australian Bank Statement extraction.\n",
    "Date Grouped Bank Statements are date ordered, with one or more transactions for each date header.\n",
    "Every transaction for a given date heading has a description, a debit/credit amount and finally a balance amount with a ' CR' suffix.\n",
    "Extract all balance amounts along with their ' CR' suffix, the transaction dates (from the date heading) and transaction descriptions,\n",
    "maintaining the same date ordering as the image, with every transaction appearing on its own row and remembering that some date headings have more than one balance.\"\"\",\n",
    "    \n",
    "    \"turn_2_count_transactions\": \"How many transactions are shown in this bank statement?\",\n",
    "    \n",
    "    \"turn_3_extract_debits\": \"From your first response, extract ONLY the debit/withdrawal amounts (amounts paid out). List them in order, one per line.\",\n",
    "    \n",
    "    \"turn_4_verify_count\": \"How many debit/withdrawal transactions did you extract in your previous response?\",\n",
    "    \n",
    "    \"turn_5_total_debits\": \"What is the total sum of all debit/withdrawal amounts in this statement?\",\n",
    "    \n",
    "    \"turn_6_date_range\": \"What is the date range covered by this bank statement?\",\n",
    "    \n",
    "    \"turn_7_verify_consistency\": \"In your very first response, you extracted all transactions. Can you verify that the debit amounts you listed in turn 3 match the debit amounts from your first extraction?\"\n",
    "}\n",
    "\n",
    "def display_prompts(prompts_dict):\n",
    "    \"\"\"Display all conversation prompts in a readable format.\"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"CONVERSATION PROMPTS\")\n",
    "    print(\"=\" * 70)\n",
    "    for i, (key, prompt) in enumerate(prompts_dict.items(), 1):\n",
    "        # Format the key for display\n",
    "        turn_name = key.replace(\"_\", \" \").title()\n",
    "        print(f\"\\n{i}. {turn_name}\")\n",
    "        print(\"-\" * 70)\n",
    "        # Truncate long prompts for preview\n",
    "        preview = prompt if len(prompt) <= 200 else prompt[:197] + \"...\"\n",
    "        print(f\"{preview}\")\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(f\"Total prompts defined: {len(prompts_dict)}\")\n",
    "\n",
    "# Display the prompts\n",
    "display_prompts(CONVERSATION_PROMPTS)\n",
    "print(\"\\nâœ… Conversation prompts defined and ready to use\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Extraction (Turn 1)\n",
    "\n",
    "Extract all transaction data from the bank statement image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image path\n",
    "imageName = \"/home/jovyan/_LMM_POC/evaluation_data/image_009.png\"\n",
    "\n",
    "# Initialize conversation\n",
    "messages = []\n",
    "images = []\n",
    "\n",
    "print(\"ðŸ“¸ Processing bank statement image...\")\n",
    "print(f\"ðŸ“ Using prompt: turn_1_initial_extraction\")\n",
    "\n",
    "response1, messages, images = chat_with_mllm(\n",
    "    model, processor, \n",
    "    CONVERSATION_PROMPTS[\"turn_1_initial_extraction\"],\n",
    "    images_path=[imageName],\n",
    "    do_sample=False,\n",
    "    max_new_tokens=2000,\n",
    "    show_image=True,\n",
    "    messages=messages,\n",
    "    images=images\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TURN 1 - INITIAL EXTRACTION:\")\n",
    "print(\"=\" * 60)\n",
    "print(response1)\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Save initial extraction\n",
    "Path(\"llama_debit_extractor_initial.txt\").write_text(response1)\n",
    "print(\"\\nâœ… Initial extraction saved to llama_debit_extractor_initial.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turn 2: Count Transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"ðŸ“ Using prompt: turn_2_count_transactions\")\n",
    "\n",
    "response2, messages, images = chat_with_mllm(\n",
    "    model, processor,\n",
    "    CONVERSATION_PROMPTS[\"turn_2_count_transactions\"],\n",
    "    messages=messages, \n",
    "    images=images,\n",
    "    max_new_tokens=500\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TURN 2 - TRANSACTION COUNT:\")\n",
    "print(\"=\" * 60)\n",
    "print(response2)\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turn 3: Extract Debit/Withdrawal Amounts Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"ðŸ“ Using prompt: turn_3_extract_debits\")\n",
    "\n",
    "response3, messages, images = chat_with_mllm(\n",
    "    model, processor,\n",
    "    CONVERSATION_PROMPTS[\"turn_3_extract_debits\"],\n",
    "    messages=messages,\n",
    "    images=images,\n",
    "    max_new_tokens=1000\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TURN 3 - DEBIT AMOUNTS ONLY:\")\n",
    "print(\"=\" * 60)\n",
    "print(response3)\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Save debit amounts\n",
    "Path(\"llama_debit_amounts.txt\").write_text(response3)\n",
    "print(\"\\nâœ… Debit amounts saved to llama_debit_amounts.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turn 4: Verify Debit Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"ðŸ“ Using prompt: turn_4_verify_count\")\n",
    "\n",
    "response4, messages, images = chat_with_mllm(\n",
    "    model, processor,\n",
    "    CONVERSATION_PROMPTS[\"turn_4_verify_count\"],\n",
    "    messages=messages,\n",
    "    images=images,\n",
    "    max_new_tokens=500\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TURN 4 - DEBIT COUNT:\")\n",
    "print(\"=\" * 60)\n",
    "print(response4)\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turn 5: Total Debit Amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"ðŸ“ Using prompt: turn_5_total_debits\")\n",
    "\n",
    "response5, messages, images = chat_with_mllm(\n",
    "    model, processor,\n",
    "    CONVERSATION_PROMPTS[\"turn_5_total_debits\"],\n",
    "    messages=messages,\n",
    "    images=images,\n",
    "    max_new_tokens=500\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TURN 5 - TOTAL DEBITS:\")\n",
    "print(\"=\" * 60)\n",
    "print(response5)\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turn 6: Date Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"ðŸ“ Using prompt: turn_6_date_range\")\n",
    "\n",
    "response6, messages, images = chat_with_mllm(\n",
    "    model, processor,\n",
    "    CONVERSATION_PROMPTS[\"turn_6_date_range\"],\n",
    "    messages=messages,\n",
    "    images=images,\n",
    "    max_new_tokens=500\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TURN 6 - DATE RANGE:\")\n",
    "print(\"=\" * 60)\n",
    "print(response6)\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turn 7: Verification - Cross-check First Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"ðŸ“ Using prompt: turn_7_verify_consistency\")\n",
    "\n",
    "response7, messages, images = chat_with_mllm(\n",
    "    model, processor,\n",
    "    CONVERSATION_PROMPTS[\"turn_7_verify_consistency\"],\n",
    "    messages=messages,\n",
    "    images=images,\n",
    "    max_new_tokens=1000\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TURN 7 - VERIFICATION:\")\n",
    "print(\"=\" * 60)\n",
    "print(response7)\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug: View Conversation Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ” Current conversation structure:\")\n",
    "print(\"=\" * 60)\n",
    "for i, msg in enumerate(messages, 1):\n",
    "    print(f\"\\nMessage {i} ({msg['role']}):\")\n",
    "    for content in msg['content']:\n",
    "        if content['type'] == 'text':\n",
    "            preview = content['text'][:100] + \"...\" if len(content['text']) > 100 else content['text']\n",
    "            print(f\"  [text]: {preview}\")\n",
    "        else:\n",
    "            print(f\"  [{content['type']}]\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nðŸ“Š Total messages: {len(messages)}\")\n",
    "print(f\"ðŸ“Š User messages: {sum(1 for m in messages if m['role'] == 'user')}\")\n",
    "print(f\"ðŸ“Š Assistant messages: {sum(1 for m in messages if m['role'] == 'assistant')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Full Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the entire conversation to a file\n",
    "output_path = Path(\"llama_multiturn_debit_conversation.txt\")\n",
    "\n",
    "with output_path.open(\"w\", encoding=\"utf-8\") as text_file:\n",
    "    text_file.write(\"=\" * 60 + \"\\n\")\n",
    "    text_file.write(\"MULTI-TURN DEBIT EXTRACTION CONVERSATION\\n\")\n",
    "    text_file.write(\"Llama-3.2-Vision-11B\\n\")\n",
    "    text_file.write(\"=\" * 60 + \"\\n\\n\")\n",
    "    \n",
    "    for i, msg in enumerate(messages, 1):\n",
    "        role = msg[\"role\"].upper()\n",
    "        text_file.write(f\"\\n{'-' * 60}\\n\")\n",
    "        text_file.write(f\"MESSAGE {i} - {role}\\n\")\n",
    "        text_file.write(f\"{'-' * 60}\\n\\n\")\n",
    "        \n",
    "        for content in msg[\"content\"]:\n",
    "            if content[\"type\"] == \"text\":\n",
    "                text_file.write(content[\"text\"] + \"\\n\")\n",
    "            elif content[\"type\"] == \"image\":\n",
    "                text_file.write(\"[IMAGE]\\n\")\n",
    "    \n",
    "    text_file.write(\"\\n\" + \"=\" * 60 + \"\\n\")\n",
    "    text_file.write(f\"Total messages: {len(messages)}\\n\")\n",
    "    text_file.write(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "print(f\"âœ… Full conversation saved to: {output_path}\")\n",
    "print(f\"ðŸ“Š File size: {output_path.stat().st_size} bytes\")\n",
    "print(f\"ðŸ’¬ Total messages in conversation: {len(messages)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
