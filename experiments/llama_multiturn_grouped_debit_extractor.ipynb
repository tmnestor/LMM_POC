{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama 3.2 Vision Multi-Turn Debit Extractor\n",
    "\n",
    "This notebook demonstrates multi-turn conversational extraction using Llama 3.2 Vision.\n",
    "Uses the `chat_with_mllm` pattern for clean, maintainable multi-turn conversations.\n",
    "\n",
    "**Reference**: [Chat with Your Images Using Multimodal LLMs](https://medium.com/data-science/chat-with-your-images-using-multimodal-llms-60af003e8bfa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add project root to path for common/ imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, MllamaForConditionalGeneration\n",
    "from transformers.image_utils import load_image\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-emptive Memory Cleanup\n",
    "\n",
    "Optional GPU memory cleanup to prevent OOM errors when switching between models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Pre-emptive memory cleanup (useful when switching models)\n",
    "try:\n",
    "    from common.gpu_optimization import emergency_cleanup\n",
    "    print(\"üßπ Clearing GPU memory...\")\n",
    "    emergency_cleanup(verbose=False)\n",
    "    print(\"‚úÖ Memory cleanup complete\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è GPU optimization module not available - skipping cleanup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Random Seed for Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.reproducibility import set_seed\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"/home/jovyan/shared_PTM/Llama-3.2-11B-Vision-Instruct\"\n",
    "\n",
    "print(\"üîß Loading Llama-3.2-Vision model...\")\n",
    "\n",
    "from common.llama_model_loader_robust import load_llama_model_robust\n",
    "\n",
    "model, processor = load_llama_model_robust(\n",
    "    model_path=model_id,\n",
    "    use_quantization=False,\n",
    "    device_map='auto',\n",
    "    max_new_tokens=2000,\n",
    "    torch_dtype='bfloat16',\n",
    "    low_cpu_mem_usage=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Add tie_weights() call\n",
    "try:\n",
    "    model.tie_weights()\n",
    "    print(\"‚úÖ Model weights tied successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è tie_weights() warning: {e}\")\n",
    "\n",
    "print(\"‚úÖ Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Manual Memory Cleanup\n",
    "\n",
    "Run this cell if you experience memory issues during the conversation. Not needed for normal operation on H200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Run this cell if you experience memory issues during conversation\n",
    "import gc\n",
    "\n",
    "print(\"üßπ Manual memory cleanup...\")\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Show memory status\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        allocated = torch.cuda.memory_allocated(i) / 1e9\n",
    "        reserved = torch.cuda.memory_reserved(i) / 1e9\n",
    "        print(f\"   GPU {i}: {allocated:.2f}GB allocated, {reserved:.2f}GB reserved\")\n",
    "print(\"‚úÖ Cleanup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define chat_with_mllm Function\n",
    "\n",
    "This function encapsulates the multi-turn conversation pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from common.llama_multiturn_chat import chat_with_mllm\n\nprint(\"‚úÖ chat_with_mllm function imported from common.llama_multiturn_chat\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Conversation Prompts\n",
    "\n",
    "All prompts are defined upfront for easy modification and review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversation prompts dictionary\n",
    "CONVERSATION_PROMPTS = {\n",
    "    \"turn_1_initial_extraction\": \"\"\"You are an expert document analyser specializing in Date Grouped Australian Bank Statement extraction.\n",
    "Date Grouped Bank Statements are date ordered, with one or more transactions for each date header.\n",
    "Every transaction for a given date heading has a description, a debit/credit amount and finally a balance amount with a ' CR' suffix.\n",
    "Extract all balance amounts along with their ' CR' suffix, the transaction dates (from the date heading) and transaction descriptions,\n",
    "maintaining the same date ordering as the image, with every transaction appearing on its own row and remembering that some date headings have more than one balance.\"\"\",\n",
    "    \n",
    "    \"turn_2_count_transactions\": \"How many transactions are shown in this bank statement?\",\n",
    "    \n",
    "    \"turn_3_extract_debits\": \"From your first response, extract ONLY the debit/withdrawal amounts (amounts paid out). List them in order, one per line.\",\n",
    "    \n",
    "    \"turn_4_verify_count\": \"How many debit/withdrawal transactions did you extract in your previous response?\",\n",
    "    \n",
    "    \"turn_5_total_debits\": \"What is the total sum of all debit/withdrawal amounts in this statement?\",\n",
    "    \n",
    "    \"turn_6_date_range\": \"What is the date range covered by this bank statement?\",\n",
    "    \n",
    "    \"turn_7_verify_consistency\": \"In your very first response, you extracted all transactions. Can you verify that the debit amounts you listed in turn 3 match the debit amounts from your first extraction?\"\n",
    "}\n",
    "\n",
    "def display_prompts(prompts_dict):\n",
    "    \"\"\"Display all conversation prompts in a readable format.\"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"CONVERSATION PROMPTS\")\n",
    "    print(\"=\" * 70)\n",
    "    for i, (key, prompt) in enumerate(prompts_dict.items(), 1):\n",
    "        # Format the key for display\n",
    "        turn_name = key.replace(\"_\", \" \").title()\n",
    "        print(f\"\\n{i}. {turn_name}\")\n",
    "        print(\"-\" * 70)\n",
    "        # Truncate long prompts for preview\n",
    "        preview = prompt if len(prompt) <= 200 else prompt[:197] + \"...\"\n",
    "        print(f\"{preview}\")\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(f\"Total prompts defined: {len(prompts_dict)}\")\n",
    "\n",
    "# Display the prompts\n",
    "display_prompts(CONVERSATION_PROMPTS)\n",
    "print(\"\\n‚úÖ Conversation prompts defined and ready to use\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Extraction (Turn 1)\n",
    "\n",
    "Extract all transaction data from the bank statement image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image path\n",
    "imageName = \"/home/jovyan/_LMM_POC/evaluation_data/image_009.png\"\n",
    "\n",
    "# Initialize conversation\n",
    "messages = []\n",
    "images = []\n",
    "\n",
    "print(\"üì∏ Processing bank statement image...\")\n",
    "print(f\"üìù Using prompt: turn_1_initial_extraction\")\n",
    "\n",
    "response1, messages, images = chat_with_mllm(\n",
    "    model, processor, \n",
    "    CONVERSATION_PROMPTS[\"turn_1_initial_extraction\"],\n",
    "    images_path=[imageName],\n",
    "    do_sample=False,\n",
    "    max_new_tokens=2000,\n",
    "    show_image=True,\n",
    "    messages=messages,\n",
    "    images=images\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TURN 1 - INITIAL EXTRACTION:\")\n",
    "print(\"=\" * 60)\n",
    "print(response1)\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Save initial extraction\n",
    "Path(\"llama_debit_extractor_initial.txt\").write_text(response1)\n",
    "print(\"\\n‚úÖ Initial extraction saved to llama_debit_extractor_initial.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# OPTIONAL: Uncomment to enable preprocessing\n# from common.image_preprocessing import enhance_for_llama, preprocess_statement_for_llama, enhance_statement_quality\n# from PIL import Image\n# import tempfile\n\n# # Choose ONE preprocessing approach:\n\n# # Option 1: Light enhancement (recommended for high-quality scans)\n# # preprocessed_img = enhance_statement_quality(imageName)\n\n# # Option 2: Moderate enhancement (upscaling + sharpness + contrast)\n# # preprocessed_img = enhance_for_llama(imageName)\n\n# # Option 3: Aggressive preprocessing (denoise + binarize + remove lines)\n# # preprocessed_img = preprocess_statement_for_llama(imageName)\n\n# # Save preprocessed image to temporary file and update imageName\n# # with tempfile.NamedTemporaryFile(suffix='.png', delete=False) as tmp:\n# #     preprocessed_img.save(tmp.name)\n# #     imageName = tmp.name\n# #     print(f\"‚úÖ Using preprocessed image: {imageName}\")\n# #     display(preprocessed_img)\n\nprint(\"‚è≠Ô∏è  Skipping preprocessing - using original image\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## OPTIONAL: Image Preprocessing\n\n**Experimental:** Test whether preprocessing improves extraction accuracy.\n\nAvailable preprocessing functions:\n- `enhance_for_llama()` - Upscale, sharpen, increase contrast\n- `preprocess_statement_for_llama()` - Denoise, binarize, remove table lines\n- `enhance_statement_quality()` - Moderate enhancement for bank statements\n\n**Note:** Modern VLMs are trained on natural images. Preprocessing may help with low-quality scans but could hurt performance on high-quality images. Test both approaches to see what works best for your data.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turn 2: Count Transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"üìù Using prompt: turn_2_count_transactions\")\n",
    "\n",
    "response2, messages, images = chat_with_mllm(\n",
    "    model, processor,\n",
    "    CONVERSATION_PROMPTS[\"turn_2_count_transactions\"],\n",
    "    messages=messages, \n",
    "    images=images,\n",
    "    max_new_tokens=500\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TURN 2 - TRANSACTION COUNT:\")\n",
    "print(\"=\" * 60)\n",
    "print(response2)\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turn 3: Extract Debit/Withdrawal Amounts Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"üìù Using prompt: turn_3_extract_debits\")\n",
    "\n",
    "response3, messages, images = chat_with_mllm(\n",
    "    model, processor,\n",
    "    CONVERSATION_PROMPTS[\"turn_3_extract_debits\"],\n",
    "    messages=messages,\n",
    "    images=images,\n",
    "    max_new_tokens=1000\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TURN 3 - DEBIT AMOUNTS ONLY:\")\n",
    "print(\"=\" * 60)\n",
    "print(response3)\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Save debit amounts\n",
    "Path(\"llama_debit_amounts.txt\").write_text(response3)\n",
    "print(\"\\n‚úÖ Debit amounts saved to llama_debit_amounts.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turn 4: Verify Debit Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"üìù Using prompt: turn_4_verify_count\")\n",
    "\n",
    "response4, messages, images = chat_with_mllm(\n",
    "    model, processor,\n",
    "    CONVERSATION_PROMPTS[\"turn_4_verify_count\"],\n",
    "    messages=messages,\n",
    "    images=images,\n",
    "    max_new_tokens=500\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TURN 4 - DEBIT COUNT:\")\n",
    "print(\"=\" * 60)\n",
    "print(response4)\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turn 5: Total Debit Amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"üìù Using prompt: turn_5_total_debits\")\n",
    "\n",
    "response5, messages, images = chat_with_mllm(\n",
    "    model, processor,\n",
    "    CONVERSATION_PROMPTS[\"turn_5_total_debits\"],\n",
    "    messages=messages,\n",
    "    images=images,\n",
    "    max_new_tokens=500\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TURN 5 - TOTAL DEBITS:\")\n",
    "print(\"=\" * 60)\n",
    "print(response5)\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turn 6: Date Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"üìù Using prompt: turn_6_date_range\")\n",
    "\n",
    "response6, messages, images = chat_with_mllm(\n",
    "    model, processor,\n",
    "    CONVERSATION_PROMPTS[\"turn_6_date_range\"],\n",
    "    messages=messages,\n",
    "    images=images,\n",
    "    max_new_tokens=500\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TURN 6 - DATE RANGE:\")\n",
    "print(\"=\" * 60)\n",
    "print(response6)\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turn 7: Verification - Cross-check First Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"üìù Using prompt: turn_7_verify_consistency\")\n",
    "\n",
    "response7, messages, images = chat_with_mllm(\n",
    "    model, processor,\n",
    "    CONVERSATION_PROMPTS[\"turn_7_verify_consistency\"],\n",
    "    messages=messages,\n",
    "    images=images,\n",
    "    max_new_tokens=1000\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TURN 7 - VERIFICATION:\")\n",
    "print(\"=\" * 60)\n",
    "print(response7)\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug: View Conversation Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç Current conversation structure:\")\n",
    "print(\"=\" * 60)\n",
    "for i, msg in enumerate(messages, 1):\n",
    "    print(f\"\\nMessage {i} ({msg['role']}):\")\n",
    "    for content in msg['content']:\n",
    "        if content['type'] == 'text':\n",
    "            preview = content['text'][:100] + \"...\" if len(content['text']) > 100 else content['text']\n",
    "            print(f\"  [text]: {preview}\")\n",
    "        else:\n",
    "            print(f\"  [{content['type']}]\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nüìä Total messages: {len(messages)}\")\n",
    "print(f\"üìä User messages: {sum(1 for m in messages if m['role'] == 'user')}\")\n",
    "print(f\"üìä Assistant messages: {sum(1 for m in messages if m['role'] == 'assistant')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Full Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the entire conversation to a file\n",
    "output_path = Path(\"llama_multiturn_debit_conversation.txt\")\n",
    "\n",
    "with output_path.open(\"w\", encoding=\"utf-8\") as text_file:\n",
    "    text_file.write(\"=\" * 60 + \"\\n\")\n",
    "    text_file.write(\"MULTI-TURN DEBIT EXTRACTION CONVERSATION\\n\")\n",
    "    text_file.write(\"Llama-3.2-Vision-11B\\n\")\n",
    "    text_file.write(\"=\" * 60 + \"\\n\\n\")\n",
    "    \n",
    "    for i, msg in enumerate(messages, 1):\n",
    "        role = msg[\"role\"].upper()\n",
    "        text_file.write(f\"\\n{'-' * 60}\\n\")\n",
    "        text_file.write(f\"MESSAGE {i} - {role}\\n\")\n",
    "        text_file.write(f\"{'-' * 60}\\n\\n\")\n",
    "        \n",
    "        for content in msg[\"content\"]:\n",
    "            if content[\"type\"] == \"text\":\n",
    "                text_file.write(content[\"text\"] + \"\\n\")\n",
    "            elif content[\"type\"] == \"image\":\n",
    "                text_file.write(\"[IMAGE]\\n\")\n",
    "    \n",
    "    text_file.write(\"\\n\" + \"=\" * 60 + \"\\n\")\n",
    "    text_file.write(f\"Total messages: {len(messages)}\\n\")\n",
    "    text_file.write(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "print(f\"‚úÖ Full conversation saved to: {output_path}\")\n",
    "print(f\"üìä File size: {output_path.stat().st_size} bytes\")\n",
    "print(f\"üí¨ Total messages in conversation: {len(messages)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}