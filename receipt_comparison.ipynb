{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 0\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, str(Path.cwd()))\n",
    "from common.evaluation_metrics import calculate_field_accuracy\n",
    "from common.config import get_document_type_fields\n",
    "\n",
    "print(\"✅ Imports loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 1\n",
    "GROUND_TRUTH_PATH = 'evaluation_data/ground_truth.csv'\n",
    "llama_files = sorted(glob.glob('output/csv/llama_batch_results_*.csv'))\n",
    "LLAMA_RESULTS_PATH = llama_files[-1] if llama_files else None\n",
    "\n",
    "print(f\"Ground truth: {GROUND_TRUTH_PATH}\")\n",
    "print(f\"Llama results: {LLAMA_RESULTS_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 2\n",
    "gt = pd.read_csv(GROUND_TRUTH_PATH)\n",
    "llama = pd.read_csv(LLAMA_RESULTS_PATH)\n",
    "\n",
    "gt_receipts = gt[gt['DOCUMENT_TYPE'] == 'RECEIPT'].copy()\n",
    "llama_receipts = llama[llama['document_type'] == 'RECEIPT'].copy()\n",
    "\n",
    "print(f\"Ground truth receipts: {len(gt_receipts)}\")\n",
    "print(f\"Llama receipts: {len(llama_receipts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 3\n",
    "gt_receipts['image_stem'] = gt_receipts['image_file'].apply(lambda x: Path(x).stem)\n",
    "llama_receipts['image_stem'] = llama_receipts['image_file'].apply(lambda x: Path(x).stem)\n",
    "\n",
    "merged = gt_receipts.merge(\n",
    "    llama_receipts,\n",
    "    on='image_stem',\n",
    "    how='inner',\n",
    "    suffixes=('_gt', '_llama')\n",
    ")\n",
    "\n",
    "print(f\"Merged receipts: {len(merged)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 4\n",
    "RECEIPT_FIELDS = [\n",
    "    'DOCUMENT_TYPE', 'BUSINESS_ABN', 'SUPPLIER_NAME', 'BUSINESS_ADDRESS',\n",
    "    'PAYER_NAME', 'PAYER_ADDRESS', 'INVOICE_DATE',\n",
    "    'LINE_ITEM_DESCRIPTIONS', 'LINE_ITEM_QUANTITIES', 'LINE_ITEM_PRICES', 'LINE_ITEM_TOTAL_PRICES',\n",
    "    'IS_GST_INCLUDED', 'GST_AMOUNT', 'TOTAL_AMOUNT'\n",
    "]\n",
    "\n",
    "results = []\n",
    "for _, row in merged.iterrows():\n",
    "    image = row['image_stem']\n",
    "    for field in RECEIPT_FIELDS:\n",
    "        gt_val = str(row[f'{field}_gt']) if pd.notna(row[f'{field}_gt']) else ''\n",
    "        llama_val = str(row[field]) if pd.notna(row[field]) else ''\n",
    "        \n",
    "        accuracy = calculate_field_accuracy(llama_val, gt_val, field)\n",
    "        \n",
    "        results.append({\n",
    "            'image': image,\n",
    "            'field': field,\n",
    "            'ground_truth': gt_val,\n",
    "            'llama': llama_val,\n",
    "            'accuracy': accuracy,\n",
    "            'match': accuracy == 1.0\n",
    "        })\n",
    "\n",
    "comparison_df = pd.DataFrame(results)\n",
    "print(f\"✅ Compared {len(comparison_df)} field values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 5\n",
    "overall_accuracy = comparison_df['accuracy'].mean() * 100\n",
    "exact_matches = comparison_df['match'].sum()\n",
    "total_fields = len(comparison_df)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"RECEIPT EXTRACTION COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Images compared: {len(merged)}\")\n",
    "print(f\"Overall accuracy: {overall_accuracy:.1f}%\")\n",
    "print(f\"Exact matches: {exact_matches}/{total_fields} ({exact_matches/total_fields*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 6\n",
    "field_accuracy = comparison_df.groupby('field').agg({\n",
    "    'accuracy': 'mean',\n",
    "    'match': 'sum'\n",
    "}).reset_index()\n",
    "field_accuracy['total'] = comparison_df.groupby('field').size().values\n",
    "field_accuracy['accuracy_pct'] = field_accuracy['accuracy'] * 100\n",
    "field_accuracy = field_accuracy.sort_values('accuracy_pct', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PER-FIELD ACCURACY\")\n",
    "print(\"=\"*80)\n",
    "print(field_accuracy[['field', 'match', 'total', 'accuracy_pct']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 7\n",
    "mismatches = comparison_df[comparison_df['accuracy'] < 1.0].copy()\n",
    "\n",
    "if len(mismatches) > 0:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"MISMATCHES ({len(mismatches)} fields)\")\n",
    "    print(\"=\"*80)\n",
    "    for _, row in mismatches.iterrows():\n",
    "        print(f\"\\n{row['image']} | {row['field']} (accuracy: {row['accuracy']:.2f})\")\n",
    "        print(f\"  GT:    {row['ground_truth'][:100]}\")\n",
    "        print(f\"  Llama: {row['llama'][:100]}\")\n",
    "else:\n",
    "    print(\"\\n✅ All fields match perfectly!\")"
   ]
  },
  {
   "cell_type": "code",
   "id": "4mcx7rly0z7",
   "source": "#Cell 8\nprint(\"=\"*80)\nprint(\"AVAILABLE RECEIPT IMAGES FOR UPDATE\")\nprint(\"=\"*80)\navailable_images = sorted(merged['image_stem'].unique())\nfor i, img in enumerate(available_images, 1):\n    print(f\"{i}. {img}\")\nprint(f\"\\nTotal: {len(available_images)} receipt images\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "c4ptg1ozjhv",
   "source": "#Cell 9\n# EDIT THESE LISTS TO SELECT WHAT TO UPDATE\n\n# Select which images to update (use image stems without extension)\nselected_images = [\n    # 'image_001',\n    # 'image_002',\n]\n\n# Select which fields to update\n# Available fields:\n# 'DOCUMENT_TYPE', 'BUSINESS_ABN', 'SUPPLIER_NAME', 'BUSINESS_ADDRESS',\n# 'PAYER_NAME', 'PAYER_ADDRESS', 'INVOICE_DATE',\n# 'LINE_ITEM_DESCRIPTIONS', 'LINE_ITEM_QUANTITIES', 'LINE_ITEM_PRICES', 'LINE_ITEM_TOTAL_PRICES',\n# 'IS_GST_INCLUDED', 'GST_AMOUNT', 'TOTAL_AMOUNT'\n\nselected_fields = [\n    # 'TOTAL_AMOUNT',\n    # 'GST_AMOUNT',\n]\n\nprint(f\"Selected {len(selected_images)} images: {selected_images}\")\nprint(f\"Selected {len(selected_fields)} fields: {selected_fields}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "4avm0ryg4vy",
   "source": "#Cell 10\ngt_updated = gt.copy()\ngt_updated['image_stem'] = gt_updated['image_file'].apply(lambda x: Path(x).stem)\n\nchanges = []\nfor img in selected_images:\n    for field in selected_fields:\n        # Get Llama's prediction for this image/field\n        llama_row = merged[merged['image_stem'] == img]\n        if len(llama_row) == 0:\n            print(f\"⚠️  Skipping {img} - not found in merged data\")\n            continue\n        \n        llama_value = llama_row.iloc[0][field]\n        \n        # Get current GT value\n        gt_idx = gt_updated[gt_updated['image_stem'] == img].index\n        if len(gt_idx) == 0:\n            print(f\"⚠️  Skipping {img} - not found in ground truth\")\n            continue\n        \n        old_value = gt_updated.loc[gt_idx[0], field]\n        \n        # Update\n        gt_updated.loc[gt_idx[0], field] = llama_value\n        \n        changes.append({\n            'image': img,\n            'field': field,\n            'old_value': old_value,\n            'new_value': llama_value\n        })\n\nchanges_df = pd.DataFrame(changes)\ngt_updated = gt_updated.drop(columns=['image_stem'])\n\nprint(f\"✅ Prepared {len(changes)} field updates\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "xc9a2bvwrzk",
   "source": "#Cell 11\nif len(changes_df) > 0:\n    print(\"=\"*80)\n    print(\"PREVIEW OF CHANGES\")\n    print(\"=\"*80)\n    pd.set_option('display.max_colwidth', 80)\n    print(changes_df.to_string(index=False))\n    print(f\"\\nTotal changes: {len(changes_df)}\")\nelse:\n    print(\"⚠️  No changes to preview. Check your selections in Cell 9.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "e1xr35zr9ph",
   "source": "#Cell 12\nif len(changes_df) > 0:\n    output_path = 'evaluation_data/ground_truth_updated.csv'\n    gt_updated.to_csv(output_path, index=False)\n    \n    print(\"=\"*80)\n    print(\"GROUND TRUTH UPDATED\")\n    print(\"=\"*80)\n    print(f\"✅ Saved to: {output_path}\")\n    print(f\"✅ Updated {len(changes_df)} fields across {len(selected_images)} images\")\n    print(f\"\\nFields updated: {selected_fields}\")\n    print(f\"Images updated: {selected_images}\")\nelse:\n    print(\"⚠️  No updates to save. Select images and fields in Cell 9.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}