{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison: Llama vs InternVL3\n",
    "\n",
    "**Business Document Processing Performance Analysis**\n",
    "\n",
    "---\n",
    "\n",
    "This notebook provides a comprehensive side-by-side comparison of Llama-3.2-Vision-11B and InternVL3-8B models for business document information extraction.\n",
    "\n",
    "## Key Business Questions Addressed:\n",
    "1. **Accuracy**: Which model extracts information more reliably?\n",
    "2. **Speed**: Which model processes documents faster?\n",
    "3. **Document Type Performance**: How do models perform on invoices, receipts, and bank statements?\n",
    "4. **Resource Efficiency**: Which model provides better ROI?\n",
    "5. **Production Readiness**: Which model is recommended for deployment?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2\n",
    "\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "from rich import print as rprint\n",
    "from rich.console import Console\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "console = Console()\n",
    "\n",
    "# Set professional styling for executive presentation\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Configuration with flexible path handling - consistent with batch notebooks\n",
    "# Define base path for flexible deployment\n",
    "\n",
    "# base_data_path = '/home/jovyan/nfs_share/tod/LMM_POC'\n",
    "# base_data_path = '/home/jovyan/LMM_POC-oct6'\n",
    "base_data_path = '/home/jovyan/_LMM_POC'\n",
    "\n",
    "CONFIG = {\n",
    "    # Path settings - Now supports both absolute and relative paths\n",
    "    'output_dir': f'{base_data_path}/output/csv',  # Can be relative or absolute path\n",
    "    'ground_truth_path': f'{base_data_path}/evaluation_data/lmm_poc_gt_20251111.csv',  # Ground truth for field accuracy\n",
    "    \n",
    "    # UPDATED: File patterns matching new notebook naming conventions\n",
    "    # Current naming: llama_batch_results_*.csv, internvl3_batch_results_*.csv, etc.\n",
    "    'llama_pattern': 'llama_batch_results_*.csv',\n",
    "    'internvl3_quantized_pattern': 'internvl3_batch_results_*.csv',  # Matches quantized InternVL3 (2B or 8B)\n",
    "    'internvl3_non_quantized_pattern': 'internvl3_non_quantized_batch_results_*.csv',  # Matches non-quantized\n",
    "    \n",
    "    # Visualization settings\n",
    "    'figure_size': (16, 10),\n",
    "    'dpi': 300,\n",
    "    'save_format': 'png'\n",
    "}\n",
    "\n",
    "# Examples of flexible path configuration:\n",
    "# Relative paths (relative to current working directory):\n",
    "#   'output_dir': 'output/csv'\n",
    "#\n",
    "# Absolute paths (can be anywhere on the system):\n",
    "#   'output_dir': '/home/user/results/vision_model_outputs/csv'\n",
    "#\n",
    "# Environment variable based paths:\n",
    "#   'output_dir': os.path.join(os.getenv('OUTPUT_DIR', 'output'), 'csv')\n",
    "#\n",
    "# Dynamic paths based on other locations:\n",
    "#   base_results_path = '/mnt/shared_data/vision_results'\n",
    "#   'output_dir': f'{base_results_path}/csv'\n",
    "\n",
    "print(\"‚úÖ Model comparison configuration loaded\")\n",
    "print(f\"üìÅ Output directory: {CONFIG['output_dir']}\")\n",
    "print(f\"üîç Looking for CSV files in: {Path(CONFIG['output_dir']).absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4\n",
    "\n",
    "def load_latest_results(output_dir: str, pattern: str, model_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load the most recent CSV results file matching the given pattern.\n",
    "    \n",
    "    Args:\n",
    "        output_dir: Directory to search for files (supports absolute and relative paths)\n",
    "        pattern: Glob pattern to match files\n",
    "        model_name: Name of the model for the 'model' column\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with results, or empty DataFrame if no files found\n",
    "    \"\"\"\n",
    "    import glob\n",
    "    from pathlib import Path\n",
    "    \n",
    "    # Handle both absolute and relative paths\n",
    "    output_path = Path(output_dir)\n",
    "    if not output_path.is_absolute():\n",
    "        # If relative, make it relative to current working directory\n",
    "        output_path = Path.cwd() / output_path\n",
    "    \n",
    "    search_path = output_path / pattern\n",
    "    files = glob.glob(str(search_path))\n",
    "    \n",
    "    if not files:\n",
    "        rprint(f\"[yellow]‚ö†Ô∏è No files found matching pattern: {pattern}[/yellow]\")\n",
    "        rprint(f\"[dim]   Searched in: {output_path}[/dim]\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Get the most recent file\n",
    "    latest_file = max(files, key=lambda x: Path(x).stat().st_mtime)\n",
    "    rprint(f\"[green]‚úÖ Loading {model_name} data from: {Path(latest_file).name}[/green]\")\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(latest_file)\n",
    "        \n",
    "        # Add model column if it doesn't exist\n",
    "        if 'model' not in df.columns:\n",
    "            df['model'] = model_name\n",
    "        \n",
    "        # Ensure required columns exist\n",
    "        required_cols = ['overall_accuracy', 'processing_time', 'document_type']\n",
    "        missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "        \n",
    "        if missing_cols:\n",
    "            rprint(f\"[yellow]‚ö†Ô∏è Missing columns in {model_name} data: {missing_cols}[/yellow]\")\n",
    "            # Create dummy columns with default values\n",
    "            for col in missing_cols:\n",
    "                if col == 'overall_accuracy':\n",
    "                    df[col] = 0.0\n",
    "                elif col == 'processing_time':\n",
    "                    df[col] = 1.0\n",
    "                elif col == 'document_type':\n",
    "                    df[col] = 'unknown'\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        rprint(f\"[red]‚ùå Error loading {model_name} data from {latest_file}: {e}[/red]\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5\n",
    "\n",
    "# Load data for all available models\n",
    "rprint(\"[bold blue]üìä Loading Model Performance Data[/bold blue]\")\n",
    "\n",
    "llama_df = load_latest_results(CONFIG['output_dir'], CONFIG['llama_pattern'], 'Llama-3.2-Vision')\n",
    "internvl3_quantized_df = load_latest_results(CONFIG['output_dir'], CONFIG['internvl3_quantized_pattern'], 'InternVL3-Quantized-8B')\n",
    "internvl3_non_quantized_df = load_latest_results(CONFIG['output_dir'], CONFIG['internvl3_non_quantized_pattern'], 'InternVL3-NonQuantized-2B')\n",
    "\n",
    "# Combine all available dataframes\n",
    "dfs_to_concat = []\n",
    "if not llama_df.empty:\n",
    "    dfs_to_concat.append(llama_df)\n",
    "if not internvl3_quantized_df.empty:\n",
    "    dfs_to_concat.append(internvl3_quantized_df)\n",
    "if not internvl3_non_quantized_df.empty:\n",
    "    dfs_to_concat.append(internvl3_non_quantized_df)\n",
    "\n",
    "if dfs_to_concat:\n",
    "    combined_df = pd.concat(dfs_to_concat, ignore_index=True)\n",
    "    rprint(f\"[green]‚úÖ Combined dataset: {len(combined_df)} records from {len(dfs_to_concat)} models[/green]\")\n",
    "    \n",
    "    # Show what models were loaded\n",
    "    loaded_models = combined_df['model'].unique()\n",
    "    rprint(f\"[cyan]üìã Models loaded: {', '.join(loaded_models)}[/cyan]\")\n",
    "else:\n",
    "    rprint(\"[red]‚ùå No data loaded for any model[/red]\")\n",
    "    rprint(\"\\n[yellow]üí° To use this notebook:[/yellow]\")\n",
    "    rprint(\"  1. Run llama_batch.ipynb to generate Llama results\")\n",
    "    rprint(\"  2. Run ivl3_batch.ipynb to generate InternVL3 quantized results\")\n",
    "    rprint(\"  3. Run ivl3_8b_batch_non_quantized.ipynb for non-quantized results\")\n",
    "    rprint(\"  4. Re-run this notebook to generate the comparison\")\n",
    "    combined_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6\n",
    "\n",
    "def generate_executive_summary(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate executive summary statistics from combined model data.\n",
    "    \n",
    "    Args:\n",
    "        df: Combined DataFrame with all model results\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with summary statistics per model\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    summary_stats = df.groupby('model').agg({\n",
    "        'overall_accuracy': ['mean', 'std', 'min', 'max'],\n",
    "        'processing_time': ['mean', 'std', 'min', 'max'],\n",
    "        'fields_extracted': 'mean',\n",
    "        'fields_matched': 'mean'\n",
    "    }).round(2)\n",
    "    \n",
    "    # Flatten column names\n",
    "    summary_stats.columns = ['_'.join(col).strip() for col in summary_stats.columns.values]\n",
    "    \n",
    "    # Calculate throughput (documents per minute)\n",
    "    summary_stats['throughput_docs_per_min'] = (60 / summary_stats['processing_time_mean']).round(2)\n",
    "    \n",
    "    # Calculate efficiency score (accuracy √ó throughput)\n",
    "    summary_stats['efficiency_score'] = (summary_stats['overall_accuracy_mean'] * summary_stats['throughput_docs_per_min']).round(2)\n",
    "    \n",
    "    return summary_stats.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7\n",
    "\n",
    "def create_executive_dashboard(df: pd.DataFrame, save_path: str = None):\n",
    "    \"\"\"\n",
    "    Create comprehensive 6-panel executive dashboard comparing model performance.\n",
    "    \n",
    "    Args:\n",
    "        df: Combined DataFrame with results from all models\n",
    "        save_path: Optional path to save the visualization\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        rprint(\"[red]‚ùå Cannot create dashboard - no data available[/red]\")\n",
    "        return\n",
    "    \n",
    "    # Define model order and colors for consistency\n",
    "    model_order = ['Llama-3.2-Vision', 'InternVL3-Quantized-8B', 'InternVL3-NonQuantized-2B']\n",
    "    fixed_colors = {\n",
    "        'Llama-3.2-Vision': '#E74C3C',\n",
    "        'InternVL3-Quantized-8B': '#3498DB', \n",
    "        'InternVL3-NonQuantized-2B': '#27AE60'\n",
    "    }\n",
    "    \n",
    "    # Filter to only available models and maintain order\n",
    "    available_models = df['model'].unique()\n",
    "    models = [model for model in model_order if model in available_models]\n",
    "    model_colors = [fixed_colors[model] for model in models if model in fixed_colors]\n",
    "    \n",
    "    # Create figure with 6 subplots\n",
    "    fig = plt.figure(figsize=(20, 12))\n",
    "    gs = fig.add_gridspec(3, 2, height_ratios=[1, 1, 1], hspace=0.45, wspace=0.35)\n",
    "    \n",
    "    # 1. Overall Accuracy Comparison (Box Plot)\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    df_plot = df[df['model'].isin(models)]\n",
    "    \n",
    "    box_parts = ax1.boxplot(\n",
    "        [df_plot[df_plot['model'] == model]['overall_accuracy'].values for model in models],\n",
    "        labels=models,\n",
    "        patch_artist=True,\n",
    "        showmeans=True\n",
    "    )\n",
    "    \n",
    "    for patch, color in zip(box_parts['boxes'], model_colors):\n",
    "        patch.set_facecolor(color)\n",
    "        patch.set_alpha(0.7)\n",
    "    \n",
    "    ax1.set_ylabel('Overall Accuracy (%)', fontsize=12)\n",
    "    ax1.set_title('Overall Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "    ax1.grid(True, alpha=0.3, axis='y')\n",
    "    ax1.set_xticklabels(models, rotation=15, ha='right')\n",
    "    \n",
    "    # 2. Processing Speed Comparison (Box Plot)\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    \n",
    "    box_parts = ax2.boxplot(\n",
    "        [df_plot[df_plot['model'] == model]['processing_time'].values for model in models],\n",
    "        labels=models,\n",
    "        patch_artist=True,\n",
    "        showmeans=True\n",
    "    )\n",
    "    \n",
    "    for patch, color in zip(box_parts['boxes'], model_colors):\n",
    "        patch.set_facecolor(color)\n",
    "        patch.set_alpha(0.7)\n",
    "    \n",
    "    ax2.set_ylabel('Processing Time (seconds)', fontsize=12)\n",
    "    ax2.set_title('Processing Speed Comparison', fontsize=14, fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "    ax2.set_xticklabels(models, rotation=15, ha='right')\n",
    "    \n",
    "    # 3. Average Accuracy by Document Type (Grouped Bar Chart)\n",
    "    ax3 = fig.add_subplot(gs[1, 0])\n",
    "    \n",
    "    doc_accuracy = df_plot.groupby(['document_type', 'model'])['overall_accuracy'].mean().unstack()\n",
    "    doc_accuracy = doc_accuracy[models]  # Ensure correct order\n",
    "    \n",
    "    x = np.arange(len(doc_accuracy.index))\n",
    "    width = 0.8 / len(models)\n",
    "    \n",
    "    for idx, model in enumerate(models):\n",
    "        offset = (idx - len(models)/2 + 0.5) * width\n",
    "        ax3.bar(x + offset, doc_accuracy[model], width, \n",
    "               label=model, color=fixed_colors[model], alpha=0.8)\n",
    "    \n",
    "    ax3.set_ylabel('Average Accuracy (%)', fontsize=12)\n",
    "    ax3.set_xlabel('Document Type', fontsize=12)\n",
    "    ax3.set_title('Average Accuracy by Document Type', fontsize=14, fontweight='bold')\n",
    "    ax3.set_xticks(x)\n",
    "    ax3.set_xticklabels(doc_accuracy.index, rotation=15, ha='right')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # 4. Average Processing Time by Document Type (Grouped Bar Chart)\n",
    "    ax4 = fig.add_subplot(gs[1, 1])\n",
    "    \n",
    "    doc_time = df_plot.groupby(['document_type', 'model'])['processing_time'].mean().unstack()\n",
    "    doc_time = doc_time[models]  # Ensure correct order\n",
    "    \n",
    "    for idx, model in enumerate(models):\n",
    "        offset = (idx - len(models)/2 + 0.5) * width\n",
    "        ax4.bar(x + offset, doc_time[model], width,\n",
    "               label=model, color=fixed_colors[model], alpha=0.8)\n",
    "    \n",
    "    ax4.set_ylabel('Average Processing Time (s)', fontsize=12)\n",
    "    ax4.set_xlabel('Document Type', fontsize=12)\n",
    "    ax4.set_title('Average Processing Time by Document Type', fontsize=14, fontweight='bold')\n",
    "    ax4.set_xticks(x)\n",
    "    ax4.set_xticklabels(doc_time.index, rotation=15, ha='right')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # 5. Efficiency Analysis: Accuracy vs Processing Time (Scatter Plot)\n",
    "    ax5 = fig.add_subplot(gs[2, 0])\n",
    "    \n",
    "    for model in models:\n",
    "        model_data = df_plot[df_plot['model'] == model]\n",
    "        ax5.scatter(model_data['processing_time'], model_data['overall_accuracy'],\n",
    "                   label=model, color=fixed_colors[model], alpha=0.6, s=100)\n",
    "    \n",
    "    ax5.set_xlabel('Processing Time (seconds)', fontsize=12)\n",
    "    ax5.set_ylabel('Overall Accuracy (%)', fontsize=12)\n",
    "    ax5.set_title('Efficiency Analysis: Accuracy vs Processing Time', fontsize=14, fontweight='bold')\n",
    "    ax5.legend()\n",
    "    ax5.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add diagonal efficiency lines\n",
    "    ax5.axline((0, 0), slope=10, color='gray', linestyle='--', alpha=0.3, label='10% per second')\n",
    "    ax5.axline((0, 0), slope=5, color='gray', linestyle='--', alpha=0.3, label='5% per second')\n",
    "    \n",
    "    # 6. Performance Summary Table\n",
    "    ax6 = fig.add_subplot(gs[2, 1])\n",
    "    ax6.axis('off')\n",
    "    \n",
    "    # Generate summary statistics\n",
    "    summary_data = []\n",
    "    for model in models:\n",
    "        model_data = df_plot[df_plot['model'] == model]\n",
    "        summary_data.append([\n",
    "            model,\n",
    "            f\"{model_data['overall_accuracy'].median():.1f}%\",\n",
    "            f\"{model_data['processing_time'].mean():.1f}s\",\n",
    "            f\"{(60 / model_data['processing_time'].mean()):.1f}\",\n",
    "            f\"{model_data['overall_accuracy'].std():.1f}%\"\n",
    "        ])\n",
    "    \n",
    "    table = ax6.table(cellText=summary_data,\n",
    "                     colLabels=['Model', 'Median Accuracy', 'Avg Time', 'Docs/min', 'Std Dev'],\n",
    "                     cellLoc='center',\n",
    "                     loc='center',\n",
    "                     colWidths=[0.35, 0.15, 0.15, 0.15, 0.15])\n",
    "    \n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    table.scale(1, 2)\n",
    "    \n",
    "    # Color header row\n",
    "    for i in range(5):\n",
    "        table[(0, i)].set_facecolor('#34495E')\n",
    "        table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "    \n",
    "    # Color data rows with model colors\n",
    "    for i, model in enumerate(models):\n",
    "        if model in fixed_colors:\n",
    "            table[(i+1, 0)].set_facecolor(fixed_colors[model])\n",
    "            table[(i+1, 0)].set_text_props(color='white', weight='bold')\n",
    "            table[(i+1, 0)].set_alpha(0.7)\n",
    "    \n",
    "    ax6.set_title('Performance Summary Table', fontsize=14, fontweight='bold', pad=20)\n",
    "    \n",
    "    # Overall title\n",
    "    fig.suptitle('MODEL PERFORMANCE COMPARISON\\nBusiness Document Information Extraction',\n",
    "                fontsize=20, fontweight='bold', y=0.98)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the visualization\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=CONFIG['dpi'], bbox_inches='tight')\n",
    "        rprint(f\"[green]‚úÖ Executive dashboard saved to: {save_path}[/green]\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Create the executive dashboard\n",
    "dashboard_timestamp = None\n",
    "if not combined_df.empty:\n",
    "    rprint(\"\\n[bold blue]üìä Creating Executive Dashboard[/bold blue]\")\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_path = Path(f\"output/visualizations/executive_comparison_{timestamp}.png\")\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    create_executive_dashboard(combined_df, str(output_path))\n",
    "    dashboard_timestamp = timestamp\n",
    "    \n",
    "    rprint(f\"[green]‚úÖ Executive dashboard created successfully[/green]\")\n",
    "else:\n",
    "    rprint(\"[red]‚ùå Cannot create executive dashboard - no data available[/red]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8\n",
    "\n",
    "def extract_field_level_accuracy_from_csv(output_dir: str, pattern: str, model_name: str, ground_truth_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract field-level accuracy by comparing CSV batch results against ground truth.\n",
    "    \n",
    "    This function:\n",
    "    1. Loads batch results CSV (which has extracted field values)\n",
    "    2. Loads ground truth CSV\n",
    "    3. Compares field-by-field using the SAME evaluation logic as llama_batch.ipynb\n",
    "    4. Returns per-field accuracy WITHOUT re-running document-type filtering\n",
    "    \n",
    "    Args:\n",
    "        output_dir: Directory containing batch results CSV files\n",
    "        pattern: Glob pattern to match files  \n",
    "        model_name: Name of the model\n",
    "        ground_truth_path: Path to ground truth CSV\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with columns: model, field_name, accuracy, correct_count, total_count\n",
    "    \"\"\"\n",
    "    import glob\n",
    "    from pathlib import Path\n",
    "    import sys\n",
    "    sys.path.insert(0, str(Path.cwd()))\n",
    "    \n",
    "    from common.evaluation_metrics import load_ground_truth, calculate_field_accuracy\n",
    "    \n",
    "    output_path = Path(output_dir)\n",
    "    if not output_path.is_absolute():\n",
    "        output_path = Path.cwd() / output_path\n",
    "    \n",
    "    search_path = output_path / pattern  \n",
    "    files = glob.glob(str(search_path))\n",
    "    \n",
    "    if not files:\n",
    "        rprint(f\"[yellow]‚ö†Ô∏è No batch results found: {pattern}[/yellow]\")\n",
    "        rprint(f\"[dim]   Searched in: {output_path}[/dim]\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Get the most recent file\n",
    "    latest_file = max(files, key=lambda x: Path(x).stat().st_mtime)\n",
    "    rprint(f\"[cyan]üìÑ Evaluating {model_name} from: {Path(latest_file).name}[/cyan]\")\n",
    "    \n",
    "    try:\n",
    "        # Load batch results CSV\n",
    "        batch_df = pd.read_csv(latest_file)\n",
    "        total_images = len(batch_df)\n",
    "        rprint(f\"[dim]  DEBUG: Loaded {total_images} rows from batch CSV[/dim]\")\n",
    "        \n",
    "        # Load ground truth\n",
    "        gt_path = Path(ground_truth_path)\n",
    "        if not gt_path.exists():\n",
    "            rprint(f\"[red]  ‚ùå Ground truth not found: {ground_truth_path}[/red]\")\n",
    "            return pd.DataFrame()\n",
    "            \n",
    "        ground_truth_map = load_ground_truth(str(gt_path), show_sample=False, verbose=False)\n",
    "        rprint(f\"[dim]  DEBUG: Ground truth has {len(ground_truth_map)} entries[/dim]\")\n",
    "        \n",
    "        # Determine which column to use for matching\n",
    "        # Ground truth uses image_file, batch CSV has both image_file and image_name\n",
    "        image_col = 'image_file' if 'image_file' in batch_df.columns else 'image_name'\n",
    "        \n",
    "        # DEBUG: Show sample image names\n",
    "        if len(batch_df) > 0:\n",
    "            sample_batch_names = batch_df[image_col].head(3).tolist()\n",
    "            rprint(f\"[dim]  DEBUG: Sample batch image names (from {image_col}): {sample_batch_names}[/dim]\")\n",
    "        \n",
    "        if len(ground_truth_map) > 0:\n",
    "            sample_gt_names = list(ground_truth_map.keys())[:3]\n",
    "            rprint(f\"[dim]  DEBUG: Sample ground truth names: {sample_gt_names}[/dim]\")\n",
    "\n",
    "        # FIX: Normalize image names by stripping extensions for matching\n",
    "        # Batch CSV has extensions (.jpeg, .png), ground truth does not\n",
    "        batch_df['image_stem'] = batch_df[image_col].apply(lambda x: Path(x).stem)\n",
    "\n",
    "        # Create ground truth mapping with stems as keys\n",
    "        ground_truth_by_stem = {}\n",
    "        for gt_name, gt_data in ground_truth_map.items():\n",
    "            stem = Path(str(gt_name)).stem  # Strip extension if present\n",
    "            ground_truth_by_stem[stem] = gt_data\n",
    "\n",
    "        rprint(f\"[dim]  DEBUG: Created stem-based mapping for {len(ground_truth_by_stem)} ground truth entries[/dim]\")\n",
    "\n",
    "\n",
    "        \n",
    "        # FILTER: Only evaluate images that have ground truth\n",
    "        # FIX: Match using image_file column (consistent with ground truth)\n",
    "        batch_df_filtered = batch_df[batch_df['image_stem'].isin(ground_truth_by_stem.keys())]\n",
    "        filtered_count = len(batch_df_filtered)\n",
    "        skipped_count = total_images - filtered_count\n",
    "        \n",
    "        rprint(f\"[dim]  DEBUG: Filtered to {filtered_count} matching images (skipped {skipped_count})[/dim]\")\n",
    "        \n",
    "        if filtered_count == 0:\n",
    "            rprint(f\"[red]  ‚ùå No images in batch match ground truth entries[/red]\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        rprint(f\"[cyan]  üìä Evaluating {filtered_count}/{total_images} images with ground truth[/cyan]\")\n",
    "        \n",
    "        # Track field accuracies - accumulate across all images\n",
    "        field_accuracies = {}\n",
    "        \n",
    "        # Get all possible field columns (exclude metadata columns)\n",
    "        metadata_cols = {'image_file', 'image_name', 'document_type', 'processing_time', \n",
    "                         'field_count', 'found_fields', 'field_coverage', 'prompt_used', \n",
    "                         'timestamp', 'overall_accuracy', 'fields_extracted', 'fields_matched', \n",
    "                         'total_fields', 'inference_only', 'model', 'image_stem'}\n",
    "        \n",
    "        field_columns = [col for col in batch_df_filtered.columns if col not in metadata_cols]\n",
    "        \n",
    "        rprint(f\"[dim]  DEBUG: Found {len(field_columns)} field columns to evaluate[/dim]\")\n",
    "        \n",
    "        # Evaluate each image\n",
    "        for _, row in batch_df_filtered.iterrows():\n",
    "            image_identifier = row[image_col]\n",
    "            \n",
    "            # Get ground truth for this image (use image_identifier as key)\n",
    "            # Get ground truth using stem (without extension)\n",
    "            image_stem = Path(str(image_identifier)).stem\n",
    "            gt_data = ground_truth_by_stem.get(image_stem)\n",
    "            if not gt_data:\n",
    "                continue\n",
    "            \n",
    "            # Compare each field\n",
    "            for field_name in field_columns:\n",
    "                extracted_value = row.get(field_name, 'NOT_FOUND')\n",
    "                ground_truth_value = gt_data.get(field_name, 'NOT_FOUND')\n",
    "                \n",
    "                # Skip if both are NOT_FOUND (field not applicable)\n",
    "                if str(extracted_value).upper() == 'NOT_FOUND' and str(ground_truth_value).upper() == 'NOT_FOUND':\n",
    "                    continue\n",
    "                \n",
    "                # Calculate accuracy score using the same logic as batch notebooks\n",
    "                accuracy_score = calculate_field_accuracy(\n",
    "                    extracted_value, ground_truth_value, field_name, debug=False\n",
    "                )\n",
    "                \n",
    "                # Initialize field tracking if needed\n",
    "                if field_name not in field_accuracies:\n",
    "                    field_accuracies[field_name] = {'correct': 0.0, 'total': 0}\n",
    "                \n",
    "                field_accuracies[field_name]['total'] += 1\n",
    "                field_accuracies[field_name]['correct'] += accuracy_score\n",
    "        \n",
    "        rprint(f\"[dim]  DEBUG: Computed accuracies for {len(field_accuracies)} fields[/dim]\")\n",
    "        \n",
    "        if not field_accuracies:\n",
    "            rprint(f\"[yellow]  ‚ö†Ô∏è No field accuracies computed[/yellow]\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        field_data = []\n",
    "        for field_name, data in field_accuracies.items():\n",
    "            accuracy = data['correct'] / data['total'] if data['total'] > 0 else 0.0\n",
    "            field_data.append({\n",
    "                'model': model_name,\n",
    "                'field_name': field_name,\n",
    "                'accuracy': accuracy,\n",
    "                'correct_count': data['correct'],\n",
    "                'total_count': data['total']\n",
    "            })\n",
    "        \n",
    "        # Calculate average accuracy\n",
    "        avg_accuracy = sum(d['accuracy'] for d in field_data) / len(field_data) if field_data else 0.0\n",
    "        \n",
    "        rprint(f\"[green]  ‚úÖ Computed accuracy for {len(field_data)} fields (avg: {avg_accuracy:.1%})[/green]\")\n",
    "        return pd.DataFrame(field_data)\n",
    "        \n",
    "    except Exception as e:\n",
    "        rprint(f\"[red]‚ùå Error evaluating {model_name}: {e}[/red]\")\n",
    "        import traceback\n",
    "        rprint(f\"[dim]{traceback.format_exc()}[/dim]\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Load field-level accuracy data from CSV files (NOT re-running evaluation)\n",
    "rprint(\"\\n[bold blue]üìä Computing Field-Level Accuracy (from CSV + Ground Truth)[/bold blue]\")\n",
    "rprint(\"[dim]Comparing batch CSV results against ground truth for field-level metrics[/dim]\\n\")\n",
    "\n",
    "# Determine ground truth path - FAIL FAST if not found\n",
    "gt_path = CONFIG['ground_truth_path']\n",
    "\n",
    "rprint(f\"[cyan]Expected ground truth path: {gt_path}[/cyan]\")\n",
    "rprint(f\"[dim]  Absolute path: {Path(gt_path).absolute()}[/dim]\")\n",
    "\n",
    "if not Path(gt_path).exists():\n",
    "    rprint(f\"[bold red]‚ùå FATAL: Ground truth file not found![/bold red]\")\n",
    "    rprint(f\"[yellow]üí° Expected location: {Path(gt_path).absolute()}[/yellow]\")\n",
    "    rprint(f\"[yellow]üí° Check CONFIG['output_dir'] setting[/yellow]\")\n",
    "    rprint(f\"[yellow]üí° Current CONFIG['output_dir']: {CONFIG.get('output_dir', 'NOT SET')}[/yellow]\")\n",
    "    raise FileNotFoundError(f\"Ground truth not found: {gt_path}\")\n",
    "\n",
    "rprint(f\"[green]‚úÖ Ground truth file exists[/green]\")\n",
    "\n",
    "\n",
    "\n",
    "field_data_frames = []\n",
    "\n",
    "if not llama_df.empty:\n",
    "    llama_fields = extract_field_level_accuracy_from_csv(\n",
    "        CONFIG['output_dir'], CONFIG['llama_pattern'], 'Llama-3.2-Vision', gt_path\n",
    "    )\n",
    "    if not llama_fields.empty:\n",
    "        field_data_frames.append(llama_fields)\n",
    "        \n",
    "if not internvl3_quantized_df.empty:\n",
    "    internvl3_q_fields = extract_field_level_accuracy_from_csv(\n",
    "        CONFIG['output_dir'], CONFIG['internvl3_quantized_pattern'], 'InternVL3-Quantized-8B', gt_path\n",
    "    )\n",
    "    if not internvl3_q_fields.empty:\n",
    "        field_data_frames.append(internvl3_q_fields)\n",
    "        \n",
    "if not internvl3_non_quantized_df.empty:\n",
    "    internvl3_nq_fields = extract_field_level_accuracy_from_csv(\n",
    "        CONFIG['output_dir'], CONFIG['internvl3_non_quantized_pattern'], 'InternVL3-NonQuantized-2B', gt_path\n",
    "    )\n",
    "    if not internvl3_nq_fields.empty:\n",
    "        field_data_frames.append(internvl3_nq_fields)\n",
    "\n",
    "if field_data_frames:\n",
    "    field_level_df = pd.concat(field_data_frames, ignore_index=True)\n",
    "    rprint(f\"\\n[green]‚úÖ Field-level accuracy computed: {len(field_level_df)} field measurements[/green]\")\n",
    "    rprint(f\"[cyan]üìã Unique fields: {field_level_df['field_name'].nunique()}[/cyan]\")\n",
    "    rprint(f\"[cyan]üìä Models analyzed: {field_level_df['model'].nunique()}[/cyan]\")\n",
    "else:\n",
    "    rprint(\"\\n[red]‚ùå No field-level accuracy data available[/red]\")\n",
    "    rprint(\"[yellow]üí° This requires batch results CSVs and ground truth for evaluation[/yellow]\")\n",
    "    field_level_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10\n",
    "\n",
    "def analyze_field_performance(field_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Analyze and compare field-level performance across models.\n",
    "    \n",
    "    Args:\n",
    "        field_df: DataFrame with field-level accuracy data\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with field performance comparison\n",
    "    \"\"\"\n",
    "    if field_df.empty:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Filter out metadata/internal fields that aren't business document fields\n",
    "    exclude_fields = ['quantization_used', 'model', 'timestamp', 'processing_time', 'image_name', 'image_stem', 'TRANSACTION_AMOUNTS_RECEIVED', 'ACCOUNT_BALANCE']\n",
    "    field_df = field_df[~field_df['field_name'].isin(exclude_fields)]\n",
    "    \n",
    "    if field_df.empty:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Pivot to get fields as rows, models as columns (using accuracy now)\n",
    "    field_comparison = field_df.pivot_table(\n",
    "        index='field_name',\n",
    "        columns='model',\n",
    "        values='accuracy',\n",
    "        aggfunc='mean'\n",
    "    )\n",
    "    \n",
    "    # Calculate average accuracy across all models\n",
    "    field_comparison['avg_accuracy'] = field_comparison.mean(axis=1)\n",
    "    \n",
    "    # Calculate variance to identify fields with inconsistent performance\n",
    "    field_comparison['variance'] = field_comparison.std(axis=1)\n",
    "    \n",
    "    # Identify best model for each field\n",
    "    model_cols = [col for col in field_comparison.columns if col not in ['avg_accuracy', 'variance']]\n",
    "    field_comparison['best_model'] = field_comparison[model_cols].idxmax(axis=1)\n",
    "    field_comparison['best_score'] = field_comparison[model_cols].max(axis=1)\n",
    "    \n",
    "    # Sort by average accuracy\n",
    "    field_comparison = field_comparison.sort_values('avg_accuracy', ascending=False)\n",
    "    \n",
    "    return field_comparison\n",
    "\n",
    "# Analyze field performance if data is available\n",
    "if not field_level_df.empty:\n",
    "    field_performance = analyze_field_performance(field_level_df)\n",
    "    \n",
    "    rprint(\"\\n[bold green]üìä FIELD-LEVEL ACCURACY ANALYSIS[/bold green]\")\n",
    "    \n",
    "    # Show top performing fields with color gradient\n",
    "    rprint(\"[bold blue]üìä All Fields Ranked by Accuracy:[/bold blue]\")\n",
    "    \n",
    "    # Get model columns for styling\n",
    "    model_cols = [col for col in field_performance.columns if col not in [\"avg_accuracy\", \"variance\", \"best_model\", \"best_score\"]]\n",
    "    \n",
    "    # Apply color gradient styling to all fields (already sorted by avg_accuracy descending)\n",
    "    styled_all = field_performance.style.background_gradient(\n",
    "        cmap=\"RdYlGn\",\n",
    "        subset=model_cols + [\"avg_accuracy\"],\n",
    "        vmin=0,\n",
    "        vmax=1\n",
    "    ).format(\n",
    "        {col: \"{:.1%}\" for col in field_performance.columns if col not in [\"best_model\", \"variance\", \"best_score\"]}\n",
    "    ).format(\n",
    "        {\"variance\": \"{:.3f}\"}\n",
    "    )\n",
    "    \n",
    "    display(styled_all)\n",
    "    \n",
    "    # Save all fields table as PNG\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_table_dir = Path(f\"{base_data_path}/output/tables\")\n",
    "    output_table_dir.mkdir(parents=True, exist_ok=True)\n",
    "    all_fields_png_path = output_table_dir / f\"field_accuracy_ranked_{timestamp}.png\"\n",
    "    \n",
    "    # Render table as image using matplotlib\n",
    "    fig, ax = plt.subplots(figsize=(16, 10))  # Taller for 17 fields\n",
    "    ax.axis(\"tight\")\n",
    "    ax.axis(\"off\")\n",
    "    \n",
    "    table_data = field_performance.values\n",
    "    col_labels = list(field_performance.columns)\n",
    "    row_labels = list(field_performance.index)\n",
    "    \n",
    "    table = ax.table(cellText=table_data, colLabels=col_labels, rowLabels=row_labels,\n",
    "                     cellLoc=\"center\", loc=\"center\")\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(8)\n",
    "    table.scale(1.2, 1.8)\n",
    "    \n",
    "    plt.savefig(str(all_fields_png_path), dpi=300, bbox_inches=\"tight\", facecolor=\"white\")\n",
    "    plt.close()\n",
    "    rprint(f\"[green]‚úÖ All fields table saved: {all_fields_png_path}[/green]\")\n",
    "    \n",
    "    # Field performance summary\n",
    "    rprint(f\"[cyan]üìà Total evaluation fields: {len(field_performance)}[/cyan]\")\n",
    "    rprint(f\"[cyan]üìä Average accuracy range: {field_performance['avg_accuracy'].min():.1%} - {field_performance['avg_accuracy'].max():.1%}[/cyan]\")\n",
    "    \n",
    "    rprint(\"\\n[bold blue]üéØ Model Field Specialization:[/bold blue]\")\n",
    "    specialization = field_performance['best_model'].value_counts()\n",
    "    for model, count in specialization.items():\n",
    "        percentage = (count / len(field_performance)) * 100\n",
    "        rprint(f\"  ‚Ä¢ {model}: Best at {count} fields ({percentage:.1f}%)\")\n",
    "        \n",
    "    # Summary statistics\n",
    "    rprint(\"\\n[bold blue]üìä Overall Field Accuracy Summary:[/bold blue]\")\n",
    "    for model in field_level_df['model'].unique():\n",
    "        model_data = field_level_df[field_level_df['model'] == model]\n",
    "        avg_acc = model_data['accuracy'].mean()\n",
    "        rprint(f\"  ‚Ä¢ {model}: {avg_acc:.1%} average field accuracy\")\n",
    "else:\n",
    "    rprint(\"[red]‚ùå Cannot analyze field performance - no field-level data available[/red]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dashboard: Side-by-Side Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12\n",
    "\n",
    "def create_field_level_visualizations(field_df: pd.DataFrame, field_performance: pd.DataFrame, save_path: str = None):\n",
    "    \"\"\"\n",
    "    Create comprehensive field-level performance visualizations.\n",
    "    \n",
    "    Args:\n",
    "        field_df: Raw field-level accuracy data\n",
    "        field_performance: Analyzed field performance comparison\n",
    "        save_path: Optional path to save visualization\n",
    "    \"\"\"\n",
    "    if field_df.empty or field_performance.empty:\n",
    "        rprint(\"[red]‚ùå Cannot create field visualizations - no data available[/red]\")\n",
    "        return\n",
    "    \n",
    "    # Create figure with improved spacing\n",
    "    fig = plt.figure(figsize=(22, 13))\n",
    "    gs = fig.add_gridspec(2, 2, height_ratios=[1.2, 1], width_ratios=[1.5, 1], \n",
    "                          hspace=0.35, wspace=0.25, top=0.90, bottom=0.06, left=0.06, right=0.97)\n",
    "    \n",
    "    # Color scheme matching main dashboard\n",
    "    model_order = ['Llama-3.2-Vision', 'InternVL3-Quantized-8B', 'InternVL3-NonQuantized-2B']\n",
    "    fixed_colors = {\n",
    "        'Llama-3.2-Vision': '#E74C3C',\n",
    "        'InternVL3-Quantized-8B': '#3498DB',\n",
    "        'InternVL3-NonQuantized-2B': '#27AE60'\n",
    "    }\n",
    "    \n",
    "    # Get available models\n",
    "    available_models = field_df['model'].unique()\n",
    "    models = [model for model in model_order if model in available_models]\n",
    "    model_colors = {model: fixed_colors[model] for model in models if model in fixed_colors}\n",
    "    \n",
    "    # 1. Field Accuracy Comparison (Horizontal Bar Chart) - IMPROVED\n",
    "    ax1 = fig.add_subplot(gs[0, :])\n",
    "    \n",
    "    # Get all evaluation fields by average accuracy\n",
    "    all_eval_fields = field_performance\n",
    "    model_cols = [col for col in all_eval_fields.columns if col in models]\n",
    "    \n",
    "    y_pos = np.arange(len(all_eval_fields))\n",
    "    bar_height = 0.22\n",
    "    \n",
    "    # Create bars with better spacing\n",
    "    for idx, model in enumerate(model_cols):\n",
    "        offset = (idx - len(model_cols)/2 + 0.5) * bar_height\n",
    "        bars = ax1.barh(y_pos + offset, all_eval_fields[model], bar_height, \n",
    "                label=model, color=model_colors.get(model, '#999999'), alpha=0.85, edgecolor='white', linewidth=0.5)\n",
    "    \n",
    "    ax1.set_yticks(y_pos)\n",
    "    ax1.set_yticklabels(all_eval_fields.index, fontsize=11)\n",
    "    ax1.invert_yaxis()  # Match heatmap ordering (top-to-bottom)\n",
    "    ax1.set_xlabel('Field Accuracy', fontsize=13, fontweight='bold')\n",
    "    ax1.set_title('All Evaluation Fields: Accuracy by Model', fontsize=16, fontweight='bold', pad=15)\n",
    "    \n",
    "    # Improved legend placement - outside plot area to avoid occlusion\n",
    "    ax1.legend(loc='upper left', bbox_to_anchor=(1.01, 1), fontsize=11, framealpha=0.98, edgecolor='gray', shadow=True)\n",
    "    ax1.grid(True, alpha=0.3, axis='x', linestyle='--')\n",
    "    ax1.set_xlim(0, 1.05)\n",
    "    ax1.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'{x:.0%}'))\n",
    "    \n",
    "    # Add vertical reference lines\n",
    "    for x in [0.25, 0.5, 0.75, 1.0]:\n",
    "        ax1.axvline(x=x, color='gray', linestyle=':', alpha=0.3, linewidth=0.8)\n",
    "    \n",
    "    # 2. Field Accuracy Heatmap - IMPROVED\n",
    "    ax2 = fig.add_subplot(gs[1, 0])\n",
    "    \n",
    "    # Get all evaluation fields for heatmap (same as bar chart for consistency)\n",
    "    heatmap_data = field_performance[model_cols]\n",
    "    \n",
    "    # Create heatmap with better colors\n",
    "    im = ax2.imshow(heatmap_data.values, cmap='RdYlGn', aspect='auto', vmin=0, vmax=1)\n",
    "    \n",
    "    # Improved tick labels\n",
    "    ax2.set_xticks(np.arange(len(model_cols)))\n",
    "    ax2.set_yticks(np.arange(len(heatmap_data)))\n",
    "    \n",
    "    # Better label formatting - shorten model names for display\n",
    "    model_labels = []\n",
    "    for model in model_cols:\n",
    "        if model == 'Llama-3.2-Vision':\n",
    "            model_labels.append('Llama-3.2')\n",
    "        elif model == 'InternVL3-Quantized-8B':\n",
    "            model_labels.append('InternVL3-Q8B')\n",
    "        elif model == 'InternVL3-NonQuantized-2B':\n",
    "            model_labels.append('InternVL3-2B')\n",
    "        else:\n",
    "            model_labels.append(model)\n",
    "    \n",
    "    ax2.set_xticklabels(model_labels, rotation=0, ha='center', fontsize=11, fontweight='bold')\n",
    "    ax2.set_yticklabels(heatmap_data.index, fontsize=10)\n",
    "    \n",
    "    # Improved colorbar\n",
    "    cbar = plt.colorbar(im, ax=ax2, fraction=0.046, pad=0.04)\n",
    "    cbar.set_label('Field Accuracy', rotation=270, labelpad=20, fontsize=11, fontweight='bold')\n",
    "    cbar.ax.tick_params(labelsize=10)\n",
    "    \n",
    "    # Add text annotations with better visibility\n",
    "    for i in range(len(heatmap_data)):\n",
    "        for j in range(len(model_cols)):\n",
    "            value = heatmap_data.iloc[i, j]\n",
    "            if not np.isnan(value):\n",
    "                text_color = 'white' if value < 0.6 else 'black'\n",
    "                ax2.text(j, i, f'{value:.0%}', ha='center', va='center', \n",
    "                        color=text_color, fontsize=9, fontweight='bold')\n",
    "    \n",
    "    ax2.set_title('Field Accuracy Heatmap: All Evaluation Fields', fontsize=15, fontweight='bold', pad=12)\n",
    "    \n",
    "    # 3. Model Specialization Pie Chart - IMPROVED\n",
    "    ax3 = fig.add_subplot(gs[1, 1])\n",
    "    \n",
    "    specialization = field_performance['best_model'].value_counts()\n",
    "    colors_list = [model_colors.get(model, '#999999') for model in specialization.index]\n",
    "    \n",
    "    # Improved pie chart with better labels\n",
    "    wedges, texts, autotexts = ax3.pie(\n",
    "        specialization.values,\n",
    "        labels=None,  # We'll add custom labels\n",
    "        autopct='%1.1f%%',\n",
    "        colors=colors_list,\n",
    "        startangle=90,\n",
    "        textprops={'fontsize': 11, 'fontweight': 'bold'},\n",
    "        explode=[0.02] * len(specialization)  # Slight separation\n",
    "    )\n",
    "    \n",
    "    # Make percentage text bold and white\n",
    "    for autotext in autotexts:\n",
    "        autotext.set_color('white')\n",
    "        autotext.set_fontweight('bold')\n",
    "    \n",
    "    # Add custom legend with shortened names\n",
    "    legend_labels = []\n",
    "    for model in specialization.index:\n",
    "        count = specialization[model]\n",
    "        pct = (count / len(field_performance)) * 100\n",
    "        if model == 'Llama-3.2-Vision':\n",
    "            legend_labels.append(f'Llama-3.2: {count} fields')\n",
    "        elif model == 'InternVL3-Quantized-8B':\n",
    "            legend_labels.append(f'InternVL3-Q8B: {count} fields')\n",
    "        elif model == 'InternVL3-NonQuantized-2B':\n",
    "            legend_labels.append(f'InternVL3-2B: {count} fields')\n",
    "        else:\n",
    "            legend_labels.append(f'{model}: {count} fields')\n",
    "    \n",
    "    ax3.legend(legend_labels, loc='upper left', fontsize=10, framealpha=0.95, \n",
    "               bbox_to_anchor=(0.02, 0.98), edgecolor='gray')\n",
    "    \n",
    "    ax3.set_title('Model Field Specialization\\n(% of Fields Where Model Performs Best)', \n",
    "                 fontsize=14, fontweight='bold', pad=12)\n",
    "    \n",
    "    # Overall title - cleaned up\n",
    "    fig.suptitle('Field-Level Accuracy Analysis\\nComparison Across Models (vs Ground Truth)', \n",
    "                fontsize=19, fontweight='bold')\n",
    "    \n",
    "    # Save the visualization\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=CONFIG['dpi'], bbox_inches='tight', facecolor='white')\n",
    "        rprint(f\"[green]‚úÖ Field-level visualization saved to: {save_path}[/green]\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Create field-level visualizations\n",
    "field_viz_timestamp = None\n",
    "if not field_level_df.empty and 'field_performance' in locals():\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_path = Path(f\"output/visualizations/field_level_accuracy_{timestamp}.png\")\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    create_field_level_visualizations(field_level_df, field_performance, str(output_path))\n",
    "    field_viz_timestamp = timestamp\n",
    "else:\n",
    "    rprint(\"[red]‚ùå Cannot create field visualizations - no field-level data available[/red]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Detailed Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14\n",
    "\n",
    "def analyze_model_strengths(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Analyze and compare model strengths and weaknesses.\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return\n",
    "    \n",
    "    rprint(\"\\n[bold green]üéØ DETAILED PERFORMANCE ANALYSIS[/bold green]\")\n",
    "    \n",
    "    # Document type winner analysis\n",
    "    doc_performance = df.groupby(['document_type', 'model'])['overall_accuracy'].mean().unstack()\n",
    "    \n",
    "    rprint(\"\\n[bold blue]üìä Document Type Performance Leaders:[/bold blue]\")\n",
    "    for doc_type in doc_performance.index:\n",
    "        best_model = doc_performance.loc[doc_type].idxmax()\n",
    "        best_score = doc_performance.loc[doc_type].max()\n",
    "        other_model = [m for m in doc_performance.columns if m != best_model][0]\n",
    "        other_score = doc_performance.loc[doc_type, other_model]\n",
    "        improvement = best_score - other_score\n",
    "        \n",
    "        rprint(f\"  ‚Ä¢ **{doc_type}**: {best_model} leads with {best_score:.1f}% (+{improvement:.1f}% advantage)\")\n",
    "    \n",
    "    # Speed analysis\n",
    "    rprint(\"\\n[bold blue]‚ö° Processing Speed Analysis:[/bold blue]\")\n",
    "    speed_comparison = df.groupby('model')['processing_time'].agg(['mean', 'min', 'max', 'std'])\n",
    "    for model in speed_comparison.index:\n",
    "        stats = speed_comparison.loc[model]\n",
    "        throughput = 60 / stats['mean']\n",
    "        rprint(f\"  ‚Ä¢ **{model}**: Avg {stats['mean']:.1f}s ({throughput:.1f} docs/min), Range {stats['min']:.1f}-{stats['max']:.1f}s\")\n",
    "    \n",
    "    # Consistency analysis\n",
    "    rprint(\"\\n[bold blue]üìà Consistency Analysis (Lower std dev = more consistent):[/bold blue]\")\n",
    "    consistency = df.groupby('model')['overall_accuracy'].std()\n",
    "    for model in consistency.index:\n",
    "        rprint(f\"  ‚Ä¢ **{model}**: ¬±{consistency[model]:.1f}% standard deviation\")\n",
    "    \n",
    "    # Efficiency score (accuracy/time ratio)\n",
    "    rprint(\"\\n[bold blue]üí° Efficiency Score (Accuracy per Second):[/bold blue]\")\n",
    "    df['efficiency_score'] = df['overall_accuracy'] / df['processing_time']\n",
    "    efficiency = df.groupby('model')['efficiency_score'].mean()\n",
    "    for model in efficiency.index:\n",
    "        rprint(f\"  ‚Ä¢ **{model}**: {efficiency[model]:.2f} accuracy points per second\")\n",
    "\n",
    "# Run detailed analysis\n",
    "if not combined_df.empty:\n",
    "    analyze_model_strengths(combined_df)\n",
    "else:\n",
    "    rprint(\"[red]‚ùå Cannot run analysis - no data available[/red]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation for Confusion Analysis\n",
    "\n",
    "Load ground truth and prepare batch data for confusion matrix analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 16\n",
    "\n",
    "# Load ground truth as DataFrame for confusion analysis\n",
    "ground_truth_path = Path(CONFIG['ground_truth_path'])\n",
    "\n",
    "if not ground_truth_path.exists():\n",
    "    rprint(f\"[red]‚ùå Ground truth file not found: {ground_truth_path}[/red]\")\n",
    "    ground_truth = pd.DataFrame()\n",
    "else:\n",
    "    # Load FULL ground truth first\n",
    "    ground_truth_full = pd.read_csv(ground_truth_path, dtype=str)\n",
    "    rprint(f\"[dim]  Loaded full ground truth: {len(ground_truth_full)} rows[/dim]\")\n",
    "\n",
    "    # Normalize column name: ground truth uses 'image_name', we need 'image_file'\n",
    "    if 'image_name' in ground_truth_full.columns and 'image_file' not in ground_truth_full.columns:\n",
    "        ground_truth_full['image_file'] = ground_truth_full['image_name']\n",
    "        rprint(f\"[dim]  Normalized: 'image_name' ‚Üí 'image_file'[/dim]\")\n",
    "    elif 'image_file' not in ground_truth_full.columns:\n",
    "        # Try other possible column names\n",
    "        possible_names = ['filename', 'file', 'image']\n",
    "        for col in possible_names:\n",
    "            if col in ground_truth_full.columns:\n",
    "                ground_truth_full['image_file'] = ground_truth_full[col]\n",
    "                rprint(f\"[dim]  Normalized: '{col}' ‚Üí 'image_file'[/dim]\")\n",
    "                break\n",
    "\n",
    "    # Add image_stem column to ground truth for matching\n",
    "    ground_truth_full['image_stem'] = ground_truth_full['image_file'].apply(lambda x: Path(x).stem)\n",
    "    ground_truth = ground_truth_full\n",
    "\n",
    "# Prepare batch dataframes with consistent naming for confusion analysis\n",
    "# Use the loaded data from cell 5\n",
    "llama_batch_df = llama_df.copy() if not llama_df.empty else pd.DataFrame()\n",
    "internvl_batch_df = internvl3_quantized_df.copy() if not internvl3_quantized_df.empty else pd.DataFrame()\n",
    "internvl_nq_batch_df = internvl3_non_quantized_df.copy() if not internvl3_non_quantized_df.empty else pd.DataFrame()\n",
    "\n",
    "# Verify data is available\n",
    "if llama_batch_df.empty:\n",
    "    rprint(\"[yellow]‚ö†Ô∏è Warning: Llama batch data not loaded. Run cell 5 first.[/yellow]\")\n",
    "else:\n",
    "    rprint(f\"[cyan]Llama batch data: {len(llama_batch_df)} rows[/cyan]\")\n",
    "    if 'document_type' in llama_batch_df.columns:\n",
    "        rprint(f\"[dim]  Predicted document types: {llama_batch_df['document_type'].value_counts().to_dict()}[/dim]\")\n",
    "\n",
    "if internvl_batch_df.empty:\n",
    "    rprint(\"[yellow]‚ö†Ô∏è Warning: InternVL3-Quantized batch data not loaded. Run cell 5 first.[/yellow]\")\n",
    "else:\n",
    "    rprint(f\"[cyan]InternVL3-Quantized batch data: {len(internvl_batch_df)} rows[/cyan]\")\n",
    "    if 'document_type' in internvl_batch_df.columns:\n",
    "        rprint(f\"[dim]  Predicted document types: {internvl_batch_df['document_type'].value_counts().to_dict()}[/dim]\")\n",
    "\n",
    "if internvl_nq_batch_df.empty:\n",
    "    rprint(\"[yellow]‚ö†Ô∏è Warning: InternVL3-NonQuantized batch data not loaded. Run cell 5 first.[/yellow]\")\n",
    "else:\n",
    "    rprint(f\"[cyan]InternVL3-NonQuantized batch data: {len(internvl_nq_batch_df)} rows[/cyan]\")\n",
    "    if 'document_type' in internvl_nq_batch_df.columns:\n",
    "        rprint(f\"[dim]  Predicted document types: {internvl_nq_batch_df['document_type'].value_counts().to_dict()}[/dim]\")\n",
    "\n",
    "if ground_truth.empty:\n",
    "    rprint(\"[yellow]‚ö†Ô∏è Warning: Ground truth not loaded. Confusion analysis will not work.[/yellow]\")\n",
    "elif 'image_file' not in ground_truth.columns:\n",
    "    rprint(\"[red]‚ùå Error: Ground truth missing 'image_file' column[/red]\")\n",
    "    rprint(f\"[yellow]   Available columns: {list(ground_truth.columns)}[/yellow]\")\n",
    "else:\n",
    "    rprint(f\"[cyan]Ground truth: {len(ground_truth)} rows[/cyan]\")\n",
    "\n",
    "rprint(\"[green]‚úÖ Data ready for confusion analysis[/green]\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Document Type Confusion Matrix\n",
    "\n",
    "**Classic confusion matrix showing document type classification accuracy.**\n",
    "\n",
    "Shows how well each model correctly identifies document types (INVOICE, RECEIPT, BANK_STATEMENT) and where misclassifications occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 18\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def abbreviate_doctype(name):\n",
    "    \"\"\"Shorten long document type names for visualization.\"\"\"\n",
    "    abbrev = {\n",
    "        'COMPULSORY THIRD PARTY PERSONAL INJURY INSURANCE GREEN SLIP CERTIFICATE': 'CTP_INSUR',\n",
    "        'E-TICKET ITINERARY, RECEIPT AND TAX INVOICE': 'E-TICKET',\n",
    "        'MOBILE APP SCREENSHOT': 'MOBILE_SS',\n",
    "        'PAYMENT ADVICE': 'PAYMENT',\n",
    "        'CRYPTO STATEMENT': 'CRYPTO',\n",
    "        'TAX INVOICE': 'TAX_INV',\n",
    "        'INVOICE': 'INVOICE',\n",
    "        'RECEIPT': 'RECEIPT',\n",
    "        'BANK_STATEMENT': 'BANK_STMT',\n",
    "        'NOT_FOUND': 'NOT_FOUND'\n",
    "    }\n",
    "    return abbrev.get(name, name[:12])  # Fallback: truncate to 12 chars\n",
    "\n",
    "def create_doctype_confusion_matrix(df_batch: pd.DataFrame, ground_truth_df: pd.DataFrame, model_name: str):\n",
    "    \"\"\"\n",
    "    Create document type confusion matrix using pandas crosstab.\n",
    "\n",
    "    Args:\n",
    "        df_batch: Batch results DataFrame with predicted document types\n",
    "        ground_truth_df: Ground truth DataFrame with true document types\n",
    "        model_name: Name of the model\n",
    "\n",
    "    Returns:\n",
    "        tuple: (confusion_matrix, classification_report_dict, col_labels, row_labels, y_true, y_pred)\n",
    "    \"\"\"\n",
    "    # Normalize image names (strip extensions) for matching\n",
    "    # Batch has extensions (.jpeg, .png), ground truth does not\n",
    "    df_batch['image_stem'] = df_batch['image_file'].apply(lambda x: Path(x).stem)\n",
    "    ground_truth_df['image_stem'] = ground_truth_df['image_file'].apply(lambda x: Path(x).stem)\n",
    "\n",
    "    # Merge to align predictions with ground truth using stems\n",
    "    merged = df_batch.merge(\n",
    "        ground_truth_df[['image_stem', 'DOCUMENT_TYPE']],\n",
    "        on='image_stem',\n",
    "        how='inner',\n",
    "        suffixes=('_pred', '_true')\n",
    "    )\n",
    "\n",
    "    # Get predicted and true document types\n",
    "    # CRITICAL: Use DOCUMENT_TYPE (extracted field) for predictions\n",
    "    #           Use DOCUMENT_TYPE from ground truth for true labels (3 types)\n",
    "\n",
    "    if 'DOCUMENT_TYPE_pred' in merged.columns:\n",
    "        y_pred = merged['DOCUMENT_TYPE_pred'].fillna('UNKNOWN').astype(str).str.strip().str.upper()\n",
    "    else:\n",
    "        y_pred = merged['DOCUMENT_TYPE'].fillna('UNKNOWN').astype(str).str.strip().str.upper()\n",
    "\n",
    "    if 'DOCUMENT_TYPE_true' in merged.columns:\n",
    "        y_true = merged['DOCUMENT_TYPE_true'].fillna('UNKNOWN').astype(str).str.strip().str.upper()\n",
    "    else:\n",
    "        y_true = merged['DOCUMENT_TYPE'].fillna('UNKNOWN').astype(str).str.strip().str.upper()\n",
    "\n",
    "    # Use pandas crosstab to create non-square confusion matrix\n",
    "    # This allows 3 rows (y_true types) √ó N columns (y_pred types)\n",
    "    cm_df = pd.crosstab(y_true, y_pred, dropna=False)\n",
    "\n",
    "    # Convert to numpy array for compatibility with seaborn heatmap\n",
    "    cm = cm_df.values\n",
    "\n",
    "    # Get labels and abbreviate them\n",
    "    labels = [abbreviate_doctype(label) for label in cm_df.columns.tolist()]\n",
    "    row_labels = [abbreviate_doctype(label) for label in cm_df.index.tolist()]\n",
    "\n",
    "    # Compute classification report (use original labels)\n",
    "    report = classification_report(y_true, y_pred, labels=cm_df.columns.tolist(), output_dict=True, zero_division=0)\n",
    "\n",
    "    return cm, report, labels, row_labels, y_true, y_pred\n",
    "\n",
    "# Create confusion matrices for all 3 models\n",
    "rprint(\"[bold cyan]Creating document type confusion matrices for 3 models...[/bold cyan]\")\n",
    "\n",
    "llama_cm, llama_report, llama_labels, llama_row_labels, llama_y_true, llama_y_pred = create_doctype_confusion_matrix(\n",
    "    llama_batch_df, ground_truth, 'Llama'\n",
    ")\n",
    "internvl_q_cm, internvl_q_report, internvl_q_labels, internvl_q_row_labels, internvl_q_y_true, internvl_q_y_pred = create_doctype_confusion_matrix(\n",
    "    internvl_batch_df, ground_truth, 'InternVL3-Quantized'\n",
    ")\n",
    "internvl_nq_cm, internvl_nq_report, internvl_nq_labels, internvl_nq_row_labels, internvl_nq_y_true, internvl_nq_y_pred = create_doctype_confusion_matrix(\n",
    "    internvl_nq_batch_df, ground_truth, 'InternVL3-NonQuantized'\n",
    ")\n",
    "\n",
    "# Plot confusion matrices in 3-panel layout\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(24, 6))\n",
    "\n",
    "# Llama confusion matrix\n",
    "sns.heatmap(\n",
    "    llama_cm,\n",
    "    annot=True,\n",
    "    fmt='d',\n",
    "    cmap='Blues',\n",
    "    xticklabels=llama_labels,\n",
    "    yticklabels=llama_row_labels,\n",
    "    cbar_kws={'label': 'Count'},\n",
    "    ax=ax1,\n",
    "    linewidths=0.5,\n",
    "    linecolor='gray'\n",
    ")\n",
    "ax1.set_title('Llama-3.2-Vision-11B', fontsize=13, fontweight='bold')\n",
    "ax1.set_xlabel('Predicted Document Type', fontsize=11, fontweight='bold')\n",
    "ax1.set_ylabel('True Document Type', fontsize=11, fontweight='bold')\n",
    "ax1.tick_params(axis='x', rotation=45, labelsize=9)\n",
    "ax1.tick_params(axis='y', rotation=0, labelsize=9)\n",
    "\n",
    "# InternVL3-Quantized confusion matrix\n",
    "sns.heatmap(\n",
    "    internvl_q_cm,\n",
    "    annot=True,\n",
    "    fmt='d',\n",
    "    cmap='Blues',\n",
    "    xticklabels=internvl_q_labels,\n",
    "    yticklabels=internvl_q_row_labels,\n",
    "    cbar_kws={'label': 'Count'},\n",
    "    ax=ax2,\n",
    "    linewidths=0.5,\n",
    "    linecolor='gray'\n",
    ")\n",
    "ax2.set_title('InternVL3-Quantized-8B', fontsize=13, fontweight='bold')\n",
    "ax2.set_xlabel('Predicted Document Type', fontsize=11, fontweight='bold')\n",
    "ax2.set_ylabel('True Document Type', fontsize=11, fontweight='bold')\n",
    "ax2.tick_params(axis='x', rotation=45, labelsize=9)\n",
    "ax2.tick_params(axis='y', rotation=0, labelsize=9)\n",
    "\n",
    "# InternVL3-NonQuantized confusion matrix\n",
    "sns.heatmap(\n",
    "    internvl_nq_cm,\n",
    "    annot=True,\n",
    "    fmt='d',\n",
    "    cmap='Blues',\n",
    "    xticklabels=internvl_nq_labels,\n",
    "    yticklabels=internvl_nq_row_labels,\n",
    "    cbar_kws={'label': 'Count'},\n",
    "    ax=ax3,\n",
    "    linewidths=0.5,\n",
    "    linecolor='gray'\n",
    ")\n",
    "ax3.set_title('InternVL3-NonQuantized-2B', fontsize=13, fontweight='bold')\n",
    "ax3.set_xlabel('Predicted Document Type', fontsize=11, fontweight='bold')\n",
    "ax3.set_ylabel('True Document Type', fontsize=11, fontweight='bold')\n",
    "ax3.tick_params(axis='x', rotation=45, labelsize=9)\n",
    "ax3.tick_params(axis='y', rotation=0, labelsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(CONFIG['output_dir'].replace('/csv', '/visualizations') + '/doctype_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "rprint(\"[green]‚úÖ 3-panel document type confusion matrices created[/green]\")\n",
    "\n",
    "# Display classification reports\n",
    "rprint(\"\\n[bold blue]Llama - Document Type Classification Report:[/bold blue]\")\n",
    "llama_report_df = pd.DataFrame(llama_report).transpose()\n",
    "display(llama_report_df)\n",
    "\n",
    "rprint(\"\\n[bold blue]InternVL3-Quantized - Document Type Classification Report:[/bold blue]\")\n",
    "internvl_q_report_df = pd.DataFrame(internvl_q_report).transpose()\n",
    "display(internvl_q_report_df)\n",
    "\n",
    "rprint(\"\\n[bold blue]InternVL3-NonQuantized - Document Type Classification Report:[/bold blue]\")\n",
    "internvl_nq_report_df = pd.DataFrame(internvl_nq_report).transpose()\n",
    "display(internvl_nq_report_df)\n",
    "\n",
    "# Save classification reports\n",
    "llama_report_df.to_csv(CONFIG['output_dir'] + '/llama_doctype_classification_report.csv')\n",
    "internvl_q_report_df.to_csv(CONFIG['output_dir'] + '/internvl3_quantized_doctype_classification_report.csv')\n",
    "internvl_nq_report_df.to_csv(CONFIG['output_dir'] + '/internvl3_nonquantized_doctype_classification_report.csv')\n",
    "\n",
    "# Summary accuracy\n",
    "llama_accuracy = (llama_y_true == llama_y_pred).sum() / len(llama_y_true) * 100\n",
    "internvl_q_accuracy = (internvl_q_y_true == internvl_q_y_pred).sum() / len(internvl_q_y_true) * 100\n",
    "internvl_nq_accuracy = (internvl_nq_y_true == internvl_nq_y_pred).sum() / len(internvl_nq_y_true) * 100\n",
    "\n",
    "rprint(\"\\n[bold blue]Document Type Classification Accuracy:[/bold blue]\")\n",
    "rprint(f\"[cyan]Llama-3.2-Vision-11B: {llama_accuracy:.1f}%[/cyan]\")\n",
    "rprint(f\"[cyan]InternVL3-Quantized-8B: {internvl_q_accuracy:.1f}%[/cyan]\")\n",
    "rprint(f\"[cyan]InternVL3-NonQuantized-2B: {internvl_nq_accuracy:.1f}%[/cyan]\")\n",
    "\n",
    "rprint(\"[green]‚úÖ Document type confusion analysis complete for all 3 models[/green]\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Field-Level Confusion Analysis\n",
    "\n",
    "**Confusion matrix showing field extraction status (correct/incorrect/not_found) for each field type.**\n",
    "\n",
    "This analysis reveals:\n",
    "- Which fields are most accurately extracted\n",
    "- Which fields are frequently incorrect vs not found\n",
    "- Model-specific strengths and weaknesses per field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 20\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define field columns (from llama_batch.ipynb)\n",
    "FIELD_COLUMNS = [\n",
    "    'DOCUMENT_TYPE', 'BUSINESS_ABN', 'SUPPLIER_NAME', 'BUSINESS_ADDRESS',\n",
    "    'PAYER_NAME', 'PAYER_ADDRESS', 'INVOICE_DATE', 'LINE_ITEM_DESCRIPTIONS',\n",
    "    'LINE_ITEM_QUANTITIES', 'LINE_ITEM_PRICES', 'LINE_ITEM_TOTAL_PRICES',\n",
    "    'IS_GST_INCLUDED', 'GST_AMOUNT', 'TOTAL_AMOUNT', 'STATEMENT_DATE_RANGE',\n",
    "    'TRANSACTION_DATES', 'TRANSACTION_AMOUNTS_PAID'\n",
    "]\n",
    "\n",
    "def create_field_confusion_data(df_batch: pd.DataFrame, ground_truth_df: pd.DataFrame, model_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create field-level confusion data showing correct/incorrect/not_found status.\n",
    "\n",
    "    Args:\n",
    "        df_batch: Batch results DataFrame with extracted field values\n",
    "        ground_truth_df: Ground truth DataFrame\n",
    "        model_name: Name of the model\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with columns: field, status (correct/incorrect/not_found), count, model\n",
    "    \"\"\"\n",
    "    confusion_data = []\n",
    "\n",
    "    for field in FIELD_COLUMNS:\n",
    "        if field not in df_batch.columns or field not in ground_truth_df.columns:\n",
    "            continue\n",
    "\n",
    "        correct_count = 0\n",
    "        incorrect_count = 0\n",
    "        not_found_count = 0\n",
    "\n",
    "        # Normalize image names (strip extensions) for matching\n",
    "        # Batch has extensions (.jpeg, .png), ground truth does not\n",
    "        if 'image_stem' not in df_batch.columns:\n",
    "            df_batch['image_stem'] = df_batch['image_file'].apply(lambda x: Path(x).stem)\n",
    "        if 'image_stem' not in ground_truth_df.columns:\n",
    "            ground_truth_df['image_stem'] = ground_truth_df['image_file'].apply(lambda x: Path(x).stem)\n",
    "\n",
    "        # Merge on image_stem to align predictions with ground truth\n",
    "        merged = df_batch.merge(\n",
    "            ground_truth_df[['image_stem', field]],\n",
    "            on='image_stem',\n",
    "            how='inner',\n",
    "            suffixes=('_pred', '_true')\n",
    "        )\n",
    "\n",
    "        pred_col = f'{field}_pred' if f'{field}_pred' in merged.columns else field\n",
    "        true_col = f'{field}_true' if f'{field}_true' in merged.columns else field\n",
    "\n",
    "        for _, row in merged.iterrows():\n",
    "            pred_val = str(row[pred_col]).strip().upper()\n",
    "            true_val = str(row[true_col]).strip().upper()\n",
    "\n",
    "            if pred_val == 'NOT_FOUND' or pred_val == 'NAN' or pred_val == '':\n",
    "                not_found_count += 1\n",
    "            elif pred_val == true_val:\n",
    "                correct_count += 1\n",
    "            else:\n",
    "                incorrect_count += 1\n",
    "\n",
    "        # Add rows for each status\n",
    "        confusion_data.append({\n",
    "            'field': field,\n",
    "            'status': 'correct',\n",
    "            'count': correct_count,\n",
    "            'model': model_name\n",
    "        })\n",
    "        confusion_data.append({\n",
    "            'field': field,\n",
    "            'status': 'incorrect',\n",
    "            'count': incorrect_count,\n",
    "            'model': model_name\n",
    "        })\n",
    "        confusion_data.append({\n",
    "            'field': field,\n",
    "            'status': 'not_found',\n",
    "            'count': not_found_count,\n",
    "            'model': model_name\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(confusion_data)\n",
    "\n",
    "# Create confusion data for all 3 models\n",
    "rprint(\"[bold cyan]Creating field-level confusion matrices...[/bold cyan]\")\n",
    "\n",
    "llama_confusion = create_field_confusion_data(llama_batch_df, ground_truth, 'Llama-11B')\n",
    "internvl_confusion = create_field_confusion_data(internvl_batch_df, ground_truth, 'InternVL3-8B')\n",
    "internvl_nq_confusion = create_field_confusion_data(internvl_nq_batch_df, ground_truth, 'InternVL3-2B')\n",
    "\n",
    "# Combine all 3 models\n",
    "all_confusion = pd.concat([llama_confusion, internvl_confusion, internvl_nq_confusion], ignore_index=True)\n",
    "\n",
    "# Create pivot table for heatmap visualization\n",
    "def plot_confusion_heatmap(confusion_df: pd.DataFrame, model_name: str, ax):\n",
    "    \"\"\"Plot confusion matrix heatmap for a single model.\"\"\"\n",
    "    # Pivot to get fields x status matrix\n",
    "    pivot = confusion_df[confusion_df['model'] == model_name].pivot(\n",
    "        index='field',\n",
    "        columns='status',\n",
    "        values='count'\n",
    "    )\n",
    "\n",
    "    # Reorder columns: correct, incorrect, not_found\n",
    "    pivot = pivot[['correct', 'incorrect', 'not_found']]\n",
    "\n",
    "    # Sort by correct count (descending) for better visualization\n",
    "    pivot = pivot.sort_values('correct', ascending=False)\n",
    "\n",
    "    # Create heatmap\n",
    "    sns.heatmap(\n",
    "        pivot,\n",
    "        annot=True,\n",
    "        fmt='g',\n",
    "        cmap='RdYlGn_r',  # Red for high incorrect/not_found, green for high correct\n",
    "        cbar_kws={'label': 'Count'},\n",
    "        ax=ax,\n",
    "        linewidths=0.5,\n",
    "        linecolor='gray'\n",
    "    )\n",
    "\n",
    "    ax.set_title(f'{model_name} - Field Extraction Status', fontsize=13, fontweight='bold')\n",
    "    ax.set_xlabel('Extraction Status', fontsize=11, fontweight='bold')\n",
    "    ax.set_ylabel('Field Name', fontsize=11, fontweight='bold')\n",
    "    ax.tick_params(axis='y', rotation=0)\n",
    "\n",
    "# Create 3-panel heatmaps\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(24, 12))\n",
    "\n",
    "plot_confusion_heatmap(all_confusion, 'Llama-11B', ax1)\n",
    "plot_confusion_heatmap(all_confusion, 'InternVL3-8B', ax2)\n",
    "plot_confusion_heatmap(all_confusion, 'InternVL3-2B', ax3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(CONFIG['output_dir'].replace('/csv', '/visualizations') + '/field_confusion_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "rprint(\"[green]‚úÖ Field-level confusion heatmaps created (3 models)[/green]\")\n",
    "\n",
    "# Create summary statistics\n",
    "rprint(\"\\n[bold blue]Field Confusion Summary:[/bold blue]\")\n",
    "for model in ['Llama-11B', 'InternVL3-8B', 'InternVL3-2B']:\n",
    "    model_data = all_confusion[all_confusion['model'] == model]\n",
    "    total = model_data['count'].sum()\n",
    "    correct = model_data[model_data['status'] == 'correct']['count'].sum()\n",
    "    incorrect = model_data[model_data['status'] == 'incorrect']['count'].sum()\n",
    "    not_found = model_data[model_data['status'] == 'not_found']['count'].sum()\n",
    "\n",
    "    rprint(f\"\\n[cyan]{model}:[/cyan]\")\n",
    "    rprint(f\"  Correct: {correct}/{total} ({correct/total*100:.1f}%)\")\n",
    "    rprint(f\"  Incorrect: {incorrect}/{total} ({incorrect/total*100:.1f}%)\")\n",
    "    rprint(f\"  Not Found: {not_found}/{total} ({not_found/total*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 Per-Field Precision, Recall, and F1 Metrics\n",
    "\n",
    "**Using sklearn's classification metrics to evaluate per-field extraction performance.**\n",
    "\n",
    "Metrics explained:\n",
    "- **Precision**: Of all predicted values, what % were correct?\n",
    "- **Recall**: Of all ground truth values, what % were correctly extracted?\n",
    "- **F1 Score**: Harmonic mean of precision and recall\n",
    "- **Support**: Number of occurrences of each field in ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 22\n",
    "\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def compute_per_field_metrics(df_batch: pd.DataFrame, ground_truth_df: pd.DataFrame, model_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute precision, recall, F1 for each field using sklearn.\n",
    "\n",
    "    Treats field extraction as a multi-class classification problem where:\n",
    "    - Each unique field value is a class\n",
    "    - 'NOT_FOUND' is treated as a special class\n",
    "\n",
    "    Args:\n",
    "        df_batch: Batch results DataFrame with extracted field values\n",
    "        ground_truth_df: Ground truth DataFrame\n",
    "        model_name: Name of the model\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with columns: field, precision, recall, f1, support, model\n",
    "    \"\"\"\n",
    "    metrics_data = []\n",
    "\n",
    "    for field in FIELD_COLUMNS:\n",
    "        if field not in df_batch.columns or field not in ground_truth_df.columns:\n",
    "            continue\n",
    "\n",
    "        # Normalize image names (strip extensions) for matching\n",
    "        # Batch has extensions (.jpeg, .png), ground truth does not\n",
    "        if 'image_stem' not in df_batch.columns:\n",
    "            df_batch['image_stem'] = df_batch['image_file'].apply(lambda x: Path(x).stem)\n",
    "        if 'image_stem' not in ground_truth_df.columns:\n",
    "            ground_truth_df['image_stem'] = ground_truth_df['image_file'].apply(lambda x: Path(x).stem)\n",
    "\n",
    "        # Merge on image_stem to align predictions with ground truth\n",
    "        merged = df_batch.merge(\n",
    "            ground_truth_df[['image_stem', field]],\n",
    "            on='image_stem',\n",
    "            how='inner',\n",
    "            suffixes=('_pred', '_true')\n",
    "        )\n",
    "\n",
    "        pred_col = f'{field}_pred' if f'{field}_pred' in merged.columns else field\n",
    "        true_col = f'{field}_true' if f'{field}_true' in merged.columns else field\n",
    "\n",
    "        # Clean and normalize values\n",
    "        y_true = merged[true_col].fillna('NOT_FOUND').astype(str).str.strip().str.upper()\n",
    "        y_pred = merged[pred_col].fillna('NOT_FOUND').astype(str).str.strip().str.upper()\n",
    "\n",
    "        # Skip if no valid data\n",
    "        if len(y_true) == 0:\n",
    "            continue\n",
    "\n",
    "        # Compute binary metrics: correct vs incorrect (treating NOT_FOUND as incorrect)\n",
    "        y_true_binary = y_true\n",
    "        y_pred_binary = y_pred\n",
    "\n",
    "        # Calculate exact match accuracy\n",
    "        matches = (y_true_binary == y_pred_binary).sum()\n",
    "        total = len(y_true_binary)\n",
    "        accuracy = matches / total if total > 0 else 0\n",
    "\n",
    "        # For precision/recall, we treat it as binary: correct extraction vs not\n",
    "        # This is more meaningful than multi-class for field extraction\n",
    "        correct_mask = (y_pred_binary == y_true_binary) & (y_pred_binary != 'NOT_FOUND')\n",
    "\n",
    "        # True positives: predicted correctly (and not NOT_FOUND)\n",
    "        tp = correct_mask.sum()\n",
    "\n",
    "        # False positives: predicted incorrectly (but not NOT_FOUND)\n",
    "        fp = ((y_pred_binary != y_true_binary) & (y_pred_binary != 'NOT_FOUND')).sum()\n",
    "\n",
    "        # False negatives: failed to extract (predicted NOT_FOUND when ground truth exists)\n",
    "        fn = ((y_pred_binary == 'NOT_FOUND') & (y_true_binary != 'NOT_FOUND')).sum()\n",
    "\n",
    "        # Calculate metrics\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "        metrics_data.append({\n",
    "            'field': field,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'accuracy': accuracy,\n",
    "            'support': total,\n",
    "            'model': model_name\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(metrics_data)\n",
    "\n",
    "# Compute metrics for all 3 models\n",
    "rprint(\"[bold cyan]Computing per-field precision/recall/F1 metrics...[/bold cyan]\")\n",
    "\n",
    "llama_metrics = compute_per_field_metrics(llama_batch_df, ground_truth, 'Llama-11B')\n",
    "internvl_metrics = compute_per_field_metrics(internvl_batch_df, ground_truth, 'InternVL3-8B')\n",
    "internvl_nq_metrics = compute_per_field_metrics(internvl_nq_batch_df, ground_truth, 'InternVL3-2B')\n",
    "\n",
    "# Combine all 3 models\n",
    "all_metrics = pd.concat([llama_metrics, internvl_metrics, internvl_nq_metrics], ignore_index=True)\n",
    "\n",
    "# Display metrics table\n",
    "rprint(\"\\n[bold blue]Per-Field Metrics Comparison:[/bold blue]\")\n",
    "display(all_metrics.sort_values(['field', 'model']))\n",
    "\n",
    "# Save to CSV\n",
    "metrics_csv_path = CONFIG['output_dir'] + '/per_field_metrics.csv'\n",
    "all_metrics.to_csv(metrics_csv_path, index=False)\n",
    "rprint(f\"[green]‚úÖ Metrics saved to: {metrics_csv_path}[/green]\")\n",
    "\n",
    "# Create visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 16))\n",
    "\n",
    "# Colors for 3 models: Llama (blue), InternVL3-8B (red), InternVL3-2B (green)\n",
    "model_colors = ['#3498db', '#e74c3c', '#2ecc71']\n",
    "\n",
    "# Plot 1: F1 Score comparison\n",
    "ax1 = axes[0, 0]\n",
    "pivot_f1 = all_metrics.pivot(index='field', columns='model', values='f1_score')\n",
    "pivot_f1.plot(kind='barh', ax=ax1, color=model_colors)\n",
    "ax1.set_title('F1 Score by Field', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('F1 Score', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Field', fontsize=12, fontweight='bold')\n",
    "ax1.legend(title='Model')\n",
    "ax1.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Plot 2: Precision comparison\n",
    "ax2 = axes[0, 1]\n",
    "pivot_precision = all_metrics.pivot(index='field', columns='model', values='precision')\n",
    "pivot_precision.plot(kind='barh', ax=ax2, color=model_colors)\n",
    "ax2.set_title('Precision by Field', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Precision', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Field', fontsize=12, fontweight='bold')\n",
    "ax2.legend(title='Model')\n",
    "ax2.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Plot 3: Recall comparison\n",
    "ax3 = axes[1, 0]\n",
    "pivot_recall = all_metrics.pivot(index='field', columns='model', values='recall')\n",
    "pivot_recall.plot(kind='barh', ax=ax3, color=model_colors)\n",
    "ax3.set_title('Recall by Field', fontsize=14, fontweight='bold')\n",
    "ax3.set_xlabel('Recall', fontsize=12, fontweight='bold')\n",
    "ax3.set_ylabel('Field', fontsize=12, fontweight='bold')\n",
    "ax3.legend(title='Model')\n",
    "ax3.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Plot 4: Accuracy comparison\n",
    "ax4 = axes[1, 1]\n",
    "pivot_accuracy = all_metrics.pivot(index='field', columns='model', values='accuracy')\n",
    "pivot_accuracy.plot(kind='barh', ax=ax4, color=model_colors)\n",
    "ax4.set_title('Accuracy by Field', fontsize=14, fontweight='bold')\n",
    "ax4.set_xlabel('Accuracy', fontsize=12, fontweight='bold')\n",
    "ax4.set_ylabel('Field', fontsize=12, fontweight='bold')\n",
    "ax4.legend(title='Model')\n",
    "ax4.grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(CONFIG['output_dir'].replace('/csv', '/visualizations') + '/per_field_metrics.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "rprint(\"[green]‚úÖ Per-field metrics visualizations created (3 models)[/green]\")\n",
    "\n",
    "# Summary statistics\n",
    "rprint(\"\\n[bold blue]Model Performance Summary:[/bold blue]\")\n",
    "summary_stats = all_metrics.groupby('model').agg({\n",
    "    'precision': 'mean',\n",
    "    'recall': 'mean',\n",
    "    'f1_score': 'mean',\n",
    "    'accuracy': 'mean'\n",
    "}).round(4)\n",
    "display(summary_stats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.7 Hallucination Analysis\n",
    "\n",
    "**Purpose**: Measure how often models invent values for fields that don't exist (NOT_FOUND in ground truth).\n",
    "\n",
    "**Key Metrics**:\n",
    "- **Hallucination Rate**: Percentage of NOT_FOUND fields where model extracted a value\n",
    "- **False Positive Rate (FPR)**: FP / (FP + TN) on NOT_FOUND fields\n",
    "- **Per-Field Hallucination**: Which fields are most hallucinated\n",
    "- **Per-Document Hallucination**: Distribution of hallucination rates across documents\n",
    "\n",
    "**Context**: From the accuracy paradox:\n",
    "- Low accuracy + high F1 ‚Üí High hallucination (Llama)\n",
    "- High accuracy + low F1 ‚Üí Low hallucination (InternVL3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 24\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def comprehensive_hallucination_analysis(model_df, ground_truth_df, model_name):\n",
    "    \"\"\"\n",
    "    Complete hallucination analysis for a model.\n",
    "    \n",
    "    Hallucination = Model extracts value when ground truth is NOT_FOUND\n",
    "    \"\"\"\n",
    "    \n",
    "    # Normalize image names for matching\n",
    "    if 'image_stem' not in model_df.columns:\n",
    "        model_df['image_stem'] = model_df['image_file'].apply(lambda x: Path(x).stem)\n",
    "    if 'image_stem' not in ground_truth_df.columns:\n",
    "        ground_truth_df['image_stem'] = ground_truth_df['image_file'].apply(lambda x: Path(x).stem)\n",
    "    \n",
    "    # Merge to align predictions with ground truth\n",
    "    merged = model_df.merge(\n",
    "        ground_truth_df,\n",
    "        on='image_stem',\n",
    "        how='inner',\n",
    "        suffixes=('_pred', '_true')\n",
    "    )\n",
    "    \n",
    "    total_hallucinations = 0\n",
    "    total_not_found_fields = 0\n",
    "    total_correct_not_found = 0\n",
    "    \n",
    "    field_hallucination = {}\n",
    "    doc_hallucination_scores = []\n",
    "    \n",
    "    for idx in range(len(merged)):\n",
    "        doc_hallucinations = 0\n",
    "        doc_not_found = 0\n",
    "        doc_correct_not_found = 0\n",
    "        \n",
    "        for field in FIELD_COLUMNS:\n",
    "            # Get column names (handle suffix)\n",
    "            true_col = f'{field}_true' if f'{field}_true' in merged.columns else field\n",
    "            pred_col = f'{field}_pred' if f'{field}_pred' in merged.columns else field\n",
    "            \n",
    "            if true_col not in merged.columns or pred_col not in merged.columns:\n",
    "                continue\n",
    "            \n",
    "            gt_val = str(merged.loc[idx, true_col]).strip().upper()\n",
    "            pred_val = str(merged.loc[idx, pred_col]).strip().upper()\n",
    "            \n",
    "            if gt_val in ['NOT_FOUND', 'NAN', '']:\n",
    "                doc_not_found += 1\n",
    "                total_not_found_fields += 1\n",
    "                \n",
    "                if pred_val not in ['NOT_FOUND', 'NAN', '']:\n",
    "                    # HALLUCINATION DETECTED\n",
    "                    doc_hallucinations += 1\n",
    "                    total_hallucinations += 1\n",
    "                    \n",
    "                    # Track per-field\n",
    "                    if field not in field_hallucination:\n",
    "                        field_hallucination[field] = {'hallucinated': 0, 'not_found_total': 0}\n",
    "                    field_hallucination[field]['hallucinated'] += 1\n",
    "                    field_hallucination[field]['not_found_total'] += 1\n",
    "                else:\n",
    "                    # Correctly said NOT_FOUND\n",
    "                    doc_correct_not_found += 1\n",
    "                    total_correct_not_found += 1\n",
    "                    \n",
    "                    if field not in field_hallucination:\n",
    "                        field_hallucination[field] = {'hallucinated': 0, 'not_found_total': 0}\n",
    "                    field_hallucination[field]['not_found_total'] += 1\n",
    "        \n",
    "        # Document-level hallucination rate\n",
    "        doc_rate = doc_hallucinations / doc_not_found if doc_not_found > 0 else 0\n",
    "        doc_hallucination_scores.append({\n",
    "            'rate': doc_rate,\n",
    "            'hallucinated_count': doc_hallucinations,\n",
    "            'not_found_count': doc_not_found\n",
    "        })\n",
    "    \n",
    "    # Calculate overall rates\n",
    "    overall_rate = total_hallucinations / total_not_found_fields if total_not_found_fields > 0 else 0\n",
    "    correct_not_found_rate = total_correct_not_found / total_not_found_fields if total_not_found_fields > 0 else 0\n",
    "    \n",
    "    # Per-field hallucination rates\n",
    "    field_rates = {}\n",
    "    for field, data in field_hallucination.items():\n",
    "        if data['not_found_total'] > 0:\n",
    "            field_rates[field] = {\n",
    "                'hallucination_rate': data['hallucinated'] / data['not_found_total'],\n",
    "                'hallucinated_count': data['hallucinated'],\n",
    "                'opportunities': data['not_found_total']\n",
    "            }\n",
    "    \n",
    "    return {\n",
    "        'model': model_name,\n",
    "        'overall_hallucination_rate': overall_rate,\n",
    "        'correct_not_found_rate': correct_not_found_rate,\n",
    "        'total_hallucinations': total_hallucinations,\n",
    "        'total_correct_not_found': total_correct_not_found,\n",
    "        'total_not_found_opportunities': total_not_found_fields,\n",
    "        'field_hallucination': field_rates,\n",
    "        'document_hallucination_scores': doc_hallucination_scores,\n",
    "        'mean_doc_hallucination': np.mean([d['rate'] for d in doc_hallucination_scores]),\n",
    "        'std_doc_hallucination': np.std([d['rate'] for d in doc_hallucination_scores])\n",
    "    }\n",
    "\n",
    "# Run hallucination analysis for all 3 models\n",
    "rprint(\"[bold cyan]Analyzing hallucination rates for all models...[/bold cyan]\")\n",
    "\n",
    "llama_hallucination = comprehensive_hallucination_analysis(\n",
    "    llama_batch_df, ground_truth, 'Llama-11B'\n",
    ")\n",
    "\n",
    "internvl_hallucination = comprehensive_hallucination_analysis(\n",
    "    internvl_batch_df, ground_truth, 'InternVL3-8B'\n",
    ")\n",
    "\n",
    "internvl_nq_hallucination = comprehensive_hallucination_analysis(\n",
    "    internvl_nq_batch_df, ground_truth, 'InternVL3-2B'\n",
    ")\n",
    "\n",
    "# Create summary table\n",
    "hallucination_summary = pd.DataFrame([\n",
    "    {\n",
    "        'Model': llama_hallucination['model'],\n",
    "        'Hallucination Rate': f\"{llama_hallucination['overall_hallucination_rate']:.1%}\",\n",
    "        'Correct NOT_FOUND Rate': f\"{llama_hallucination['correct_not_found_rate']:.1%}\",\n",
    "        'Total Hallucinations': llama_hallucination['total_hallucinations'],\n",
    "        'Total Opportunities': llama_hallucination['total_not_found_opportunities'],\n",
    "        'Mean Doc Hallucination': f\"{llama_hallucination['mean_doc_hallucination']:.1%}\",\n",
    "        'Std Dev': f\"{llama_hallucination['std_doc_hallucination']:.1%}\"\n",
    "    },\n",
    "    {\n",
    "        'Model': internvl_hallucination['model'],\n",
    "        'Hallucination Rate': f\"{internvl_hallucination['overall_hallucination_rate']:.1%}\",\n",
    "        'Correct NOT_FOUND Rate': f\"{internvl_hallucination['correct_not_found_rate']:.1%}\",\n",
    "        'Total Hallucinations': internvl_hallucination['total_hallucinations'],\n",
    "        'Total Opportunities': internvl_hallucination['total_not_found_opportunities'],\n",
    "        'Mean Doc Hallucination': f\"{internvl_hallucination['mean_doc_hallucination']:.1%}\",\n",
    "        'Std Dev': f\"{internvl_hallucination['std_doc_hallucination']:.1%}\"\n",
    "    },\n",
    "    {\n",
    "        'Model': internvl_nq_hallucination['model'],\n",
    "        'Hallucination Rate': f\"{internvl_nq_hallucination['overall_hallucination_rate']:.1%}\",\n",
    "        'Correct NOT_FOUND Rate': f\"{internvl_nq_hallucination['correct_not_found_rate']:.1%}\",\n",
    "        'Total Hallucinations': internvl_nq_hallucination['total_hallucinations'],\n",
    "        'Total Opportunities': internvl_nq_hallucination['total_not_found_opportunities'],\n",
    "        'Mean Doc Hallucination': f\"{internvl_nq_hallucination['mean_doc_hallucination']:.1%}\",\n",
    "        'Std Dev': f\"{internvl_nq_hallucination['std_doc_hallucination']:.1%}\"\n",
    "    }\n",
    "])\n",
    "\n",
    "rprint(\"\\n[bold blue]Hallucination Summary:[/bold blue]\")\n",
    "display(hallucination_summary)\n",
    "\n",
    "# Interpretation\n",
    "rprint(\"\\n[bold yellow]Interpretation:[/bold yellow]\")\n",
    "rprint(\"[dim]Hallucination Rate = % of NOT_FOUND fields where model invented a value[/dim]\")\n",
    "rprint(\"[dim]Correct NOT_FOUND Rate = % of NOT_FOUND fields correctly identified[/dim]\")\n",
    "rprint(\"[dim]Higher hallucination = More aggressive extraction (high recall, low accuracy)[/dim]\")\n",
    "rprint(\"[dim]Lower hallucination = More conservative extraction (low recall, high accuracy)[/dim]\")\n",
    "\n",
    "# Create visualizations\n",
    "fig = plt.figure(figsize=(24, 16))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# Plot 1: Overall Hallucination Rate Comparison\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "models = [llama_hallucination['model'], internvl_hallucination['model'], internvl_nq_hallucination['model']]\n",
    "halluc_rates = [\n",
    "    llama_hallucination['overall_hallucination_rate'],\n",
    "    internvl_hallucination['overall_hallucination_rate'],\n",
    "    internvl_nq_hallucination['overall_hallucination_rate']\n",
    "]\n",
    "colors = ['#e74c3c', '#3498db', '#2ecc71']  # Red, Blue, Green\n",
    "\n",
    "bars = ax1.bar(models, halluc_rates, color=colors, alpha=0.7)\n",
    "ax1.set_ylabel('Hallucination Rate', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Overall Hallucination Rate by Model', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylim(0, max(halluc_rates) * 1.2)\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, rate in zip(bars, halluc_rates):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{rate:.1%}',\n",
    "             ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "\n",
    "# Plot 2: Hallucination vs Correct NOT_FOUND\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "x = np.arange(len(models))\n",
    "width = 0.35\n",
    "\n",
    "halluc_counts = [\n",
    "    llama_hallucination['total_hallucinations'],\n",
    "    internvl_hallucination['total_hallucinations'],\n",
    "    internvl_nq_hallucination['total_hallucinations']\n",
    "]\n",
    "correct_counts = [\n",
    "    llama_hallucination['total_correct_not_found'],\n",
    "    internvl_hallucination['total_correct_not_found'],\n",
    "    internvl_nq_hallucination['total_correct_not_found']\n",
    "]\n",
    "\n",
    "bars1 = ax2.bar(x - width/2, halluc_counts, width, label='Hallucinated', color='#e74c3c', alpha=0.7)\n",
    "bars2 = ax2.bar(x + width/2, correct_counts, width, label='Correct NOT_FOUND', color='#2ecc71', alpha=0.7)\n",
    "\n",
    "ax2.set_ylabel('Count', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Hallucinations vs Correct NOT_FOUND', fontsize=14, fontweight='bold')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(models, rotation=15, ha='right')\n",
    "ax2.legend()\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Plot 3: Hallucination Rate vs Recall (Tradeoff)\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "recalls = [\n",
    "    llama_hallucination.get('recall', 0.7797),  # From previous metrics\n",
    "    internvl_hallucination.get('recall', 0.4367),\n",
    "    internvl_nq_hallucination.get('recall', 0.3873)\n",
    "]\n",
    "\n",
    "ax3.scatter(halluc_rates, recalls, s=200, c=colors, alpha=0.7, edgecolors='black', linewidth=2)\n",
    "for i, model in enumerate(models):\n",
    "    ax3.annotate(model, (halluc_rates[i], recalls[i]), \n",
    "                xytext=(10, -5), textcoords='offset points', fontsize=10, fontweight='bold')\n",
    "\n",
    "ax3.set_xlabel('Hallucination Rate', fontsize=12, fontweight='bold')\n",
    "ax3.set_ylabel('Recall', fontsize=12, fontweight='bold')\n",
    "ax3.set_title('Hallucination vs Recall Tradeoff', fontsize=14, fontweight='bold')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4, 5, 6: Per-Field Hallucination Rates (3 models)\n",
    "for idx, (halluc_data, ax_pos, model_color) in enumerate([\n",
    "    (llama_hallucination, gs[1, 0], colors[0]),\n",
    "    (internvl_hallucination, gs[1, 1], colors[1]),\n",
    "    (internvl_nq_hallucination, gs[1, 2], colors[2])\n",
    "]):\n",
    "    ax = fig.add_subplot(ax_pos)\n",
    "    \n",
    "    # Sort fields by hallucination rate\n",
    "    field_data = halluc_data['field_hallucination']\n",
    "    sorted_fields = sorted(field_data.items(), key=lambda x: x[1]['hallucination_rate'], reverse=True)\n",
    "    \n",
    "    fields = [f[0] for f in sorted_fields][:15]  # Top 15\n",
    "    rates = [f[1]['hallucination_rate'] for f in sorted_fields][:15]\n",
    "    \n",
    "    bars = ax.barh(fields, rates, color=model_color, alpha=0.7)\n",
    "    ax.set_xlabel('Hallucination Rate', fontsize=11, fontweight='bold')\n",
    "    ax.set_title(f'{halluc_data[\"model\"]} - Field Hallucination', fontsize=13, fontweight='bold')\n",
    "    ax.set_xlim(0, 1.0)\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, rate in zip(bars, rates):\n",
    "        width = bar.get_width()\n",
    "        ax.text(width, bar.get_y() + bar.get_height()/2.,\n",
    "                f'{rate:.1%}',\n",
    "                ha='left', va='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "# Plot 7, 8, 9: Document Hallucination Distribution (histograms)\n",
    "for idx, (halluc_data, ax_pos, model_color) in enumerate([\n",
    "    (llama_hallucination, gs[2, 0], colors[0]),\n",
    "    (internvl_hallucination, gs[2, 1], colors[1]),\n",
    "    (internvl_nq_hallucination, gs[2, 2], colors[2])\n",
    "]):\n",
    "    ax = fig.add_subplot(ax_pos)\n",
    "    \n",
    "    doc_rates = [d['rate'] for d in halluc_data['document_hallucination_scores']]\n",
    "    \n",
    "    ax.hist(doc_rates, bins=20, color=model_color, alpha=0.7, edgecolor='black')\n",
    "    ax.axvline(halluc_data['mean_doc_hallucination'], color='red', linestyle='--', linewidth=2, label=f'Mean: {halluc_data[\"mean_doc_hallucination\"]:.1%}')\n",
    "    ax.set_xlabel('Document Hallucination Rate', fontsize=11, fontweight='bold')\n",
    "    ax.set_ylabel('Number of Documents', fontsize=11, fontweight='bold')\n",
    "    ax.set_title(f'{halluc_data[\"model\"]} - Document Distribution', fontsize=13, fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.savefig(CONFIG['output_dir'].replace('/csv', '/visualizations') + '/hallucination_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "rprint(\"[green]‚úÖ Hallucination analysis complete (9 visualizations created)[/green]\")\n",
    "rprint(f\"[green]üíæ Saved: visualizations/hallucination_analysis.png[/green]\")\n",
    "\n",
    "# Additional insights\n",
    "rprint(\"\\n[bold magenta]Key Insights:[/bold magenta]\")\n",
    "\n",
    "# Find highest hallucination model\n",
    "max_halluc_model = max(\n",
    "    [llama_hallucination, internvl_hallucination, internvl_nq_hallucination],\n",
    "    key=lambda x: x['overall_hallucination_rate']\n",
    ")\n",
    "min_halluc_model = min(\n",
    "    [llama_hallucination, internvl_hallucination, internvl_nq_hallucination],\n",
    "    key=lambda x: x['overall_hallucination_rate']\n",
    ")\n",
    "\n",
    "rprint(f\"[red]üî¥ Highest hallucination: {max_halluc_model['model']} ({max_halluc_model['overall_hallucination_rate']:.1%})[/red]\")\n",
    "rprint(f\"[green]üü¢ Lowest hallucination: {min_halluc_model['model']} ({min_halluc_model['overall_hallucination_rate']:.1%})[/green]\")\n",
    "\n",
    "# Find most hallucinated fields across all models\n",
    "all_field_halluc = {}\n",
    "for halluc_data in [llama_hallucination, internvl_hallucination, internvl_nq_hallucination]:\n",
    "    for field, data in halluc_data['field_hallucination'].items():\n",
    "        if field not in all_field_halluc:\n",
    "            all_field_halluc[field] = []\n",
    "        all_field_halluc[field].append(data['hallucination_rate'])\n",
    "\n",
    "avg_field_halluc = {field: np.mean(rates) for field, rates in all_field_halluc.items()}\n",
    "most_hallucinated = sorted(avg_field_halluc.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "\n",
    "rprint(\"\\n[yellow]Most Hallucinated Fields (avg across models):[/yellow]\")\n",
    "for field, rate in most_hallucinated:\n",
    "    rprint(f\"  ‚Ä¢ {field}: {rate:.1%}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Business Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 26\n",
    "\n",
    "def generate_business_recommendations(df: pd.DataFrame, field_performance: pd.DataFrame = None):\n",
    "    \"\"\"\n",
    "    Generate executive business recommendations based on CSV data analysis only.\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return\n",
    "    \n",
    "    rprint(\"\\n[bold green]üíº EXECUTIVE BUSINESS RECOMMENDATIONS[/bold green]\")\n",
    "    \n",
    "    # Overall performance leader\n",
    "    overall_leader = df.groupby('model')['overall_accuracy'].mean().idxmax()\n",
    "    speed_leader = df.groupby('model')['processing_time'].mean().idxmin()\n",
    "    \n",
    "    # Calculate key metrics\n",
    "    model_stats = df.groupby('model').agg({\n",
    "        'overall_accuracy': 'mean',\n",
    "        'processing_time': 'mean'\n",
    "    })\n",
    "    \n",
    "    rprint(\"\\n[bold blue]üéØ PRIMARY RECOMMENDATION:[/bold blue]\")\n",
    "    \n",
    "    if overall_leader == speed_leader:\n",
    "        rprint(f\"  ‚Ä¢ **Deploy {overall_leader}** - Clear winner on both accuracy and speed\")\n",
    "    else:\n",
    "        accuracy_leader_score = model_stats.loc[overall_leader, 'overall_accuracy']\n",
    "        speed_leader_score = model_stats.loc[speed_leader, 'processing_time']\n",
    "        \n",
    "        rprint(f\"  ‚Ä¢ **For Accuracy-Critical Applications**: Deploy {overall_leader} ({accuracy_leader_score:.1f}% avg accuracy)\")\n",
    "        rprint(f\"  ‚Ä¢ **For High-Volume Processing**: Deploy {speed_leader} ({speed_leader_score:.1f}s avg processing)\")\n",
    "    \n",
    "    rprint(\"\\n[bold blue]üìã USE CASE SPECIFIC RECOMMENDATIONS:[/bold blue]\")\n",
    "    \n",
    "    # Document-specific recommendations\n",
    "    doc_performance = df.groupby(['document_type', 'model'])['overall_accuracy'].mean().unstack()\n",
    "    \n",
    "    for doc_type in doc_performance.index:\n",
    "        best_model = doc_performance.loc[doc_type].idxmax()\n",
    "        best_score = doc_performance.loc[doc_type].max()\n",
    "        rprint(f\"  ‚Ä¢ **{doc_type.title()} Processing**: Use {best_model} ({best_score:.1f}% accuracy)\")\n",
    "    \n",
    "    # Field-specific recommendations\n",
    "    if field_performance is not None and not field_performance.empty:\n",
    "        rprint(\"\\n[bold blue]üîç FIELD-SPECIFIC RECOMMENDATIONS:[/bold blue]\")\n",
    "        \n",
    "        # Identify fields with significant performance differences between models\n",
    "        model_cols = [col for col in field_performance.columns if col not in ['avg_accuracy', 'variance', 'best_model', 'best_score', 'max_diff']]\n",
    "        \n",
    "        if len(model_cols) >= 2:\n",
    "            # Calculate performance spread for each field\n",
    "            field_performance['max_diff'] = field_performance[model_cols].max(axis=1) - field_performance[model_cols].min(axis=1)\n",
    "            \n",
    "            # Get fields with significant differences (>20% spread)\n",
    "            significant_diff_fields = field_performance[field_performance['max_diff'] > 0.2].nlargest(5, 'max_diff')\n",
    "            \n",
    "            if not significant_diff_fields.empty:\n",
    "                rprint(\"  **Fields with Significant Model Performance Differences:**\")\n",
    "                for field_name, row in significant_diff_fields.iterrows():\n",
    "                    best_model = row['best_model']\n",
    "                    best_score = row['best_score']\n",
    "                    worst_score = row[model_cols].min()\n",
    "                    advantage = best_score - worst_score\n",
    "                    rprint(f\"    ‚Ä¢ {field_name}: Use **{best_model}** ({best_score:.0%} vs {worst_score:.0%}, +{advantage:.0%} advantage)\")\n",
    "        \n",
    "        # Identify consistently problematic fields\n",
    "        problematic_fields = field_performance[field_performance['avg_accuracy'] < 0.5]\n",
    "        if not problematic_fields.empty:\n",
    "            rprint(\"\\n  **‚ö†Ô∏è Fields Requiring Attention (Low Accuracy):**\")\n",
    "            for field_name, row in problematic_fields.head(5).iterrows():\n",
    "                avg_acc = row['avg_accuracy']\n",
    "                rprint(f\"    ‚Ä¢ {field_name}: {avg_acc:.0%} average accuracy - Consider prompt optimization\")\n",
    "    \n",
    "    rprint(\"\\n[bold blue]üìä PERFORMANCE EFFICIENCY (CSV Data Only):[/bold blue]\")\n",
    "    \n",
    "    # Calculate pure performance efficiency metrics from CSV data only\n",
    "    for model in df['model'].unique():\n",
    "        model_data = df[df['model'] == model]\n",
    "        avg_accuracy = model_data['overall_accuracy'].mean()\n",
    "        avg_time = model_data['processing_time'].mean()\n",
    "        throughput = 60 / avg_time\n",
    "        \n",
    "        # Pure efficiency score based only on CSV performance data\n",
    "        efficiency_score = avg_accuracy * throughput\n",
    "        rprint(f\"  ‚Ä¢ **{model}**: Efficiency score {efficiency_score:.1f} (accuracy √ó throughput)\")\n",
    "    \n",
    "    rprint(\"\\n[bold blue]‚ö° PERFORMANCE COMPARISON SUMMARY:[/bold blue]\")\n",
    "    \n",
    "    # Generate data-driven insights based only on CSV metrics\n",
    "    accuracy_winner = df.groupby('model')['overall_accuracy'].mean().idxmax()\n",
    "    speed_winner = df.groupby('model')['processing_time'].mean().idxmin()\n",
    "    consistency_winner = df.groupby('model')['overall_accuracy'].std().idxmin()\n",
    "    \n",
    "    rprint(f\"  ‚Ä¢ **Highest Average Accuracy**: {accuracy_winner}\")\n",
    "    rprint(f\"  ‚Ä¢ **Fastest Processing**: {speed_winner}\")  \n",
    "    rprint(f\"  ‚Ä¢ **Most Consistent Results**: {consistency_winner}\")\n",
    "\n",
    "# Generate recommendations with field-level insights\n",
    "if not combined_df.empty:\n",
    "    field_perf = field_performance if 'field_performance' in locals() and not field_performance.empty else None\n",
    "    generate_business_recommendations(combined_df, field_perf)\n",
    "else:\n",
    "    rprint(\"[red]‚ùå Cannot generate recommendations - no data available[/red]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 27\n",
    "\n",
    "def export_executive_report(df: pd.DataFrame, output_dir: str = \"output/reports\", visualization_timestamp: str = None):\n",
    "    \"\"\"\n",
    "    Export a comprehensive executive report in multiple formats.\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        rprint(\"[red]‚ùå Cannot export report - no data available[/red]\")\n",
    "        return\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Export detailed CSV comparison\n",
    "    comparison_file = output_path / f\"executive_model_comparison_{timestamp}.csv\"\n",
    "    df.to_csv(comparison_file, index=False)\n",
    "    \n",
    "    # Export summary statistics\n",
    "    summary_file = output_path / f\"executive_summary_{timestamp}.csv\"\n",
    "    summary_stats = generate_executive_summary(df)\n",
    "    summary_stats.to_csv(summary_file, index=False)\n",
    "    \n",
    "    rprint(\"\\n[green]‚úÖ Executive reports exported:[/green]\")\n",
    "    rprint(f\"  ‚Ä¢ Detailed comparison: {comparison_file}\")\n",
    "    rprint(f\"  ‚Ä¢ Executive summary: {summary_file}\")\n",
    "    \n",
    "    # Generate markdown report with visualization\n",
    "    markdown_file = output_path / f\"executive_report_{timestamp}.md\"\n",
    "    with open(markdown_file, 'w') as f:\n",
    "        f.write(\"# Executive Model Comparison Report\\n\\n\")\n",
    "        f.write(f\"**Generated**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "        \n",
    "        # Include visualization if available\n",
    "        if visualization_timestamp:\n",
    "            viz_path = f\"../visualizations/executive_comparison_{visualization_timestamp}.png\"\n",
    "            f.write(\"## Performance Dashboard\\n\\n\")\n",
    "            f.write(f\"![Executive Performance Comparison]({viz_path})\\n\\n\")\n",
    "        \n",
    "        f.write(\"## Executive Summary\\n\\n\")\n",
    "        \n",
    "        # Model comparison summary\n",
    "        for model in df['model'].unique():\n",
    "            model_data = df[df['model'] == model]\n",
    "            avg_acc = model_data['overall_accuracy'].mean()\n",
    "            avg_time = model_data['processing_time'].mean()\n",
    "            throughput = 60 / avg_time\n",
    "            \n",
    "            f.write(f\"### {model}\\n\")\n",
    "            f.write(f\"- **Average Accuracy**: {avg_acc:.1f}%\\n\")\n",
    "            f.write(f\"- **Average Processing Time**: {avg_time:.1f} seconds\\n\")\n",
    "            f.write(f\"- **Throughput**: {throughput:.1f} documents per minute\\n\")\n",
    "            f.write(f\"- **Documents Processed**: {len(model_data)}\\n\\n\")\n",
    "        \n",
    "        f.write(\"## Document Type Performance\\n\\n\")\n",
    "        doc_perf = df.groupby(['document_type', 'model'])['overall_accuracy'].mean().unstack()\n",
    "        f.write(doc_perf.to_markdown())\n",
    "        f.write(\"\\n\\n## Key Findings\\n\\n\")\n",
    "        \n",
    "        # Add key findings based on data\n",
    "        accuracy_leader = df.groupby('model')['overall_accuracy'].mean().idxmax()\n",
    "        speed_leader = df.groupby('model')['processing_time'].mean().idxmin()\n",
    "        \n",
    "        f.write(f\"- **Accuracy Leader**: {accuracy_leader}\\n\")\n",
    "        f.write(f\"- **Speed Leader**: {speed_leader}\\n\")\n",
    "        f.write(f\"- **Best for Invoices**: {df[df['document_type']=='invoice'].groupby('model')['overall_accuracy'].mean().idxmax()}\\n\")\n",
    "        f.write(f\"- **Best for Receipts**: {df[df['document_type']=='receipt'].groupby('model')['overall_accuracy'].mean().idxmax()}\\n\")\n",
    "        f.write(f\"- **Best for Bank Statements**: {df[df['document_type']=='bank_statement'].groupby('model')['overall_accuracy'].mean().idxmax()}\\n\")\n",
    "        \n",
    "        f.write(\"\\n\\n## Recommendations\\n\\n\")\n",
    "        f.write(\"Detailed recommendations and analysis available in the full comparison notebook.\\n\")\n",
    "    \n",
    "    rprint(f\"  ‚Ä¢ Markdown report: {markdown_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 28\n",
    "\n",
    "# Export reports with visualization reference\n",
    "if not combined_df.empty:\n",
    "    # Use the dashboard timestamp from earlier\n",
    "    export_executive_report(combined_df, visualization_timestamp=dashboard_timestamp)\n",
    "    \n",
    "    rprint(\"\\n[bold green]üéâ Executive Analysis Complete![/bold green]\")\n",
    "    rprint(\"[cyan]üìä Dashboard and reports are ready for executive presentation[/cyan]\")\n",
    "else:\n",
    "    rprint(\"[red]‚ùå Cannot export reports - no data available[/red]\")\n",
    "    rprint(\"\\n[yellow]üí° To use this notebook:[/yellow]\")\n",
    "    rprint(\"  1. Run llama_document_aware_batch.ipynb to generate Llama results\")\n",
    "    rprint(\"  2. Run internvl3_document_aware_batch.ipynb to generate InternVL3 results\")\n",
    "    rprint(\"  3. Re-run this notebook to generate the executive comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VERSION 104"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "du",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
