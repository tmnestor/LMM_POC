{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### InternVL3.5-8B Document Extraction\n",
    "\n",
    "**Primary notebook for InternVL3.5-8B vision-language model document extraction.**\n",
    "\n",
    "**Model**: InternVL3.5-8B (8.5B parameters: 0.3B vision + 8.2B language)\n",
    "\n",
    "**Features:**\n",
    "- Cascade Reinforcement Learning (Cascade RL) for enhanced reasoning\n",
    "- Visual Resolution Router (ViR) for dynamic resolution adjustment\n",
    "- Flash Attention 2 support\n",
    "- Sophisticated bank statement extraction with multi-turn processing\n",
    "\n",
    "**Requirements**: \n",
    "- `transformers>=4.52.1` (critical for InternVL3.5 support)\n",
    "- PyTorch with CUDA support\n",
    "- H200/H100/A100 GPU (native bfloat16) or V100 (float32)\n",
    "\n",
    "**Configuration:**\n",
    "- `USE_SOPHISTICATED_BANK_EXTRACTION`: True (default) uses multi-turn extraction\n",
    "- `ENABLE_BALANCE_CORRECTION`: Optional mathematical balance validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Project root: /home/jovyan/nfs_share/tod_2026/LMM_POC\n",
      "‚úÖ Added /home/jovyan/nfs_share/tod_2026/LMM_POC to sys.path\n",
      "‚úÖ Common module found at: /home/jovyan/nfs_share/tod_2026/LMM_POC/common/__init__.py\n",
      "‚úÖ Path setup complete\n"
     ]
    }
   ],
   "source": [
    "#Cell 1\n",
    "# Project root setup - notebook is at project root\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "os.environ['EVALUATION_METHOD'] = 'order_aware_f1'  # or 'f1', 'kieval', 'order_aware_f1', 'correlation'\n",
    "\n",
    "# Project root is current directory (notebook at root)\n",
    "PROJECT_ROOT = Path().absolute()\n",
    "print(f\"üìÇ Project root: {PROJECT_ROOT}\")\n",
    "\n",
    "# Ensure the project root is in the Python path\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "    print(f\"‚úÖ Added {PROJECT_ROOT} to sys.path\")\n",
    "\n",
    "# Verify common module can be found\n",
    "try:\n",
    "    import common\n",
    "    print(f\"‚úÖ Common module found at: {common.__file__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Common module not found: {e}\")\n",
    "    raise\n",
    "\n",
    "print(\"‚úÖ Path setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#Cell 2\n# Enable autoreload for module changes\n%load_ext autoreload\n%autoreload 2\n\n# Standard library imports\nimport sys\nimport warnings\nfrom datetime import datetime\nfrom pathlib import Path\n\n# Add current directory to path to ensure proper module resolution\nnotebook_dir = Path.cwd()\nif str(notebook_dir) not in sys.path:\n    sys.path.insert(0, str(notebook_dir))\n\n# Third-party imports\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom IPython.display import display\nfrom rich import print as rprint\nfrom rich.console import Console\nfrom transformers import AutoModel, AutoTokenizer\n\n# V2: Sophisticated bank statement processing\nfrom common.bank_statement_adapter import BankStatementAdapter\n\n# Project-specific imports - consistent with ivl3_cli.py\nfrom common.batch_analytics import BatchAnalytics\nfrom common.batch_processor import BatchDocumentProcessor, load_document_field_definitions\nfrom common.batch_reporting import BatchReporter\nfrom common.batch_visualizations import BatchVisualizer\nfrom common.evaluation_metrics import load_ground_truth\nfrom common.pipeline_config import discover_images\nfrom common.gpu_optimization import emergency_cleanup\nfrom models.document_aware_internvl3_processor import (\n    DocumentAwareInternVL3HybridProcessor,\n)\n\nprint(\"‚úÖ All imports loaded successfully\")\nprint(\"‚úÖ InternVL3 Hybrid Processor imported successfully\") \nprint(\"‚úÖ Batch processing modules imported successfully\")\nprint(\"‚úÖ V2: BankStatementAdapter imported for sophisticated bank extraction\")\nprint(\"‚úÖ load_document_field_definitions imported for YAML config\")\nprint(f\"üìÇ Working directory: {notebook_dir}\")\nprint(\"üöÄ InternVL3.5-8B: Cascade RL + Visual Resolution Router\")\nwarnings.filterwarnings('ignore')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pre-emptive Memory Cleanup\n",
    "\n",
    "**CRITICAL for V100**: Run this cell first to prevent OOM errors when switching between models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 3\n",
    "# Pre-emptive V100 Memory Cleanup - Run FIRST to prevent OOM errors\n",
    "rprint(\"[bold red]üßπ PRE-EMPTIVE V100 MEMORY CLEANUP[/bold red]\")\n",
    "rprint(\"[yellow]Clearing any existing model caches before loading...[/yellow]\")\n",
    "rprint(\"[cyan]üí° This prevents OOM errors when switching between models on V100[/cyan]\")\n",
    "\n",
    "# Emergency cleanup to ensure clean slate\n",
    "emergency_cleanup(verbose=True)\n",
    "\n",
    "rprint(\"[green]‚úÖ Memory cleanup complete - ready for model loading[/green]\")\n",
    "rprint(\"[dim]üìã Next: Import modules and configure settings[/dim]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#Cell 4\n# Initialize console and environment configuration\nconsole = Console()\n\n# Environment-specific base paths\nENVIRONMENT_BASES = {\n    'sandbox': '/home/jovyan/nfs_share/tod_2026',\n    'efs': '/efs/shared/PoC_data'\n}\nbase_data_path = ENVIRONMENT_BASES['sandbox']\n\nCONFIG = {\n    # Model settings - InternVL3.5-8B\n    'MODEL_PATH': '/home/jovyan/nfs_share/models/InternVL3_5-8B',\n    # Alternative paths:\n    # 'MODEL_PATH': '/efs/shared/PTM/InternVL3_5-8B',\n    # 'MODEL_PATH': 'OpenGVLab/InternVL3_5-8B',  # Auto-download from HuggingFace\n\n    # Batch settings - Using base path for consistency\n    # 'DATA_DIR': f'{base_data_path}/LMM_POC/evaluation_data/travel/logbooks',\n    # 'GROUND_TRUTH': f'{base_data_path}/LMM_POC/evaluation_data/travel/logbooks/ground_truth_logbook.csv',\n    'DATA_DIR': f'{base_data_path}/LMM_POC/evaluation_data/synthetic',\n    'GROUND_TRUTH': f'{base_data_path}/LMM_POC/evaluation_data/synthetic/ground_truth_synthetic.csv',\n\n    'OUTPUT_BASE': f'{base_data_path}/LMM_POC/output',\n    'MAX_IMAGES': None,  # None for all, or set limit\n    'DOCUMENT_TYPES': None,  # None for all, or ['invoice', 'receipt']\n    'ENABLE_MATH_ENHANCEMENT': False,  # Disable mathematical correction for bank statements\n    \n    # Batch inference settings (consistent with ivl3_cli.py --batch-size)\n    'BATCH_SIZE': None,  # None for auto-detect from VRAM, or set explicitly\n    \n    # Inference and evaluation mode\n    'INFERENCE_ONLY': False,  # Default: False (evaluation mode)\n    \n    # Verbosity control\n    'VERBOSE': True,\n    'SHOW_PROMPTS': True,\n    \n    # ============================================================================\n    # H200 BFLOAT16 CONFIGURATION - OPTIMAL PERFORMANCE BASELINE\n    # ============================================================================\n    'USE_QUANTIZATION': False,  # Full precision for H200\n    'DEVICE_MAP': 'auto',\n    'MAX_NEW_TOKENS': 2000,\n    'TORCH_DTYPE': 'bfloat16',  # Native H200 support\n    'LOW_CPU_MEM_USAGE': False,  # Must be False - InternVL3.5 vision encoder incompatible with meta device init\n    'USE_FLASH_ATTN': True,  # H200 optimized - faster inference\n    'USE_TORCH_COMPILE': False,\n    'MAX_TILES': 11,  # H200 optimized - InternVL3.5 training max for dense OCR\n    \n    # ============================================================================\n    # V2: SOPHISTICATED BANK STATEMENT EXTRACTION\n    # ============================================================================\n    'USE_SOPHISTICATED_BANK_EXTRACTION': True,\n    'ENABLE_BALANCE_CORRECTION': True,\n}\n\n# Make GROUND_TRUTH conditional based on INFERENCE_ONLY mode\nif CONFIG['INFERENCE_ONLY']:\n    CONFIG['GROUND_TRUTH'] = None\n\n# ============================================================================\n# PROMPT CONFIGURATION - Uses PROJECT_ROOT for subdirectory compatibility\n# ============================================================================\nPROMPT_CONFIG = {\n    # Document type detection configuration\n    'detection_file': str(PROJECT_ROOT / 'prompts/document_type_detection.yaml'),\n    'detection_key': 'detection',\n    \n    # Extraction prompt file mapping (REQUIRED)\n    'extraction_files': {\n        'INVOICE': str(PROJECT_ROOT / 'prompts/internvl3_prompts.yaml'),\n        'RECEIPT': str(PROJECT_ROOT / 'prompts/internvl3_prompts.yaml'),\n        'BANK_STATEMENT': str(PROJECT_ROOT / 'prompts/internvl3_prompts.yaml'),\n        'TRAVEL_EXPENSE': str(PROJECT_ROOT / 'prompts/internvl3_prompts.yaml'),\n        'VEHICLE_LOGBOOK': str(PROJECT_ROOT / 'prompts/internvl3_prompts.yaml'),\n    },\n}\n\n# ============================================================================\n# FIELD DEFINITIONS - LOADED FROM YAML (SINGLE SOURCE OF TRUTH)\n# ============================================================================\n# Load document-type-specific field lists from config/field_definitions.yaml\n# This replaces the hardcoded UNIVERSAL_FIELDS list to ensure consistency\n# across all components (processor, evaluation, CSV output).\n# ============================================================================\nDOCUMENT_FIELD_DEFINITIONS = load_document_field_definitions()\n\n# Create a union of all fields for multi-document-type batches\n# This is used for CSV column headers when processing mixed document types\nALL_FIELDS = set()\nfor doc_type, fields in DOCUMENT_FIELD_DEFINITIONS.items():\n    ALL_FIELDS.update(fields)\nALL_FIELDS = sorted(list(ALL_FIELDS))\n\n# For backwards compatibility, UNIVERSAL_FIELDS is now the union of all fields\nUNIVERSAL_FIELDS = ALL_FIELDS\n\nprint(\"‚úÖ Configuration set up successfully\")\nprint(f\"üìÇ Evaluation data: {CONFIG['DATA_DIR']}\")\nprint(f\"üìä Ground truth: {CONFIG['GROUND_TRUTH']}\")\nprint(\"ü§ñ Model: InternVL3.5-8B\")\nprint(f\"üìÅ Model path: {CONFIG['MODEL_PATH']}\")\nprint(f\"üìÅ Output base: {CONFIG['OUTPUT_BASE']}\")\nprint(f\"üè† Project root: {PROJECT_ROOT}\")\nprint(f\"üìã Field definitions loaded from YAML:\")\nfor doc_type, fields in DOCUMENT_FIELD_DEFINITIONS.items():\n    print(f\"   - {doc_type}: {len(fields)} fields\")\nprint(f\"üìã Universal fields (union): {len(UNIVERSAL_FIELDS)}\")\nprint(f\"üéØ Mode: {'Inference-only' if CONFIG['INFERENCE_ONLY'] else 'Evaluation mode'}\")\nprint(f\"‚öôÔ∏è  Precision: {CONFIG['TORCH_DTYPE'].upper()}\")\nprint(f\"‚ö° Flash Attention: {'ENABLED' if CONFIG['USE_FLASH_ATTN'] else 'DISABLED'}\")\nprint(f\"üî≤ Max Tiles: {CONFIG['MAX_TILES']}\")\nprint(f\"üì¶ Batch Size: {CONFIG['BATCH_SIZE'] or 'auto'}\")\nprint(f\"üè¶ V2 Bank Extraction: {'Enabled' if CONFIG['USE_SOPHISTICATED_BANK_EXTRACTION'] else 'Disabled'}\")\nprint(f\"üî• torch.compile: {'ENABLED' if CONFIG['USE_TORCH_COMPILE'] else 'DISABLED'}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Output Directory Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 5\n",
    "# Setup output directories - Handle both absolute and relative paths\n",
    "\n",
    "# Convert OUTPUT_BASE to Path and handle absolute/relative paths\n",
    "OUTPUT_BASE = Path(CONFIG['OUTPUT_BASE'])\n",
    "if not OUTPUT_BASE.is_absolute():\n",
    "    # If relative, make it relative to current working directory\n",
    "    OUTPUT_BASE = Path.cwd() / OUTPUT_BASE\n",
    "\n",
    "BATCH_TIMESTAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "OUTPUT_DIRS = {\n",
    "    'base': OUTPUT_BASE,\n",
    "    'batch': OUTPUT_BASE / 'batch_results',\n",
    "    'csv': OUTPUT_BASE / 'csv',\n",
    "    'visualizations': OUTPUT_BASE / 'visualizations',\n",
    "    'reports': OUTPUT_BASE / 'reports'\n",
    "}\n",
    "\n",
    "for dir_path in OUTPUT_DIRS.values():\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Model Loading\n",
    "\n",
    "**InternVL3.5-8B**: Loading the latest InternVL3.5 model with Cascade RL and Visual Resolution Router.\n",
    "\n",
    "**Requirements**: Ensure `transformers>=4.52.1` is installed for InternVL3.5 compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#Cell 6\n# Load InternVL3.5-8B model\n# Model page: https://huggingface.co/OpenGVLab/InternVL3_5-8B\n# Using official loading pattern from HuggingFace model card\n\nrprint(\"[bold green]Loading InternVL3.5-8B model...[/bold green]\")\nrprint(\"[cyan]üöÄ Model: InternVL3.5-8B (8.5B params: 0.3B vision + 8.2B language)[/cyan]\")\nrprint(\"[cyan]üìñ Features: Cascade RL, Visual Resolution Router (ViR)[/cyan]\")\n\n# Resolve torch dtype from config string (consistent with ivl3_cli.py)\nTORCH_DTYPE = getattr(torch, CONFIG['TORCH_DTYPE'])\n\ntry:\n    # Clear any existing CUDA cache\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        rprint(\"[blue]üßπ CUDA cache cleared[/blue]\")\n    \n    world_size = torch.cuda.device_count()\n    rprint(f\"[cyan]üñ•Ô∏è  Detected {world_size} GPU(s)[/cyan]\")\n    rprint(\"[cyan]üì• Loading model (official HuggingFace pattern)...[/cyan]\")\n    \n    # Official loading pattern from https://huggingface.co/OpenGVLab/InternVL3_5-8B\n    # Using device_map='auto' for multi-GPU distribution\n    model = AutoModel.from_pretrained(\n        CONFIG['MODEL_PATH'],\n        torch_dtype=TORCH_DTYPE,\n        low_cpu_mem_usage=True,\n        use_flash_attn=CONFIG['USE_FLASH_ATTN'],\n        trust_remote_code=True,\n        device_map='auto',\n    ).eval()\n    \n    # Load tokenizer\n    rprint(\"[cyan]üì• Loading tokenizer...[/cyan]\")\n    tokenizer = AutoTokenizer.from_pretrained(\n        CONFIG['MODEL_PATH'],\n        trust_remote_code=True,\n        use_fast=False\n    )\n    \n    # Set generation parameters\n    model.config.max_new_tokens = CONFIG['MAX_NEW_TOKENS']\n    \n    rprint(\"[green]‚úÖ Model and tokenizer loaded successfully![/green]\")\n    \n    # ============================================================================\n    # torch.compile: JIT OPTIMIZATION (PyTorch 2.x)\n    # ============================================================================\n    torch_compile_active = False\n    if CONFIG.get('USE_TORCH_COMPILE', False):\n        rprint(\"[cyan]üî• Applying torch.compile (JIT optimization)...[/cyan]\")\n        try:\n            model = torch.compile(model, mode=\"default\")\n            torch_compile_active = True\n            rprint(\"[green]‚úÖ torch.compile applied (mode=default)[/green]\")\n            rprint(\"[dim]   First inference will be slower (JIT compilation), subsequent runs faster[/dim]\")\n        except Exception as e:\n            rprint(f\"[yellow]‚ö†Ô∏è torch.compile failed: {e}[/yellow]\")\n            rprint(\"[yellow]   Continuing without JIT optimization (model works normally)[/yellow]\")\n    \n    # ============================================================================\n    # GPU MEMORY DISPLAY\n    # ============================================================================\n    if torch.cuda.is_available():\n        device_count = torch.cuda.device_count()\n        \n        if device_count > 1:\n            rprint(f\"[blue]üîÑ GPU Memory Status ({device_count} GPUs available, device_map='auto'):[/blue]\")\n            \n            total_allocated = 0\n            total_capacity = 0\n            \n            for gpu_id in range(device_count):\n                gpu_allocated = torch.cuda.memory_allocated(gpu_id) / 1e9\n                gpu_capacity = torch.cuda.get_device_properties(gpu_id).total_memory / 1e9\n                gpu_name = torch.cuda.get_device_name(gpu_id)\n                \n                total_allocated += gpu_allocated\n                total_capacity += gpu_capacity\n                \n                usage_pct = (gpu_allocated / gpu_capacity) * 100 if gpu_capacity > 0 else 0\n                rprint(f\"   GPU {gpu_id} ({gpu_name}): {gpu_allocated:.1f}GB/{gpu_capacity:.0f}GB ({usage_pct:.1f}%)\")\n            \n            rprint(f\"[blue]üìä Total: {total_allocated:.1f}GB allocated, {total_capacity:.0f}GB capacity[/blue]\")\n        else:\n            allocated = torch.cuda.memory_allocated() / 1e9\n            total = torch.cuda.get_device_properties(0).total_memory / 1e9\n            gpu_name = torch.cuda.get_device_name(0)\n            usage_pct = (allocated / total) * 100 if total > 0 else 0\n            \n            rprint(f\"[blue]üìä GPU {gpu_name}: {allocated:.1f}GB/{total:.0f}GB ({usage_pct:.1f}%)[/blue]\")\n    \n    # ============================================================================\n    # CONFIGURATION TABLE\n    # ============================================================================\n    from rich.table import Table\n    \n    config_table = Table(title=\"üîß InternVL3.5-8B Model Configuration\")\n    config_table.add_column(\"Setting\", style=\"cyan\")\n    config_table.add_column(\"Value\", style=\"yellow\")\n    config_table.add_column(\"Status\", style=\"green\")\n    \n    model_name = Path(CONFIG['MODEL_PATH']).name\n    config_table.add_row(\"Model Path\", model_name, \"‚úÖ Valid\")\n    config_table.add_row(\"Model Type\", \"InternVL3.5-8B\", \"‚úÖ Cascade RL + ViR\")\n    config_table.add_row(\"Device Placement\", \"device_map='auto'\", \"‚úÖ Multi-GPU\")\n    \n    quant_method = \"16-bit\" if not CONFIG['USE_QUANTIZATION'] else \"8-bit\"\n    config_table.add_row(\"Quantization Method\", quant_method, \"‚úÖ Configured\")\n    config_table.add_row(\"Data Type\", CONFIG['TORCH_DTYPE'], \"‚úÖ Recommended\")\n    config_table.add_row(\"Max New Tokens\", str(CONFIG['MAX_NEW_TOKENS']), \"‚úÖ Generation Ready\")\n    config_table.add_row(\"Max Tiles\", str(CONFIG['MAX_TILES']), \"‚úÖ Configured\")\n    \n    if torch.cuda.is_available():\n        device_count = torch.cuda.device_count()\n        if device_count > 1:\n            gpu_info = f\"{device_count}x {torch.cuda.get_device_name(0)} ({total_capacity:.0f}GB total)\"\n            gpu_status = \"‚úÖ Multi-GPU (auto)\"\n        else:\n            gpu_info = f\"{torch.cuda.get_device_name(0)} ({total:.0f}GB)\"\n            gpu_status = \"‚úÖ Single GPU\"\n    else:\n        gpu_info = \"CPU\"\n        gpu_status = \"üíª CPU Mode\"\n    config_table.add_row(\"GPU Configuration\", gpu_info, gpu_status)\n    \n    param_count = sum(p.numel() for p in model.parameters())\n    config_table.add_row(\"Model Parameters\", f\"{param_count:,}\", \"‚úÖ Loaded\")\n    \n    flash_attn_status = \"‚úÖ Enabled\" if CONFIG['USE_FLASH_ATTN'] else \"‚ö†Ô∏è Disabled\"\n    config_table.add_row(\"Flash Attention\", str(CONFIG['USE_FLASH_ATTN']), flash_attn_status)\n    \n    compile_status = \"‚úÖ Active\" if torch_compile_active else \"‚ö†Ô∏è Disabled\"\n    config_table.add_row(\"torch.compile\", str(torch_compile_active), compile_status)\n    \n    batch_size_display = str(CONFIG['BATCH_SIZE']) if CONFIG['BATCH_SIZE'] else \"auto\"\n    config_table.add_row(\"Batch Size\", batch_size_display, \"‚úÖ Configured\")\n    \n    console.print(config_table)\n    \n    # Initialize the hybrid processor with loaded model components\n    # (consistent with ivl3_cli.py create_processor())\n    rprint(\"[cyan]üîß Initializing document-aware processor...[/cyan]\")\n    hybrid_processor = DocumentAwareInternVL3HybridProcessor(\n        field_list=UNIVERSAL_FIELDS,\n        model_path=CONFIG['MODEL_PATH'],\n        debug=CONFIG['VERBOSE'],\n        pre_loaded_model=model,\n        pre_loaded_tokenizer=tokenizer,\n        prompt_config=PROMPT_CONFIG,\n        max_tiles=CONFIG['MAX_TILES'],\n        field_definitions=DOCUMENT_FIELD_DEFINITIONS,\n    )\n    \n    rprint(\"[bold green]‚úÖ InternVL3.5-8B ready for document-aware processing[/bold green]\")\n    rprint(f\"[cyan]üî≤ Using {CONFIG['MAX_TILES']} tiles for dense OCR[/cyan]\")\n    if torch_compile_active:\n        rprint(\"[cyan]üî• torch.compile active: first image will be slower (JIT warmup)[/cyan]\")\n    \nexcept Exception as e:\n    rprint(f\"[red]‚ùå Error loading model: {e}[/red]\")\n    rprint(\"[yellow]üí° Ensure transformers>=4.52.1 is installed[/yellow]\")\n    rprint(\"[yellow]üí° Ensure flash-attn is installed: pip install flash-attn --no-build-isolation[/yellow]\")\n    rprint(\"[yellow]üí° Check model path is correct[/yellow]\")\n    raise"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Image Discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#Cell 7\n# Discover and filter images - consistent with ivl3_cli.py run_pipeline()\n\n# Convert DATA_DIR to Path and handle absolute/relative paths\ndata_dir = Path(CONFIG['DATA_DIR'])\nif not data_dir.is_absolute():\n    data_dir = Path.cwd() / data_dir\n\n# Discover images using pipeline_config.discover_images (same as CLI)\nall_images = list(discover_images(data_dir, CONFIG['DOCUMENT_TYPES']))\n\n# Conditionally load ground truth only when not in inference-only mode\nground_truth = {}\nif not CONFIG['INFERENCE_ONLY'] and CONFIG['GROUND_TRUTH']:\n    ground_truth_path = Path(CONFIG['GROUND_TRUTH'])\n    if not ground_truth_path.is_absolute():\n        ground_truth_path = Path.cwd() / ground_truth_path\n    \n    ground_truth = load_ground_truth(str(ground_truth_path), verbose=CONFIG['VERBOSE'])\n    rprint(f\"[green]‚úÖ Ground truth loaded for {len(ground_truth)} images[/green]\")\nelse:\n    rprint(\"[cyan]üìã Running in inference-only mode (no ground truth required)[/cyan]\")\n\nif CONFIG['MAX_IMAGES']:\n    all_images = all_images[:CONFIG['MAX_IMAGES']]\n\nrprint(f\"[bold green]Ready to process {len(all_images)} images[/bold green]\")\nrprint(f\"[cyan]Data directory: {data_dir}[/cyan]\")\nif not CONFIG['INFERENCE_ONLY'] and CONFIG['GROUND_TRUTH']:\n    rprint(f\"[cyan]Ground truth: {ground_truth_path}[/cyan]\")\nrprint(f\"[cyan]Mode: {'Inference-only' if CONFIG['INFERENCE_ONLY'] else 'Evaluation mode'}[/cyan]\")\nfor i, img in enumerate(all_images[:5], 1):\n    print(f\"  {i}. {Path(img).name}\")\nif len(all_images) > 5:\n    print(f\"  ... and {len(all_images) - 5} more\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Batch Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": "#Cell 8\n# ============================================================================\n# V2: BATCH PROCESSING WITH SOPHISTICATED BANK STATEMENT EXTRACTION\n# ============================================================================\n# Document routing is handled internally by BatchDocumentProcessor:\n# - Bank statements ‚Üí BankStatementAdapter (multi-turn extraction)\n# - Invoice/Receipt ‚Üí Standard document-aware extraction\n# ============================================================================\n\n# Create bank adapter when V2 sophisticated extraction is enabled\n# (consistent with ivl3_cli.py run_batch_processing())\nbank_adapter = None\nif CONFIG.get('USE_SOPHISTICATED_BANK_EXTRACTION', False):\n    rprint(\"[bold cyan]üè¶ V2: Setting up sophisticated bank statement extraction...[/bold cyan]\")\n\n    # Detect model dtype from the loaded model\n    try:\n        model_dtype = next(model.parameters()).dtype\n    except (StopIteration, AttributeError):\n        model_dtype = TORCH_DTYPE\n\n    bank_adapter = BankStatementAdapter(\n        model=hybrid_processor,\n        verbose=CONFIG['VERBOSE'],\n        use_balance_correction=CONFIG.get('ENABLE_BALANCE_CORRECTION', False),\n        model_dtype=model_dtype,\n    )\n\n    rprint(\"[green]‚úÖ V2: Sophisticated bank statement extraction enabled[/green]\")\n    rprint(f\"[cyan]   Balance correction: {'Enabled' if CONFIG.get('ENABLE_BALANCE_CORRECTION', False) else 'Disabled'}[/cyan]\")\n    rprint(f\"[cyan]   Model dtype: {model_dtype}[/cyan]\")\nelse:\n    rprint(\"[dim]‚è≠Ô∏è  V2: Sophisticated bank extraction disabled - using original single-turn[/dim]\")\n\n# Initialize batch processor (consistent with ivl3_cli.py run_batch_processing())\nbatch_processor = BatchDocumentProcessor(\n    model=hybrid_processor,\n    prompt_config=PROMPT_CONFIG,\n    ground_truth_csv=CONFIG['GROUND_TRUTH'],\n    console=console,\n    enable_math_enhancement=CONFIG['ENABLE_MATH_ENHANCEMENT'],\n    bank_adapter=bank_adapter,\n    field_definitions=DOCUMENT_FIELD_DEFINITIONS,\n    batch_size=CONFIG['BATCH_SIZE'],\n)\n\n# Process batch using proven evaluation infrastructure\nbatch_results, processing_times, document_types_found = batch_processor.process_batch(\n    all_images, verbose=CONFIG['VERBOSE']\n)\n\n# Brief summary\nrprint(f\"[bold green]‚úÖ Processed {len(batch_results)} images[/bold green]\")\nrprint(f\"[cyan]Average time: {np.mean(processing_times):.2f}s[/cyan]\")\n\n# Batch stats (consistent with ivl3_cli.py print_summary())\nbatch_stats = batch_processor.batch_stats\nif batch_stats:\n    configured = batch_stats.get(\"configured_batch_size\", 1)\n    avg_extract = batch_stats.get(\"avg_extraction_batch\", 1.0)\n    rprint(f\"[cyan]Batch size (configured): {configured}[/cyan]\")\n    rprint(f\"[cyan]Avg batch size (actual): {avg_extract:.1f}[/cyan]\")\n\nif CONFIG['INFERENCE_ONLY']:\n    rprint(\"[cyan]üìã Inference-only mode: No accuracy evaluation performed[/cyan]\")\nelse:\n    avg_accuracy = np.mean([r.get('evaluation', {}).get('overall_accuracy', 0) * 100 for r in batch_results if 'evaluation' in r])\n    rprint(f\"[cyan]Average accuracy: {avg_accuracy:.1f}%[/cyan]\")\n\n    # V2: Show bank statement specific results if sophisticated extraction was used\n    if CONFIG.get('USE_SOPHISTICATED_BANK_EXTRACTION', False):\n        bank_results = [r for r in batch_results if r.get('document_type', '').upper() == 'BANK_STATEMENT']\n        if bank_results:\n            bank_accuracy = np.mean([r.get('evaluation', {}).get('overall_accuracy', 0) * 100 for r in bank_results if 'evaluation' in r])\n            rprint(f\"[cyan]üè¶ Bank statement accuracy (V2): {bank_accuracy:.1f}%[/cyan]\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Generate Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 9\n",
    "# Create model-specific CSV file to match Llama structure\n",
    "# Use UNIVERSAL_FIELDS (already filtered to exclude validation-only fields)\n",
    "# CRITICAL: TRANSACTION_AMOUNTS_RECEIVED and ACCOUNT_BALANCE are excluded\n",
    "FIELD_COLUMNS = UNIVERSAL_FIELDS\n",
    "\n",
    "# Create comprehensive results data matching Llama structure\n",
    "internvl3_5_csv_data = []\n",
    "\n",
    "for i, result in enumerate(batch_results):\n",
    "    # Basic metadata\n",
    "    image_name = Path(result['image_path']).name\n",
    "    doc_type = result.get('document_type', '').lower()\n",
    "    processing_time = processing_times[i] if i < len(processing_times) else 0\n",
    "    \n",
    "    # Extract fields from result\n",
    "    extraction_result = result.get('extraction_result', {})\n",
    "    extracted_fields = extraction_result.get('extracted_data', {})\n",
    "    accuracy_data = result.get('evaluation', {})\n",
    "    \n",
    "    # Count fields (using filtered field list)\n",
    "    total_fields = len(FIELD_COLUMNS)\n",
    "    found_fields = sum(1 for field in FIELD_COLUMNS if extracted_fields.get(field, 'NOT_FOUND') != 'NOT_FOUND')\n",
    "    field_coverage = (found_fields / total_fields * 100) if total_fields > 0 else 0\n",
    "    \n",
    "    # Handle both inference-only and evaluation modes\n",
    "    if CONFIG['INFERENCE_ONLY'] or accuracy_data.get('inference_only', False):\n",
    "        # Inference-only mode\n",
    "        overall_accuracy = None\n",
    "        fields_extracted = found_fields\n",
    "        fields_matched = 0  # No matching in inference mode\n",
    "        eval_total_fields = total_fields\n",
    "    else:\n",
    "        # Evaluation mode\n",
    "        overall_accuracy = accuracy_data.get('overall_accuracy', 0) * 100 if accuracy_data else 0\n",
    "        fields_extracted = accuracy_data.get('fields_extracted', 0) if accuracy_data else 0\n",
    "        fields_matched = accuracy_data.get('fields_matched', 0) if accuracy_data else 0\n",
    "        eval_total_fields = accuracy_data.get('total_fields', total_fields) if accuracy_data else total_fields\n",
    "    \n",
    "    # Create prompt identifier for InternVL3.5-8B\n",
    "    prompt_used = f\"internvl3_5_8b_{doc_type}\" if doc_type else \"internvl3_5_8b_unknown\"\n",
    "    \n",
    "    # Create row data\n",
    "    row_data = {\n",
    "        'image_file': image_name,\n",
    "        'image_name': image_name,\n",
    "        'document_type': doc_type,\n",
    "        'processing_time': processing_time,\n",
    "        'field_count': eval_total_fields,\n",
    "        'found_fields': fields_extracted,\n",
    "        'field_coverage': field_coverage,\n",
    "        'prompt_used': prompt_used,\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'overall_accuracy': overall_accuracy,\n",
    "        'fields_extracted': fields_extracted,\n",
    "        'fields_matched': fields_matched,\n",
    "        'total_fields': eval_total_fields,\n",
    "        'inference_only': CONFIG['INFERENCE_ONLY'],\n",
    "        'model_version': 'InternVL3.5-8B',\n",
    "        'gpu_type': 'H200',\n",
    "        'precision': 'bfloat16'\n",
    "    }\n",
    "    \n",
    "    # Add all field values (only for fields in filtered list)\n",
    "    for field in FIELD_COLUMNS:\n",
    "        row_data[field] = extracted_fields.get(field, 'NOT_FOUND')\n",
    "    \n",
    "    internvl3_5_csv_data.append(row_data)\n",
    "\n",
    "# Create DataFrame and save\n",
    "internvl3_5_df = pd.DataFrame(internvl3_5_csv_data)\n",
    "internvl3_5_csv_path = OUTPUT_DIRS['csv'] / f\"internvl3_5_8b_batch_results_{BATCH_TIMESTAMP}.csv\"\n",
    "internvl3_5_df.to_csv(internvl3_5_csv_path, index=False)\n",
    "\n",
    "rprint(\"[bold green]‚úÖ InternVL3.5-8B model-specific CSV exported:[/bold green]\")\n",
    "rprint(f\"[cyan]üìÑ File: {internvl3_5_csv_path}[/cyan]\")\n",
    "rprint(f\"[cyan]üìä Structure: {len(internvl3_5_df)} rows √ó {len(internvl3_5_df.columns)} columns[/cyan]\")\n",
    "rprint(f\"[cyan]üìã Fields: {len(FIELD_COLUMNS)} (validation-only fields excluded)[/cyan]\")\n",
    "rprint(\"[cyan]üñ•Ô∏è  Hardware: H200 + bfloat16 (optimal configuration)[/cyan]\")\n",
    "rprint(\"[cyan]üîó Compatible with model_comparison.ipynb pattern: *internvl3_5_8b*batch*results*.csv[/cyan]\")\n",
    "\n",
    "# Display sample of the exported data (conditional based on mode)\n",
    "if CONFIG['INFERENCE_ONLY']:\n",
    "    rprint(\"\\n[bold blue]üìã Sample exported data (inference-only mode):[/bold blue]\")\n",
    "    sample_cols = ['image_file', 'document_type', 'processing_time', 'found_fields', 'field_coverage', 'model_version', 'gpu_type', 'precision']\n",
    "    if len(internvl3_5_df) > 0:\n",
    "        display(internvl3_5_df[sample_cols].head(10))\n",
    "    else:\n",
    "        rprint(\"[yellow]‚ö†Ô∏è No data to display[/yellow]\")\n",
    "else:\n",
    "    rprint(\"\\n[bold blue]üìã Sample exported data (first 3 rows, key columns):[/bold blue]\")\n",
    "    sample_cols = ['image_file', 'document_type', 'overall_accuracy', 'processing_time', 'found_fields', 'model_version', 'gpu_type']\n",
    "    if len(internvl3_5_df) > 0:\n",
    "        display(internvl3_5_df[sample_cols].head(3))\n",
    "    else:\n",
    "        rprint(\"[yellow]‚ö†Ô∏è No data to display[/yellow]\")\n",
    "\n",
    "    # Verification: Show accuracy values to confirm they're correct (evaluation mode only)\n",
    "    rprint(\"\\n[bold blue]üîç Accuracy verification:[/bold blue]\")\n",
    "    for i, result in enumerate(batch_results[:10]):  # Show first 10\n",
    "        evaluation = result.get('evaluation', {})\n",
    "        original_accuracy = evaluation.get('overall_accuracy', 0)\n",
    "        percentage_accuracy = original_accuracy * 100\n",
    "        rprint(f\"  {result['image_name']}: {original_accuracy:.4f} ‚Üí {percentage_accuracy:.2f}%\")\n",
    "\n",
    "# Create analytics using proven infrastructure (same pattern as llama_batch.ipynb)\n",
    "analytics = BatchAnalytics(batch_results, processing_times)\n",
    "\n",
    "# Generate and save DataFrames using established patterns\n",
    "saved_files, df_results, df_summary, df_doctype_stats, df_field_stats = analytics.save_all_dataframes(\n",
    "    OUTPUT_DIRS['csv'], BATCH_TIMESTAMP, verbose=CONFIG['VERBOSE']\n",
    ")\n",
    "\n",
    "# Display key results based on mode\n",
    "rprint(\"\\n[bold blue]üìä InternVL3.5-8B Results Summary (H200 + bfloat16)[/bold blue]\")\n",
    "if CONFIG['INFERENCE_ONLY']:\n",
    "    rprint(\"[cyan]üìã Running in inference-only mode - no accuracy metrics available[/cyan]\")\n",
    "    # Show extraction statistics instead\n",
    "    rprint(f\"[cyan]‚úÖ Total images processed: {len(batch_results)}[/cyan]\")\n",
    "    rprint(f\"[cyan]‚úÖ Average fields found: {internvl3_5_df['found_fields'].mean():.1f}[/cyan]\")\n",
    "    rprint(f\"[cyan]‚úÖ Average field coverage: {internvl3_5_df['field_coverage'].mean():.1f}%[/cyan]\")\n",
    "else:\n",
    "    display(df_summary)\n",
    "    rprint(\"[blue]üìä This establishes the H200 bfloat16 baseline for comparison with V100 float32[/blue]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "internvl3_5_df[['image_file','overall_accuracy']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Export Model-Specific CSV for Comparison\n",
    "\n",
    "Create InternVL3 NON-QUANTIZED specific CSV file that matches Llama structure for model comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 10\n",
    "# Create visualizations using proven infrastructure (same pattern as llama_batch.ipynb)\n",
    "visualizer = BatchVisualizer()\n",
    "\n",
    "viz_files = visualizer.create_all_visualizations(\n",
    "    df_results, \n",
    "    df_doctype_stats,\n",
    "    OUTPUT_DIRS['visualizations'],\n",
    "    BATCH_TIMESTAMP,\n",
    "    show=False  # Disable display to reduce output\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Generate Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 11\n",
    "# Generate reports using proven infrastructure (same pattern as llama_batch.ipynb)\n",
    "reporter = BatchReporter(\n",
    "    batch_results, \n",
    "    processing_times,\n",
    "    document_types_found,\n",
    "    BATCH_TIMESTAMP\n",
    ")\n",
    "\n",
    "# Save all reports using CONFIG verbose setting\n",
    "report_files = reporter.save_all_reports(\n",
    "    OUTPUT_DIRS,\n",
    "    df_results,\n",
    "    df_summary,\n",
    "    df_doctype_stats,\n",
    "    CONFIG['MODEL_PATH'],\n",
    "    {\n",
    "        'data_dir': CONFIG['DATA_DIR'],\n",
    "        'ground_truth': CONFIG['GROUND_TRUTH'],\n",
    "        'max_images': CONFIG['MAX_IMAGES'],\n",
    "        'document_types': CONFIG['DOCUMENT_TYPES']\n",
    "    },\n",
    "    {\n",
    "        'use_quantization': CONFIG['USE_QUANTIZATION'],\n",
    "        'device_map': CONFIG['DEVICE_MAP'],\n",
    "        'max_new_tokens': CONFIG['MAX_NEW_TOKENS'],\n",
    "        'torch_dtype': CONFIG['TORCH_DTYPE'],\n",
    "        'low_cpu_mem_usage': CONFIG['LOW_CPU_MEM_USAGE']\n",
    "    },\n",
    "    verbose=CONFIG['VERBOSE']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Display Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 12\n",
    "# Display final summary\n",
    "console.rule(\"[bold green]InternVL3.5-8B Batch Processing Complete[/bold green]\")\n",
    "\n",
    "total_images = len(batch_results)\n",
    "successful = len([r for r in batch_results if 'error' not in r])\n",
    "avg_accuracy = df_results['overall_accuracy'].mean() if len(df_results) > 0 else 0\n",
    "\n",
    "rprint(f\"[bold green]‚úÖ Processed: {total_images} images[/bold green]\")\n",
    "rprint(f\"[cyan]Success Rate: {(successful/total_images*100):.1f}%[/cyan]\")\n",
    "rprint(f\"[cyan]Overall Average Accuracy (across all images): {avg_accuracy:.2f}%[/cyan]\")\n",
    "\n",
    "# Show per-document-type breakdown\n",
    "if len(df_results) > 0 and 'document_type' in df_results.columns:\n",
    "    rprint(\"\\n[bold blue]üìä Breakdown by Document Type:[/bold blue]\")\n",
    "    doc_type_stats = df_results.groupby('document_type').agg({\n",
    "        'overall_accuracy': 'mean',\n",
    "        'image_name': 'count'\n",
    "    }).sort_values('overall_accuracy', ascending=False)\n",
    "    \n",
    "    for doc_type, row in doc_type_stats.iterrows():\n",
    "        count = int(row['image_name'])\n",
    "        acc = row['overall_accuracy']\n",
    "        rprint(f\"[cyan]  {doc_type.upper()}: {acc:.1f}% (n={count} images)[/cyan]\")\n",
    "\n",
    "rprint(f\"\\n[cyan]Output: {OUTPUT_BASE}[/cyan]\")\n",
    "rprint(\"[blue]üöÄ Model: InternVL3.5-8B with Cascade RL and ViR[/blue]\")\n",
    "\n",
    "# Performance assessment\n",
    "if successful == total_images and avg_accuracy > 50:\n",
    "    rprint(\"\\n[bold green]üéâ SUCCESS: InternVL3.5-8B processing completed successfully![/bold green]\")\n",
    "    rprint(\"[green]‚úÖ Enhanced reasoning with Cascade RL is working[/green]\")\n",
    "    rprint(\"[green]‚úÖ Dynamic resolution adjustment with ViR is active[/green]\")\n",
    "elif successful < total_images:\n",
    "    rprint(\"\\n[bold red]‚ùå FAILURE: Processing errors occurred[/bold red]\")\n",
    "    rprint(\"[red]üîç Review error logs for diagnostic information[/red]\")\n",
    "elif avg_accuracy < 30:\n",
    "    rprint(\"\\n[bold yellow]‚ö†Ô∏è POOR PERFORMANCE: Low accuracy detected[/bold yellow]\")\n",
    "    rprint(\"[yellow]üîç Review extraction results for quality issues[/yellow]\")\n",
    "else:\n",
    "    rprint(\"\\n[bold blue]üìä MIXED RESULTS: Partially working[/bold blue]\")\n",
    "    rprint(\"[blue]üîç Review individual results to assess performance[/blue]\")\n",
    "\n",
    "# Document type distribution\n",
    "if document_types_found:\n",
    "    rprint(\"\\n[bold blue]üìã Document Type Distribution:[/bold blue]\")\n",
    "    for doc_type, count in document_types_found.items():\n",
    "        percentage = (count / total_images * 100) if total_images > 0 else 0\n",
    "        rprint(f\"[cyan]  {doc_type}: {count} documents ({percentage:.1f}%)[/cyan]\")\n",
    "\n",
    "# Display dashboard if available\n",
    "dashboard_files = list(OUTPUT_DIRS['visualizations'].glob(f\"dashboard_{BATCH_TIMESTAMP}.png\"))\n",
    "if dashboard_files:\n",
    "    from IPython.display import Image, display\n",
    "    dashboard_path = dashboard_files[0]\n",
    "    rprint(\"\\n[bold blue]üìä Visual Dashboard:[/bold blue]\")\n",
    "    display(Image(str(dashboard_path)))\n",
    "else:\n",
    "    rprint(f\"\\n[yellow]‚ö†Ô∏è Dashboard not found in {OUTPUT_DIRS['visualizations']}[/yellow]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Failed Extractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 13\n",
    "# Calculate zero accuracy extractions\n",
    "zero_accuracy_count = 0\n",
    "zero_accuracy_images = []\n",
    "total_evaluated = 0\n",
    "\n",
    "for result in batch_results:\n",
    "    # Check if evaluation data exists (not inference-only mode)\n",
    "    evaluation = result.get(\"evaluation\", {})\n",
    "\n",
    "    if evaluation and not evaluation.get(\"inference_only\", False):\n",
    "        total_evaluated += 1\n",
    "        accuracy = evaluation.get(\"overall_accuracy\", 0)\n",
    "\n",
    "        if accuracy == 0.0:\n",
    "            zero_accuracy_count += 1\n",
    "            zero_accuracy_images.append(\n",
    "                {\n",
    "                    \"image_name\": result.get(\"image_name\", \"unknown\"),\n",
    "                    \"document_type\": result.get(\"document_type\", \"unknown\"),\n",
    "                    \"fields_extracted\": evaluation.get(\"fields_extracted\", 0),\n",
    "                    \"total_fields\": evaluation.get(\"total_fields\", 0),\n",
    "                }\n",
    "            )\n",
    "\n",
    "# Display results\n",
    "if total_evaluated > 0:\n",
    "    console.rule(\"[bold red]Zero Accuracy Analysis[/bold red]\")\n",
    "\n",
    "    rprint(f\"[cyan]Total documents evaluated: {total_evaluated}[/cyan]\")\n",
    "    rprint(f\"[red]Documents with 0% accuracy: {zero_accuracy_count}[/red]\")\n",
    "\n",
    "    if zero_accuracy_count > 0:\n",
    "        percentage = (zero_accuracy_count / total_evaluated) * 100\n",
    "        rprint(f\"[red]Zero accuracy rate: {percentage:.1f}%[/red]\")\n",
    "\n",
    "        rprint(\"\\n[bold red]Documents with 0% Accuracy:[/bold red]\")\n",
    "        for i, img_info in enumerate(zero_accuracy_images, 1):\n",
    "            rprint(f\"  {i}. {img_info['image_name']} ({img_info['document_type']})\")\n",
    "            rprint(\n",
    "                f\"     Fields extracted: {img_info['fields_extracted']}/{img_info['total_fields']}\"\n",
    "            )\n",
    "    else:\n",
    "        rprint(\n",
    "            \"[green]‚úÖ No documents with 0% accuracy - all extractions had some success![/green]\"\n",
    "        )\n",
    "else:\n",
    "    rprint(\n",
    "        \"[yellow]‚ö†Ô∏è Running in inference-only mode - no accuracy metrics available[/yellow]\"\n",
    "    )\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (LMM_POC_IVL3.5)",
   "language": "python",
   "name": "lmm_poc_ivl3.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}