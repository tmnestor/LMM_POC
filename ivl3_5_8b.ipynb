{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### InternVL3.5-8B Document Extraction\n",
    "\n",
    "**Primary notebook for InternVL3.5-8B vision-language model document extraction.**\n",
    "\n",
    "**Model**: InternVL3.5-8B (8.5B parameters: 0.3B vision + 8.2B language)\n",
    "\n",
    "**Features:**\n",
    "- Cascade Reinforcement Learning (Cascade RL) for enhanced reasoning\n",
    "- Visual Resolution Router (ViR) for dynamic resolution adjustment\n",
    "- Flash Attention 2 support\n",
    "- Sophisticated bank statement extraction with multi-turn processing\n",
    "\n",
    "**Requirements**: \n",
    "- `transformers>=4.52.1` (critical for InternVL3.5 support)\n",
    "- PyTorch with CUDA support\n",
    "- H200/H100/A100 GPU (native bfloat16) or V100 (float32)\n",
    "\n",
    "**Configuration:**\n",
    "- `USE_SOPHISTICATED_BANK_EXTRACTION`: True (default) uses multi-turn extraction\n",
    "- `ENABLE_BALANCE_CORRECTION`: Optional mathematical balance validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ Project root: /home/jovyan/nfs_share/tod_2026/LMM_POC\n",
      "âœ… Added /home/jovyan/nfs_share/tod_2026/LMM_POC to sys.path\n",
      "âœ… Common module found at: /home/jovyan/nfs_share/tod_2026/LMM_POC/common/__init__.py\n",
      "âœ… Path setup complete\n"
     ]
    }
   ],
   "source": [
    "#Cell 1\n",
    "# Project root setup - notebook is at project root\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "os.environ['EVALUATION_METHOD'] = 'order_aware_f1'  # or 'f1', 'kieval', 'order_aware_f1', 'correlation'\n",
    "\n",
    "# Project root is current directory (notebook at root)\n",
    "PROJECT_ROOT = Path().absolute()\n",
    "print(f\"ğŸ“‚ Project root: {PROJECT_ROOT}\")\n",
    "\n",
    "# Ensure the project root is in the Python path\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "    print(f\"âœ… Added {PROJECT_ROOT} to sys.path\")\n",
    "\n",
    "# Verify common module can be found\n",
    "try:\n",
    "    import common\n",
    "    print(f\"âœ… Common module found at: {common.__file__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"âŒ Common module not found: {e}\")\n",
    "    raise\n",
    "\n",
    "print(\"âœ… Path setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: field_types_from_yaml = {'monetary': ['GST_AMOUNT', 'TOTAL_AMOUNT', 'LINE_ITEM_PRICES', 'LINE_ITEM_TOTAL_PRICES', 'TRANSACTION_AMOUNTS_PAID', 'TRANSACTION_AMOUNTS_RECEIVED', 'ACCOUNT_BALANCE'], 'boolean': ['IS_GST_INCLUDED'], 'list': ['LINE_ITEM_DESCRIPTIONS', 'LINE_ITEM_QUANTITIES', 'LINE_ITEM_PRICES', 'LINE_ITEM_TOTAL_PRICES', 'TRANSACTION_DATES', 'TRANSACTION_AMOUNTS_PAID', 'TRANSACTION_AMOUNTS_RECEIVED', 'ACCOUNT_BALANCE'], 'date': ['INVOICE_DATE', 'STATEMENT_DATE_RANGE', 'TRANSACTION_DATES'], 'transaction_list': ['TRANSACTION_DATES', 'TRANSACTION_AMOUNTS_PAID', 'TRANSACTION_AMOUNTS_RECEIVED', 'ACCOUNT_BALANCE', 'TRAVEL_DATES']}\n",
      "DEBUG: boolean fields from YAML = ['IS_GST_INCLUDED']\n",
      "DEBUG: self.boolean_fields = ['IS_GST_INCLUDED']\n",
      "DEBUG _ensure_fields_loaded: BOOLEAN_FIELDS = ['IS_GST_INCLUDED']\n",
      "âœ… All imports loaded successfully\n",
      "âœ… InternVL3 Hybrid Processor imported successfully\n",
      "âœ… Batch processing modules imported successfully\n",
      "âœ… V2: BankStatementAdapter imported for sophisticated bank extraction\n",
      "âœ… load_document_field_definitions imported for YAML config\n",
      "ğŸ“‚ Working directory: /home/jovyan/nfs_share/tod_2026/LMM_POC\n",
      "ğŸš€ InternVL3.5-8B: Cascade RL + Visual Resolution Router\n"
     ]
    }
   ],
   "source": [
    "#Cell 2\n",
    "# Enable autoreload for module changes\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Standard library imports\n",
    "import sys\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Add current directory to path to ensure proper module resolution\n",
    "notebook_dir = Path.cwd()\n",
    "if str(notebook_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(notebook_dir))\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from IPython.display import display\n",
    "from rich import print as rprint\n",
    "from rich.console import Console\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# V2: Sophisticated bank statement processing\n",
    "from common.bank_statement_adapter import BankStatementAdapter\n",
    "\n",
    "# Project-specific imports - using absolute imports to avoid conflicts\n",
    "from common.batch_analytics import BatchAnalytics\n",
    "from common.batch_processor import BatchDocumentProcessor, load_document_field_definitions\n",
    "from common.batch_reporting import BatchReporter\n",
    "from common.batch_visualizations import BatchVisualizer\n",
    "from common.evaluation_metrics import load_ground_truth\n",
    "from common.extraction_parser import discover_images\n",
    "from common.gpu_optimization import emergency_cleanup\n",
    "from models.document_aware_internvl3_processor import (\n",
    "    DocumentAwareInternVL3HybridProcessor,\n",
    ")\n",
    "\n",
    "print(\"âœ… All imports loaded successfully\")\n",
    "print(\"âœ… InternVL3 Hybrid Processor imported successfully\") \n",
    "print(\"âœ… Batch processing modules imported successfully\")\n",
    "print(\"âœ… V2: BankStatementAdapter imported for sophisticated bank extraction\")\n",
    "print(\"âœ… load_document_field_definitions imported for YAML config\")\n",
    "print(f\"ğŸ“‚ Working directory: {notebook_dir}\")\n",
    "print(\"ğŸš€ InternVL3.5-8B: Cascade RL + Visual Resolution Router\")\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pre-emptive Memory Cleanup\n",
    "\n",
    "**CRITICAL for V100**: Run this cell first to prevent OOM errors when switching between models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">ğŸ§¹ PRE-EMPTIVE V100 MEMORY CLEANUP</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mğŸ§¹ PRE-EMPTIVE V100 MEMORY CLEANUP\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">Clearing any existing model caches before loading...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[33mClearing any existing model caches before loading\u001b[0m\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">ğŸ’¡ This prevents OOM errors when switching between models on V100</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mğŸ’¡ This prevents OOM errors when switching between models on V100\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš¨ Running emergency GPU cleanup...\n",
      "ğŸ§¹ Starting GPU memory cleanup...\n",
      "   ğŸ“Š Initial GPU memory: 0.00GB allocated, 0.00GB reserved\n",
      "   âœ… Final GPU memory: 0.00GB allocated, 0.00GB reserved\n",
      "   ğŸ’¾ Memory freed: 0.00GB\n",
      "âœ… GPU memory cleanup complete\n",
      "âœ… Emergency cleanup complete\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">âœ… Memory cleanup complete - ready for model loading</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mâœ… Memory cleanup complete - ready for model loading\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">ğŸ“‹ Next: Import modules and configure settings</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2mğŸ“‹ Next: Import modules and configure settings\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Cell 3\n",
    "# Pre-emptive V100 Memory Cleanup - Run FIRST to prevent OOM errors\n",
    "rprint(\"[bold red]ğŸ§¹ PRE-EMPTIVE V100 MEMORY CLEANUP[/bold red]\")\n",
    "rprint(\"[yellow]Clearing any existing model caches before loading...[/yellow]\")\n",
    "rprint(\"[cyan]ğŸ’¡ This prevents OOM errors when switching between models on V100[/cyan]\")\n",
    "\n",
    "# Emergency cleanup to ensure clean slate\n",
    "emergency_cleanup(verbose=True)\n",
    "\n",
    "rprint(\"[green]âœ… Memory cleanup complete - ready for model loading[/green]\")\n",
    "rprint(\"[dim]ğŸ“‹ Next: Import modules and configure settings[/dim]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Configuration set up successfully\n",
      "ğŸ“‚ Evaluation data: /home/jovyan/nfs_share/tod_2026/LMM_POC/evaluation_data/travel\n",
      "ğŸ“Š Ground truth: /home/jovyan/nfs_share/tod_2026/LMM_POC/evaluation_data/travel/ground_truth_travel.csv\n",
      "ğŸ¤– Model: InternVL3.5-8B\n",
      "ğŸ“ Model path: /home/jovyan/nfs_share/models/InternVL3_5-8B\n",
      "ğŸ“ Output base: /home/jovyan/nfs_share/tod_2026/LMM_POC/output\n",
      "ğŸ  Project root: /home/jovyan/nfs_share/tod_2026/LMM_POC\n",
      "ğŸ“‹ Field definitions loaded from YAML:\n",
      "   - invoice: 14 fields\n",
      "   - receipt: 14 fields\n",
      "   - bank_statement: 5 fields\n",
      "   - travel_expense: 9 fields\n",
      "ğŸ“‹ Universal fields (union): 21\n",
      "ğŸ¯ Mode: Evaluation mode\n",
      "âš™ï¸  Precision: BFLOAT16 (H200 native support)\n",
      "âš¡ Flash Attention: ENABLED\n",
      "ğŸ”² Max Tiles: 11\n",
      "ğŸ¦ V2 Bank Extraction: Enabled\n",
      "ğŸ”¥ torch.compile: DISABLED\n"
     ]
    }
   ],
   "source": [
    "#Cell 4\n",
    "# Initialize console and environment configuration\n",
    "console = Console()\n",
    "\n",
    "# Environment-specific base paths\n",
    "ENVIRONMENT_BASES = {\n",
    "    'sandbox': '/home/jovyan/nfs_share/tod_2026',\n",
    "    'efs': '/efs/shared/PoC_data'\n",
    "}\n",
    "base_data_path = ENVIRONMENT_BASES['sandbox']\n",
    "\n",
    "CONFIG = {\n",
    "    # Model settings - InternVL3.5-8B\n",
    "    'MODEL_PATH': '/home/jovyan/nfs_share/models/InternVL3_5-8B',\n",
    "    # Alternative paths:\n",
    "    # 'MODEL_PATH': '/efs/shared/PTM/InternVL3_5-8B',\n",
    "    # 'MODEL_PATH': 'OpenGVLab/InternVL3_5-8B',  # Auto-download from HuggingFace\n",
    "\n",
    "    # Batch settings - Using base path for consistency\n",
    "    # 'DATA_DIR': f'{base_data_path}/LMM_POC/evaluation_data/synthetic',\n",
    "    # 'GROUND_TRUTH': f'{base_data_path}/LMM_POC/evaluation_data/synthetic/ground_truth_synthetic.csv',\n",
    "    'DATA_DIR': f'{base_data_path}/LMM_POC/evaluation_data/travel',\n",
    "    'GROUND_TRUTH': f'{base_data_path}/LMM_POC/evaluation_data/travel/ground_truth_travel.csv',\n",
    "    'OUTPUT_BASE': f'{base_data_path}/LMM_POC/output',\n",
    "    'MAX_IMAGES': None,  # None for all, or set limit\n",
    "    'DOCUMENT_TYPES': None,  # None for all, or ['invoice', 'receipt']\n",
    "    'ENABLE_MATH_ENHANCEMENT': False,  # Disable mathematical correction for bank statements\n",
    "    \n",
    "    # Inference and evaluation mode\n",
    "    'INFERENCE_ONLY': False,  # Default: False (evaluation mode)\n",
    "    \n",
    "    # Verbosity control\n",
    "    'VERBOSE': True,\n",
    "    'SHOW_PROMPTS': True,\n",
    "    \n",
    "    # ============================================================================\n",
    "    # H200 BFLOAT16 CONFIGURATION - OPTIMAL PERFORMANCE BASELINE\n",
    "    # ============================================================================\n",
    "    'USE_QUANTIZATION': False,  # Full precision for H200\n",
    "    'DEVICE_MAP': 'auto',\n",
    "    'MAX_NEW_TOKENS': 2000,\n",
    "    'TORCH_DTYPE': 'bfloat16',  # Native H200 support\n",
    "    'LOW_CPU_MEM_USAGE': False,  # Must be False - InternVL3.5 vision encoder incompatible with meta device init\n",
    "    'USE_FLASH_ATTN': True,  # H200 optimized - faster inference\n",
    "    'USE_TORCH_COMPILE': False,\n",
    "    'MAX_TILES': 11,  # H200 optimized - InternVL3.5 training max for dense OCR\n",
    "    \n",
    "    # ============================================================================\n",
    "    # V2: SOPHISTICATED BANK STATEMENT EXTRACTION\n",
    "    # ============================================================================\n",
    "    'USE_SOPHISTICATED_BANK_EXTRACTION': True,\n",
    "    'ENABLE_BALANCE_CORRECTION': True,\n",
    "}\n",
    "\n",
    "# Make GROUND_TRUTH conditional based on INFERENCE_ONLY mode\n",
    "if CONFIG['INFERENCE_ONLY']:\n",
    "    CONFIG['GROUND_TRUTH'] = None\n",
    "\n",
    "# ============================================================================\n",
    "# PROMPT CONFIGURATION - Uses PROJECT_ROOT for subdirectory compatibility\n",
    "# ============================================================================\n",
    "PROMPT_CONFIG = {\n",
    "    # Document type detection configuration\n",
    "    'detection_file': str(PROJECT_ROOT / 'prompts/document_type_detection.yaml'),\n",
    "    'detection_key': 'detection',\n",
    "    \n",
    "    # Extraction prompt file mapping (REQUIRED)\n",
    "    'extraction_files': {\n",
    "        'INVOICE': str(PROJECT_ROOT / 'prompts/internvl3_prompts.yaml'),\n",
    "        'RECEIPT': str(PROJECT_ROOT / 'prompts/internvl3_prompts.yaml'),\n",
    "        'BANK_STATEMENT': str(PROJECT_ROOT / 'prompts/internvl3_prompts.yaml'),\n",
    "        'TRAVEL_EXPENSE': str(PROJECT_ROOT / 'prompts/internvl3_prompts.yaml')\n",
    "    },\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# FIELD DEFINITIONS - LOADED FROM YAML (SINGLE SOURCE OF TRUTH)\n",
    "# ============================================================================\n",
    "# Load document-type-specific field lists from config/field_definitions.yaml\n",
    "# This replaces the hardcoded UNIVERSAL_FIELDS list to ensure consistency\n",
    "# across all components (processor, evaluation, CSV output).\n",
    "# ============================================================================\n",
    "DOCUMENT_FIELD_DEFINITIONS = load_document_field_definitions()\n",
    "\n",
    "# Create a union of all fields for multi-document-type batches\n",
    "# This is used for CSV column headers when processing mixed document types\n",
    "ALL_FIELDS = set()\n",
    "for doc_type, fields in DOCUMENT_FIELD_DEFINITIONS.items():\n",
    "    ALL_FIELDS.update(fields)\n",
    "ALL_FIELDS = sorted(list(ALL_FIELDS))\n",
    "\n",
    "# For backwards compatibility, UNIVERSAL_FIELDS is now the union of all fields\n",
    "UNIVERSAL_FIELDS = ALL_FIELDS\n",
    "\n",
    "print(\"âœ… Configuration set up successfully\")\n",
    "print(f\"ğŸ“‚ Evaluation data: {CONFIG['DATA_DIR']}\")\n",
    "print(f\"ğŸ“Š Ground truth: {CONFIG['GROUND_TRUTH']}\")\n",
    "print(\"ğŸ¤– Model: InternVL3.5-8B\")\n",
    "print(f\"ğŸ“ Model path: {CONFIG['MODEL_PATH']}\")\n",
    "print(f\"ğŸ“ Output base: {CONFIG['OUTPUT_BASE']}\")\n",
    "print(f\"ğŸ  Project root: {PROJECT_ROOT}\")\n",
    "print(f\"ğŸ“‹ Field definitions loaded from YAML:\")\n",
    "for doc_type, fields in DOCUMENT_FIELD_DEFINITIONS.items():\n",
    "    print(f\"   - {doc_type}: {len(fields)} fields\")\n",
    "print(f\"ğŸ“‹ Universal fields (union): {len(UNIVERSAL_FIELDS)}\")\n",
    "print(f\"ğŸ¯ Mode: {'Inference-only' if CONFIG['INFERENCE_ONLY'] else 'Evaluation mode'}\")\n",
    "print(\"âš™ï¸  Precision: BFLOAT16 (H200 native support)\")\n",
    "print(f\"âš¡ Flash Attention: {'ENABLED' if CONFIG['USE_FLASH_ATTN'] else 'DISABLED'}\")\n",
    "print(f\"ğŸ”² Max Tiles: {CONFIG['MAX_TILES']}\")\n",
    "print(f\"ğŸ¦ V2 Bank Extraction: {'Enabled' if CONFIG['USE_SOPHISTICATED_BANK_EXTRACTION'] else 'Disabled'}\")\n",
    "print(f\"ğŸ”¥ torch.compile: {'ENABLED' if CONFIG['USE_TORCH_COMPILE'] else 'DISABLED'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Output Directory Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 5\n",
    "# Setup output directories - Handle both absolute and relative paths\n",
    "\n",
    "# Convert OUTPUT_BASE to Path and handle absolute/relative paths\n",
    "OUTPUT_BASE = Path(CONFIG['OUTPUT_BASE'])\n",
    "if not OUTPUT_BASE.is_absolute():\n",
    "    # If relative, make it relative to current working directory\n",
    "    OUTPUT_BASE = Path.cwd() / OUTPUT_BASE\n",
    "\n",
    "BATCH_TIMESTAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "OUTPUT_DIRS = {\n",
    "    'base': OUTPUT_BASE,\n",
    "    'batch': OUTPUT_BASE / 'batch_results',\n",
    "    'csv': OUTPUT_BASE / 'csv',\n",
    "    'visualizations': OUTPUT_BASE / 'visualizations',\n",
    "    'reports': OUTPUT_BASE / 'reports'\n",
    "}\n",
    "\n",
    "for dir_path in OUTPUT_DIRS.values():\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Model Loading\n",
    "\n",
    "**InternVL3.5-8B**: Loading the latest InternVL3.5 model with Cascade RL and Visual Resolution Router.\n",
    "\n",
    "**Requirements**: Ensure `transformers>=4.52.1` is installed for InternVL3.5 compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Loading InternVL3.</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">5</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">-8B model...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mLoading InternVL3.\u001b[0m\u001b[1;32m5\u001b[0m\u001b[1;32m-8B model\u001b[0m\u001b[1;32m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">ğŸš€ Model: InternVL3.</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"color: #008080; text-decoration-color: #008080\">-8B </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8.</span><span style=\"color: #008080; text-decoration-color: #008080\">5B params: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.</span><span style=\"color: #008080; text-decoration-color: #008080\">3B vision + </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8.</span><span style=\"color: #008080; text-decoration-color: #008080\">2B language</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mğŸš€ Model: InternVL3.\u001b[0m\u001b[1;36m5\u001b[0m\u001b[36m-8B \u001b[0m\u001b[1;36m(\u001b[0m\u001b[1;36m8.\u001b[0m\u001b[36m5B params: \u001b[0m\u001b[1;36m0.\u001b[0m\u001b[36m3B vision + \u001b[0m\u001b[1;36m8.\u001b[0m\u001b[36m2B language\u001b[0m\u001b[1;36m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">ğŸ“– Features: Cascade RL, Visual Resolution Router </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080\">ViR</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mğŸ“– Features: Cascade RL, Visual Resolution Router \u001b[0m\u001b[1;36m(\u001b[0m\u001b[36mViR\u001b[0m\u001b[1;36m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">ğŸ§¹ CUDA cache cleared</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34mğŸ§¹ CUDA cache cleared\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">ğŸ–¥ï¸  Detected </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #008080; text-decoration-color: #008080\"> </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">GPU(</span><span style=\"color: #008080; text-decoration-color: #008080\">s</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mğŸ–¥ï¸  Detected \u001b[0m\u001b[1;36m2\u001b[0m\u001b[36m \u001b[0m\u001b[1;36mGPU\u001b[0m\u001b[1;36m(\u001b[0m\u001b[36ms\u001b[0m\u001b[1;36m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">ğŸ“¥ Loading model </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080\">official HuggingFace pattern</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">)</span><span style=\"color: #008080; text-decoration-color: #008080\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mğŸ“¥ Loading model \u001b[0m\u001b[1;36m(\u001b[0m\u001b[36mofficial HuggingFace pattern\u001b[0m\u001b[1;36m)\u001b[0m\u001b[36m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb476448af9746acb103423c0a84d261",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">ğŸ“¥ Loading tokenizer...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mğŸ“¥ Loading tokenizer\u001b[0m\u001b[36m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">âœ… Model and tokenizer loaded successfully!</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mâœ… Model and tokenizer loaded successfully!\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">ğŸ”„ GPU Memory Status </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">(</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">2</span><span style=\"color: #000080; text-decoration-color: #000080\"> GPUs available, model on GPU </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">0</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">)</span><span style=\"color: #000080; text-decoration-color: #000080\">:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34mğŸ”„ GPU Memory Status \u001b[0m\u001b[1;34m(\u001b[0m\u001b[1;34m2\u001b[0m\u001b[34m GPUs available, model on GPU \u001b[0m\u001b[1;34m0\u001b[0m\u001b[1;34m)\u001b[0m\u001b[34m:\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   GPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> <span style=\"font-weight: bold\">(</span>NVIDIA L4<span style=\"font-weight: bold\">)</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17.</span>1GB/24GB <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">72.1</span>%<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   GPU \u001b[1;36m0\u001b[0m \u001b[1m(\u001b[0mNVIDIA L4\u001b[1m)\u001b[0m: \u001b[1;36m17.\u001b[0m1GB/24GB \u001b[1m(\u001b[0m\u001b[1;36m72.1\u001b[0m%\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   GPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> <span style=\"font-weight: bold\">(</span>NVIDIA L4<span style=\"font-weight: bold\">)</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.</span>0GB/24GB <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>%<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "   GPU \u001b[1;36m1\u001b[0m \u001b[1m(\u001b[0mNVIDIA L4\u001b[1m)\u001b[0m: \u001b[1;36m0.\u001b[0m0GB/24GB \u001b[1m(\u001b[0m\u001b[1;36m0.0\u001b[0m%\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">ğŸ“Š Total: </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">17.</span><span style=\"color: #000080; text-decoration-color: #000080\">1GB allocated, 47GB capacity</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34mğŸ“Š Total: \u001b[0m\u001b[1;34m17.\u001b[0m\u001b[34m1GB allocated, 47GB capacity\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                  ğŸ”§ InternVL3.5-8B Model Configuration                  </span>\n",
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Setting             </span>â”ƒ<span style=\"font-weight: bold\"> Value                     </span>â”ƒ<span style=\"font-weight: bold\"> Status              </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\"> Model Path          </span>â”‚<span style=\"color: #808000; text-decoration-color: #808000\"> InternVL3_5-8B            </span>â”‚<span style=\"color: #008000; text-decoration-color: #008000\"> âœ… Valid            </span>â”‚\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\"> Model Type          </span>â”‚<span style=\"color: #808000; text-decoration-color: #808000\"> InternVL3.5-8B            </span>â”‚<span style=\"color: #008000; text-decoration-color: #008000\"> âœ… Cascade RL + ViR </span>â”‚\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\"> Device Placement    </span>â”‚<span style=\"color: #808000; text-decoration-color: #808000\"> cuda:0                    </span>â”‚<span style=\"color: #008000; text-decoration-color: #008000\"> âœ… Loaded           </span>â”‚\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\"> Quantization Method </span>â”‚<span style=\"color: #808000; text-decoration-color: #808000\"> 16-bit                    </span>â”‚<span style=\"color: #008000; text-decoration-color: #008000\"> âœ… Configured       </span>â”‚\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\"> Data Type           </span>â”‚<span style=\"color: #808000; text-decoration-color: #808000\"> bfloat16                  </span>â”‚<span style=\"color: #008000; text-decoration-color: #008000\"> âœ… Recommended      </span>â”‚\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\"> Max New Tokens      </span>â”‚<span style=\"color: #808000; text-decoration-color: #808000\"> 2000                      </span>â”‚<span style=\"color: #008000; text-decoration-color: #008000\"> âœ… Generation Ready </span>â”‚\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\"> Max Tiles           </span>â”‚<span style=\"color: #808000; text-decoration-color: #808000\"> 11                        </span>â”‚<span style=\"color: #008000; text-decoration-color: #008000\"> âœ… H200 Optimized   </span>â”‚\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\"> GPU Configuration   </span>â”‚<span style=\"color: #808000; text-decoration-color: #808000\"> 2x NVIDIA L4 (47GB total) </span>â”‚<span style=\"color: #008000; text-decoration-color: #008000\"> âœ… Model on GPU 0   </span>â”‚\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\"> Model Parameters    </span>â”‚<span style=\"color: #808000; text-decoration-color: #808000\"> 8,528,318,464             </span>â”‚<span style=\"color: #008000; text-decoration-color: #008000\"> âœ… Loaded           </span>â”‚\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\"> Flash Attention     </span>â”‚<span style=\"color: #808000; text-decoration-color: #808000\"> True                      </span>â”‚<span style=\"color: #008000; text-decoration-color: #008000\"> âœ… Enabled          </span>â”‚\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\"> torch.compile       </span>â”‚<span style=\"color: #808000; text-decoration-color: #808000\"> False                     </span>â”‚<span style=\"color: #008000; text-decoration-color: #008000\"> âš ï¸ Disabled         </span>â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                  ğŸ”§ InternVL3.5-8B Model Configuration                  \u001b[0m\n",
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mSetting            \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mValue                    \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mStatus             \u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36mModel Path         \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[33m \u001b[0m\u001b[33mInternVL3_5-8B           \u001b[0m\u001b[33m \u001b[0mâ”‚\u001b[32m \u001b[0m\u001b[32mâœ… Valid           \u001b[0m\u001b[32m \u001b[0mâ”‚\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36mModel Type         \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[33m \u001b[0m\u001b[33mInternVL3.5-8B           \u001b[0m\u001b[33m \u001b[0mâ”‚\u001b[32m \u001b[0m\u001b[32mâœ… Cascade RL + ViR\u001b[0m\u001b[32m \u001b[0mâ”‚\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36mDevice Placement   \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[33m \u001b[0m\u001b[33mcuda:0                   \u001b[0m\u001b[33m \u001b[0mâ”‚\u001b[32m \u001b[0m\u001b[32mâœ… Loaded          \u001b[0m\u001b[32m \u001b[0mâ”‚\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36mQuantization Method\u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[33m \u001b[0m\u001b[33m16-bit                   \u001b[0m\u001b[33m \u001b[0mâ”‚\u001b[32m \u001b[0m\u001b[32mâœ… Configured      \u001b[0m\u001b[32m \u001b[0mâ”‚\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36mData Type          \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[33m \u001b[0m\u001b[33mbfloat16                 \u001b[0m\u001b[33m \u001b[0mâ”‚\u001b[32m \u001b[0m\u001b[32mâœ… Recommended     \u001b[0m\u001b[32m \u001b[0mâ”‚\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36mMax New Tokens     \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[33m \u001b[0m\u001b[33m2000                     \u001b[0m\u001b[33m \u001b[0mâ”‚\u001b[32m \u001b[0m\u001b[32mâœ… Generation Ready\u001b[0m\u001b[32m \u001b[0mâ”‚\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36mMax Tiles          \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[33m \u001b[0m\u001b[33m11                       \u001b[0m\u001b[33m \u001b[0mâ”‚\u001b[32m \u001b[0m\u001b[32mâœ… H200 Optimized  \u001b[0m\u001b[32m \u001b[0mâ”‚\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36mGPU Configuration  \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[33m \u001b[0m\u001b[33m2x NVIDIA L4 (47GB total)\u001b[0m\u001b[33m \u001b[0mâ”‚\u001b[32m \u001b[0m\u001b[32mâœ… Model on GPU 0  \u001b[0m\u001b[32m \u001b[0mâ”‚\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36mModel Parameters   \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[33m \u001b[0m\u001b[33m8,528,318,464            \u001b[0m\u001b[33m \u001b[0mâ”‚\u001b[32m \u001b[0m\u001b[32mâœ… Loaded          \u001b[0m\u001b[32m \u001b[0mâ”‚\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36mFlash Attention    \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[33m \u001b[0m\u001b[33mTrue                     \u001b[0m\u001b[33m \u001b[0mâ”‚\u001b[32m \u001b[0m\u001b[32mâœ… Enabled         \u001b[0m\u001b[32m \u001b[0mâ”‚\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36mtorch.compile      \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[33m \u001b[0m\u001b[33mFalse                    \u001b[0m\u001b[33m \u001b[0mâ”‚\u001b[32m \u001b[0m\u001b[32mâš ï¸ Disabled        \u001b[0m\u001b[32m \u001b[0mâ”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">ğŸ”§ Initializing document-aware processor...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mğŸ”§ Initializing document-aware processor\u001b[0m\u001b[36m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ InternVL3 Hybrid processor initialized for 21 fields: BUSINESS_ABN â†’ TRAVEL_ROUTE\n",
      "ğŸ”§ GPU detected: NVIDIA L4\n",
      "ğŸ”§ CUDA memory allocation configured: max_split_size_mb:128\n",
      "ğŸ“Š Initial CUDA state (Multi-GPU Total): Allocated=15.89GB, Reserved=16.07GB\n",
      "âš ï¸ Could not detect GPU memory: No module named 'common.robust_gpu_memory'\n",
      "ğŸ¤– Auto-detected batch size: 8 (GPU Memory: 24.0GB)\n",
      "ğŸ¯ DOCUMENT AWARE REDUCTION: 21 fields (~28% fewer than original 29)\n",
      "ğŸ¯ Generation config: max_new_tokens=2000, do_sample=False (greedy decoding)\n",
      "âœ… Using pre-loaded InternVL3 model and tokenizer\n",
      "ğŸ”§ Device: cuda:0\n",
      "ğŸ’¾ Model parameters: 8,528,318,464\n",
      "ğŸš€ GPU optimizations applied (TF32 enabled)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">âœ… InternVL3.</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">5</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">-8B ready for document-aware processing</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mâœ… InternVL3.\u001b[0m\u001b[1;32m5\u001b[0m\u001b[1;32m-8B ready for document-aware processing\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">ğŸ”² Using </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span><span style=\"color: #008080; text-decoration-color: #008080\"> tiles for dense OCR</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mğŸ”² Using \u001b[0m\u001b[1;36m11\u001b[0m\u001b[36m tiles for dense OCR\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Cell 6\n",
    "# Load InternVL3.5-8B model\n",
    "# Model page: https://huggingface.co/OpenGVLab/InternVL3_5-8B\n",
    "# Using official loading pattern from HuggingFace model card\n",
    "\n",
    "rprint(\"[bold green]Loading InternVL3.5-8B model...[/bold green]\")\n",
    "rprint(\"[cyan]ğŸš€ Model: InternVL3.5-8B (8.5B params: 0.3B vision + 8.2B language)[/cyan]\")\n",
    "rprint(\"[cyan]ğŸ“– Features: Cascade RL, Visual Resolution Router (ViR)[/cyan]\")\n",
    "\n",
    "try:\n",
    "    # Clear any existing CUDA cache\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        rprint(\"[blue]ğŸ§¹ CUDA cache cleared[/blue]\")\n",
    "    \n",
    "    world_size = torch.cuda.device_count()\n",
    "    rprint(f\"[cyan]ğŸ–¥ï¸  Detected {world_size} GPU(s)[/cyan]\")\n",
    "    rprint(\"[cyan]ğŸ“¥ Loading model (official HuggingFace pattern)...[/cyan]\")\n",
    "    \n",
    "    # Official loading pattern from https://huggingface.co/OpenGVLab/InternVL3_5-8B\n",
    "    model = AutoModel.from_pretrained(\n",
    "        CONFIG['MODEL_PATH'],\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        low_cpu_mem_usage=True,\n",
    "        use_flash_attn=CONFIG['USE_FLASH_ATTN'],\n",
    "        trust_remote_code=True,\n",
    "    ).eval().cuda()\n",
    "    \n",
    "    # Load tokenizer\n",
    "    rprint(\"[cyan]ğŸ“¥ Loading tokenizer...[/cyan]\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        CONFIG['MODEL_PATH'],\n",
    "        trust_remote_code=True,\n",
    "        use_fast=False\n",
    "    )\n",
    "    \n",
    "    # Set generation parameters\n",
    "    model.config.max_new_tokens = CONFIG['MAX_NEW_TOKENS']\n",
    "    \n",
    "    rprint(\"[green]âœ… Model and tokenizer loaded successfully![/green]\")\n",
    "    \n",
    "    # ============================================================================\n",
    "    # torch.compile: JIT OPTIMIZATION (PyTorch 2.x)\n",
    "    # ============================================================================\n",
    "    # Compiles the model's forward pass for optimized CUDA kernel execution.\n",
    "    # - First inference is slower (JIT compilation overhead)\n",
    "    # - Subsequent inferences are faster (optimized kernels cached)\n",
    "    # - mode=\"default\": Safe for variable input sizes (different tile counts)\n",
    "    # - Graceful fallback: If compile fails with custom code, model still works\n",
    "    # ============================================================================\n",
    "    torch_compile_active = False\n",
    "    if CONFIG.get('USE_TORCH_COMPILE', False):\n",
    "        rprint(\"[cyan]ğŸ”¥ Applying torch.compile (JIT optimization)...[/cyan]\")\n",
    "        try:\n",
    "            model = torch.compile(model, mode=\"default\")\n",
    "            torch_compile_active = True\n",
    "            rprint(\"[green]âœ… torch.compile applied (mode=default)[/green]\")\n",
    "            rprint(\"[dim]   First inference will be slower (JIT compilation), subsequent runs faster[/dim]\")\n",
    "        except Exception as e:\n",
    "            rprint(f\"[yellow]âš ï¸ torch.compile failed: {e}[/yellow]\")\n",
    "            rprint(\"[yellow]   Continuing without JIT optimization (model works normally)[/yellow]\")\n",
    "    \n",
    "    # ============================================================================\n",
    "    # GPU MEMORY DISPLAY\n",
    "    # ============================================================================\n",
    "    if torch.cuda.is_available():\n",
    "        device_count = torch.cuda.device_count()\n",
    "        \n",
    "        if device_count > 1:\n",
    "            rprint(f\"[blue]ğŸ”„ GPU Memory Status ({device_count} GPUs available, model on GPU 0):[/blue]\")\n",
    "            \n",
    "            total_allocated = 0\n",
    "            total_capacity = 0\n",
    "            \n",
    "            for gpu_id in range(device_count):\n",
    "                gpu_allocated = torch.cuda.memory_allocated(gpu_id) / 1e9\n",
    "                gpu_capacity = torch.cuda.get_device_properties(gpu_id).total_memory / 1e9\n",
    "                gpu_name = torch.cuda.get_device_name(gpu_id)\n",
    "                \n",
    "                total_allocated += gpu_allocated\n",
    "                total_capacity += gpu_capacity\n",
    "                \n",
    "                usage_pct = (gpu_allocated / gpu_capacity) * 100 if gpu_capacity > 0 else 0\n",
    "                rprint(f\"   GPU {gpu_id} ({gpu_name}): {gpu_allocated:.1f}GB/{gpu_capacity:.0f}GB ({usage_pct:.1f}%)\")\n",
    "            \n",
    "            rprint(f\"[blue]ğŸ“Š Total: {total_allocated:.1f}GB allocated, {total_capacity:.0f}GB capacity[/blue]\")\n",
    "        else:\n",
    "            allocated = torch.cuda.memory_allocated() / 1e9\n",
    "            total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "            gpu_name = torch.cuda.get_device_name(0)\n",
    "            usage_pct = (allocated / total) * 100 if total > 0 else 0\n",
    "            \n",
    "            rprint(f\"[blue]ğŸ“Š GPU {gpu_name}: {allocated:.1f}GB/{total:.0f}GB ({usage_pct:.1f}%)[/blue]\")\n",
    "    \n",
    "    # ============================================================================\n",
    "    # CONFIGURATION TABLE\n",
    "    # ============================================================================\n",
    "    from rich.table import Table\n",
    "    \n",
    "    config_table = Table(title=\"ğŸ”§ InternVL3.5-8B Model Configuration\")\n",
    "    config_table.add_column(\"Setting\", style=\"cyan\")\n",
    "    config_table.add_column(\"Value\", style=\"yellow\")\n",
    "    config_table.add_column(\"Status\", style=\"green\")\n",
    "    \n",
    "    model_name = Path(CONFIG['MODEL_PATH']).name\n",
    "    config_table.add_row(\"Model Path\", model_name, \"âœ… Valid\")\n",
    "    config_table.add_row(\"Model Type\", \"InternVL3.5-8B\", \"âœ… Cascade RL + ViR\")\n",
    "    config_table.add_row(\"Device Placement\", str(next(model.parameters()).device), \"âœ… Loaded\")\n",
    "    \n",
    "    quant_method = \"16-bit\" if not CONFIG['USE_QUANTIZATION'] else \"8-bit\"\n",
    "    config_table.add_row(\"Quantization Method\", quant_method, \"âœ… Configured\")\n",
    "    config_table.add_row(\"Data Type\", \"bfloat16\", \"âœ… Recommended\")\n",
    "    config_table.add_row(\"Max New Tokens\", str(CONFIG['MAX_NEW_TOKENS']), \"âœ… Generation Ready\")\n",
    "    config_table.add_row(\"Max Tiles\", str(CONFIG['MAX_TILES']), \"âœ… H200 Optimized\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        device_count = torch.cuda.device_count()\n",
    "        if device_count > 1:\n",
    "            gpu_info = f\"{device_count}x {torch.cuda.get_device_name(0)} ({total_capacity:.0f}GB total)\"\n",
    "            gpu_status = \"âœ… Model on GPU 0\"\n",
    "        else:\n",
    "            gpu_info = f\"{torch.cuda.get_device_name(0)} ({total:.0f}GB)\"\n",
    "            gpu_status = \"âœ… Single GPU\"\n",
    "    else:\n",
    "        gpu_info = \"CPU\"\n",
    "        gpu_status = \"ğŸ’» CPU Mode\"\n",
    "    config_table.add_row(\"GPU Configuration\", gpu_info, gpu_status)\n",
    "    \n",
    "    param_count = sum(p.numel() for p in model.parameters())\n",
    "    config_table.add_row(\"Model Parameters\", f\"{param_count:,}\", \"âœ… Loaded\")\n",
    "    \n",
    "    flash_attn_status = \"âœ… Enabled\" if CONFIG['USE_FLASH_ATTN'] else \"âš ï¸ Disabled\"\n",
    "    config_table.add_row(\"Flash Attention\", str(CONFIG['USE_FLASH_ATTN']), flash_attn_status)\n",
    "    \n",
    "    compile_status = \"âœ… Active\" if torch_compile_active else \"âš ï¸ Disabled\"\n",
    "    config_table.add_row(\"torch.compile\", str(torch_compile_active), compile_status)\n",
    "    \n",
    "    console.print(config_table)\n",
    "    \n",
    "    # Initialize the hybrid processor with loaded model components\n",
    "    rprint(\"[cyan]ğŸ”§ Initializing document-aware processor...[/cyan]\")\n",
    "    hybrid_processor = DocumentAwareInternVL3HybridProcessor(\n",
    "        field_list=UNIVERSAL_FIELDS,\n",
    "        model_path=CONFIG['MODEL_PATH'],\n",
    "        debug=CONFIG['VERBOSE'],\n",
    "        pre_loaded_model=model,\n",
    "        pre_loaded_tokenizer=tokenizer,\n",
    "        prompt_config=PROMPT_CONFIG,\n",
    "        max_tiles=CONFIG['MAX_TILES']\n",
    "    )\n",
    "    \n",
    "    rprint(\"[bold green]âœ… InternVL3.5-8B ready for document-aware processing[/bold green]\")\n",
    "    rprint(f\"[cyan]ğŸ”² Using {CONFIG['MAX_TILES']} tiles for dense OCR[/cyan]\")\n",
    "    if torch_compile_active:\n",
    "        rprint(\"[cyan]ğŸ”¥ torch.compile active: first image will be slower (JIT warmup)[/cyan]\")\n",
    "    \n",
    "except Exception as e:\n",
    "    rprint(f\"[red]âŒ Error loading model: {e}[/red]\")\n",
    "    rprint(\"[yellow]ğŸ’¡ Ensure transformers>=4.52.1 is installed[/yellow]\")\n",
    "    rprint(\"[yellow]ğŸ’¡ Ensure flash-attn is installed: pip install flash-attn --no-build-isolation[/yellow]\")\n",
    "    rprint(\"[yellow]ğŸ’¡ Check model path is correct[/yellow]\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Image Discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 7\n",
    "# Discover and filter images - Handle both absolute and relative paths\n",
    "\n",
    "# Convert DATA_DIR to Path and handle absolute/relative paths\n",
    "data_dir = Path(CONFIG['DATA_DIR'])\n",
    "if not data_dir.is_absolute():\n",
    "    # If relative, make it relative to current working directory\n",
    "    data_dir = Path.cwd() / data_dir\n",
    "\n",
    "# Discover images from the resolved data directory\n",
    "all_images = discover_images(str(data_dir))\n",
    "\n",
    "# Conditionally load ground truth only when not in inference-only mode\n",
    "ground_truth = {}\n",
    "if not CONFIG['INFERENCE_ONLY'] and CONFIG['GROUND_TRUTH']:\n",
    "    # Convert GROUND_TRUTH to Path and handle absolute/relative paths\n",
    "    ground_truth_path = Path(CONFIG['GROUND_TRUTH'])\n",
    "    if not ground_truth_path.is_absolute():\n",
    "        # If relative, make it relative to current working directory\n",
    "        ground_truth_path = Path.cwd() / ground_truth_path\n",
    "    \n",
    "    # Load ground truth from the resolved path\n",
    "    ground_truth = load_ground_truth(str(ground_truth_path), verbose=CONFIG['VERBOSE'])\n",
    "    \n",
    "    rprint(f\"[green]âœ… Ground truth loaded for {len(ground_truth)} images[/green]\")\n",
    "else:\n",
    "    rprint(\"[cyan]ğŸ“‹ Running in inference-only mode (no ground truth required)[/cyan]\")\n",
    "\n",
    "# Apply filters (only if ground truth is available)\n",
    "if CONFIG['DOCUMENT_TYPES'] and ground_truth:\n",
    "    filtered = []\n",
    "    for img in all_images:\n",
    "        img_name = Path(img).name\n",
    "        if img_name in ground_truth:\n",
    "            doc_type = ground_truth[img_name].get('DOCUMENT_TYPE', '').lower()\n",
    "            if any(dt.lower() in doc_type for dt in CONFIG['DOCUMENT_TYPES']):\n",
    "                filtered.append(img)\n",
    "    all_images = filtered\n",
    "\n",
    "if CONFIG['MAX_IMAGES']:\n",
    "    all_images = all_images[:CONFIG['MAX_IMAGES']]\n",
    "\n",
    "rprint(f\"[bold green]Ready to process {len(all_images)} images[/bold green]\")\n",
    "rprint(f\"[cyan]Data directory: {data_dir}[/cyan]\")\n",
    "if not CONFIG['INFERENCE_ONLY'] and CONFIG['GROUND_TRUTH']:\n",
    "    rprint(f\"[cyan]Ground truth: {ground_truth_path}[/cyan]\")\n",
    "rprint(f\"[cyan]Mode: {'Inference-only' if CONFIG['INFERENCE_ONLY'] else 'Evaluation mode'}[/cyan]\")\n",
    "for i, img in enumerate(all_images[:5], 1):\n",
    "    print(f\"  {i}. {Path(img).name}\")\n",
    "if len(all_images) > 5:\n",
    "    print(f\"  ... and {len(all_images) - 5} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Batch Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Cell 8\n",
    "# ============================================================================\n",
    "# V2: BATCH PROCESSING WITH SOPHISTICATED BANK STATEMENT EXTRACTION\n",
    "# ============================================================================\n",
    "# This cell initializes the batch processor and sets up document routing:\n",
    "#\n",
    "# Document Flow:\n",
    "# 1. Document type detection (Turn 0) - classify as INVOICE, RECEIPT, or BANK_STATEMENT\n",
    "# 2. If BANK_STATEMENT:\n",
    "#    - Turn 1: Header detection via BankStatementAdapter\n",
    "#    - Turn 2: Adaptive extraction based on detected columns\n",
    "# 3. If INVOICE/RECEIPT:\n",
    "#    - Standard single-turn extraction\n",
    "#\n",
    "# This ensures NO duplicate extraction - each document is processed once.\n",
    "# ============================================================================\n",
    "\n",
    "# Initialize batch processor with proven infrastructure (same pattern as llama_batch.ipynb)\n",
    "batch_processor = BatchDocumentProcessor(\n",
    "    model=hybrid_processor,  # InternVL3 hybrid processor (handler)\n",
    "    processor=None,          # Not needed for InternVL3\n",
    "    prompt_config=PROMPT_CONFIG,\n",
    "    ground_truth_csv=CONFIG['GROUND_TRUTH'],  # None in inference-only mode\n",
    "    console=console,\n",
    "    enable_math_enhancement=CONFIG['ENABLE_MATH_ENHANCEMENT']\n",
    ")\n",
    "\n",
    "# V2: Set up sophisticated bank statement extraction if enabled\n",
    "if CONFIG.get('USE_SOPHISTICATED_BANK_EXTRACTION', False):\n",
    "    rprint(\"[bold cyan]ğŸ¦ V2: Setting up sophisticated bank statement extraction...[/bold cyan]\")\n",
    "    \n",
    "    # Detect model dtype from the loaded model\n",
    "    try:\n",
    "        model_dtype = next(model.parameters()).dtype\n",
    "    except (StopIteration, AttributeError):\n",
    "        model_dtype = torch.bfloat16  # Default for H200\n",
    "    \n",
    "    # Create bank adapter for multi-turn extraction (InternVL3 mode)\n",
    "    bank_adapter = BankStatementAdapter(\n",
    "        model=hybrid_processor,  # Pass hybrid processor - adapter extracts model/tokenizer\n",
    "        processor=None,\n",
    "        verbose=CONFIG['VERBOSE'],\n",
    "        use_balance_correction=CONFIG.get('ENABLE_BALANCE_CORRECTION', False),\n",
    "        model_type=\"internvl3\",\n",
    "        model_dtype=model_dtype,\n",
    "    )\n",
    "    \n",
    "    # Get reference to the InternVL3 handler for document-aware processing\n",
    "    internvl3_handler = batch_processor.internvl3_handler\n",
    "    \n",
    "    def enhanced_process_internvl3(image_path, verbose):\n",
    "        \"\"\"\n",
    "        Enhanced processing with proper routing:\n",
    "        - Turn 0: Document type detection (all documents)\n",
    "        - If BANK_STATEMENT: BankStatementAdapter handles extraction (Turn 1 headers + Turn 2 extraction)\n",
    "        - If INVOICE/RECEIPT: Standard extraction via internvl3_handler\n",
    "        \n",
    "        NO duplicate extraction - each document processed exactly once.\n",
    "        \"\"\"\n",
    "        import sys\n",
    "        from pathlib import Path\n",
    "        \n",
    "        def _safe_print(msg: str) -> None:\n",
    "            \"\"\"Print without triggering Rich console recursion in Jupyter.\"\"\"\n",
    "            try:\n",
    "                sys.__stdout__.write(msg + \"\\n\")\n",
    "                sys.__stdout__.flush()\n",
    "            except Exception:\n",
    "                pass\n",
    "        \n",
    "        # =====================================================================\n",
    "        # TURN 0: Document Type Detection ONLY (no extraction yet)\n",
    "        # =====================================================================\n",
    "        if verbose:\n",
    "            _safe_print(f\"\\nğŸ“‹ Turn 0: Document type detection for {Path(image_path).name}\")\n",
    "        \n",
    "        try:\n",
    "            classification_info = internvl3_handler.detect_and_classify_document(\n",
    "                image_path, verbose=verbose\n",
    "            )\n",
    "            doc_type = classification_info[\"document_type\"]\n",
    "        except Exception as e:\n",
    "            rprint(f\"[red]Error in document type detection: {e}[/red]\")\n",
    "            raise\n",
    "        \n",
    "        if verbose:\n",
    "            _safe_print(f\"âœ… Detected: {doc_type}\")\n",
    "        \n",
    "        # =====================================================================\n",
    "        # ROUTING: Bank statements vs other documents\n",
    "        # =====================================================================\n",
    "        if doc_type.upper() == \"BANK_STATEMENT\":\n",
    "            # -----------------------------------------------------------------\n",
    "            # BANK STATEMENT: Use BankStatementAdapter (Turn 1 + Turn 2)\n",
    "            # -----------------------------------------------------------------\n",
    "            if verbose:\n",
    "                _safe_print(\"ğŸ¦ Routing to BankStatementAdapter for multi-turn extraction\")\n",
    "            \n",
    "            try:\n",
    "                # BankStatementAdapter handles:\n",
    "                # - Turn 1: Header detection (identifies column names)\n",
    "                # - Turn 2: Adaptive extraction based on detected columns\n",
    "                schema_fields, metadata = bank_adapter.extract_bank_statement(image_path)\n",
    "                \n",
    "                # Build result structure compatible with BatchDocumentProcessor\n",
    "                extraction_result = {\n",
    "                    \"extracted_data\": schema_fields,\n",
    "                    \"raw_response\": metadata.get(\"raw_responses\", {}).get(\"turn1\", \"\"),\n",
    "                    \"field_list\": list(schema_fields.keys()),\n",
    "                    \"metadata\": metadata,\n",
    "                }\n",
    "                \n",
    "                # Create prompt name indicating strategy used\n",
    "                strategy = metadata.get(\"strategy_used\", \"unknown\")\n",
    "                prompt_name = f\"unified_bank_{strategy}\"\n",
    "                \n",
    "                if verbose:\n",
    "                    _safe_print(f\"  âœ… Strategy: {strategy}\")\n",
    "                    tx_count = len(schema_fields.get('TRANSACTION_DATES', '').split('|')) if schema_fields.get('TRANSACTION_DATES') != 'NOT_FOUND' else 0\n",
    "                    _safe_print(f\"  âœ… Transactions extracted: {tx_count}\")\n",
    "                \n",
    "                return doc_type, extraction_result, prompt_name\n",
    "                \n",
    "            except Exception as e:\n",
    "                rprint(f\"[yellow]âš ï¸  BankStatementAdapter failed: {e}[/yellow]\")\n",
    "                rprint(\"[yellow]   Falling back to standard extraction...[/yellow]\")\n",
    "                # Fall through to standard extraction\n",
    "        \n",
    "        # -----------------------------------------------------------------\n",
    "        # INVOICE/RECEIPT (or bank fallback): Standard extraction\n",
    "        # -----------------------------------------------------------------\n",
    "        if verbose:\n",
    "            _safe_print(f\"ğŸ“„ Using standard extraction for {doc_type}\")\n",
    "        \n",
    "        # Use document-aware extraction (single turn)\n",
    "        extraction_result = internvl3_handler.process_document_aware(\n",
    "            image_path, classification_info, verbose=verbose\n",
    "        )\n",
    "        \n",
    "        # Extract the actual extracted_data for evaluation\n",
    "        extracted_data = extraction_result.get(\"extracted_data\", {})\n",
    "        \n",
    "        # Create extraction_result in format expected by batch processor\n",
    "        formatted_result = {\n",
    "            \"extracted_data\": extracted_data,\n",
    "            \"document_type\": doc_type,\n",
    "            \"image_file\": Path(image_path).name,\n",
    "            \"processing_time\": extraction_result.get(\"processing_time\", 0),\n",
    "        }\n",
    "        \n",
    "        prompt_name = f\"internvl3_{doc_type.lower()}\"\n",
    "        \n",
    "        return doc_type, formatted_result, prompt_name\n",
    "    \n",
    "    # Replace the processing method\n",
    "    batch_processor._process_internvl3_image = enhanced_process_internvl3\n",
    "    \n",
    "    rprint(\"[green]âœ… V2: Sophisticated bank statement extraction enabled[/green]\")\n",
    "    rprint(\"[cyan]   Flow: Detection â†’ Route â†’ Extract (no duplicate processing)[/cyan]\")\n",
    "    rprint(\"[cyan]   Bank statements: Turn 0 (detect) â†’ Turn 1 (headers) â†’ Turn 2 (extract)[/cyan]\")\n",
    "    rprint(\"[cyan]   Invoice/Receipt: Turn 0 (detect) â†’ Standard extraction[/cyan]\")\n",
    "    rprint(f\"[cyan]   Balance correction: {'Enabled' if CONFIG.get('ENABLE_BALANCE_CORRECTION', False) else 'Disabled'}[/cyan]\")\n",
    "    rprint(f\"[cyan]   Model dtype: {model_dtype}[/cyan]\")\n",
    "else:\n",
    "    rprint(\"[dim]â­ï¸  V2: Sophisticated bank extraction disabled - using original single-turn[/dim]\")\n",
    "\n",
    "# Process batch using proven evaluation infrastructure\n",
    "batch_results, processing_times, document_types_found = batch_processor.process_batch(\n",
    "    all_images, verbose=CONFIG['VERBOSE']\n",
    ")\n",
    "\n",
    "# Brief summary\n",
    "rprint(f\"[bold green]âœ… Processed {len(batch_results)} images[/bold green]\")\n",
    "rprint(f\"[cyan]Average time: {np.mean(processing_times):.2f}s[/cyan]\")\n",
    "if CONFIG['INFERENCE_ONLY']:\n",
    "    rprint(\"[cyan]ğŸ“‹ Inference-only mode: No accuracy evaluation performed[/cyan]\")\n",
    "else:\n",
    "    avg_accuracy = np.mean([r.get('evaluation', {}).get('overall_accuracy', 0) * 100 for r in batch_results if 'evaluation' in r])\n",
    "    rprint(f\"[cyan]Average accuracy: {avg_accuracy:.1f}%[/cyan]\")\n",
    "    \n",
    "    # V2: Show bank statement specific results if sophisticated extraction was used\n",
    "    if CONFIG.get('USE_SOPHISTICATED_BANK_EXTRACTION', False):\n",
    "        bank_results = [r for r in batch_results if r.get('document_type', '').upper() == 'BANK_STATEMENT']\n",
    "        if bank_results:\n",
    "            bank_accuracy = np.mean([r.get('evaluation', {}).get('overall_accuracy', 0) * 100 for r in bank_results if 'evaluation' in r])\n",
    "            rprint(f\"[cyan]ğŸ¦ Bank statement accuracy (V2): {bank_accuracy:.1f}%[/cyan]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Generate Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 9\n",
    "# Create model-specific CSV file to match Llama structure\n",
    "# Use UNIVERSAL_FIELDS (already filtered to exclude validation-only fields)\n",
    "# CRITICAL: TRANSACTION_AMOUNTS_RECEIVED and ACCOUNT_BALANCE are excluded\n",
    "FIELD_COLUMNS = UNIVERSAL_FIELDS\n",
    "\n",
    "# Create comprehensive results data matching Llama structure\n",
    "internvl3_5_csv_data = []\n",
    "\n",
    "for i, result in enumerate(batch_results):\n",
    "    # Basic metadata\n",
    "    image_name = Path(result['image_path']).name\n",
    "    doc_type = result.get('document_type', '').lower()\n",
    "    processing_time = processing_times[i] if i < len(processing_times) else 0\n",
    "    \n",
    "    # Extract fields from result\n",
    "    extraction_result = result.get('extraction_result', {})\n",
    "    extracted_fields = extraction_result.get('extracted_data', {})\n",
    "    accuracy_data = result.get('evaluation', {})\n",
    "    \n",
    "    # Count fields (using filtered field list)\n",
    "    total_fields = len(FIELD_COLUMNS)\n",
    "    found_fields = sum(1 for field in FIELD_COLUMNS if extracted_fields.get(field, 'NOT_FOUND') != 'NOT_FOUND')\n",
    "    field_coverage = (found_fields / total_fields * 100) if total_fields > 0 else 0\n",
    "    \n",
    "    # Handle both inference-only and evaluation modes\n",
    "    if CONFIG['INFERENCE_ONLY'] or accuracy_data.get('inference_only', False):\n",
    "        # Inference-only mode\n",
    "        overall_accuracy = None\n",
    "        fields_extracted = found_fields\n",
    "        fields_matched = 0  # No matching in inference mode\n",
    "        eval_total_fields = total_fields\n",
    "    else:\n",
    "        # Evaluation mode\n",
    "        overall_accuracy = accuracy_data.get('overall_accuracy', 0) * 100 if accuracy_data else 0\n",
    "        fields_extracted = accuracy_data.get('fields_extracted', 0) if accuracy_data else 0\n",
    "        fields_matched = accuracy_data.get('fields_matched', 0) if accuracy_data else 0\n",
    "        eval_total_fields = accuracy_data.get('total_fields', total_fields) if accuracy_data else total_fields\n",
    "    \n",
    "    # Create prompt identifier for InternVL3.5-8B\n",
    "    prompt_used = f\"internvl3_5_8b_{doc_type}\" if doc_type else \"internvl3_5_8b_unknown\"\n",
    "    \n",
    "    # Create row data\n",
    "    row_data = {\n",
    "        'image_file': image_name,\n",
    "        'image_name': image_name,\n",
    "        'document_type': doc_type,\n",
    "        'processing_time': processing_time,\n",
    "        'field_count': eval_total_fields,\n",
    "        'found_fields': fields_extracted,\n",
    "        'field_coverage': field_coverage,\n",
    "        'prompt_used': prompt_used,\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'overall_accuracy': overall_accuracy,\n",
    "        'fields_extracted': fields_extracted,\n",
    "        'fields_matched': fields_matched,\n",
    "        'total_fields': eval_total_fields,\n",
    "        'inference_only': CONFIG['INFERENCE_ONLY'],\n",
    "        'model_version': 'InternVL3.5-8B',\n",
    "        'gpu_type': 'H200',\n",
    "        'precision': 'bfloat16'\n",
    "    }\n",
    "    \n",
    "    # Add all field values (only for fields in filtered list)\n",
    "    for field in FIELD_COLUMNS:\n",
    "        row_data[field] = extracted_fields.get(field, 'NOT_FOUND')\n",
    "    \n",
    "    internvl3_5_csv_data.append(row_data)\n",
    "\n",
    "# Create DataFrame and save\n",
    "internvl3_5_df = pd.DataFrame(internvl3_5_csv_data)\n",
    "internvl3_5_csv_path = OUTPUT_DIRS['csv'] / f\"internvl3_5_8b_batch_results_{BATCH_TIMESTAMP}.csv\"\n",
    "internvl3_5_df.to_csv(internvl3_5_csv_path, index=False)\n",
    "\n",
    "rprint(\"[bold green]âœ… InternVL3.5-8B model-specific CSV exported:[/bold green]\")\n",
    "rprint(f\"[cyan]ğŸ“„ File: {internvl3_5_csv_path}[/cyan]\")\n",
    "rprint(f\"[cyan]ğŸ“Š Structure: {len(internvl3_5_df)} rows Ã— {len(internvl3_5_df.columns)} columns[/cyan]\")\n",
    "rprint(f\"[cyan]ğŸ“‹ Fields: {len(FIELD_COLUMNS)} (validation-only fields excluded)[/cyan]\")\n",
    "rprint(\"[cyan]ğŸ–¥ï¸  Hardware: H200 + bfloat16 (optimal configuration)[/cyan]\")\n",
    "rprint(\"[cyan]ğŸ”— Compatible with model_comparison.ipynb pattern: *internvl3_5_8b*batch*results*.csv[/cyan]\")\n",
    "\n",
    "# Display sample of the exported data (conditional based on mode)\n",
    "if CONFIG['INFERENCE_ONLY']:\n",
    "    rprint(\"\\n[bold blue]ğŸ“‹ Sample exported data (inference-only mode):[/bold blue]\")\n",
    "    sample_cols = ['image_file', 'document_type', 'processing_time', 'found_fields', 'field_coverage', 'model_version', 'gpu_type', 'precision']\n",
    "    if len(internvl3_5_df) > 0:\n",
    "        display(internvl3_5_df[sample_cols].head(10))\n",
    "    else:\n",
    "        rprint(\"[yellow]âš ï¸ No data to display[/yellow]\")\n",
    "else:\n",
    "    rprint(\"\\n[bold blue]ğŸ“‹ Sample exported data (first 3 rows, key columns):[/bold blue]\")\n",
    "    sample_cols = ['image_file', 'document_type', 'overall_accuracy', 'processing_time', 'found_fields', 'model_version', 'gpu_type']\n",
    "    if len(internvl3_5_df) > 0:\n",
    "        display(internvl3_5_df[sample_cols].head(3))\n",
    "    else:\n",
    "        rprint(\"[yellow]âš ï¸ No data to display[/yellow]\")\n",
    "\n",
    "    # Verification: Show accuracy values to confirm they're correct (evaluation mode only)\n",
    "    rprint(\"\\n[bold blue]ğŸ” Accuracy verification:[/bold blue]\")\n",
    "    for i, result in enumerate(batch_results[:10]):  # Show first 10\n",
    "        evaluation = result.get('evaluation', {})\n",
    "        original_accuracy = evaluation.get('overall_accuracy', 0)\n",
    "        percentage_accuracy = original_accuracy * 100\n",
    "        rprint(f\"  {result['image_name']}: {original_accuracy:.4f} â†’ {percentage_accuracy:.2f}%\")\n",
    "\n",
    "# Create analytics using proven infrastructure (same pattern as llama_batch.ipynb)\n",
    "analytics = BatchAnalytics(batch_results, processing_times)\n",
    "\n",
    "# Generate and save DataFrames using established patterns\n",
    "saved_files, df_results, df_summary, df_doctype_stats, df_field_stats = analytics.save_all_dataframes(\n",
    "    OUTPUT_DIRS['csv'], BATCH_TIMESTAMP, verbose=CONFIG['VERBOSE']\n",
    ")\n",
    "\n",
    "# Display key results based on mode\n",
    "rprint(\"\\n[bold blue]ğŸ“Š InternVL3.5-8B Results Summary (H200 + bfloat16)[/bold blue]\")\n",
    "if CONFIG['INFERENCE_ONLY']:\n",
    "    rprint(\"[cyan]ğŸ“‹ Running in inference-only mode - no accuracy metrics available[/cyan]\")\n",
    "    # Show extraction statistics instead\n",
    "    rprint(f\"[cyan]âœ… Total images processed: {len(batch_results)}[/cyan]\")\n",
    "    rprint(f\"[cyan]âœ… Average fields found: {internvl3_5_df['found_fields'].mean():.1f}[/cyan]\")\n",
    "    rprint(f\"[cyan]âœ… Average field coverage: {internvl3_5_df['field_coverage'].mean():.1f}%[/cyan]\")\n",
    "else:\n",
    "    display(df_summary)\n",
    "    rprint(\"[blue]ğŸ“Š This establishes the H200 bfloat16 baseline for comparison with V100 float32[/blue]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "internvl3_5_df[['image_file','overall_accuracy']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Export Model-Specific CSV for Comparison\n",
    "\n",
    "Create InternVL3 NON-QUANTIZED specific CSV file that matches Llama structure for model comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 10\n",
    "# Create visualizations using proven infrastructure (same pattern as llama_batch.ipynb)\n",
    "visualizer = BatchVisualizer()\n",
    "\n",
    "viz_files = visualizer.create_all_visualizations(\n",
    "    df_results, \n",
    "    df_doctype_stats,\n",
    "    OUTPUT_DIRS['visualizations'],\n",
    "    BATCH_TIMESTAMP,\n",
    "    show=False  # Disable display to reduce output\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Generate Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 11\n",
    "# Generate reports using proven infrastructure (same pattern as llama_batch.ipynb)\n",
    "reporter = BatchReporter(\n",
    "    batch_results, \n",
    "    processing_times,\n",
    "    document_types_found,\n",
    "    BATCH_TIMESTAMP\n",
    ")\n",
    "\n",
    "# Save all reports using CONFIG verbose setting\n",
    "report_files = reporter.save_all_reports(\n",
    "    OUTPUT_DIRS,\n",
    "    df_results,\n",
    "    df_summary,\n",
    "    df_doctype_stats,\n",
    "    CONFIG['MODEL_PATH'],\n",
    "    {\n",
    "        'data_dir': CONFIG['DATA_DIR'],\n",
    "        'ground_truth': CONFIG['GROUND_TRUTH'],\n",
    "        'max_images': CONFIG['MAX_IMAGES'],\n",
    "        'document_types': CONFIG['DOCUMENT_TYPES']\n",
    "    },\n",
    "    {\n",
    "        'use_quantization': CONFIG['USE_QUANTIZATION'],\n",
    "        'device_map': CONFIG['DEVICE_MAP'],\n",
    "        'max_new_tokens': CONFIG['MAX_NEW_TOKENS'],\n",
    "        'torch_dtype': CONFIG['TORCH_DTYPE'],\n",
    "        'low_cpu_mem_usage': CONFIG['LOW_CPU_MEM_USAGE']\n",
    "    },\n",
    "    verbose=CONFIG['VERBOSE']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Display Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 12\n",
    "# Display final summary\n",
    "console.rule(\"[bold green]InternVL3.5-8B Batch Processing Complete[/bold green]\")\n",
    "\n",
    "total_images = len(batch_results)\n",
    "successful = len([r for r in batch_results if 'error' not in r])\n",
    "avg_accuracy = df_results['overall_accuracy'].mean() if len(df_results) > 0 else 0\n",
    "\n",
    "rprint(f\"[bold green]âœ… Processed: {total_images} images[/bold green]\")\n",
    "rprint(f\"[cyan]Success Rate: {(successful/total_images*100):.1f}%[/cyan]\")\n",
    "rprint(f\"[cyan]Overall Average Accuracy (across all images): {avg_accuracy:.2f}%[/cyan]\")\n",
    "\n",
    "# Show per-document-type breakdown\n",
    "if len(df_results) > 0 and 'document_type' in df_results.columns:\n",
    "    rprint(\"\\n[bold blue]ğŸ“Š Breakdown by Document Type:[/bold blue]\")\n",
    "    doc_type_stats = df_results.groupby('document_type').agg({\n",
    "        'overall_accuracy': 'mean',\n",
    "        'image_name': 'count'\n",
    "    }).sort_values('overall_accuracy', ascending=False)\n",
    "    \n",
    "    for doc_type, row in doc_type_stats.iterrows():\n",
    "        count = int(row['image_name'])\n",
    "        acc = row['overall_accuracy']\n",
    "        rprint(f\"[cyan]  {doc_type.upper()}: {acc:.1f}% (n={count} images)[/cyan]\")\n",
    "\n",
    "rprint(f\"\\n[cyan]Output: {OUTPUT_BASE}[/cyan]\")\n",
    "rprint(\"[blue]ğŸš€ Model: InternVL3.5-8B with Cascade RL and ViR[/blue]\")\n",
    "\n",
    "# Performance assessment\n",
    "if successful == total_images and avg_accuracy > 50:\n",
    "    rprint(\"\\n[bold green]ğŸ‰ SUCCESS: InternVL3.5-8B processing completed successfully![/bold green]\")\n",
    "    rprint(\"[green]âœ… Enhanced reasoning with Cascade RL is working[/green]\")\n",
    "    rprint(\"[green]âœ… Dynamic resolution adjustment with ViR is active[/green]\")\n",
    "elif successful < total_images:\n",
    "    rprint(\"\\n[bold red]âŒ FAILURE: Processing errors occurred[/bold red]\")\n",
    "    rprint(\"[red]ğŸ” Review error logs for diagnostic information[/red]\")\n",
    "elif avg_accuracy < 30:\n",
    "    rprint(\"\\n[bold yellow]âš ï¸ POOR PERFORMANCE: Low accuracy detected[/bold yellow]\")\n",
    "    rprint(\"[yellow]ğŸ” Review extraction results for quality issues[/yellow]\")\n",
    "else:\n",
    "    rprint(\"\\n[bold blue]ğŸ“Š MIXED RESULTS: Partially working[/bold blue]\")\n",
    "    rprint(\"[blue]ğŸ” Review individual results to assess performance[/blue]\")\n",
    "\n",
    "# Document type distribution\n",
    "if document_types_found:\n",
    "    rprint(\"\\n[bold blue]ğŸ“‹ Document Type Distribution:[/bold blue]\")\n",
    "    for doc_type, count in document_types_found.items():\n",
    "        percentage = (count / total_images * 100) if total_images > 0 else 0\n",
    "        rprint(f\"[cyan]  {doc_type}: {count} documents ({percentage:.1f}%)[/cyan]\")\n",
    "\n",
    "# Display dashboard if available\n",
    "dashboard_files = list(OUTPUT_DIRS['visualizations'].glob(f\"dashboard_{BATCH_TIMESTAMP}.png\"))\n",
    "if dashboard_files:\n",
    "    from IPython.display import Image, display\n",
    "    dashboard_path = dashboard_files[0]\n",
    "    rprint(\"\\n[bold blue]ğŸ“Š Visual Dashboard:[/bold blue]\")\n",
    "    display(Image(str(dashboard_path)))\n",
    "else:\n",
    "    rprint(f\"\\n[yellow]âš ï¸ Dashboard not found in {OUTPUT_DIRS['visualizations']}[/yellow]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Failed Extractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 13\n",
    "# Calculate zero accuracy extractions\n",
    "zero_accuracy_count = 0\n",
    "zero_accuracy_images = []\n",
    "total_evaluated = 0\n",
    "\n",
    "for result in batch_results:\n",
    "    # Check if evaluation data exists (not inference-only mode)\n",
    "    evaluation = result.get(\"evaluation\", {})\n",
    "\n",
    "    if evaluation and not evaluation.get(\"inference_only\", False):\n",
    "        total_evaluated += 1\n",
    "        accuracy = evaluation.get(\"overall_accuracy\", 0)\n",
    "\n",
    "        if accuracy == 0.0:\n",
    "            zero_accuracy_count += 1\n",
    "            zero_accuracy_images.append(\n",
    "                {\n",
    "                    \"image_name\": result.get(\"image_name\", \"unknown\"),\n",
    "                    \"document_type\": result.get(\"document_type\", \"unknown\"),\n",
    "                    \"fields_extracted\": evaluation.get(\"fields_extracted\", 0),\n",
    "                    \"total_fields\": evaluation.get(\"total_fields\", 0),\n",
    "                }\n",
    "            )\n",
    "\n",
    "# Display results\n",
    "if total_evaluated > 0:\n",
    "    console.rule(\"[bold red]Zero Accuracy Analysis[/bold red]\")\n",
    "\n",
    "    rprint(f\"[cyan]Total documents evaluated: {total_evaluated}[/cyan]\")\n",
    "    rprint(f\"[red]Documents with 0% accuracy: {zero_accuracy_count}[/red]\")\n",
    "\n",
    "    if zero_accuracy_count > 0:\n",
    "        percentage = (zero_accuracy_count / total_evaluated) * 100\n",
    "        rprint(f\"[red]Zero accuracy rate: {percentage:.1f}%[/red]\")\n",
    "\n",
    "        rprint(\"\\n[bold red]Documents with 0% Accuracy:[/bold red]\")\n",
    "        for i, img_info in enumerate(zero_accuracy_images, 1):\n",
    "            rprint(f\"  {i}. {img_info['image_name']} ({img_info['document_type']})\")\n",
    "            rprint(\n",
    "                f\"     Fields extracted: {img_info['fields_extracted']}/{img_info['total_fields']}\"\n",
    "            )\n",
    "    else:\n",
    "        rprint(\n",
    "            \"[green]âœ… No documents with 0% accuracy - all extractions had some success![/green]\"\n",
    "        )\n",
    "else:\n",
    "    rprint(\n",
    "        \"[yellow]âš ï¸ Running in inference-only mode - no accuracy metrics available[/yellow]\"\n",
    "    )\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (LMM_POC_IVL3.5)",
   "language": "python",
   "name": "lmm_poc_ivl3.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
