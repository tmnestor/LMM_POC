{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c12cb8ec",
   "metadata": {},
   "source": [
    "# InternVL3-2B Pure Multi-Turn Adaptive Extraction ‚≠ê\n",
    "\n",
    "**TRUE multi-turn chat using official InternVL3 pattern** - exactly like Llama.\n",
    "\n",
    "This notebook implements genuine multi-turn conversation using the official InternVL3 pattern:\n",
    "1. Direct model loading\n",
    "2. Manual prompt loading from YAML  \n",
    "3. TRUE multi-turn via `return_history=True` (official pattern)\n",
    "4. Manual response parsing\n",
    "5. Full control over generation parameters\n",
    "\n",
    "**Official Pattern** (from InternVL3 docs):\n",
    "```python\n",
    "# Load pixel_values ONCE\n",
    "pixel_values = load_image(image_path)\n",
    "\n",
    "# First turn: history=None\n",
    "response, history = model.chat(tokenizer, pixel_values, prompt, \n",
    "                               generation_config, history=None, return_history=True)\n",
    "\n",
    "# Second turn: pass history from first turn\n",
    "response, history = model.chat(tokenizer, pixel_values, prompt,\n",
    "                               generation_config, history=history, return_history=True)\n",
    "```\n",
    "\n",
    "**Key difference from hybrid**: Uses `return_history=True` to maintain conversation context across turns with the SAME `pixel_values` tensor (no reloading needed!).\n",
    "\n",
    "Outputs compatible with model_comparison.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d0d5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "import traceback\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import yaml\n",
    "from PIL import Image\n",
    "from rich import print as rprint\n",
    "from rich.console import Console\n",
    "from rich.progress import track\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# Initialize console for rich output\n",
    "console = Console()\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "rprint(\"[green]‚úÖ Imports loaded[/green]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b69b25",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d126c74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment-specific base paths\n",
    "ENVIRONMENT_BASES = {\n",
    "    'sandbox': '/home/jovyan/nfs_share/tod',\n",
    "    'efs': '/efs/shared/PoC_data'\n",
    "}\n",
    "base_data_path = ENVIRONMENT_BASES['sandbox']\n",
    "\n",
    "CONFIG = {\n",
    "    # Model settings\n",
    "    'MODEL_PATH': '/home/jovyan/nfs_share/models/InternVL3-2B',\n",
    "\n",
    "    # Data paths\n",
    "    'DATA_DIR': f'{base_data_path}/evaluation_data',\n",
    "\n",
    "    # Prompt files\n",
    "    'PROMPT_FILE_DOCTYPE': f'{base_data_path}/LMM_POC/prompts/document_type_detection.yaml',\n",
    "    'PROMPT_FILE_INVOICE': f'{base_data_path}/LMM_POC/prompts/internvl3_prompts.yaml',\n",
    "    'PROMPT_FILE_RECEIPT': f'{base_data_path}/LMM_POC/prompts/internvl3_prompts.yaml',\n",
    "    'PROMPT_FILE_BANK': f'{base_data_path}/LMM_POC/prompts/internvl3_prompts.yaml',\n",
    "\n",
    "    # Output directory\n",
    "    'OUTPUT_DIR': f'{base_data_path}/LMM_POC/output',\n",
    "\n",
    "    # Token limits\n",
    "    'MAX_NEW_TOKENS_DOCTYPE': 50,\n",
    "    'MAX_NEW_TOKENS_STRUCTURE': 50,\n",
    "    'MAX_NEW_TOKENS_EXTRACT': 2000,\n",
    "    \n",
    "    # Verbosity control\n",
    "    'VERBOSE': True,  # Show stage-by-stage progress\n",
    "    'SHOW_PROMPTS': True,  # Show actual prompts being used\n",
    "}\n",
    "\n",
    "# Create output directory\n",
    "output_dir = Path(CONFIG['OUTPUT_DIR'])\n",
    "csv_dir = output_dir / 'csv'\n",
    "csv_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Timestamp for output files\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "rprint(\"[green]‚úÖ Configuration loaded[/green]\")\n",
    "rprint(f\"[cyan]  Environment: {[k for k, v in ENVIRONMENT_BASES.items() if v == base_data_path][0]}[/cyan]\")\n",
    "rprint(f\"[cyan]  Base path: {base_data_path}[/cyan]\")\n",
    "rprint(f\"[cyan]  Output directory: {output_dir}[/cyan]\")\n",
    "rprint(f\"[cyan]  Timestamp: {TIMESTAMP}[/cyan]\")\n",
    "rprint(f\"[cyan]  Verbosity: {'ON' if CONFIG['VERBOSE'] else 'OFF'}[/cyan]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd950364",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2799393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load InternVL3 model directly (no processor wrapper)\n",
    "rprint(\"[bold green]üîß Loading InternVL3 model...[/bold green]\")\n",
    "\n",
    "model = AutoModel.from_pretrained(\n",
    "    CONFIG['MODEL_PATH'],\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    use_flash_attn=False,  # V100 compatible\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\"\n",
    ").eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    CONFIG['MODEL_PATH'],\n",
    "    trust_remote_code=True,\n",
    "    use_fast=False\n",
    ")\n",
    "\n",
    "# Fix pad_token_id\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "rprint(\"[green]‚úÖ Model and tokenizer loaded[/green]\")\n",
    "\n",
    "# Display model info\n",
    "if torch.cuda.is_available():\n",
    "    allocated = torch.cuda.memory_allocated() / 1e9\n",
    "    total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    rprint(f\"[blue]üìä GPU Memory: {allocated:.2f}GB / {total:.0f}GB ({(allocated/total*100):.1f}%)[/blue]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382eb2b8",
   "metadata": {},
   "source": [
    "## Image Preprocessing\n",
    "\n",
    "InternVL3-specific image loading with dynamic tiling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5be3978",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "\n",
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "def build_transform(input_size):\n",
    "    \"\"\"Build InternVL3 image transform.\"\"\"\n",
    "    return T.Compose([\n",
    "        T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
    "        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "    ])\n",
    "\n",
    "def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):\n",
    "    \"\"\"Find closest aspect ratio for tiling.\"\"\"\n",
    "    best_ratio_diff = float('inf')\n",
    "    best_ratio = (1, 1)\n",
    "    area = width * height\n",
    "    for ratio in target_ratios:\n",
    "        target_aspect_ratio = ratio[0] / ratio[1]\n",
    "        ratio_diff = abs(aspect_ratio - target_aspect_ratio)\n",
    "        if ratio_diff < best_ratio_diff:\n",
    "            best_ratio_diff = ratio_diff\n",
    "            best_ratio = ratio\n",
    "        elif ratio_diff == best_ratio_diff:\n",
    "            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\n",
    "                best_ratio = ratio\n",
    "    return best_ratio\n",
    "\n",
    "def dynamic_preprocess(image, min_num=1, max_num=12, image_size=448, use_thumbnail=False):\n",
    "    \"\"\"InternVL3 dynamic preprocessing into tiles.\"\"\"\n",
    "    orig_width, orig_height = image.size\n",
    "    aspect_ratio = orig_width / orig_height\n",
    "\n",
    "    # Calculate target ratios\n",
    "    target_ratios = set(\n",
    "        (i, j) for n in range(min_num, max_num + 1)\n",
    "        for i in range(1, n + 1) for j in range(1, n + 1)\n",
    "        if i * j <= max_num and i * j >= min_num\n",
    "    )\n",
    "    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n",
    "\n",
    "    # Find best fit\n",
    "    target_aspect_ratio = find_closest_aspect_ratio(\n",
    "        aspect_ratio, target_ratios, orig_width, orig_height, image_size\n",
    "    )\n",
    "\n",
    "    # Calculate dimensions\n",
    "    target_width = image_size * target_aspect_ratio[0]\n",
    "    target_height = image_size * target_aspect_ratio[1]\n",
    "    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]\n",
    "\n",
    "    # Resize image\n",
    "    resized_img = image.resize((target_width, target_height))\n",
    "    processed_images = []\n",
    "\n",
    "    for i in range(blocks):\n",
    "        box = (\n",
    "            (i % (target_width // image_size)) * image_size,\n",
    "            (i // (target_width // image_size)) * image_size,\n",
    "            ((i % (target_width // image_size)) + 1) * image_size,\n",
    "            ((i // (target_width // image_size)) + 1) * image_size\n",
    "        )\n",
    "        split_img = resized_img.crop(box)\n",
    "        processed_images.append(split_img)\n",
    "\n",
    "    if use_thumbnail and len(processed_images) != 1:\n",
    "        thumbnail_img = image.resize((image_size, image_size))\n",
    "        processed_images.append(thumbnail_img)\n",
    "\n",
    "    return processed_images\n",
    "\n",
    "def load_image(image_path, input_size=448, max_num=24, debug=None):\n",
    "    \"\"\"Complete InternVL3 image loading pipeline - matches working processor.\"\"\"\n",
    "    # Use CONFIG VERBOSE if debug not specified\n",
    "    if debug is None:\n",
    "        debug = CONFIG.get('VERBOSE', False)\n",
    "    \n",
    "    if debug:\n",
    "        rprint(f\"[blue]üîç LOAD_IMAGE: max_num={max_num}, input_size={input_size}[/blue]\")\n",
    "    \n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    images = dynamic_preprocess(image, min_num=1, max_num=max_num,\n",
    "                                image_size=input_size, use_thumbnail=True)\n",
    "    transform = build_transform(input_size=input_size)\n",
    "    pixel_values = [transform(img) for img in images]\n",
    "    pixel_values = torch.stack(pixel_values)\n",
    "\n",
    "    # CRITICAL: Robust dtype detection (copied from working processor)\n",
    "    try:\n",
    "        # Try vision model's embedding layer dtype (most reliable)\n",
    "        if hasattr(model, 'vision_model') and hasattr(model.vision_model, 'embeddings'):\n",
    "            vision_dtype = next(model.vision_model.embeddings.parameters()).dtype\n",
    "            pixel_values = pixel_values.to(dtype=vision_dtype)\n",
    "            if debug:\n",
    "                rprint(f\"[blue]üîß TENSOR_DTYPE: Using vision model dtype {vision_dtype}[/blue]\")\n",
    "        elif hasattr(model, 'dtype'):\n",
    "            pixel_values = pixel_values.to(dtype=model.dtype)\n",
    "            if debug:\n",
    "                rprint(f\"[blue]üîß TENSOR_DTYPE: Using model.dtype {model.dtype}[/blue]\")\n",
    "        else:\n",
    "            # Fall back to parameter dtype\n",
    "            model_dtype = next(model.parameters()).dtype\n",
    "            pixel_values = pixel_values.to(dtype=model_dtype)\n",
    "            if debug:\n",
    "                rprint(f\"[blue]üîß TENSOR_DTYPE: Using parameter dtype {model_dtype}[/blue]\")\n",
    "    except Exception:\n",
    "        # Safe fallback: use bfloat16 for 2B model\n",
    "        pixel_values = pixel_values.to(dtype=torch.bfloat16)\n",
    "        if debug:\n",
    "            rprint(\"[blue]üîß TENSOR_DTYPE: Using bfloat16 (fallback)[/blue]\")\n",
    "    \n",
    "    if debug:\n",
    "        rprint(f\"[blue]üìê TENSOR_SHAPE: {pixel_values.shape} (batch_size={pixel_values.shape[0]} tiles)[/blue]\")\n",
    "        rprint(f\"[blue]üìä TENSOR_DTYPE: {pixel_values.dtype}[/blue]\")\n",
    "\n",
    "    return pixel_values\n",
    "\n",
    "rprint(\"[green]‚úÖ Image preprocessing functions defined[/green]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8feca160",
   "metadata": {},
   "source": [
    "## Load All Prompts\n",
    "\n",
    "Loading prompts for:\n",
    "- Document type detection\n",
    "- Invoice extraction\n",
    "- Receipt extraction\n",
    "- Bank statement extraction (flat and grouped variants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7750b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all prompts from YAML files\n",
    "\n",
    "# Document type detection prompt\n",
    "with open(CONFIG['PROMPT_FILE_DOCTYPE'], 'r') as f:\n",
    "    doctype_data = yaml.safe_load(f)\n",
    "    DOCTYPE_PROMPT = doctype_data['prompts']['detection']['prompt']\n",
    "\n",
    "# Load all InternVL3 prompts from single file\n",
    "with open(CONFIG['PROMPT_FILE_INVOICE'], 'r') as f:\n",
    "    internvl3_data = yaml.safe_load(f)\n",
    "    INVOICE_PROMPT = internvl3_data['prompts']['invoice']['prompt']\n",
    "    RECEIPT_PROMPT = internvl3_data['prompts']['receipt']['prompt']\n",
    "    BANK_PROMPTS = {\n",
    "        'flat': internvl3_data['prompts']['bank_statement_flat']['prompt'],\n",
    "        'date_grouped': internvl3_data['prompts']['bank_statement_date_grouped']['prompt']\n",
    "    }\n",
    "\n",
    "# Bank statement structure classification prompt\n",
    "STRUCTURE_CLASSIFICATION_PROMPT = \"\"\"Look at how dates are displayed in this bank statement's transaction list.\n",
    "\n",
    "Answer with ONLY one word:\n",
    "- FLAT (if dates appear as the FIRST COLUMN in a table row, like: \"05/05/2025 | Purchase | $22.50\")\n",
    "- GROUPED (if dates appear as SECTION HEADERS above transactions, like: \"Thu 05 Sep 2025\" followed by indented transaction details below)\n",
    "\n",
    "The key difference: FLAT has dates IN the table columns, GROUPED has dates AS headers ABOVE the rows.\n",
    "\n",
    "Answer (one word only):\"\"\"\n",
    "\n",
    "rprint(\"[green]‚úÖ All prompts loaded[/green]\")\n",
    "rprint(f\"[cyan]  Document type detection: {len(DOCTYPE_PROMPT)} chars[/cyan]\")\n",
    "rprint(f\"[cyan]  Invoice extraction: {len(INVOICE_PROMPT)} chars[/cyan]\")\n",
    "rprint(f\"[cyan]  Receipt extraction: {len(RECEIPT_PROMPT)} chars[/cyan]\")\n",
    "rprint(f\"[cyan]  Bank flat extraction: {len(BANK_PROMPTS['flat'])} chars[/cyan]\")\n",
    "rprint(f\"[cyan]  Bank grouped extraction: {len(BANK_PROMPTS['date_grouped'])} chars[/cyan]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6a78eb",
   "metadata": {},
   "source": [
    "## Multi-Turn Chat Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1821f915",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_internvl(model, tokenizer, prompt, pixel_values, history=None,\n",
    "                       max_new_tokens=2000, do_sample=False, debug=None):\n",
    "    \"\"\"\n",
    "    Multi-turn chat with InternVL3 using conversation history.\n",
    "\n",
    "    OFFICIAL PATTERN: Uses SAME pixel_values with return_history=True.\n",
    "    Based on: https://internvl.readthedocs.io/en/latest/internvl3.0/quick_start.html\n",
    "\n",
    "    Args:\n",
    "        model: InternVL3 model\n",
    "        tokenizer: InternVL3 tokenizer\n",
    "        prompt: Text prompt for this turn\n",
    "        pixel_values: Preprocessed image tensor (REUSED across turns)\n",
    "        history: Conversation history from previous turn or None\n",
    "        max_new_tokens: Maximum tokens to generate\n",
    "        do_sample: Whether to use sampling\n",
    "        debug: Show debug output (uses CONFIG['VERBOSE'] if None)\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (response, updated_history)\n",
    "    \"\"\"\n",
    "    # Use CONFIG VERBOSE if debug not specified\n",
    "    if debug is None:\n",
    "        debug = CONFIG.get('VERBOSE', False)\n",
    "    \n",
    "    if debug:\n",
    "        rprint(f\"[magenta]üí≠ Generating with max_new_tokens={max_new_tokens}[/magenta]\")\n",
    "        if CONFIG.get('SHOW_PROMPTS', False):\n",
    "            rprint(f\"[yellow]üìù Prompt ({len(prompt)} chars):[/yellow]\")\n",
    "            rprint(\"[dim]\" + \"=\"*80 + \"[/dim]\")\n",
    "            # Show first 500 chars of prompt\n",
    "            preview = prompt[:500] + (\"...\" if len(prompt) > 500 else \"\")\n",
    "            rprint(f\"[dim]{preview}[/dim]\")\n",
    "            rprint(\"[dim]\" + \"=\"*80 + \"[/dim]\")\n",
    "    \n",
    "    # Build generation config\n",
    "    generation_config = {\n",
    "        \"max_new_tokens\": max_new_tokens,\n",
    "        \"temperature\": None if not do_sample else 0.6,\n",
    "        \"do_sample\": do_sample,\n",
    "        \"top_p\": 0.9 if do_sample else None,\n",
    "        \"pad_token_id\": tokenizer.eos_token_id,\n",
    "    }\n",
    "\n",
    "    # OFFICIAL PATTERN: Use model.chat() with return_history=True\n",
    "    response, history = model.chat(\n",
    "        tokenizer,\n",
    "        pixel_values,\n",
    "        prompt,\n",
    "        generation_config=generation_config,\n",
    "        history=history,  # None for first turn, then pass returned history\n",
    "        return_history=True  # CRITICAL: Must be True for multi-turn\n",
    "    )\n",
    "    \n",
    "    if debug:\n",
    "        rprint(f\"[magenta]üìÑ Model response ({len(response)} chars):[/magenta]\")\n",
    "        if CONFIG.get('SHOW_PROMPTS', False):\n",
    "            rprint(\"[dim]\" + \"=\"*80 + \"[/dim]\")\n",
    "            # Show first 500 chars of response\n",
    "            preview = response[:500] + (\"...\" if len(response) > 500 else \"\")\n",
    "            rprint(f\"[dim]{preview}[/dim]\")\n",
    "            rprint(\"[dim]\" + \"=\"*80 + \"[/dim]\")\n",
    "\n",
    "    return response, history\n",
    "\n",
    "rprint(\"[green]‚úÖ Multi-turn chat function defined[/green]\")\n",
    "rprint(\"[cyan]üí° Using official InternVL3 multi-turn pattern (return_history=True)[/cyan]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93ff557",
   "metadata": {},
   "source": [
    "## Parser Functions\n",
    "\n",
    "Functions to parse VLM responses:\n",
    "- Document type classification\n",
    "- Bank statement structure classification\n",
    "- Field extraction parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f0e48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_document_type(response):\n",
    "    \"\"\"Parse document type from VLM response.\"\"\"\n",
    "    response = response.strip().upper()\n",
    "    if \"INVOICE\" in response:\n",
    "        return \"INVOICE\"\n",
    "    elif \"RECEIPT\" in response:\n",
    "        return \"RECEIPT\"\n",
    "    elif \"BANK\" in response or \"STATEMENT\" in response:\n",
    "        return \"BANK_STATEMENT\"\n",
    "    else:\n",
    "        return \"INVOICE\"  # Default fallback\n",
    "\n",
    "def parse_structure_type(response):\n",
    "    \"\"\"Parse bank statement structure type from VLM response.\"\"\"\n",
    "    response = response.strip().upper()\n",
    "    if \"FLAT\" in response:\n",
    "        return \"flat\"\n",
    "    elif \"GROUPED\" in response or \"DATE\" in response:\n",
    "        return \"date_grouped\"\n",
    "    else:\n",
    "        return \"flat\"  # Default fallback\n",
    "\n",
    "def parse_extraction(extraction_text):\n",
    "    \"\"\"Parse extraction text into field dictionary.\"\"\"\n",
    "    extracted_fields = {}\n",
    "\n",
    "    for line in extraction_text.split('\\n'):\n",
    "        line = line.strip()\n",
    "        if ':' in line and not line.startswith('#'):\n",
    "            parts = line.split(':', 1)\n",
    "            if len(parts) == 2:\n",
    "                field_name = parts[0].strip()\n",
    "                field_value = parts[1].strip()\n",
    "                extracted_fields[field_name] = field_value if field_value else 'NOT_FOUND'\n",
    "\n",
    "    return extracted_fields\n",
    "\n",
    "rprint(\"[green]‚úÖ Parser functions defined[/green]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19f5f24",
   "metadata": {},
   "source": [
    "## Discover Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5e08c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discover all images (no filtering by document type)\n",
    "data_dir = Path(CONFIG['DATA_DIR'])\n",
    "image_files = sorted(data_dir.glob(\"*.png\"))\n",
    "\n",
    "rprint(f\"[green]‚úÖ Found {len(image_files)} images to process[/green]\")\n",
    "\n",
    "rprint(\"\\n[bold blue]Images to process:[/bold blue]\")\n",
    "for img in image_files:\n",
    "    rprint(f\"[cyan]  - {img.name}[/cyan]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97869efb",
   "metadata": {},
   "source": [
    "## Multi-Stage Batch Processing\n",
    "\n",
    "**Explicit multi-stage processing** with true multi-turn conversations:\n",
    "- **Stage 0**: Document Type Classification (INVOICE/RECEIPT/BANK_STATEMENT)\n",
    "- **Stage 1**: Structure Classification (for BANK_STATEMENT only: FLAT/GROUPED)\n",
    "- **Stage 2**: Document-Type-Aware Extraction (using appropriate prompt)\n",
    "\n",
    "Each image maintains a conversation history across all stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511dad43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-stage adaptive extraction with TRUE multi-turn chat (official pattern)\n",
    "results = []\n",
    "processing_times = []\n",
    "doctype_counts = {'INVOICE': 0, 'RECEIPT': 0, 'BANK_STATEMENT': 0}\n",
    "structure_counts = {'flat': 0, 'date_grouped': 0}\n",
    "\n",
    "rprint(\"\\n[bold green]üöÄ Starting multi-stage adaptive extraction...[/bold green]\\n\")\n",
    "\n",
    "for idx, image_path in enumerate(track(image_files, description=\"Processing images\"), 1):\n",
    "    image_name = image_path.name\n",
    "\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Load image ONCE for all stages (official pattern)\n",
    "        pixel_values = load_image(str(image_path))\n",
    "\n",
    "        # Initialize conversation history (official pattern: history=None for first turn)\n",
    "        history = None\n",
    "\n",
    "        # ===================================================================\n",
    "        # STAGE 0: Document Type Classification (Turn 1)\n",
    "        # ===================================================================\n",
    "        if CONFIG['VERBOSE']:\n",
    "            rprint(f\"\\n[bold blue]Processing [{idx}/{len(image_files)}]: {image_name}[/bold blue]\")\n",
    "            rprint(\"[dim]Stage 0: Document type detection...[/dim]\")\n",
    "\n",
    "        doctype_answer, history = chat_with_internvl(\n",
    "            model, tokenizer, DOCTYPE_PROMPT, pixel_values, history,\n",
    "            max_new_tokens=CONFIG['MAX_NEW_TOKENS_DOCTYPE']\n",
    "        )\n",
    "\n",
    "        # Parse document type\n",
    "        document_type = parse_document_type(doctype_answer)\n",
    "        doctype_counts[document_type] += 1\n",
    "\n",
    "        # ===================================================================\n",
    "        # STAGE 1: Structure Classification (Turn 2 - only for BANK_STATEMENT)\n",
    "        # ===================================================================\n",
    "        structure_type = \"N/A\"\n",
    "        structure_answer = \"N/A\"\n",
    "\n",
    "        if document_type == \"BANK_STATEMENT\":\n",
    "            if CONFIG['VERBOSE']:\n",
    "                rprint(\"[dim]Stage 1: Bank statement structure classification...[/dim]\")\n",
    "\n",
    "            structure_answer, history = chat_with_internvl(\n",
    "                model, tokenizer, STRUCTURE_CLASSIFICATION_PROMPT, pixel_values, history,\n",
    "                max_new_tokens=CONFIG['MAX_NEW_TOKENS_STRUCTURE']\n",
    "            )\n",
    "\n",
    "            structure_type = parse_structure_type(structure_answer)\n",
    "            structure_counts[structure_type] += 1\n",
    "            extraction_prompt = BANK_PROMPTS[structure_type]\n",
    "            prompt_key = f\"internvl3_bank_statement_{structure_type}\"\n",
    "\n",
    "        elif document_type == \"INVOICE\":\n",
    "            extraction_prompt = INVOICE_PROMPT\n",
    "            prompt_key = \"internvl3_invoice\"\n",
    "\n",
    "        elif document_type == \"RECEIPT\":\n",
    "            extraction_prompt = RECEIPT_PROMPT\n",
    "            prompt_key = \"internvl3_receipt\"\n",
    "\n",
    "        # ===================================================================\n",
    "        # STAGE 2: Document-Type-Aware Extraction (Turn 2/3)\n",
    "        # ===================================================================\n",
    "        if CONFIG['VERBOSE']:\n",
    "            rprint(f\"[dim]Stage 2: Extraction using {prompt_key}...[/dim]\")\n",
    "\n",
    "        extraction_result, history = chat_with_internvl(\n",
    "            model, tokenizer, extraction_prompt, pixel_values, history,\n",
    "            max_new_tokens=CONFIG['MAX_NEW_TOKENS_EXTRACT']\n",
    "        )\n",
    "\n",
    "        # Parse extraction\n",
    "        extracted_fields = parse_extraction(extraction_result)\n",
    "\n",
    "        # Store results\n",
    "        result = {\n",
    "            'image_file': image_name,\n",
    "            'document_type': document_type,\n",
    "            'structure_type': structure_type,\n",
    "            'prompt_used': prompt_key,\n",
    "            'doctype_classification': doctype_answer.strip(),\n",
    "            'structure_classification': structure_answer.strip() if isinstance(structure_answer, str) else structure_answer,\n",
    "            'extraction_raw': extraction_result,\n",
    "            **extracted_fields\n",
    "        }\n",
    "        results.append(result)\n",
    "\n",
    "        processing_time = time.time() - start_time\n",
    "        processing_times.append(processing_time)\n",
    "\n",
    "        structure_display = structure_type if structure_type != 'N/A' else 'direct'\n",
    "        rprint(f\"[green]‚úÖ {image_name}: {document_type} ({structure_display}) - {processing_time:.2f}s[/green]\")\n",
    "\n",
    "    except Exception as e:\n",
    "        error_msg = f\"{type(e).__name__}: {str(e)}\"\n",
    "        rprint(f\"[red]‚ùå {image_name}: Error - {error_msg}[/red]\")\n",
    "        \n",
    "        # Print full traceback for debugging\n",
    "        if CONFIG.get('VERBOSE', True):\n",
    "            rprint(\"[yellow]Full traceback:[/yellow]\")\n",
    "            traceback.print_exc()\n",
    "        \n",
    "        results.append({\n",
    "            'image_file': image_name,\n",
    "            'document_type': 'ERROR',\n",
    "            'structure_type': 'ERROR',\n",
    "            'error': error_msg\n",
    "        })\n",
    "        processing_times.append(0)\n",
    "\n",
    "    finally:\n",
    "        # Memory cleanup after each image\n",
    "        if 'pixel_values' in locals():\n",
    "            del pixel_values\n",
    "\n",
    "        # Clear GPU cache\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # Periodic garbage collection every 3 images\n",
    "        if idx % 3 == 0:\n",
    "            gc.collect()\n",
    "\n",
    "console.rule(\"[bold green]Batch Processing Complete[/bold green]\")\n",
    "\n",
    "# Display summary statistics\n",
    "rprint(f\"\\n[bold blue]üìä Document Type Classification Summary:[/bold blue]\")\n",
    "rprint(f\"[cyan]  Invoices: {doctype_counts['INVOICE']}[/cyan]\")\n",
    "rprint(f\"[cyan]  Receipts: {doctype_counts['RECEIPT']}[/cyan]\")\n",
    "rprint(f\"[cyan]  Bank Statements: {doctype_counts['BANK_STATEMENT']}[/cyan]\")\n",
    "\n",
    "if doctype_counts['BANK_STATEMENT'] > 0:\n",
    "    rprint(f\"\\n[bold blue]üìä Bank Statement Structure Summary:[/bold blue]\")\n",
    "    rprint(f\"[cyan]  Flat table: {structure_counts['flat']}[/cyan]\")\n",
    "    rprint(f\"[cyan]  Date-grouped: {structure_counts['date_grouped']}[/cyan]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415bbcb2",
   "metadata": {},
   "source": [
    "## Save Results (Llama-Compatible Format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b379e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert results to DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Save to CSV (compatible with model_comparison.ipynb)\n",
    "csv_output = csv_dir / f\"internvl3_pure_adaptive_results_{TIMESTAMP}.csv\"\n",
    "df.to_csv(csv_output, index=False)\n",
    "\n",
    "rprint(f\"[green]‚úÖ CSV saved to: {csv_output}[/green]\")\n",
    "rprint(f\"[cyan]  Rows: {len(df)}[/cyan]\")\n",
    "rprint(f\"[cyan]  Columns: {len(df.columns)}[/cyan]\")\n",
    "\n",
    "# Show column names to verify Llama-compatible structure\n",
    "rprint(\"\\n[bold blue]üìã CSV Columns (Llama-compatible):[/bold blue]\")\n",
    "core_cols = ['image_file', 'document_type', 'structure_type', 'prompt_used',\n",
    "             'doctype_classification', 'structure_classification', 'extraction_raw']\n",
    "rprint(f\"[cyan]Core columns: {', '.join(core_cols)}[/cyan]\")\n",
    "field_cols = [col for col in df.columns if col not in core_cols and col != 'error']\n",
    "rprint(f\"[cyan]Field columns ({len(field_cols)}): {', '.join(field_cols[:5])}{'...' if len(field_cols) > 5 else ''}[/cyan]\")\n",
    "\n",
    "# Save detailed JSON results\n",
    "json_output = csv_dir / f\"internvl3_pure_adaptive_results_{TIMESTAMP}.json\"\n",
    "with open(json_output, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "rprint(f\"[green]‚úÖ JSON saved to: {json_output}[/green]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28211dd",
   "metadata": {},
   "source": [
    "## Display Sample Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0721c0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample results\n",
    "console.rule(\"[bold blue]Sample Results[/bold blue]\")\n",
    "\n",
    "display_cols = ['image_file', 'document_type', 'structure_type', 'prompt_used']\n",
    "rprint(df[display_cols].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f684d36",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb01ea06",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìä DOCUMENT-TYPE-AWARE ADAPTIVE EXTRACTION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total images processed: {len(results)}\")\n",
    "print(f\"Successful extractions: {len([r for r in results if 'error' not in r])}\")\n",
    "print(f\"Errors: {len([r for r in results if 'error' in r])}\")\n",
    "\n",
    "print(\"\\nDocument Type Classification:\")\n",
    "print(f\"  Invoices: {doctype_counts['INVOICE']}\")\n",
    "print(f\"  Receipts: {doctype_counts['RECEIPT']}\")\n",
    "print(f\"  Bank Statements: {doctype_counts['BANK_STATEMENT']}\")\n",
    "\n",
    "if doctype_counts['BANK_STATEMENT'] > 0:\n",
    "    print(\"\\nBank Statement Structure Classification:\")\n",
    "    print(f\"  Flat table format: {structure_counts['flat']}\")\n",
    "    print(f\"  Date-grouped format: {structure_counts['date_grouped']}\")\n",
    "\n",
    "print(\"\\nPrompts Used:\")\n",
    "prompt_usage = {}\n",
    "for result in results:\n",
    "    if 'prompt_used' in result:\n",
    "        prompt = result['prompt_used']\n",
    "        prompt_usage[prompt] = prompt_usage.get(prompt, 0) + 1\n",
    "\n",
    "for prompt, count in sorted(prompt_usage.items()):\n",
    "    print(f\"  {prompt}: {count}\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Field extraction statistics\n",
    "if len(df) > 0:\n",
    "    field_cols = [col for col in df.columns if col not in [\n",
    "        'image_file', 'document_type', 'structure_type', 'prompt_used',\n",
    "        'doctype_classification', 'structure_classification', 'extraction_raw', 'error'\n",
    "    ]]\n",
    "\n",
    "    if field_cols:\n",
    "        print(\"\\nüìà Field Extraction Coverage:\")\n",
    "        for field in field_cols:\n",
    "            if field in df.columns:\n",
    "                found_count = df[field].notna().sum()\n",
    "                coverage = (found_count / len(df)) * 100\n",
    "                print(f\"  {field}: {found_count}/{len(df)} ({coverage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3430c9",
   "metadata": {},
   "source": [
    "## View Individual Extraction\n",
    "\n",
    "Change `image_to_view` to view detailed extraction for a specific image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59171298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View detailed extraction for specific image\n",
    "image_to_view = \"image_003.png\"  # Change this\n",
    "\n",
    "result = next((r for r in results if r['image_file'] == image_to_view), None)\n",
    "\n",
    "if result:\n",
    "    print(f\"\\nüîç Detailed Extraction: {image_to_view}\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Document Type: {result['document_type']}\")\n",
    "    print(f\"Structure Type: {result['structure_type']}\")\n",
    "    print(f\"Prompt Used: {result['prompt_used']}\")\n",
    "    print(f\"\\nDocument Type Classification Response:\")\n",
    "    print(result.get('doctype_classification', 'N/A'))\n",
    "    print(f\"\\nStructure Classification Response:\")\n",
    "    print(result.get('structure_classification', 'N/A'))\n",
    "    print(f\"\\nExtraction Result:\")\n",
    "    extraction_display = result.get('extraction_raw', 'N/A')\n",
    "    if len(extraction_display) > 1000:\n",
    "        extraction_display = extraction_display[:1000] + \"\\n...[truncated]...\"\n",
    "    print(extraction_display)\n",
    "    print(\"=\"*80)\n",
    "else:\n",
    "    print(f\"Image {image_to_view} not found in results\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
