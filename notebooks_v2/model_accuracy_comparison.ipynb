{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Accuracy Comparison\n",
    "\n",
    "Compare field-level accuracy between the current model and a competing model across 195 mixed documents (invoices, receipts, bank statements).\n",
    "\n",
    "## Overview\n",
    "- **Current Model**: Results from our evaluation pipeline\n",
    "- **Competing Model**: Field-level accuracy from external CSV\n",
    "- **Schema**: 17 common fields across document types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "from typing import Any\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Plot style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.precision', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# CONFIGURATION - Update these paths before running\n# =============================================================================\n\n# Path to competing model's field-level accuracy CSV\nCOMPETING_MODEL_CSV = Path(\"../output/competing_model_accuracy.csv\")\n\n# Path to current model's results\nCURRENT_MODEL_CSV = Path(\"../output/current_model_accuracy.csv\")\n\n# Model names for display\nCURRENT_MODEL_NAME = \"InternVL3-8B\"\nCOMPETING_MODEL_NAME = \"Competing Model\"\n\n# Output directory for comparison results\nOUTPUT_DIR = Path(\"../output/comparison\")\nOUTPUT_DIR.mkdir(parents=True, exist_ok=True)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SCHEMA FIELDS - 17 fields for comparison\n",
    "# =============================================================================\n",
    "\n",
    "SCHEMA_FIELDS = [\n",
    "    \"DOCUMENT_TYPE\",\n",
    "    \"BUSINESS_ABN\",\n",
    "    \"SUPPLIER_NAME\",\n",
    "    \"BUSINESS_ADDRESS\",\n",
    "    \"PAYER_NAME\",\n",
    "    \"PAYER_ADDRESS\",\n",
    "    \"INVOICE_DATE\",\n",
    "    \"LINE_ITEM_DESCRIPTIONS\",\n",
    "    \"LINE_ITEM_QUANTITIES\",\n",
    "    \"LINE_ITEM_PRICES\",\n",
    "    \"LINE_ITEM_TOTAL_PRICES\",\n",
    "    \"IS_GST_INCLUDED\",\n",
    "    \"GST_AMOUNT\",\n",
    "    \"TOTAL_AMOUNT\",\n",
    "    \"STATEMENT_DATE_RANGE\",\n",
    "    \"TRANSACTION_DATES\",\n",
    "    \"TRANSACTION_AMOUNTS_PAID\",\n",
    "]\n",
    "\n",
    "# Field categories for grouped analysis\n",
    "FIELD_CATEGORIES = {\n",
    "    \"Identity\": [\"DOCUMENT_TYPE\", \"BUSINESS_ABN\", \"SUPPLIER_NAME\"],\n",
    "    \"Address\": [\"BUSINESS_ADDRESS\", \"PAYER_NAME\", \"PAYER_ADDRESS\"],\n",
    "    \"Dates\": [\"INVOICE_DATE\", \"STATEMENT_DATE_RANGE\", \"TRANSACTION_DATES\"],\n",
    "    \"Line Items\": [\"LINE_ITEM_DESCRIPTIONS\", \"LINE_ITEM_QUANTITIES\", \"LINE_ITEM_PRICES\", \"LINE_ITEM_TOTAL_PRICES\"],\n",
    "    \"Financial\": [\"IS_GST_INCLUDED\", \"GST_AMOUNT\", \"TOTAL_AMOUNT\", \"TRANSACTION_AMOUNTS_PAID\"],\n",
    "}\n",
    "\n",
    "# Critical fields (require high accuracy)\n",
    "CRITICAL_FIELDS = [\"BUSINESS_ABN\", \"GST_AMOUNT\", \"TOTAL_AMOUNT\", \"SUPPLIER_NAME\"]\n",
    "\n",
    "print(f\"Schema fields: {len(SCHEMA_FIELDS)}\")\n",
    "print(f\"Field categories: {list(FIELD_CATEGORIES.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_competing_model_data(csv_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load competing model's field-level accuracy from CSV.\n",
    "    \n",
    "    Expected formats:\n",
    "    - Format A: Columns = field names, single row of accuracy values\n",
    "    - Format B: Columns = [Field, Accuracy] with fields as rows\n",
    "    - Format C: Columns include 'field' and 'accuracy' (case-insensitive)\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"Loaded competing model CSV: {csv_path}\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "    \n",
    "    # Detect format and normalize to {field: accuracy} structure\n",
    "    cols_lower = [c.lower() for c in df.columns]\n",
    "    \n",
    "    if 'field' in cols_lower and 'accuracy' in cols_lower:\n",
    "        # Format B/C: Field and Accuracy columns\n",
    "        field_col = df.columns[cols_lower.index('field')]\n",
    "        acc_col = df.columns[cols_lower.index('accuracy')]\n",
    "        result = pd.DataFrame({\n",
    "            'field': df[field_col].str.upper(),\n",
    "            'accuracy': pd.to_numeric(df[acc_col], errors='coerce')\n",
    "        })\n",
    "    elif len(df) == 1:\n",
    "        # Format A: Single row with field names as columns\n",
    "        result = pd.DataFrame({\n",
    "            'field': [c.upper() for c in df.columns],\n",
    "            'accuracy': df.iloc[0].values\n",
    "        })\n",
    "    else:\n",
    "        # Try first column as field names\n",
    "        result = pd.DataFrame({\n",
    "            'field': df.iloc[:, 0].str.upper(),\n",
    "            'accuracy': pd.to_numeric(df.iloc[:, 1], errors='coerce')\n",
    "        })\n",
    "    \n",
    "    # Normalize accuracy to 0-1 scale if needed (detect if 0-100 scale)\n",
    "    if result['accuracy'].max() > 1:\n",
    "        result['accuracy'] = result['accuracy'] / 100.0\n",
    "        print(\"Normalized accuracy from 0-100 to 0-1 scale\")\n",
    "    \n",
    "    return result.set_index('field')['accuracy'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_current_model_data(csv_path: Path) -> dict[str, float]:\n",
    "    \"\"\"\n",
    "    Load current model's field-level accuracy.\n",
    "    \n",
    "    Handles:\n",
    "    - per_field_metrics.csv format (Field, Accuracy columns)\n",
    "    - batch_results.csv format (per-image results, needs aggregation)\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"Loaded current model CSV: {csv_path}\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Columns: {list(df.columns)[:10]}...\" if len(df.columns) > 10 else f\"Columns: {list(df.columns)}\")\n",
    "    \n",
    "    cols_lower = [c.lower() for c in df.columns]\n",
    "    \n",
    "    # Check for per_field_metrics.csv format\n",
    "    if 'field' in cols_lower and 'accuracy' in cols_lower:\n",
    "        field_col = df.columns[cols_lower.index('field')]\n",
    "        acc_col = df.columns[cols_lower.index('accuracy')]\n",
    "        result = dict(zip(df[field_col].str.upper(), df[acc_col]))\n",
    "        return result\n",
    "    \n",
    "    # Check for batch results format (aggregate per-image to field-level)\n",
    "    if 'image_file' in cols_lower or 'image_name' in cols_lower:\n",
    "        print(\"Detected batch results format - aggregating to field-level accuracy\")\n",
    "        field_accuracies = {}\n",
    "        \n",
    "        for field in SCHEMA_FIELDS:\n",
    "            # Look for field column or field_accuracy column\n",
    "            acc_col = f\"{field}_accuracy\"\n",
    "            if acc_col in df.columns:\n",
    "                field_accuracies[field] = df[acc_col].mean()\n",
    "            elif field in df.columns:\n",
    "                # Binary accuracy: check if field has valid (non-empty, non-NOT_FOUND) values\n",
    "                valid = df[field].notna() & (df[field] != 'NOT_FOUND') & (df[field] != '')\n",
    "                field_accuracies[field] = valid.mean()\n",
    "        \n",
    "        return field_accuracies\n",
    "    \n",
    "    raise ValueError(f\"Unable to parse CSV format. Columns: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load both datasets\n",
    "print(\"=\" * 60)\n",
    "print(\"LOADING COMPETING MODEL DATA\")\n",
    "print(\"=\" * 60)\n",
    "competing_accuracy = load_competing_model_data(COMPETING_MODEL_CSV)\n",
    "print(f\"\\nFields loaded: {len(competing_accuracy)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"LOADING CURRENT MODEL DATA\")\n",
    "print(\"=\" * 60)\n",
    "current_accuracy = load_current_model_data(CURRENT_MODEL_CSV)\n",
    "print(f\"\\nFields loaded: {len(current_accuracy)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Data Alignment & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_field_data(\n",
    "    current: dict[str, float], \n",
    "    competing: dict[str, float], \n",
    "    schema_fields: list[str]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Align field-level accuracy data from both models into a comparison DataFrame.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    \n",
    "    for field in schema_fields:\n",
    "        current_acc = current.get(field, np.nan)\n",
    "        competing_acc = competing.get(field, np.nan)\n",
    "        \n",
    "        # Find category for this field\n",
    "        category = \"Other\"\n",
    "        for cat, fields in FIELD_CATEGORIES.items():\n",
    "            if field in fields:\n",
    "                category = cat\n",
    "                break\n",
    "        \n",
    "        data.append({\n",
    "            'field': field,\n",
    "            'category': category,\n",
    "            'current_accuracy': current_acc,\n",
    "            'competing_accuracy': competing_acc,\n",
    "            'difference': current_acc - competing_acc if not (np.isnan(current_acc) or np.isnan(competing_acc)) else np.nan,\n",
    "            'is_critical': field in CRITICAL_FIELDS,\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create aligned comparison DataFrame\n",
    "comparison_df = align_field_data(current_accuracy, competing_accuracy, SCHEMA_FIELDS)\n",
    "\n",
    "print(\"Field Alignment Summary\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total schema fields: {len(SCHEMA_FIELDS)}\")\n",
    "print(f\"Current model fields available: {comparison_df['current_accuracy'].notna().sum()}\")\n",
    "print(f\"Competing model fields available: {comparison_df['competing_accuracy'].notna().sum()}\")\n",
    "print(f\"Fields with both models: {(comparison_df['current_accuracy'].notna() & comparison_df['competing_accuracy'].notna()).sum()}\")\n",
    "\n",
    "# Show missing fields\n",
    "missing_current = comparison_df[comparison_df['current_accuracy'].isna()]['field'].tolist()\n",
    "missing_competing = comparison_df[comparison_df['competing_accuracy'].isna()]['field'].tolist()\n",
    "\n",
    "if missing_current:\n",
    "    print(f\"\\nMissing from current model: {missing_current}\")\n",
    "if missing_competing:\n",
    "    print(f\"Missing from competing model: {missing_competing}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the comparison table\n",
    "display_df = comparison_df.copy()\n",
    "display_df['current_accuracy'] = display_df['current_accuracy'].apply(lambda x: f\"{x:.1%}\" if pd.notna(x) else \"N/A\")\n",
    "display_df['competing_accuracy'] = display_df['competing_accuracy'].apply(lambda x: f\"{x:.1%}\" if pd.notna(x) else \"N/A\")\n",
    "display_df['difference'] = display_df['difference'].apply(lambda x: f\"{x:+.1%}\" if pd.notna(x) else \"N/A\")\n",
    "\n",
    "print(\"\\nField-Level Accuracy Comparison\")\n",
    "print(\"=\" * 80)\n",
    "print(display_df[['field', 'category', 'current_accuracy', 'competing_accuracy', 'difference', 'is_critical']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Aggregate Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_aggregate_metrics(df: pd.DataFrame) -> dict[str, Any]:\n",
    "    \"\"\"Calculate aggregate comparison metrics.\"\"\"\n",
    "    \n",
    "    # Filter to fields with both values\n",
    "    valid_df = df.dropna(subset=['current_accuracy', 'competing_accuracy'])\n",
    "    \n",
    "    metrics = {\n",
    "        'n_fields_compared': len(valid_df),\n",
    "        \n",
    "        # Current model metrics\n",
    "        'current_mean': valid_df['current_accuracy'].mean(),\n",
    "        'current_median': valid_df['current_accuracy'].median(),\n",
    "        'current_std': valid_df['current_accuracy'].std(),\n",
    "        'current_min': valid_df['current_accuracy'].min(),\n",
    "        'current_max': valid_df['current_accuracy'].max(),\n",
    "        \n",
    "        # Competing model metrics\n",
    "        'competing_mean': valid_df['competing_accuracy'].mean(),\n",
    "        'competing_median': valid_df['competing_accuracy'].median(),\n",
    "        'competing_std': valid_df['competing_accuracy'].std(),\n",
    "        'competing_min': valid_df['competing_accuracy'].min(),\n",
    "        'competing_max': valid_df['competing_accuracy'].max(),\n",
    "        \n",
    "        # Comparison metrics\n",
    "        'mean_difference': valid_df['difference'].mean(),\n",
    "        'fields_current_better': (valid_df['difference'] > 0).sum(),\n",
    "        'fields_competing_better': (valid_df['difference'] < 0).sum(),\n",
    "        'fields_equal': (valid_df['difference'] == 0).sum(),\n",
    "        \n",
    "        # Critical fields\n",
    "        'critical_current_mean': valid_df[valid_df['is_critical']]['current_accuracy'].mean(),\n",
    "        'critical_competing_mean': valid_df[valid_df['is_critical']]['competing_accuracy'].mean(),\n",
    "    }\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = calculate_aggregate_metrics(comparison_df)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"AGGREGATE METRICS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nFields Compared: {metrics['n_fields_compared']}\")\n",
    "\n",
    "print(f\"\\n{CURRENT_MODEL_NAME}:\")\n",
    "print(f\"  Mean Accuracy:   {metrics['current_mean']:.1%}\")\n",
    "print(f\"  Median Accuracy: {metrics['current_median']:.1%}\")\n",
    "print(f\"  Std Dev:         {metrics['current_std']:.1%}\")\n",
    "print(f\"  Range:           {metrics['current_min']:.1%} - {metrics['current_max']:.1%}\")\n",
    "\n",
    "print(f\"\\n{COMPETING_MODEL_NAME}:\")\n",
    "print(f\"  Mean Accuracy:   {metrics['competing_mean']:.1%}\")\n",
    "print(f\"  Median Accuracy: {metrics['competing_median']:.1%}\")\n",
    "print(f\"  Std Dev:         {metrics['competing_std']:.1%}\")\n",
    "print(f\"  Range:           {metrics['competing_min']:.1%} - {metrics['competing_max']:.1%}\")\n",
    "\n",
    "print(f\"\\nComparison:\")\n",
    "print(f\"  Mean Difference: {metrics['mean_difference']:+.1%} (positive = current better)\")\n",
    "print(f\"  Fields where {CURRENT_MODEL_NAME} better: {metrics['fields_current_better']}\")\n",
    "print(f\"  Fields where {COMPETING_MODEL_NAME} better: {metrics['fields_competing_better']}\")\n",
    "print(f\"  Fields equal: {metrics['fields_equal']}\")\n",
    "\n",
    "print(f\"\\nCritical Fields (ABN, GST, Total, Supplier):\")\n",
    "print(f\"  {CURRENT_MODEL_NAME} Mean:   {metrics['critical_current_mean']:.1%}\")\n",
    "print(f\"  {COMPETING_MODEL_NAME} Mean: {metrics['critical_competing_mean']:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Statistical Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_statistical_tests(df: pd.DataFrame) -> dict[str, Any]:\n",
    "    \"\"\"Run statistical significance tests on accuracy differences.\"\"\"\n",
    "    \n",
    "    valid_df = df.dropna(subset=['current_accuracy', 'competing_accuracy'])\n",
    "    current = valid_df['current_accuracy'].values\n",
    "    competing = valid_df['competing_accuracy'].values\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Paired t-test (assumes normality)\n",
    "    t_stat, t_pvalue = stats.ttest_rel(current, competing)\n",
    "    results['paired_ttest'] = {'statistic': t_stat, 'pvalue': t_pvalue}\n",
    "    \n",
    "    # Wilcoxon signed-rank test (non-parametric alternative)\n",
    "    # Only works if there are non-zero differences\n",
    "    differences = current - competing\n",
    "    non_zero_diff = differences[differences != 0]\n",
    "    if len(non_zero_diff) > 0:\n",
    "        w_stat, w_pvalue = stats.wilcoxon(non_zero_diff)\n",
    "        results['wilcoxon'] = {'statistic': w_stat, 'pvalue': w_pvalue}\n",
    "    else:\n",
    "        results['wilcoxon'] = {'statistic': np.nan, 'pvalue': 1.0}\n",
    "    \n",
    "    # Effect size (Cohen's d)\n",
    "    pooled_std = np.sqrt((current.std()**2 + competing.std()**2) / 2)\n",
    "    cohens_d = (current.mean() - competing.mean()) / pooled_std if pooled_std > 0 else 0\n",
    "    results['cohens_d'] = cohens_d\n",
    "    \n",
    "    # Bootstrap confidence interval for mean difference\n",
    "    n_bootstrap = 10000\n",
    "    bootstrap_diffs = []\n",
    "    n = len(current)\n",
    "    for _ in range(n_bootstrap):\n",
    "        idx = np.random.choice(n, n, replace=True)\n",
    "        boot_diff = current[idx].mean() - competing[idx].mean()\n",
    "        bootstrap_diffs.append(boot_diff)\n",
    "    \n",
    "    ci_95 = np.percentile(bootstrap_diffs, [2.5, 97.5])\n",
    "    results['bootstrap_ci_95'] = ci_95\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_results = run_statistical_tests(comparison_df)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STATISTICAL SIGNIFICANCE TESTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nPaired t-test:\")\n",
    "print(f\"  t-statistic: {stat_results['paired_ttest']['statistic']:.4f}\")\n",
    "print(f\"  p-value:     {stat_results['paired_ttest']['pvalue']:.4f}\")\n",
    "sig_t = \"YES\" if stat_results['paired_ttest']['pvalue'] < 0.05 else \"NO\"\n",
    "print(f\"  Significant at α=0.05: {sig_t}\")\n",
    "\n",
    "print(f\"\\nWilcoxon signed-rank test (non-parametric):\")\n",
    "print(f\"  W-statistic: {stat_results['wilcoxon']['statistic']:.4f}\")\n",
    "print(f\"  p-value:     {stat_results['wilcoxon']['pvalue']:.4f}\")\n",
    "sig_w = \"YES\" if stat_results['wilcoxon']['pvalue'] < 0.05 else \"NO\"\n",
    "print(f\"  Significant at α=0.05: {sig_w}\")\n",
    "\n",
    "print(f\"\\nEffect Size:\")\n",
    "print(f\"  Cohen's d: {stat_results['cohens_d']:.4f}\")\n",
    "effect_interp = \"negligible\" if abs(stat_results['cohens_d']) < 0.2 else \\\n",
    "                \"small\" if abs(stat_results['cohens_d']) < 0.5 else \\\n",
    "                \"medium\" if abs(stat_results['cohens_d']) < 0.8 else \"large\"\n",
    "print(f\"  Interpretation: {effect_interp}\")\n",
    "\n",
    "print(f\"\\n95% Bootstrap Confidence Interval for Mean Difference:\")\n",
    "print(f\"  [{stat_results['bootstrap_ci_95'][0]:+.1%}, {stat_results['bootstrap_ci_95'][1]:+.1%}]\")\n",
    "contains_zero = stat_results['bootstrap_ci_95'][0] <= 0 <= stat_results['bootstrap_ci_95'][1]\n",
    "print(f\"  Contains zero: {'YES (not significant)' if contains_zero else 'NO (significant)'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for plotting\n",
    "plot_df = comparison_df.dropna(subset=['current_accuracy', 'competing_accuracy']).copy()\n",
    "plot_df = plot_df.sort_values('current_accuracy', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 1: Side-by-side bar chart\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "y_pos = np.arange(len(plot_df))\n",
    "bar_height = 0.35\n",
    "\n",
    "bars1 = ax.barh(y_pos - bar_height/2, plot_df['current_accuracy'], bar_height, \n",
    "                label=CURRENT_MODEL_NAME, color='#2ecc71', alpha=0.8)\n",
    "bars2 = ax.barh(y_pos + bar_height/2, plot_df['competing_accuracy'], bar_height,\n",
    "                label=COMPETING_MODEL_NAME, color='#3498db', alpha=0.8)\n",
    "\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(plot_df['field'])\n",
    "ax.set_xlabel('Accuracy', fontsize=12)\n",
    "ax.set_title('Field-Level Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='lower right')\n",
    "ax.set_xlim(0, 1.05)\n",
    "ax.axvline(x=0.9, color='red', linestyle='--', alpha=0.5, label='90% threshold')\n",
    "\n",
    "# Add value labels\n",
    "for bar, val in zip(bars1, plot_df['current_accuracy']):\n",
    "    ax.text(val + 0.01, bar.get_y() + bar.get_height()/2, f'{val:.0%}', \n",
    "            va='center', fontsize=8)\n",
    "for bar, val in zip(bars2, plot_df['competing_accuracy']):\n",
    "    ax.text(val + 0.01, bar.get_y() + bar.get_height()/2, f'{val:.0%}', \n",
    "            va='center', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'accuracy_comparison_bars.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 2: Difference chart (lollipop plot)\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Sort by difference\n",
    "diff_df = plot_df.sort_values('difference')\n",
    "y_pos = np.arange(len(diff_df))\n",
    "\n",
    "colors = ['#2ecc71' if d >= 0 else '#e74c3c' for d in diff_df['difference']]\n",
    "\n",
    "ax.hlines(y=y_pos, xmin=0, xmax=diff_df['difference'], color=colors, linewidth=2)\n",
    "ax.scatter(diff_df['difference'], y_pos, color=colors, s=100, zorder=3)\n",
    "\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(diff_df['field'])\n",
    "ax.axvline(x=0, color='black', linewidth=1)\n",
    "ax.set_xlabel('Accuracy Difference (positive = current model better)', fontsize=12)\n",
    "ax.set_title(f'Accuracy Difference: {CURRENT_MODEL_NAME} vs {COMPETING_MODEL_NAME}', \n",
    "             fontsize=14, fontweight='bold')\n",
    "\n",
    "# Add value labels\n",
    "for y, diff in zip(y_pos, diff_df['difference']):\n",
    "    ax.text(diff + 0.01 if diff >= 0 else diff - 0.01, y, f'{diff:+.1%}',\n",
    "            va='center', ha='left' if diff >= 0 else 'right', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'accuracy_difference_lollipop.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 3: Radar/Spider chart by category\n",
    "def plot_radar_chart(df: pd.DataFrame, categories: dict, ax=None):\n",
    "    \"\"\"Plot radar chart comparing models by field category.\"\"\"\n",
    "    \n",
    "    # Calculate category averages\n",
    "    cat_data = []\n",
    "    for cat, fields in categories.items():\n",
    "        cat_df = df[df['field'].isin(fields)]\n",
    "        if len(cat_df) > 0:\n",
    "            cat_data.append({\n",
    "                'category': cat,\n",
    "                'current': cat_df['current_accuracy'].mean(),\n",
    "                'competing': cat_df['competing_accuracy'].mean()\n",
    "            })\n",
    "    \n",
    "    cat_df = pd.DataFrame(cat_data)\n",
    "    \n",
    "    # Radar chart setup\n",
    "    categories_list = cat_df['category'].tolist()\n",
    "    N = len(categories_list)\n",
    "    \n",
    "    angles = [n / float(N) * 2 * np.pi for n in range(N)]\n",
    "    angles += angles[:1]  # Complete the loop\n",
    "    \n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(polar=True))\n",
    "    \n",
    "    # Plot current model\n",
    "    values = cat_df['current'].tolist()\n",
    "    values += values[:1]\n",
    "    ax.plot(angles, values, 'o-', linewidth=2, label=CURRENT_MODEL_NAME, color='#2ecc71')\n",
    "    ax.fill(angles, values, alpha=0.25, color='#2ecc71')\n",
    "    \n",
    "    # Plot competing model\n",
    "    values = cat_df['competing'].tolist()\n",
    "    values += values[:1]\n",
    "    ax.plot(angles, values, 'o-', linewidth=2, label=COMPETING_MODEL_NAME, color='#3498db')\n",
    "    ax.fill(angles, values, alpha=0.25, color='#3498db')\n",
    "    \n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(categories_list)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
    "    \n",
    "    return ax\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8), subplot_kw=dict(polar=True))\n",
    "plot_radar_chart(comparison_df, FIELD_CATEGORIES, ax)\n",
    "plt.title('Accuracy by Field Category', fontsize=14, fontweight='bold', y=1.08)\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'accuracy_radar_chart.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 4: Heatmap of accuracy values\n",
    "fig, ax = plt.subplots(figsize=(6, 10))\n",
    "\n",
    "heatmap_data = plot_df[['current_accuracy', 'competing_accuracy']].copy()\n",
    "heatmap_data.index = plot_df['field']\n",
    "heatmap_data.columns = [CURRENT_MODEL_NAME, COMPETING_MODEL_NAME]\n",
    "\n",
    "sns.heatmap(heatmap_data, annot=True, fmt='.0%', cmap='RdYlGn', \n",
    "            vmin=0, vmax=1, ax=ax, cbar_kws={'label': 'Accuracy'})\n",
    "ax.set_title('Field-Level Accuracy Heatmap', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'accuracy_heatmap.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 5: Box plot comparison\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "box_data = pd.DataFrame({\n",
    "    CURRENT_MODEL_NAME: plot_df['current_accuracy'],\n",
    "    COMPETING_MODEL_NAME: plot_df['competing_accuracy']\n",
    "})\n",
    "\n",
    "box_data.boxplot(ax=ax, patch_artist=True,\n",
    "                  boxprops=dict(facecolor='lightblue', alpha=0.7),\n",
    "                  medianprops=dict(color='red', linewidth=2))\n",
    "\n",
    "# Add individual points\n",
    "for i, col in enumerate(box_data.columns, 1):\n",
    "    y = box_data[col].dropna()\n",
    "    x = np.random.normal(i, 0.04, size=len(y))\n",
    "    ax.scatter(x, y, alpha=0.6, s=50)\n",
    "\n",
    "ax.set_ylabel('Accuracy', fontsize=12)\n",
    "ax.set_title('Distribution of Field-Level Accuracy', fontsize=14, fontweight='bold')\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.axhline(y=0.9, color='red', linestyle='--', alpha=0.5, label='90% threshold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'accuracy_boxplot.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 6: Scatter plot with identity line\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "# Plot points\n",
    "colors = ['red' if c else 'blue' for c in plot_df['is_critical']]\n",
    "ax.scatter(plot_df['competing_accuracy'], plot_df['current_accuracy'], \n",
    "           c=colors, s=100, alpha=0.7)\n",
    "\n",
    "# Add field labels\n",
    "for _, row in plot_df.iterrows():\n",
    "    ax.annotate(row['field'][:12], (row['competing_accuracy'], row['current_accuracy']),\n",
    "                textcoords=\"offset points\", xytext=(5, 5), fontsize=8)\n",
    "\n",
    "# Identity line\n",
    "ax.plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Equal accuracy')\n",
    "\n",
    "# Fill regions\n",
    "ax.fill_between([0, 1], [0, 1], [1, 1], alpha=0.1, color='green', label=f'{CURRENT_MODEL_NAME} better')\n",
    "ax.fill_between([0, 1], [0, 0], [0, 1], alpha=0.1, color='blue', label=f'{COMPETING_MODEL_NAME} better')\n",
    "\n",
    "ax.set_xlabel(f'{COMPETING_MODEL_NAME} Accuracy', fontsize=12)\n",
    "ax.set_ylabel(f'{CURRENT_MODEL_NAME} Accuracy', fontsize=12)\n",
    "ax.set_title('Model Accuracy Comparison (red = critical fields)', fontsize=14, fontweight='bold')\n",
    "ax.set_xlim(0, 1.05)\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.legend(loc='lower right')\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'accuracy_scatter.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Category-Level Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_by_category(df: pd.DataFrame, categories: dict) -> pd.DataFrame:\n",
    "    \"\"\"Analyze accuracy by field category.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for cat, fields in categories.items():\n",
    "        cat_df = df[df['field'].isin(fields)].dropna(subset=['current_accuracy', 'competing_accuracy'])\n",
    "        \n",
    "        if len(cat_df) > 0:\n",
    "            results.append({\n",
    "                'category': cat,\n",
    "                'n_fields': len(cat_df),\n",
    "                'current_mean': cat_df['current_accuracy'].mean(),\n",
    "                'competing_mean': cat_df['competing_accuracy'].mean(),\n",
    "                'difference': cat_df['difference'].mean(),\n",
    "                'current_wins': (cat_df['difference'] > 0).sum(),\n",
    "                'competing_wins': (cat_df['difference'] < 0).sum(),\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_analysis = analyze_by_category(comparison_df, FIELD_CATEGORIES)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CATEGORY-LEVEL ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for _, row in category_analysis.iterrows():\n",
    "    print(f\"\\n{row['category']} ({row['n_fields']} fields):\")\n",
    "    print(f\"  {CURRENT_MODEL_NAME}:   {row['current_mean']:.1%}\")\n",
    "    print(f\"  {COMPETING_MODEL_NAME}: {row['competing_mean']:.1%}\")\n",
    "    print(f\"  Difference: {row['difference']:+.1%}\")\n",
    "    print(f\"  Winner breakdown: {CURRENT_MODEL_NAME}={row['current_wins']}, {COMPETING_MODEL_NAME}={row['competing_wins']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Category comparison bar chart\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "x = np.arange(len(category_analysis))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, category_analysis['current_mean'], width, \n",
    "               label=CURRENT_MODEL_NAME, color='#2ecc71', alpha=0.8)\n",
    "bars2 = ax.bar(x + width/2, category_analysis['competing_mean'], width,\n",
    "               label=COMPETING_MODEL_NAME, color='#3498db', alpha=0.8)\n",
    "\n",
    "ax.set_ylabel('Mean Accuracy', fontsize=12)\n",
    "ax.set_title('Accuracy by Field Category', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(category_analysis['category'], rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1.1)\n",
    "ax.axhline(y=0.9, color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars1:\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "            f'{bar.get_height():.0%}', ha='center', va='bottom', fontsize=9)\n",
    "for bar in bars2:\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "            f'{bar.get_height():.0%}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'accuracy_by_category.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export comparison DataFrame\n",
    "comparison_df.to_csv(OUTPUT_DIR / 'field_comparison_results.csv', index=False)\n",
    "print(f\"Saved: {OUTPUT_DIR / 'field_comparison_results.csv'}\")\n",
    "\n",
    "# Export category analysis\n",
    "category_analysis.to_csv(OUTPUT_DIR / 'category_comparison_results.csv', index=False)\n",
    "print(f\"Saved: {OUTPUT_DIR / 'category_comparison_results.csv'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary report\n",
    "report = f\"\"\"# Model Accuracy Comparison Report\n",
    "\n",
    "## Overview\n",
    "- **Current Model**: {CURRENT_MODEL_NAME}\n",
    "- **Competing Model**: {COMPETING_MODEL_NAME}\n",
    "- **Fields Compared**: {metrics['n_fields_compared']}\n",
    "- **Document Types**: Invoices, Receipts, Bank Statements (195 documents)\n",
    "\n",
    "## Summary Statistics\n",
    "\n",
    "| Metric | {CURRENT_MODEL_NAME} | {COMPETING_MODEL_NAME} |\n",
    "|--------|-----------|----------|\n",
    "| Mean Accuracy | {metrics['current_mean']:.1%} | {metrics['competing_mean']:.1%} |\n",
    "| Median Accuracy | {metrics['current_median']:.1%} | {metrics['competing_median']:.1%} |\n",
    "| Std Dev | {metrics['current_std']:.1%} | {metrics['competing_std']:.1%} |\n",
    "| Min | {metrics['current_min']:.1%} | {metrics['competing_min']:.1%} |\n",
    "| Max | {metrics['current_max']:.1%} | {metrics['competing_max']:.1%} |\n",
    "\n",
    "## Comparison Results\n",
    "\n",
    "- **Mean Difference**: {metrics['mean_difference']:+.1%} (positive = {CURRENT_MODEL_NAME} better)\n",
    "- **Fields where {CURRENT_MODEL_NAME} better**: {metrics['fields_current_better']}\n",
    "- **Fields where {COMPETING_MODEL_NAME} better**: {metrics['fields_competing_better']}\n",
    "- **Fields equal**: {metrics['fields_equal']}\n",
    "\n",
    "## Critical Fields Analysis\n",
    "\n",
    "Critical fields: {', '.join(CRITICAL_FIELDS)}\n",
    "\n",
    "- **{CURRENT_MODEL_NAME} Mean**: {metrics['critical_current_mean']:.1%}\n",
    "- **{COMPETING_MODEL_NAME} Mean**: {metrics['critical_competing_mean']:.1%}\n",
    "\n",
    "## Statistical Significance\n",
    "\n",
    "- **Paired t-test p-value**: {stat_results['paired_ttest']['pvalue']:.4f} ({'Significant' if stat_results['paired_ttest']['pvalue'] < 0.05 else 'Not significant'} at α=0.05)\n",
    "- **Wilcoxon test p-value**: {stat_results['wilcoxon']['pvalue']:.4f} ({'Significant' if stat_results['wilcoxon']['pvalue'] < 0.05 else 'Not significant'} at α=0.05)\n",
    "- **Cohen's d effect size**: {stat_results['cohens_d']:.4f} ({effect_interp})\n",
    "- **95% CI for difference**: [{stat_results['bootstrap_ci_95'][0]:+.1%}, {stat_results['bootstrap_ci_95'][1]:+.1%}]\n",
    "\n",
    "## Field-Level Details\n",
    "\n",
    "| Field | {CURRENT_MODEL_NAME} | {COMPETING_MODEL_NAME} | Difference |\n",
    "|-------|-----------|----------|------------|\n",
    "\"\"\"\n",
    "\n",
    "for _, row in comparison_df.iterrows():\n",
    "    curr = f\"{row['current_accuracy']:.1%}\" if pd.notna(row['current_accuracy']) else \"N/A\"\n",
    "    comp = f\"{row['competing_accuracy']:.1%}\" if pd.notna(row['competing_accuracy']) else \"N/A\"\n",
    "    diff = f\"{row['difference']:+.1%}\" if pd.notna(row['difference']) else \"N/A\"\n",
    "    critical = \" ⚠️\" if row['is_critical'] else \"\"\n",
    "    report += f\"| {row['field']}{critical} | {curr} | {comp} | {diff} |\\n\"\n",
    "\n",
    "report += f\"\"\"\n",
    "## Output Files\n",
    "\n",
    "- `field_comparison_results.csv` - Detailed field-level comparison\n",
    "- `category_comparison_results.csv` - Category-level aggregation\n",
    "- `accuracy_comparison_bars.png` - Side-by-side bar chart\n",
    "- `accuracy_difference_lollipop.png` - Difference visualization\n",
    "- `accuracy_radar_chart.png` - Category radar chart\n",
    "- `accuracy_heatmap.png` - Heatmap comparison\n",
    "- `accuracy_boxplot.png` - Distribution comparison\n",
    "- `accuracy_scatter.png` - Scatter plot with identity line\n",
    "\"\"\"\n",
    "\n",
    "# Save report\n",
    "report_path = OUTPUT_DIR / 'comparison_report.md'\n",
    "with open(report_path, 'w') as f:\n",
    "    f.write(report)\n",
    "\n",
    "print(f\"Saved: {report_path}\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"COMPARISON COMPLETE\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Quick Reference: Data Format Requirements\n",
    "\n",
    "### Competing Model CSV Format\n",
    "\n",
    "The notebook accepts multiple CSV formats:\n",
    "\n",
    "**Format A - Single row with field columns:**\n",
    "```csv\n",
    "DOCUMENT_TYPE,BUSINESS_ABN,SUPPLIER_NAME,...\n",
    "0.95,0.87,0.92,...\n",
    "```\n",
    "\n",
    "**Format B - Field and Accuracy columns:**\n",
    "```csv\n",
    "Field,Accuracy\n",
    "DOCUMENT_TYPE,95.0\n",
    "BUSINESS_ABN,87.0\n",
    "...\n",
    "```\n",
    "\n",
    "**Format C - Any column names (case-insensitive):**\n",
    "```csv\n",
    "field_name,accuracy_score\n",
    "document_type,0.95\n",
    "...\n",
    "```\n",
    "\n",
    "### Current Model Data Sources\n",
    "\n",
    "1. **per_field_metrics.csv** - Direct field-level accuracy from evaluation pipeline\n",
    "2. **batch_results_*.csv** - Per-image results (will be aggregated automatically)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}