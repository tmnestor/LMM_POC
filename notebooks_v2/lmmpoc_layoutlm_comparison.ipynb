{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model F1 Comparison\n",
    "\n",
    "Compare field-level F1 between the current model and a competing model across 195 mixed documents (invoices, receipts, bank statements).\n",
    "\n",
    "## Overview\n",
    "- **Current Model**: Results from our evaluation pipeline\n",
    "- **Competing Model**: Field-level accuracy from external CSV\n",
    "- **Schema**: 17 common fields across document types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "from typing import Any\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Plot style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.precision', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION - Update these paths before running\n",
    "# =============================================================================\n",
    "\n",
    "# Path to competing model's field-level accuracy CSV\n",
    "COMPETING_MODEL_CSV = Path(\"/home/jovyan/shared_annotations/output/competing_model_accuracy.csv\")\n",
    "\n",
    "# Path to current model's results\n",
    "CURRENT_MODEL_CSV = Path(\"/home/jovyan/shared_annotations/output/current_model_accuracy.csv\")\n",
    "\n",
    "# Model names for display\n",
    "# CURRENT_MODEL_NAME = \"InternVL3-8B\"\n",
    "# CURRENT_MODEL_NAME = \"InternVL3-2B\"\n",
    "CURRENT_MODEL_NAME = \"InternVL3.5-8B\"\n",
    "# CURRENT_MODEL_NAME = \"Llama-3.2-11B\"\n",
    "# CURRENT_MODEL_NAME = \"Llama-4-Scout\"\n",
    "COMPETING_MODEL_NAME = \"LayoutLM\"\n",
    "\n",
    "# Output directory for comparison results\n",
    "OUTPUT_DIR = Path(\"/home/jovyan/shared_annotations/output/comparison\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "# SVG exports directory\n",
    "EXPORTS_DIR = Path('/home/jovyan/LMM_POC-main/notebooks_v2/exports')\n",
    "EXPORTS_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SCHEMA FIELDS - 17 fields for comparison\n",
    "# =============================================================================\n",
    "\n",
    "SCHEMA_FIELDS = [\n",
    "    \"DOCUMENT_TYPE\",\n",
    "    \"BUSINESS_ABN\",\n",
    "    \"SUPPLIER_NAME\",\n",
    "    \"BUSINESS_ADDRESS\",\n",
    "    \"PAYER_NAME\",\n",
    "    \"PAYER_ADDRESS\",\n",
    "    \"INVOICE_DATE\",\n",
    "    \"LINE_ITEM_DESCRIPTIONS\",\n",
    "    \"LINE_ITEM_QUANTITIES\",\n",
    "    \"LINE_ITEM_PRICES\",\n",
    "    \"LINE_ITEM_TOTAL_PRICES\",\n",
    "    \"IS_GST_INCLUDED\",\n",
    "    \"GST_AMOUNT\",\n",
    "    \"TOTAL_AMOUNT\",\n",
    "    \"STATEMENT_DATE_RANGE\",\n",
    "    \"TRANSACTION_DATES\",\n",
    "    \"TRANSACTION_AMOUNTS_PAID\",\n",
    "]\n",
    "\n",
    "# Field categories for grouped analysis\n",
    "FIELD_CATEGORIES = {\n",
    "    \"Identity\": [\"DOCUMENT_TYPE\", \"BUSINESS_ABN\", \"SUPPLIER_NAME\"],\n",
    "    \"Address\": [\"BUSINESS_ADDRESS\", \"PAYER_NAME\", \"PAYER_ADDRESS\"],\n",
    "    \"Dates\": [\"INVOICE_DATE\", \"STATEMENT_DATE_RANGE\", \"TRANSACTION_DATES\"],\n",
    "    \"Line Items\": [\"LINE_ITEM_DESCRIPTIONS\", \"LINE_ITEM_QUANTITIES\", \"LINE_ITEM_PRICES\", \"LINE_ITEM_TOTAL_PRICES\"],\n",
    "    \"Financial\": [\"IS_GST_INCLUDED\", \"GST_AMOUNT\", \"TOTAL_AMOUNT\", \"TRANSACTION_AMOUNTS_PAID\"],\n",
    "}\n",
    "\n",
    "# Critical fields (require high accuracy)\n",
    "CRITICAL_FIELDS = [\"BUSINESS_ABN\", \"GST_AMOUNT\", \"TOTAL_AMOUNT\", \"SUPPLIER_NAME\"]\n",
    "\n",
    "# =============================================================================\n",
    "# EXCLUSIVE FIELDS - Capabilities only our model has (not in competing model)\n",
    "# =============================================================================\n",
    "# These fields are EXCLUDED from direct comparison metrics but shown separately\n",
    "# as \"Additional Capabilities\" to highlight competitive advantage\n",
    "EXCLUSIVE_FIELDS = [\"DOCUMENT_TYPE\", \"STATEMENT_DATE_RANGE\"]\n",
    "\n",
    "print(f\"Schema fields: {len(SCHEMA_FIELDS)}\")\n",
    "print(f\"Exclusive fields (our model only): {EXCLUSIVE_FIELDS}\")\n",
    "print(f\"Comparable fields: {len(SCHEMA_FIELDS) - len(EXCLUSIVE_FIELDS)}\")\n",
    "print(f\"Field categories: {list(FIELD_CATEGORIES.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_competing_model_data(csv_path: Path) -> tuple[dict, dict]:\n",
    "    \"\"\"\n",
    "    Load competing model's field-level F1 scores from CSV.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (f1_dict, precision_dict) - mapping field names to values\n",
    "    \n",
    "    Expected columns: Subset, Field, F1\n",
    "    For backward compatibility, also checks for 'Accuracy' column.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"Loaded competing model CSV: {csv_path}\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "    \n",
    "    cols_lower = [c.lower() for c in df.columns]\n",
    "    \n",
    "    f1_dict = {}\n",
    "    precision_dict = {}\n",
    "    \n",
    "    if 'field' not in cols_lower:\n",
    "        raise ValueError(f\"No 'Field' column found. Columns: {list(df.columns)}\")\n",
    "    \n",
    "    field_col = df.columns[cols_lower.index('field')]\n",
    "    \n",
    "    # Look for F1 column (primary metric)\n",
    "    f1_col_name = None\n",
    "    for possible in ['f1', 'f1_score', 'f1-score', 'f1_agnostic', 'accuracy']:\n",
    "        if possible in cols_lower:\n",
    "            f1_col_name = df.columns[cols_lower.index(possible)]\n",
    "            break\n",
    "    \n",
    "    if f1_col_name:\n",
    "        print(f\"Using column for comparison: {f1_col_name}\")\n",
    "        for _, row in df.iterrows():\n",
    "            field = str(row[field_col]).upper()\n",
    "            f1_dict[field] = pd.to_numeric(row[f1_col_name], errors='coerce')\n",
    "    else:\n",
    "        print(\"No F1 or Accuracy column found\")\n",
    "    \n",
    "    # Look for Precision column\n",
    "    if 'precision' in cols_lower:\n",
    "        prec_col = df.columns[cols_lower.index('precision')]\n",
    "        for _, row in df.iterrows():\n",
    "            field = str(row[field_col]).upper()\n",
    "            precision_dict[field] = pd.to_numeric(row[prec_col], errors='coerce')\n",
    "    \n",
    "    print(f\"Fields loaded: {len(f1_dict)} F1 values\")\n",
    "    \n",
    "    return f1_dict, precision_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_current_model_data(csv_path: Path) -> tuple[dict, dict, set]:\n",
    "    \"\"\"\n",
    "    Load current model's field-level F1 scores and exclusive fields.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (f1_dict, precision_dict, exclusive_set)\n",
    "    \n",
    "    Note: We now use F1 as the primary metric (not accuracy).\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"Loaded current model CSV: {csv_path}\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "    \n",
    "    cols_lower = [c.lower() for c in df.columns]\n",
    "    \n",
    "    f1_dict = {}\n",
    "    precision_dict = {}\n",
    "    exclusive_set = set()\n",
    "    \n",
    "    # Check for Field column\n",
    "    if 'field' not in cols_lower:\n",
    "        raise ValueError(f\"No 'Field' column found. Columns: {list(df.columns)}\")\n",
    "    \n",
    "    field_col = df.columns[cols_lower.index('field')]\n",
    "    \n",
    "    # Look for F1 column (try multiple possible names)\n",
    "    f1_col_name = None\n",
    "    for possible in ['f1', 'f1_score', 'f1-score', 'f1_agnostic', 'f1_smart']:\n",
    "        if possible in cols_lower:\n",
    "            f1_col_name = df.columns[cols_lower.index(possible)]\n",
    "            break\n",
    "    \n",
    "    if f1_col_name:\n",
    "        print(f\"Found F1 column: {f1_col_name}\")\n",
    "        for _, row in df.iterrows():\n",
    "            field = str(row[field_col]).upper()\n",
    "            f1_dict[field] = pd.to_numeric(row[f1_col_name], errors='coerce')\n",
    "    else:\n",
    "        print(\"No F1 column found\")\n",
    "    \n",
    "    # Look for Precision column\n",
    "    if 'precision' in cols_lower:\n",
    "        prec_col = df.columns[cols_lower.index('precision')]\n",
    "        for _, row in df.iterrows():\n",
    "            field = str(row[field_col]).upper()\n",
    "            precision_dict[field] = pd.to_numeric(row[prec_col], errors='coerce')\n",
    "    \n",
    "    # Read Exclusive column from CSV\n",
    "    if 'exclusive' in cols_lower:\n",
    "        exc_col = df.columns[cols_lower.index('exclusive')]\n",
    "        for _, row in df.iterrows():\n",
    "            field = str(row[field_col]).upper()\n",
    "            exc_val = row[exc_col]\n",
    "            # Handle string 'True'/'False' or boolean\n",
    "            if str(exc_val).lower() == 'true' or exc_val == True:\n",
    "                exclusive_set.add(field)\n",
    "        print(f\"Exclusive fields from CSV: {exclusive_set}\")\n",
    "    \n",
    "    print(f\"Fields loaded: {len(f1_dict)} F1 values\")\n",
    "    \n",
    "    return f1_dict, precision_dict, exclusive_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load both datasets (F1 scores)\n",
    "print(\"=\" * 60)\n",
    "print(\"LOADING COMPETING MODEL DATA\")\n",
    "print(\"=\" * 60)\n",
    "competing_f1, competing_precision = load_competing_model_data(COMPETING_MODEL_CSV)\n",
    "\n",
    "print(\"\" + \"=\" * 60)\n",
    "print(\"LOADING CURRENT MODEL DATA\")\n",
    "print(\"=\" * 60)\n",
    "current_f1, current_precision, csv_exclusive_fields = load_current_model_data(CURRENT_MODEL_CSV)\n",
    "\n",
    "# Override EXCLUSIVE_FIELDS with values from CSV if available\n",
    "if csv_exclusive_fields:\n",
    "    EXCLUSIVE_FIELDS_DYNAMIC = csv_exclusive_fields\n",
    "    print(f\"Using exclusive fields from CSV: {EXCLUSIVE_FIELDS_DYNAMIC}\")\n",
    "else:\n",
    "    EXCLUSIVE_FIELDS_DYNAMIC = set(EXCLUSIVE_FIELDS)\n",
    "    print(f\"Using hardcoded exclusive fields: {EXCLUSIVE_FIELDS_DYNAMIC}\")\n",
    "\n",
    "# For backward compatibility with rest of notebook, create accuracy aliases\n",
    "current_accuracy = current_f1\n",
    "competing_accuracy = competing_f1\n",
    "\n",
    "# Summary\n",
    "print(\"\" + \"=\" * 60)\n",
    "print(\"DATA SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Current model fields: {len(current_f1)}\")\n",
    "print(f\"Competing model fields: {len(competing_f1)}\")\n",
    "common_fields = set(current_f1.keys()) & set(competing_f1.keys())\n",
    "print(f\"Common fields: {len(common_fields)}\")\n",
    "print(f\"Exclusive fields: {len(EXCLUSIVE_FIELDS_DYNAMIC)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Data Alignment & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_field_data(\n",
    "    current_acc: dict[str, float], \n",
    "    competing_acc: dict[str, float], \n",
    "    current_f1: dict[str, float],\n",
    "    competing_f1: dict[str, float],\n",
    "    schema_fields: list[str]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Align field-level accuracy AND F1 data from both models into a comparison DataFrame.\n",
    "    \n",
    "    F1 scores are POSITION-AWARE for both models, enabling fair comparison.\n",
    "    Marks fields as 'is_exclusive' if they are in EXCLUSIVE_FIELDS.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    \n",
    "    for field in schema_fields:\n",
    "        current_accuracy = current_acc.get(field, np.nan)\n",
    "        competing_accuracy = competing_acc.get(field, np.nan)\n",
    "        \n",
    "        # F1 scores (Position-Aware for fair comparison)\n",
    "        current_f1_val = current_f1.get(field, np.nan) if current_f1 else np.nan\n",
    "        competing_f1_val = competing_f1.get(field, np.nan) if competing_f1 else np.nan\n",
    "        \n",
    "        # Find category for this field\n",
    "        category = \"Other\"\n",
    "        for cat, fields in FIELD_CATEGORIES.items():\n",
    "            if field in fields:\n",
    "                category = cat\n",
    "                break\n",
    "        \n",
    "        # Check if this is an exclusive capability (our model only)\n",
    "        is_exclusive = field in EXCLUSIVE_FIELDS_DYNAMIC\n",
    "        \n",
    "        data.append({\n",
    "            'field': field,\n",
    "            'category': category,\n",
    "            'current_accuracy': current_accuracy,\n",
    "            'competing_accuracy': competing_accuracy,\n",
    "            'difference': current_accuracy - competing_accuracy if not (np.isnan(current_accuracy) or np.isnan(competing_accuracy)) else np.nan,\n",
    "            # F1 scores (Position-Aware)\n",
    "            'current_f1': current_f1_val,\n",
    "            'competing_f1': competing_f1_val,\n",
    "            'f1_difference': current_f1_val - competing_f1_val if not (np.isnan(current_f1_val) or np.isnan(competing_f1_val)) else np.nan,\n",
    "            'is_critical': field in CRITICAL_FIELDS,\n",
    "            'is_exclusive': is_exclusive,\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create aligned comparison DataFrame (with F1 scores)\n",
    "comparison_df = align_field_data(\n",
    "    current_accuracy, competing_accuracy, \n",
    "    current_f1, competing_f1, \n",
    "    SCHEMA_FIELDS\n",
    ")\n",
    "\n",
    "# Separate comparable fields from exclusive capabilities\n",
    "comparable_df = comparison_df[~comparison_df['is_exclusive']].copy()\n",
    "exclusive_df = comparison_df[comparison_df['is_exclusive']].copy()\n",
    "\n",
    "print(\"Field Alignment Summary\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total schema fields: {len(SCHEMA_FIELDS)}\")\n",
    "print(f\"Comparable fields: {len(comparable_df)}\")\n",
    "print(f\"Exclusive capabilities (our model only): {len(exclusive_df)}\")\n",
    "\n",
    "print(f\"\\nF1 Score data:\")\n",
    "print(f\"  Fields with both models: {(comparable_df['current_accuracy'].notna() & comparable_df['competing_accuracy'].notna()).sum()}\")\n",
    "\n",
    "# F1 comparison summary\n",
    "f1_both = (comparable_df['current_f1'].notna() & comparable_df['competing_f1'].notna()).sum()\n",
    "print(f\"\\nF1 Score data (Position-Aware):\")\n",
    "print(f\"  Fields with both models: {f1_both}\")\n",
    "if f1_both > 0:\n",
    "    mean_f1_diff = comparable_df['f1_difference'].mean()\n",
    "    print(f\"  Mean F1 difference: {mean_f1_diff:+.1%} (positive = {CURRENT_MODEL_NAME} better)\")\n",
    "\n",
    "# Show exclusive fields\n",
    "if len(exclusive_df) > 0:\n",
    "    print(f\"\\n*** ADDITIONAL CAPABILITIES ({CURRENT_MODEL_NAME} only) ***\")\n",
    "    for _, row in exclusive_df.iterrows():\n",
    "        acc = f\"{row['current_accuracy']:.1%}\" if pd.notna(row['current_accuracy']) else \"N/A\"\n",
    "        print(f\"  - {row['field']}: {acc}\")\n",
    "\n",
    "# Show missing fields (excluding exclusive ones)\n",
    "missing_current = comparable_df[comparable_df['current_accuracy'].isna()]['field'].tolist()\n",
    "missing_competing = comparable_df[comparable_df['competing_accuracy'].isna()]['field'].tolist()\n",
    "\n",
    "if missing_current:\n",
    "    print(f\"\\nMissing from current model: {missing_current}\")\n",
    "if missing_competing:\n",
    "    print(f\"Missing from competing model: {missing_competing}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the comparison table (comparable fields only)\n",
    "display_df = comparable_df.copy()\n",
    "display_df['current_accuracy'] = display_df['current_accuracy'].apply(lambda x: f\"{x:.1%}\" if pd.notna(x) else \"N/A\")\n",
    "display_df['competing_accuracy'] = display_df['competing_accuracy'].apply(lambda x: f\"{x:.1%}\" if pd.notna(x) else \"N/A\")\n",
    "display_df['difference'] = display_df['difference'].apply(lambda x: f\"{x:+.1%}\" if pd.notna(x) else \"N/A\")\n",
    "\n",
    "# Format F1 columns if available\n",
    "has_f1 = comparable_df['current_f1'].notna().any() or comparable_df['competing_f1'].notna().any()\n",
    "if has_f1:\n",
    "    display_df['current_f1'] = display_df['current_f1'].apply(lambda x: f\"{x:.1%}\" if pd.notna(x) else \"N/A\")\n",
    "    display_df['competing_f1'] = display_df['competing_f1'].apply(lambda x: f\"{x:.1%}\" if pd.notna(x) else \"N/A\")\n",
    "    display_df['f1_difference'] = display_df['f1_difference'].apply(lambda x: f\"{x:+.1%}\" if pd.notna(x) else \"N/A\")\n",
    "\n",
    "print(\"\\nField-Level Comparison (Comparable Fields)\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Show accuracy columns\n",
    "acc_cols = ['field', 'category', 'current_accuracy', 'competing_accuracy', 'difference', 'is_critical']\n",
    "print(\"\\n\ud83d\udcca F1 SCORES:\")\n",
    "print(display_df[acc_cols].to_string(index=False))\n",
    "\n",
    "# Show F1 columns if available\n",
    "if has_f1:\n",
    "    f1_cols = ['field', 'current_f1', 'competing_f1', 'f1_difference']\n",
    "    print(\"\\n\ud83d\udcc8 F1 SCORES (Position-Aware):\")\n",
    "    print(display_df[f1_cols].to_string(index=False))\n",
    "\n",
    "# Display exclusive capabilities separately\n",
    "if len(exclusive_df) > 0:\n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(f\"ADDITIONAL CAPABILITIES ({CURRENT_MODEL_NAME} only - not in {COMPETING_MODEL_NAME})\")\n",
    "    print(\"=\" * 100)\n",
    "    exc_display = exclusive_df.copy()\n",
    "    exc_display['current_accuracy'] = exc_display['current_accuracy'].apply(lambda x: f\"{x:.1%}\" if pd.notna(x) else \"N/A\")\n",
    "    if has_f1:\n",
    "        exc_display['current_f1'] = exc_display['current_f1'].apply(lambda x: f\"{x:.1%}\" if pd.notna(x) else \"N/A\")\n",
    "        print(exc_display[['field', 'category', 'current_accuracy', 'current_f1']].to_string(index=False))\n",
    "    else:\n",
    "        print(exc_display[['field', 'category', 'current_accuracy']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Aggregate Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_aggregate_metrics(df: pd.DataFrame, exclusive_df: pd.DataFrame = None) -> dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Calculate aggregate comparison metrics including F1 scores.\n",
    "    \n",
    "    Only uses comparable fields (excludes exclusive capabilities) for fair comparison.\n",
    "    F1 metrics are included when both models have F1 data.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Filter to comparable fields with both accuracy values\n",
    "    comparable_only = df[~df['is_exclusive']] if 'is_exclusive' in df.columns else df\n",
    "    valid_df = comparable_only.dropna(subset=['current_accuracy', 'competing_accuracy'])\n",
    "    \n",
    "    metrics = {\n",
    "        'n_fields_compared': len(valid_df),\n",
    "        'n_exclusive_fields': len(exclusive_df) if exclusive_df is not None else 0,\n",
    "        \n",
    "        # Current model accuracy metrics (comparable fields only)\n",
    "        'current_mean': valid_df['current_accuracy'].mean(),\n",
    "        'current_median': valid_df['current_accuracy'].median(),\n",
    "        'current_std': valid_df['current_accuracy'].std(),\n",
    "        'current_min': valid_df['current_accuracy'].min(),\n",
    "        'current_max': valid_df['current_accuracy'].max(),\n",
    "        \n",
    "        # Competing model accuracy metrics\n",
    "        'competing_mean': valid_df['competing_accuracy'].mean(),\n",
    "        'competing_median': valid_df['competing_accuracy'].median(),\n",
    "        'competing_std': valid_df['competing_accuracy'].std(),\n",
    "        'competing_min': valid_df['competing_accuracy'].min(),\n",
    "        'competing_max': valid_df['competing_accuracy'].max(),\n",
    "        \n",
    "        # Accuracy comparison metrics\n",
    "        'mean_difference': valid_df['difference'].mean(),\n",
    "        'fields_current_better': (valid_df['difference'] > 0).sum(),\n",
    "        'fields_competing_better': (valid_df['difference'] < 0).sum(),\n",
    "        'fields_equal': (valid_df['difference'] == 0).sum(),\n",
    "        \n",
    "        # Critical fields (from comparable only)\n",
    "        'critical_current_mean': valid_df[valid_df['is_critical']]['current_accuracy'].mean(),\n",
    "        'critical_competing_mean': valid_df[valid_df['is_critical']]['competing_accuracy'].mean(),\n",
    "    }\n",
    "    \n",
    "    # F1 Score metrics (Position-Aware for fair comparison)\n",
    "    f1_valid = comparable_only.dropna(subset=['current_f1', 'competing_f1'])\n",
    "    metrics['n_fields_with_f1'] = len(f1_valid)\n",
    "    \n",
    "    if len(f1_valid) > 0:\n",
    "        metrics['current_f1_mean'] = f1_valid['current_f1'].mean()\n",
    "        metrics['competing_f1_mean'] = f1_valid['competing_f1'].mean()\n",
    "        metrics['f1_mean_difference'] = f1_valid['f1_difference'].mean()\n",
    "        metrics['f1_fields_current_better'] = (f1_valid['f1_difference'] > 0).sum()\n",
    "        metrics['f1_fields_competing_better'] = (f1_valid['f1_difference'] < 0).sum()\n",
    "    else:\n",
    "        metrics['current_f1_mean'] = np.nan\n",
    "        metrics['competing_f1_mean'] = np.nan\n",
    "        metrics['f1_mean_difference'] = np.nan\n",
    "        metrics['f1_fields_current_better'] = 0\n",
    "        metrics['f1_fields_competing_better'] = 0\n",
    "    \n",
    "    # Add exclusive capabilities metrics\n",
    "    if exclusive_df is not None and len(exclusive_df) > 0:\n",
    "        exclusive_with_data = exclusive_df[exclusive_df['current_accuracy'].notna()]\n",
    "        metrics['exclusive_fields'] = exclusive_df['field'].tolist()\n",
    "        metrics['exclusive_mean'] = exclusive_with_data['current_accuracy'].mean() if len(exclusive_with_data) > 0 else np.nan\n",
    "    else:\n",
    "        metrics['exclusive_fields'] = []\n",
    "        metrics['exclusive_mean'] = np.nan\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = calculate_aggregate_metrics(comparison_df, exclusive_df)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"AGGREGATE METRICS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nComparable Fields: {metrics['n_fields_compared']}\")\n",
    "print(f\"Additional Capabilities ({CURRENT_MODEL_NAME} only): {metrics['n_exclusive_fields']}\")\n",
    "\n",
    "print(f\"\\n--- ACCURACY METRICS ---\")\n",
    "print(f\"\\n{CURRENT_MODEL_NAME} (comparable fields):\")\n",
    "print(f\"  Mean Accuracy:   {metrics['current_mean']:.1%}\")\n",
    "print(f\"  Median Accuracy: {metrics['current_median']:.1%}\")\n",
    "print(f\"  Std Dev:         {metrics['current_std']:.1%}\")\n",
    "print(f\"  Range:           {metrics['current_min']:.1%} - {metrics['current_max']:.1%}\")\n",
    "\n",
    "print(f\"\\n{COMPETING_MODEL_NAME}:\")\n",
    "print(f\"  Mean Accuracy:   {metrics['competing_mean']:.1%}\")\n",
    "print(f\"  Median Accuracy: {metrics['competing_median']:.1%}\")\n",
    "print(f\"  Std Dev:         {metrics['competing_std']:.1%}\")\n",
    "print(f\"  Range:           {metrics['competing_min']:.1%} - {metrics['competing_max']:.1%}\")\n",
    "\n",
    "print(f\"\\nAccuracy Comparison:\")\n",
    "print(f\"  Mean Difference: {metrics['mean_difference']:+.1%} (positive = {CURRENT_MODEL_NAME} better)\")\n",
    "print(f\"  Fields where {CURRENT_MODEL_NAME} better: {metrics['fields_current_better']}\")\n",
    "print(f\"  Fields where {COMPETING_MODEL_NAME} better: {metrics['fields_competing_better']}\")\n",
    "print(f\"  Fields equal: {metrics['fields_equal']}\")\n",
    "\n",
    "print(f\"\\nCritical Fields (ABN, GST, Total, Supplier):\")\n",
    "print(f\"  {CURRENT_MODEL_NAME} Mean:   {metrics['critical_current_mean']:.1%}\")\n",
    "print(f\"  {COMPETING_MODEL_NAME} Mean: {metrics['critical_competing_mean']:.1%}\")\n",
    "\n",
    "# F1 Score comparison (if available)\n",
    "if metrics['n_fields_with_f1'] > 0:\n",
    "    print(f\"\\n--- F1 SCORE METRICS (Position-Aware) ---\")\n",
    "    print(f\"\\nFields with F1 comparison: {metrics['n_fields_with_f1']}\")\n",
    "    print(f\"\\n{CURRENT_MODEL_NAME}:\")\n",
    "    print(f\"  Mean F1: {metrics['current_f1_mean']:.1%}\")\n",
    "    print(f\"\\n{COMPETING_MODEL_NAME}:\")\n",
    "    print(f\"  Mean F1: {metrics['competing_f1_mean']:.1%}\")\n",
    "    print(f\"\\nF1 Comparison:\")\n",
    "    print(f\"  Mean F1 Difference: {metrics['f1_mean_difference']:+.1%}\")\n",
    "    print(f\"  Fields where {CURRENT_MODEL_NAME} better: {metrics['f1_fields_current_better']}\")\n",
    "    print(f\"  Fields where {COMPETING_MODEL_NAME} better: {metrics['f1_fields_competing_better']}\")\n",
    "else:\n",
    "    print(f\"\\n--- F1 SCORES ---\")\n",
    "    print(\"  No F1 comparison available (add F1 columns to both CSV files)\")\n",
    "\n",
    "if metrics['n_exclusive_fields'] > 0:\n",
    "    print(f\"\\n*** ADDITIONAL CAPABILITIES ({CURRENT_MODEL_NAME} only) ***\")\n",
    "    print(f\"  Fields: {', '.join(metrics['exclusive_fields'])}\")\n",
    "    if not np.isnan(metrics['exclusive_mean']):\n",
    "        print(f\"  Mean Accuracy: {metrics['exclusive_mean']:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Statistical Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_statistical_tests(df: pd.DataFrame) -> dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Run statistical significance tests on accuracy differences.\n",
    "    \n",
    "    Uses only comparable fields (excludes exclusive capabilities).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Filter to comparable fields only\n",
    "    comparable_only = df[~df['is_exclusive']] if 'is_exclusive' in df.columns else df\n",
    "    valid_df = comparable_only.dropna(subset=['current_accuracy', 'competing_accuracy'])\n",
    "    current = valid_df['current_accuracy'].values\n",
    "    competing = valid_df['competing_accuracy'].values\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Paired t-test (assumes normality)\n",
    "    t_stat, t_pvalue = stats.ttest_rel(current, competing)\n",
    "    results['paired_ttest'] = {'statistic': t_stat, 'pvalue': t_pvalue}\n",
    "    \n",
    "    # Wilcoxon signed-rank test (non-parametric alternative)\n",
    "    # Only works if there are non-zero differences\n",
    "    differences = current - competing\n",
    "    non_zero_diff = differences[differences != 0]\n",
    "    if len(non_zero_diff) > 0:\n",
    "        w_stat, w_pvalue = stats.wilcoxon(non_zero_diff)\n",
    "        results['wilcoxon'] = {'statistic': w_stat, 'pvalue': w_pvalue}\n",
    "    else:\n",
    "        results['wilcoxon'] = {'statistic': np.nan, 'pvalue': 1.0}\n",
    "    \n",
    "    # Effect size (Cohen's d)\n",
    "    pooled_std = np.sqrt((current.std()**2 + competing.std()**2) / 2)\n",
    "    cohens_d = (current.mean() - competing.mean()) / pooled_std if pooled_std > 0 else 0\n",
    "    results['cohens_d'] = cohens_d\n",
    "    \n",
    "    # Bootstrap confidence interval for mean difference\n",
    "    n_bootstrap = 10000\n",
    "    bootstrap_diffs = []\n",
    "    n = len(current)\n",
    "    for _ in range(n_bootstrap):\n",
    "        idx = np.random.choice(n, n, replace=True)\n",
    "        boot_diff = current[idx].mean() - competing[idx].mean()\n",
    "        bootstrap_diffs.append(boot_diff)\n",
    "    \n",
    "    ci_95 = np.percentile(bootstrap_diffs, [2.5, 97.5])\n",
    "    results['bootstrap_ci_95'] = ci_95\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_results = run_statistical_tests(comparison_df)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STATISTICAL SIGNIFICANCE TESTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nPaired t-test:\")\n",
    "print(f\"  t-statistic: {stat_results['paired_ttest']['statistic']:.4f}\")\n",
    "print(f\"  p-value:     {stat_results['paired_ttest']['pvalue']:.4f}\")\n",
    "sig_t = \"YES\" if stat_results['paired_ttest']['pvalue'] < 0.05 else \"NO\"\n",
    "print(f\"  Significant at \u03b1=0.05: {sig_t}\")\n",
    "\n",
    "print(f\"\\nWilcoxon signed-rank test (non-parametric):\")\n",
    "print(f\"  W-statistic: {stat_results['wilcoxon']['statistic']:.4f}\")\n",
    "print(f\"  p-value:     {stat_results['wilcoxon']['pvalue']:.4f}\")\n",
    "sig_w = \"YES\" if stat_results['wilcoxon']['pvalue'] < 0.05 else \"NO\"\n",
    "print(f\"  Significant at \u03b1=0.05: {sig_w}\")\n",
    "\n",
    "print(f\"\\nEffect Size:\")\n",
    "print(f\"  Cohen's d: {stat_results['cohens_d']:.4f}\")\n",
    "effect_interp = \"negligible\" if abs(stat_results['cohens_d']) < 0.2 else \\\n",
    "                \"small\" if abs(stat_results['cohens_d']) < 0.5 else \\\n",
    "                \"medium\" if abs(stat_results['cohens_d']) < 0.8 else \"large\"\n",
    "print(f\"  Interpretation: {effect_interp}\")\n",
    "\n",
    "print(f\"\\n95% Bootstrap Confidence Interval for Mean Difference:\")\n",
    "print(f\"  [{stat_results['bootstrap_ci_95'][0]:+.1%}, {stat_results['bootstrap_ci_95'][1]:+.1%}]\")\n",
    "contains_zero = stat_results['bootstrap_ci_95'][0] <= 0 <= stat_results['bootstrap_ci_95'][1]\n",
    "print(f\"  Contains zero: {'YES (not significant)' if contains_zero else 'NO (significant)'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# COHEN'S D CALCULATION - DETAILED BREAKDOWN\n",
    "# =============================================================================\n",
    "# This table shows all values used in the Cohen's d effect size calculation\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"COHEN'S D EFFECT SIZE - CALCULATION DETAILS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Get the comparable data used in statistical tests\n",
    "comparable_only = comparison_df[~comparison_df['is_exclusive']] if 'is_exclusive' in comparison_df.columns else comparison_df\n",
    "valid_df = comparable_only.dropna(subset=['current_accuracy', 'competing_accuracy'])\n",
    "current_values = valid_df['current_accuracy'].values\n",
    "competing_values = valid_df['competing_accuracy'].values\n",
    "\n",
    "# Calculate components\n",
    "current_mean = current_values.mean()\n",
    "competing_mean = competing_values.mean()\n",
    "current_std = current_values.std()\n",
    "competing_std = competing_values.std()\n",
    "pooled_std = np.sqrt((current_std**2 + competing_std**2) / 2)\n",
    "mean_diff = current_mean - competing_mean\n",
    "cohens_d_calc = mean_diff / pooled_std if pooled_std > 0 else 0\n",
    "\n",
    "# Print detailed table\n",
    "print(f\"\")\n",
    "print(f\"{'Component':<35} {'Value':>15} {'Formula':<30}\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'N (comparable fields)':<35} {len(current_values):>15}\")\n",
    "print(f\"{'Current Model Mean':<35} {current_mean:>15.4f}\")\n",
    "print(f\"{'Competing Model Mean':<35} {competing_mean:>15.4f}\")\n",
    "print(f\"{'Current Model Std Dev':<35} {current_std:>15.4f}\")\n",
    "print(f\"{'Competing Model Std Dev':<35} {competing_std:>15.4f}\")\n",
    "print(\"-\" * 80)\n",
    "pooled_label = \"Pooled Std Dev\"\n",
    "print(f\"{pooled_label:<35} {pooled_std:>15.4f} {'sqrt((s1^2 + s2^2) / 2)':<30}\")\n",
    "print(f\"{'Mean Difference':<35} {mean_diff:>15.4f}\")\n",
    "print(\"-\" * 80)\n",
    "cohens_label = \"Cohen's d\"\n",
    "print(f\"{cohens_label:<35} {cohens_d_calc:>15.4f} {'(mean1 - mean2) / pooled_std':<30}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Effect size interpretation\n",
    "if abs(cohens_d_calc) < 0.2:\n",
    "    interpretation = \"Negligible\"\n",
    "elif abs(cohens_d_calc) < 0.5:\n",
    "    interpretation = \"Small\"\n",
    "elif abs(cohens_d_calc) < 0.8:\n",
    "    interpretation = \"Medium\"\n",
    "else:\n",
    "    interpretation = \"Large\"\n",
    "\n",
    "print(f\"\")\n",
    "print(f\"Interpretation: |d| = {abs(cohens_d_calc):.2f} -> {interpretation} effect size\")\n",
    "print(f\"(Thresholds: <0.2 Negligible, 0.2-0.5 Small, 0.5-0.8 Medium, >0.8 Large)\")\n",
    "\n",
    "# Show individual field values used\n",
    "print(f\"\")\n",
    "print(f\"INDIVIDUAL FIELD VALUES USED IN CALCULATION\")\n",
    "print(\"-\" * 70)\n",
    "field_values = valid_df[['field', 'current_accuracy', 'competing_accuracy']].copy()\n",
    "field_values['difference'] = field_values['current_accuracy'] - field_values['competing_accuracy']\n",
    "field_values = field_values.sort_values('difference', ascending=False)\n",
    "for _, row in field_values.iterrows():\n",
    "    curr = row['current_accuracy']\n",
    "    comp = row['competing_accuracy']\n",
    "    diff = row['difference']\n",
    "    print(f\"  {row['field']:<25} Current: {curr:.3f}  Competing: {comp:.3f}  Diff: {diff:+.3f}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for plotting - use COMPARABLE fields only (exclude exclusive capabilities)\n",
    "plot_df = comparable_df.dropna(subset=['current_accuracy', 'competing_accuracy']).copy()\n",
    "plot_df = plot_df.sort_values('current_accuracy', ascending=True)\n",
    "\n",
    "print(f\"Plotting {len(plot_df)} comparable fields (excluding {len(exclusive_df)} exclusive capabilities)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 1: Side-by-side bar chart\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "y_pos = np.arange(len(plot_df))\n",
    "bar_height = 0.35\n",
    "\n",
    "bars1 = ax.barh(y_pos - bar_height/2, plot_df['current_accuracy'], bar_height, \n",
    "                label=CURRENT_MODEL_NAME, color='#2ecc71', alpha=0.8)\n",
    "bars2 = ax.barh(y_pos + bar_height/2, plot_df['competing_accuracy'], bar_height,\n",
    "                label=COMPETING_MODEL_NAME, color='#3498db', alpha=0.8)\n",
    "\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(plot_df['field'])\n",
    "ax.set_xlabel('F1 Score', fontsize=12)\n",
    "ax.set_title('Field-Level F1 Comparison', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='lower right')\n",
    "ax.set_xlim(0, 1.05)\n",
    "ax.axvline(x=0.9, color='red', linestyle='--', alpha=0.5, label='90% threshold')\n",
    "\n",
    "# Add value labels\n",
    "for bar, val in zip(bars1, plot_df['current_accuracy']):\n",
    "    ax.text(val + 0.01, bar.get_y() + bar.get_height()/2, f'{val:.0%}', \n",
    "            va='center', fontsize=8)\n",
    "for bar, val in zip(bars2, plot_df['competing_accuracy']):\n",
    "    ax.text(val + 0.01, bar.get_y() + bar.get_height()/2, f'{val:.0%}', \n",
    "            va='center', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'accuracy_comparison_bars.png', dpi=600, bbox_inches='tight')\n",
    "plt.savefig(EXPORTS_DIR / 'accuracy_comparison_bars.svg', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 2: Difference chart (lollipop plot)\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Sort by difference\n",
    "diff_df = plot_df.sort_values('difference')\n",
    "y_pos = np.arange(len(diff_df))\n",
    "\n",
    "colors = ['#2ecc71' if d >= 0 else '#e74c3c' for d in diff_df['difference']]\n",
    "\n",
    "# Use stronger markers for critical fields\n",
    "marker_sizes = [160 if f in CRITICAL_FIELDS else 100 for f in diff_df['field']]\n",
    "edge_colors = ['#333333' if f in CRITICAL_FIELDS else 'none' for f in diff_df['field']]\n",
    "linewidths = [3 if f in CRITICAL_FIELDS else 2 for f in diff_df['field']]\n",
    "\n",
    "ax.hlines(y=y_pos, xmin=0, xmax=diff_df['difference'], color=colors,\n",
    "          linewidth=linewidths)\n",
    "ax.scatter(diff_df['difference'], y_pos, color=colors, s=marker_sizes,\n",
    "           edgecolors=edge_colors, linewidths=1.5, zorder=3)\n",
    "\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(diff_df['field'])\n",
    "ax.axvline(x=0, color='black', linewidth=1)\n",
    "ax.set_xlabel('F1 Difference (positive = InternVL3.5-8B model better)', fontsize=12)\n",
    "ax.set_title(f'F1 Difference: {CURRENT_MODEL_NAME} vs {COMPETING_MODEL_NAME}',\n",
    "             fontsize=14, fontweight='bold')\n",
    "\n",
    "# Bold y-tick labels for critical fields\n",
    "for label in ax.get_yticklabels():\n",
    "    if label.get_text() in CRITICAL_FIELDS:\n",
    "        label.set_fontweight('bold')\n",
    "        label.set_fontsize(11)\n",
    "\n",
    "# Dynamic x-limits with padding for value labels\n",
    "x_min = diff_df['difference'].min()\n",
    "x_max = diff_df['difference'].max()\n",
    "x_pad = (x_max - x_min) * 0.15  # 15% padding for labels\n",
    "ax.set_xlim(x_min - x_pad, x_max + x_pad)\n",
    "\n",
    "# Add value labels\n",
    "for y, diff in zip(y_pos, diff_df['difference']):\n",
    "    ax.text(diff + 0.01 if diff >= 0 else diff - 0.01, y, f'{diff:+.1%}',\n",
    "            va='center', ha='left' if diff >= 0 else 'right', fontsize=9)\n",
    "\n",
    "# Legend for critical fields\n",
    "from matplotlib.lines import Line2D\n",
    "legend_elements = [\n",
    "    Line2D([0], [0], marker='o', color='gray', markerfacecolor='gray',\n",
    "           markeredgecolor='#333333', markersize=10, linewidth=2,\n",
    "           label='Critical field'),\n",
    "    Line2D([0], [0], marker='o', color='gray', markerfacecolor='gray',\n",
    "           markeredgecolor='none', markersize=8, linewidth=1.5,\n",
    "           label='Standard field'),\n",
    "]\n",
    "ax.legend(handles=legend_elements, loc='lower right', fontsize=9)\n",
    "\n",
    "plt.subplots_adjust(left=0.22, right=0.95, top=0.92, bottom=0.08)\n",
    "plt.savefig(OUTPUT_DIR / 'accuracy_difference_lollipop.png', dpi=600, bbox_inches='tight')\n",
    "plt.savefig(EXPORTS_DIR / 'accuracy_difference_lollipop.svg', bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 3: Radar/Spider chart by category\n",
    "def plot_radar_chart(df: pd.DataFrame, categories: dict, ax=None):\n",
    "    \"\"\"Plot radar chart comparing models by field category.\"\"\"\n",
    "    \n",
    "    # Calculate category averages\n",
    "    cat_data = []\n",
    "    for cat, fields in categories.items():\n",
    "        cat_df = df[df['field'].isin(fields)]\n",
    "        if len(cat_df) > 0:\n",
    "            cat_data.append({\n",
    "                'category': cat,\n",
    "                'current': cat_df['current_accuracy'].mean(),\n",
    "                'competing': cat_df['competing_accuracy'].mean()\n",
    "            })\n",
    "    \n",
    "    cat_df = pd.DataFrame(cat_data)\n",
    "    \n",
    "    # Radar chart setup\n",
    "    categories_list = cat_df['category'].tolist()\n",
    "    N = len(categories_list)\n",
    "    \n",
    "    angles = [n / float(N) * 2 * np.pi for n in range(N)]\n",
    "    angles += angles[:1]  # Complete the loop\n",
    "    \n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(polar=True))\n",
    "    \n",
    "    # Plot current model\n",
    "    values = cat_df['current'].tolist()\n",
    "    values += values[:1]\n",
    "    ax.plot(angles, values, 'o-', linewidth=2, label=CURRENT_MODEL_NAME, color='#2ecc71')\n",
    "    ax.fill(angles, values, alpha=0.25, color='#2ecc71')\n",
    "    \n",
    "    # Plot competing model\n",
    "    values = cat_df['competing'].tolist()\n",
    "    values += values[:1]\n",
    "    ax.plot(angles, values, 'o-', linewidth=2, label=COMPETING_MODEL_NAME, color='#3498db')\n",
    "    ax.fill(angles, values, alpha=0.25, color='#3498db')\n",
    "    \n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(categories_list)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
    "    \n",
    "    return ax\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8), subplot_kw=dict(polar=True))\n",
    "plot_radar_chart(comparison_df, FIELD_CATEGORIES, ax)\n",
    "plt.title('F1 by Field Category', fontsize=14, fontweight='bold', y=1.08)\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'accuracy_radar_chart.png', dpi=600, bbox_inches='tight')\n",
    "plt.savefig(EXPORTS_DIR / 'accuracy_radar_chart.svg', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 4: Heatmap of accuracy values\n",
    "fig, ax = plt.subplots(figsize=(6, 10))\n",
    "\n",
    "heatmap_data = plot_df[['current_accuracy', 'competing_accuracy']].copy()\n",
    "heatmap_data.index = plot_df['field']\n",
    "heatmap_data.columns = [CURRENT_MODEL_NAME, COMPETING_MODEL_NAME]\n",
    "\n",
    "sns.heatmap(heatmap_data, annot=True, fmt='.0%', cmap='RdYlGn', \n",
    "            vmin=0, vmax=1, ax=ax, cbar_kws={'label': 'Accuracy'})\n",
    "ax.set_title('Field-Level F1 Heatmap', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'accuracy_heatmap.png', dpi=600, bbox_inches='tight')\n",
    "plt.savefig(EXPORTS_DIR / 'accuracy_heatmap.svg', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 5: Box plot comparison\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "box_data = pd.DataFrame({\n",
    "    CURRENT_MODEL_NAME: plot_df['current_accuracy'],\n",
    "    COMPETING_MODEL_NAME: plot_df['competing_accuracy']\n",
    "})\n",
    "\n",
    "box_data.boxplot(ax=ax, patch_artist=True,\n",
    "                  boxprops=dict(facecolor='lightblue', alpha=0.7),\n",
    "                  medianprops=dict(color='red', linewidth=2))\n",
    "\n",
    "# Add individual points\n",
    "for i, col in enumerate(box_data.columns, 1):\n",
    "    y = box_data[col].dropna()\n",
    "    x = np.random.normal(i, 0.04, size=len(y))\n",
    "    ax.scatter(x, y, alpha=0.6, s=50)\n",
    "\n",
    "ax.set_ylabel('F1 Score', fontsize=12)\n",
    "ax.set_title('Distribution of Field-Level F1', fontsize=14, fontweight='bold')\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.axhline(y=0.9, color='red', linestyle='--', alpha=0.5, label='90% threshold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'accuracy_boxplot.png', dpi=600, bbox_inches='tight')\n",
    "plt.savefig(EXPORTS_DIR / 'accuracy_boxplot.svg', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 6: Scatter plot with identity line\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "# Plot points\n",
    "colors = ['red' if c else 'blue' for c in plot_df['is_critical']]\n",
    "ax.scatter(plot_df['competing_accuracy'], plot_df['current_accuracy'], \n",
    "           c=colors, s=100, alpha=0.7)\n",
    "\n",
    "# Add field labels\n",
    "for _, row in plot_df.iterrows():\n",
    "    ax.annotate(row['field'][:12], (row['competing_accuracy'], row['current_accuracy']),\n",
    "                textcoords=\"offset points\", xytext=(5, 5), fontsize=8)\n",
    "\n",
    "# Identity line\n",
    "ax.plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Equal accuracy')\n",
    "\n",
    "# Fill regions\n",
    "ax.fill_between([0, 1], [0, 1], [1, 1], alpha=0.1, color='green', label=f'{CURRENT_MODEL_NAME} better')\n",
    "ax.fill_between([0, 1], [0, 0], [0, 1], alpha=0.1, color='blue', label=f'{COMPETING_MODEL_NAME} better')\n",
    "\n",
    "ax.set_xlabel(f'{COMPETING_MODEL_NAME} F1', fontsize=12)\n",
    "ax.set_ylabel(f'{CURRENT_MODEL_NAME} F1', fontsize=12)\n",
    "ax.set_title('Model F1 Comparison (red = critical fields)', fontsize=14, fontweight='bold')\n",
    "ax.set_xlim(0, 1.05)\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.legend(loc='lower right')\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'accuracy_scatter.png', dpi=600, bbox_inches='tight')\n",
    "plt.savefig(EXPORTS_DIR / 'accuracy_scatter.svg', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 7: F1 Score Comparison (if F1 data available)\n",
    "f1_plot_df = comparable_df.dropna(subset=['current_f1', 'competing_f1']).copy()\n",
    "\n",
    "if len(f1_plot_df) > 0:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Left: F1 bar comparison\n",
    "    ax1 = axes[0]\n",
    "    f1_sorted = f1_plot_df.sort_values('current_f1', ascending=True)\n",
    "    y_pos = np.arange(len(f1_sorted))\n",
    "    bar_height = 0.35\n",
    "    \n",
    "    ax1.barh(y_pos - bar_height/2, f1_sorted['current_f1'], bar_height, \n",
    "             label=CURRENT_MODEL_NAME, color='#2ecc71', alpha=0.8)\n",
    "    ax1.barh(y_pos + bar_height/2, f1_sorted['competing_f1'], bar_height,\n",
    "             label=COMPETING_MODEL_NAME, color='#3498db', alpha=0.8)\n",
    "    \n",
    "    ax1.set_yticks(y_pos)\n",
    "    ax1.set_yticklabels(f1_sorted['field'], fontsize=9)\n",
    "    ax1.set_xlabel('F1 Score (Position-Aware)', fontsize=11)\n",
    "    ax1.set_title('F1 Score Comparison by Field', fontsize=12, fontweight='bold')\n",
    "    ax1.legend(loc='lower right')\n",
    "    ax1.set_xlim(0, 1.05)\n",
    "    ax1.axvline(x=0.5, color='gray', linestyle=':', alpha=0.5)\n",
    "    \n",
    "    # Right: F1 difference lollipop\n",
    "    ax2 = axes[1]\n",
    "    f1_diff_sorted = f1_plot_df.sort_values('f1_difference')\n",
    "    y_diff = np.arange(len(f1_diff_sorted))\n",
    "    diff_colors = ['#2ecc71' if d >= 0 else '#e74c3c' for d in f1_diff_sorted['f1_difference']]\n",
    "    \n",
    "    ax2.hlines(y=y_diff, xmin=0, xmax=f1_diff_sorted['f1_difference'], color=diff_colors, linewidth=2)\n",
    "    ax2.scatter(f1_diff_sorted['f1_difference'], y_diff, color=diff_colors, s=80, zorder=3)\n",
    "    ax2.axvline(x=0, color='black', linewidth=1)\n",
    "    \n",
    "    ax2.set_yticks(y_diff)\n",
    "    ax2.set_yticklabels(f1_diff_sorted['field'], fontsize=9)\n",
    "    ax2.set_xlabel('F1 Difference (positive = current better)', fontsize=11)\n",
    "    ax2.set_title('F1 Score Improvement per Field', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Add value labels\n",
    "    for y, diff in zip(y_diff, f1_diff_sorted['f1_difference']):\n",
    "        ax2.text(diff + 0.01 if diff >= 0 else diff - 0.01, y, f'{diff:+.1%}',\n",
    "                va='center', ha='left' if diff >= 0 else 'right', fontsize=9)\n",
    "    \n",
    "    plt.subplots_adjust(left=0.22, right=0.95, top=0.92, bottom=0.08)\n",
    "    plt.savefig(OUTPUT_DIR / 'f1_comparison.png', dpi=600, bbox_inches='tight')\n",
    "    plt.savefig(EXPORTS_DIR / 'f1_comparison.svg', bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\nF1 Score Summary (Position-Aware):\")\n",
    "    print(f\"  Fields compared: {len(f1_plot_df)}\")\n",
    "    print(f\"  {CURRENT_MODEL_NAME} mean F1: {f1_plot_df['current_f1'].mean():.1%}\")\n",
    "    print(f\"  {COMPETING_MODEL_NAME} mean F1: {f1_plot_df['competing_f1'].mean():.1%}\")\n",
    "    print(f\"  Mean F1 difference: {f1_plot_df['f1_difference'].mean():+.1%}\")\n",
    "else:\n",
    "    print(\"No F1 comparison chart generated - F1 data not available for both models\")\n",
    "    print(\"To enable F1 comparison:\")\n",
    "    print(\"  1. Run harvest notebook to generate current model F1 scores\")\n",
    "    print(\"  2. Add F1 column to competing_model_accuracy.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Category-Level Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_by_category(df: pd.DataFrame, categories: dict) -> pd.DataFrame:\n",
    "    \"\"\"Analyze accuracy by field category.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for cat, fields in categories.items():\n",
    "        cat_df = df[df['field'].isin(fields)].dropna(subset=['current_accuracy', 'competing_accuracy'])\n",
    "        \n",
    "        if len(cat_df) > 0:\n",
    "            results.append({\n",
    "                'category': cat,\n",
    "                'n_fields': len(cat_df),\n",
    "                'current_mean': cat_df['current_accuracy'].mean(),\n",
    "                'competing_mean': cat_df['competing_accuracy'].mean(),\n",
    "                'difference': cat_df['difference'].mean(),\n",
    "                'current_wins': (cat_df['difference'] > 0).sum(),\n",
    "                'competing_wins': (cat_df['difference'] < 0).sum(),\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_analysis = analyze_by_category(comparison_df, FIELD_CATEGORIES)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CATEGORY-LEVEL ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for _, row in category_analysis.iterrows():\n",
    "    print(f\"\\n{row['category']} ({row['n_fields']} fields):\")\n",
    "    print(f\"  {CURRENT_MODEL_NAME}:   {row['current_mean']:.1%}\")\n",
    "    print(f\"  {COMPETING_MODEL_NAME}: {row['competing_mean']:.1%}\")\n",
    "    print(f\"  Difference: {row['difference']:+.1%}\")\n",
    "    print(f\"  Winner breakdown: {CURRENT_MODEL_NAME}={row['current_wins']}, {COMPETING_MODEL_NAME}={row['competing_wins']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Category comparison bar chart\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "x = np.arange(len(category_analysis))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, category_analysis['current_mean'], width, \n",
    "               label=CURRENT_MODEL_NAME, color='#2ecc71', alpha=0.8)\n",
    "bars2 = ax.bar(x + width/2, category_analysis['competing_mean'], width,\n",
    "               label=COMPETING_MODEL_NAME, color='#3498db', alpha=0.8)\n",
    "\n",
    "ax.set_ylabel('Mean F1', fontsize=12)\n",
    "ax.set_title('F1 by Field Category', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(category_analysis['category'], rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1.1)\n",
    "ax.axhline(y=0.9, color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars1:\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "            f'{bar.get_height():.0%}', ha='center', va='bottom', fontsize=9)\n",
    "for bar in bars2:\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "            f'{bar.get_height():.0%}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'accuracy_by_category.png', dpi=600, bbox_inches='tight')\n",
    "plt.savefig(EXPORTS_DIR / 'accuracy_by_category.svg', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. A4 Dashboard Summary\n",
    "\n",
    "Single-page dashboard suitable for printing or reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# A4 PORTRAIT DASHBOARD - Single page summary with Additional Capabilities\n",
    "# =============================================================================\n",
    "# A4 portrait: 8.27 x 11.69 inches, using 7.5 x 10.5 for margins\n",
    "\n",
    "fig = plt.figure(figsize=(7.5, 13), facecolor='white')\n",
    "\n",
    "# Create grid: 4 rows with different heights\n",
    "gs = fig.add_gridspec(4, 2, height_ratios=[1.2, 1, 1, 0.7], hspace=0.5, wspace=0.6, top=0.87, bottom=0.04)\n",
    "\n",
    "# --- Title & Summary Stats (top spanning both columns) ---\n",
    "ax_title = fig.add_axes([0.05, 0.93, 0.9, 0.05])\n",
    "ax_title.axis('off')\n",
    "ax_title.text(0.5, 0.7, 'Model F1 Comparison Dashboard', \n",
    "              fontsize=16, fontweight='bold', ha='center', va='center')\n",
    "ax_title.text(0.5, 0.1, \n",
    "              f'{CURRENT_MODEL_NAME} vs {COMPETING_MODEL_NAME}  |  '\n",
    "              f'{metrics[\"n_fields_compared\"]} comparable fields  |  '\n",
    "              f'Mean improvement: {metrics[\"mean_difference\"]:+.1%}', \n",
    "              fontsize=9, ha='center', va='center', color='#555')\n",
    "\n",
    "# Add single shared legend below title\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [\n",
    "    Patch(facecolor='#2ecc71', alpha=0.85, label=CURRENT_MODEL_NAME),\n",
    "    Patch(facecolor='#3498db', alpha=0.85, label=COMPETING_MODEL_NAME)\n",
    "]\n",
    "fig.legend(handles=legend_elements, loc='upper center', bbox_to_anchor=(0.5, 0.935),\n",
    "           ncol=2, fontsize=9, frameon=True, fancybox=True)\n",
    "\n",
    "# --- Panel A: Field-level comparison (horizontal bar) ---\n",
    "ax1 = fig.add_subplot(gs[0, :])\n",
    "plot_sorted = plot_df.sort_values('current_accuracy', ascending=True)\n",
    "y_pos = np.arange(len(plot_sorted))\n",
    "\n",
    "# Use seaborn colors\n",
    "colors_current = '#2ecc71'\n",
    "colors_competing = '#3498db'\n",
    "\n",
    "ax1.barh(y_pos + 0.2, plot_sorted['current_accuracy'], 0.4, \n",
    "         label=CURRENT_MODEL_NAME, color=colors_current, alpha=0.85)\n",
    "ax1.barh(y_pos - 0.2, plot_sorted['competing_accuracy'], 0.4,\n",
    "         label=COMPETING_MODEL_NAME, color=colors_competing, alpha=0.85)\n",
    "\n",
    "ax1.set_yticks(y_pos)\n",
    "ax1.set_yticklabels(plot_sorted['field'], fontsize=8)\n",
    "ax1.set_xlabel('F1 Score', fontsize=9)\n",
    "ax1.set_xlim(0, 1)\n",
    "ax1.set_title('A. Field-Level F1 (Comparable Fields)', fontsize=11, fontweight='bold', loc='left')\n",
    "ax1.legend().set_visible(False)\n",
    "ax1.axvline(x=0.5, color='gray', linestyle=':', alpha=0.5)\n",
    "sns.despine(ax=ax1)\n",
    "\n",
    "# Add percentage labels at end of bars\n",
    "for i, (_, row) in enumerate(plot_sorted.iterrows()):\n",
    "    # Current model (top bar)\n",
    "    ax1.text(row['current_accuracy'] + 0.01, i + 0.2, f\"{row['current_accuracy']:.0%}\", \n",
    "             va='center', ha='left', fontsize=6, color=colors_current)\n",
    "    # Competing model (bottom bar)\n",
    "    ax1.text(row['competing_accuracy'] + 0.01, i - 0.2, f\"{row['competing_accuracy']:.0%}\", \n",
    "             va='center', ha='left', fontsize=6, color=colors_competing)\n",
    "\n",
    "# --- Panel B: Category comparison (Radar Chart) ---\n",
    "ax2 = fig.add_subplot(gs[1, 0], polar=True)\n",
    "\n",
    "# Calculate category averages\n",
    "cat_data = []\n",
    "for cat, fields in FIELD_CATEGORIES.items():\n",
    "    cat_df_temp = comparison_df[comparison_df['field'].isin(fields)]\n",
    "    if len(cat_df_temp) > 0:\n",
    "        cat_data.append({\n",
    "            'category': cat,\n",
    "            'current': cat_df_temp['current_accuracy'].mean(),\n",
    "            'competing': cat_df_temp['competing_accuracy'].mean()\n",
    "        })\n",
    "\n",
    "radar_df = pd.DataFrame(cat_data)\n",
    "categories_list = radar_df['category'].tolist()\n",
    "N = len(categories_list)\n",
    "\n",
    "angles = [n / float(N) * 2 * np.pi for n in range(N)]\n",
    "angles += angles[:1]\n",
    "\n",
    "# Plot current model\n",
    "values_current = radar_df['current'].tolist()\n",
    "values_current += values_current[:1]\n",
    "ax2.plot(angles, values_current, 'o-', linewidth=2, color=colors_current, markersize=4)\n",
    "ax2.fill(angles, values_current, alpha=0.25, color=colors_current)\n",
    "\n",
    "# Plot competing model\n",
    "values_competing = radar_df['competing'].tolist()\n",
    "values_competing += values_competing[:1]\n",
    "ax2.plot(angles, values_competing, 'o-', linewidth=2, color=colors_competing, markersize=4)\n",
    "ax2.fill(angles, values_competing, alpha=0.25, color=colors_competing)\n",
    "\n",
    "ax2.set_xticks(angles[:-1])\n",
    "ax2.set_xticklabels(categories_list, fontsize=7)\n",
    "ax2.set_ylim(0, 1)\n",
    "ax2.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "ax2.set_yticklabels(['0.2', '', '0.6', '', '1.0'], fontsize=6)\n",
    "ax2.set_title('B. By Category', fontsize=11, fontweight='bold', loc='left', pad=15)\n",
    "\n",
    "# --- Panel C: Distribution comparison (box + strip) ---\n",
    "ax3 = fig.add_subplot(gs[1, 1])\n",
    "box_df = pd.DataFrame({\n",
    "    'Model': [CURRENT_MODEL_NAME] * len(plot_df) + [COMPETING_MODEL_NAME] * len(plot_df),\n",
    "    'Accuracy': list(plot_df['current_accuracy']) + list(plot_df['competing_accuracy'])\n",
    "})\n",
    "\n",
    "sns.boxplot(data=box_df, x='Model', y='Accuracy', ax=ax3, \n",
    "            palette=[colors_current, colors_competing], width=0.5)\n",
    "sns.stripplot(data=box_df, x='Model', y='Accuracy', ax=ax3,\n",
    "              color='black', alpha=0.4, size=4, jitter=0.1)\n",
    "\n",
    "ax3.set_ylim(0, 1)\n",
    "ax3.set_xlabel('')\n",
    "ax3.set_ylabel('F1 Score', fontsize=9)\n",
    "ax3.set_title('C. Distribution', fontsize=11, fontweight='bold', loc='left')\n",
    "ax3.tick_params(axis='x', labelsize=8)\n",
    "sns.despine(ax=ax3)\n",
    "\n",
    "# --- Panel D: Difference lollipop ---\n",
    "ax4 = fig.add_subplot(gs[2, 0])\n",
    "diff_sorted = plot_df.sort_values('difference')\n",
    "y_diff = np.arange(len(diff_sorted))\n",
    "diff_colors = [colors_current if d >= 0 else '#e74c3c' for d in diff_sorted['difference']]\n",
    "\n",
    "ax4.hlines(y=y_diff, xmin=0, xmax=diff_sorted['difference'], color=diff_colors, linewidth=1.5)\n",
    "ax4.scatter(diff_sorted['difference'], y_diff, color=diff_colors, s=30, zorder=3)\n",
    "ax4.axvline(x=0, color='black', linewidth=0.8)\n",
    "\n",
    "ax4.set_yticks(y_diff)\n",
    "ax4.set_yticklabels(diff_sorted['field'], fontsize=7)\n",
    "ax4.set_xlabel('F1 Difference', fontsize=9)\n",
    "ax4.set_title('D. F1 Improvement per Field', fontsize=11, fontweight='bold', loc='left')\n",
    "ax4.set_xlim(min(diff_sorted['difference']) - 0.05, max(diff_sorted['difference']) + 0.05)\n",
    "sns.despine(ax=ax4)\n",
    "\n",
    "# --- Panel E: Summary metrics table ---\n",
    "ax5 = fig.add_subplot(gs[2, 1])\n",
    "ax5.axis('off')\n",
    "\n",
    "# Create summary table\n",
    "# Use abbreviated names for table headers\n",
    "current_short = CURRENT_MODEL_NAME.replace('InternVL', 'IVL').replace('-', '')\n",
    "competing_short = COMPETING_MODEL_NAME\n",
    "\n",
    "# Model comparison metrics\n",
    "metrics_data = [\n",
    "    ['', current_short, competing_short],\n",
    "    ['Mean F1', f\"{metrics['current_mean']:.1%}\", f\"{metrics['competing_mean']:.1%}\"],\n",
    "    ['Median', f\"{metrics['current_median']:.1%}\", f\"{metrics['competing_median']:.1%}\"],\n",
    "    ['Std Dev', f\"{metrics['current_std']:.1%}\", f\"{metrics['competing_std']:.1%}\"],\n",
    "    ['Critical', f\"{metrics['critical_current_mean']:.1%}\", f\"{metrics['critical_competing_mean']:.1%}\"],\n",
    "]\n",
    "\n",
    "# Statistical tests\n",
    "stats_data = [\n",
    "    ['Test', 'p-value', 'Effect'],\n",
    "    ['t-test', f\"{stat_results['paired_ttest']['pvalue']:.4f}\", \n",
    "     '***' if stat_results['paired_ttest']['pvalue'] < 0.001 else ('**' if stat_results['paired_ttest']['pvalue'] < 0.01 else ('*' if stat_results['paired_ttest']['pvalue'] < 0.05 else 'ns'))],\n",
    "    [\"Cohen's d\", f\"{stat_results['cohens_d']:.2f}\", effect_interp],\n",
    "    ['95% CI', f\"{stat_results['bootstrap_ci_95'][0]:+.0%} to {stat_results['bootstrap_ci_95'][1]:+.0%}\", ''],\n",
    "]\n",
    "\n",
    "# Draw metrics table\n",
    "table1 = ax5.table(cellText=metrics_data, loc='upper center', cellLoc='center',\n",
    "                   colWidths=[0.28, 0.36, 0.36], bbox=[0, 0.55, 1, 0.45])\n",
    "table1.auto_set_font_size(False)\n",
    "table1.set_fontsize(8)\n",
    "\n",
    "# Style metrics table\n",
    "for j in range(3):\n",
    "    table1[(0, j)].set_facecolor('#2ecc71' if j == 1 else ('#3498db' if j == 2 else '#f5f5f5'))\n",
    "    table1[(0, j)].set_text_props(fontweight='bold', color='white' if j > 0 else 'black')\n",
    "for i in range(1, 5):\n",
    "    for j in range(3):\n",
    "        table1[(i, j)].set_facecolor('#f9f9f9' if i % 2 == 0 else 'white')\n",
    "\n",
    "# Draw stats table\n",
    "table2 = ax5.table(cellText=stats_data, loc='lower center', cellLoc='center',\n",
    "                   colWidths=[0.28, 0.36, 0.36], bbox=[0, 0, 1, 0.45])\n",
    "table2.auto_set_font_size(False)\n",
    "table2.set_fontsize(8)\n",
    "\n",
    "# Style stats table\n",
    "for j in range(3):\n",
    "    table2[(0, j)].set_facecolor('#555555')\n",
    "    table2[(0, j)].set_text_props(fontweight='bold', color='white')\n",
    "for i in range(1, 4):\n",
    "    for j in range(3):\n",
    "        table2[(i, j)].set_facecolor('#f9f9f9' if i % 2 == 0 else 'white')\n",
    "\n",
    "ax5.set_title('E. Summary Statistics', fontsize=11, fontweight='bold', loc='left', pad=10)\n",
    "\n",
    "# --- Panel F: Additional Capabilities (bottom spanning both columns) ---\n",
    "ax6 = fig.add_subplot(gs[3, 0])\n",
    "ax6.axis('off')\n",
    "ax6.set_title('F. Additional Capabilities', fontsize=11, fontweight='bold', loc='left')\n",
    "\n",
    "# Build additional capabilities text\n",
    "if len(exclusive_df) > 0:\n",
    "    exc_text = f\"({CURRENT_MODEL_NAME} only)\\n\\n\"\n",
    "    exc_text += \"Fields unique to our model:\\n\\n\"\n",
    "    for _, row in exclusive_df.iterrows():\n",
    "        acc_str = f\"{row['current_accuracy']:.1%}\" if pd.notna(row['current_accuracy']) else \"N/A\"\n",
    "        exc_text += f\"   \u2022 {row['field']}: {acc_str} accuracy\\n\"\n",
    "    if not np.isnan(metrics.get('exclusive_mean', np.nan)):\n",
    "        exc_text += f\"\\n   Mean F1 of exclusive capabilities: {metrics['exclusive_mean']:.1%}\"\n",
    "else:\n",
    "    exc_text = \"No exclusive capabilities to report.\"\n",
    "\n",
    "# Add styled box for additional capabilities\n",
    "ax6.text(0.5, 0.5, exc_text, transform=ax6.transAxes, fontsize=9,\n",
    "         ha='center', va='center',\n",
    "         bbox=dict(boxstyle='round,pad=0.3', facecolor='#e8f5e9', edgecolor='#2ecc71', linewidth=2))\n",
    "\n",
    "\n",
    "\n",
    "# --- Panel G: Throughput Comparison ---\n",
    "ax7 = fig.add_subplot(gs[3, 1])\n",
    "\n",
    "# Throughput data - use shorter labels to avoid overlap\n",
    "throughput_models = ['IVL3.5-8B', 'LayoutLM']\n",
    "throughput_values = [1.6, 3.52]\n",
    "throughput_colors = ['#2ecc71', '#3498db']\n",
    "\n",
    "bars = ax7.barh(throughput_models, throughput_values, color=throughput_colors, height=0.5, edgecolor='black', linewidth=1.2)\n",
    "\n",
    "# Add value labels\n",
    "for bar, val in zip(bars, throughput_values):\n",
    "    ax7.text(val + 0.08, bar.get_y() + bar.get_height()/2, \n",
    "             f'{val:.1f} pages/min', va='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "ax7.set_xlabel('Throughput (pages/min)', fontsize=9)\n",
    "ax7.set_title('G. Throughput', fontsize=11, fontweight='bold', loc='left')\n",
    "ax7.set_xlim(0, 5)\n",
    "ax7.tick_params(axis='y', labelsize=9)\n",
    "ax7.xaxis.grid(True, linestyle='--', alpha=0.7)\n",
    "ax7.set_axisbelow(True)\n",
    "sns.despine(ax=ax7)\n",
    "\n",
    "plt.savefig(OUTPUT_DIR / 'comparison_dashboard_a4.png', dpi=600, bbox_inches='tight', \n",
    "            facecolor='white', edgecolor='none')\n",
    "plt.savefig(EXPORTS_DIR / 'comparison_dashboard_a4.svg', bbox_inches='tight')\n",
    "plt.savefig(OUTPUT_DIR / 'comparison_dashboard_a4.pdf', bbox_inches='tight',\n",
    "            facecolor='white', edgecolor='none')\n",
    "print(f\"Saved: {OUTPUT_DIR / 'comparison_dashboard_a4.png'}\")\n",
    "print(f\"Saved: {OUTPUT_DIR / 'comparison_dashboard_a4.pdf'}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \ud83d\udcca A4 Dashboard - Supporting Data Tables\n",
    "\n",
    "The tables below contain the exact values used to generate each panel in the A4 dashboard above.\n",
    "Data Scientists can use these to verify the accuracy of all visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# A4 DASHBOARD - SUPPORTING DATA TABLES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"\ud83d\udcca A4 DASHBOARD - SUPPORTING DATA TABLES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# TABLE A: Field-Level F1 Scores\n",
    "print(\"\\n\ud83d\udcc8 TABLE A: Field-Level F1 Scores\")\n",
    "print(\"-\" * 80)\n",
    "field_table = plot_df[['field', 'current_accuracy', 'competing_accuracy', 'difference']].copy()\n",
    "field_table = field_table.sort_values('current_accuracy', ascending=False)\n",
    "field_display = field_table.copy()\n",
    "field_display['current_accuracy'] = field_display['current_accuracy'].apply(lambda x: f\"{x:.1%}\")\n",
    "field_display['competing_accuracy'] = field_display['competing_accuracy'].apply(lambda x: f\"{x:.1%}\")\n",
    "field_display['difference'] = field_display['difference'].apply(lambda x: f\"{x:+.1%}\")\n",
    "field_display.columns = ['Field', CURRENT_MODEL_NAME, COMPETING_MODEL_NAME, 'Difference']\n",
    "print(field_display.to_string(index=False))\n",
    "\n",
    "# TABLE B: Category Averages\n",
    "print(\"\\n\\n\ud83d\udcc8 TABLE B: Category Average F1 Scores\")\n",
    "print(\"-\" * 80)\n",
    "cat_table_data = []\n",
    "for cat, fields in FIELD_CATEGORIES.items():\n",
    "    cat_df_temp = comparison_df[comparison_df['field'].isin(fields)]\n",
    "    if len(cat_df_temp) > 0:\n",
    "        cat_table_data.append({\n",
    "            'Category': cat,\n",
    "            CURRENT_MODEL_NAME: f\"{cat_df_temp['current_accuracy'].mean():.1%}\",\n",
    "            COMPETING_MODEL_NAME: f\"{cat_df_temp['competing_accuracy'].mean():.1%}\",\n",
    "            'Difference': f\"{cat_df_temp['current_accuracy'].mean() - cat_df_temp['competing_accuracy'].mean():+.1%}\",\n",
    "        })\n",
    "print(pd.DataFrame(cat_table_data).to_string(index=False))\n",
    "\n",
    "# TABLE C: Distribution Statistics\n",
    "print(\"\\n\\n\ud83d\udcc8 TABLE C: Distribution Statistics\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Statistic':<12} {CURRENT_MODEL_NAME:<20} {COMPETING_MODEL_NAME:<20}\")\n",
    "print(f\"{'Mean':<12} {plot_df['current_accuracy'].mean():<20.1%} {plot_df['competing_accuracy'].mean():<20.1%}\")\n",
    "print(f\"{'Median':<12} {plot_df['current_accuracy'].median():<20.1%} {plot_df['competing_accuracy'].median():<20.1%}\")\n",
    "print(f\"{'Std Dev':<12} {plot_df['current_accuracy'].std():<20.1%} {plot_df['competing_accuracy'].std():<20.1%}\")\n",
    "print(f\"{'Min':<12} {plot_df['current_accuracy'].min():<20.1%} {plot_df['competing_accuracy'].min():<20.1%}\")\n",
    "print(f\"{'Max':<12} {plot_df['current_accuracy'].max():<20.1%} {plot_df['competing_accuracy'].max():<20.1%}\")\n",
    "\n",
    "# TABLE E: Summary Metrics\n",
    "print(\"\\n\\n\ud83d\udcc8 TABLE E: Summary Metrics\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Mean F1: {CURRENT_MODEL_NAME}={metrics['current_mean']:.1%}, {COMPETING_MODEL_NAME}={metrics['competing_mean']:.1%}\")\n",
    "print(f\"Paired t-test p-value: {stat_results['paired_ttest']['pvalue']:.4f}\")\n",
    "print(f\"Cohen's d: {stat_results['cohens_d']:.2f} ({effect_interp})\")\n",
    "print(f\"95% CI: {stat_results['bootstrap_ci_95'][0]:+.1%} to {stat_results['bootstrap_ci_95'][1]:+.1%}\")\n",
    "\n",
    "# TABLE F: Additional Capabilities\n",
    "print(\"\\n\\n\ud83d\udcc8 TABLE F: Additional Capabilities\")\n",
    "print(\"-\" * 80)\n",
    "if len(exclusive_df) > 0:\n",
    "    exc_display = exclusive_df[['field', 'current_accuracy']].copy()\n",
    "    exc_display['current_accuracy'] = exc_display['current_accuracy'].apply(lambda x: f\"{x:.1%}\" if pd.notna(x) else \"N/A\")\n",
    "    exc_display.columns = ['Field', 'F1 Score']\n",
    "    print(exc_display.to_string(index=False))\n",
    "else:\n",
    "    print(\"No exclusive capabilities.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LANDSCAPE DASHBOARD - Slide presentation format (16:9)\n",
    "# =============================================================================\n",
    "\n",
    "fig = plt.figure(figsize=(14, 8), facecolor='white')\n",
    "\n",
    "# Grid: 3 rows, 4 columns - F and G stack under D\n",
    "gs = fig.add_gridspec(3, 4, height_ratios=[1.1, 0.5, 0.5], width_ratios=[1.5, 0.8, 0.8, 1],\n",
    "                      hspace=0.3, wspace=0.35, top=0.88, bottom=0.08, left=0.06, right=0.98)\n",
    "\n",
    "# Colors\n",
    "colors_current = '#2ecc71'\n",
    "colors_competing = '#3498db'\n",
    "\n",
    "# --- Title & Legend (top) ---\n",
    "ax_title = fig.add_axes([0.02, 0.91, 0.96, 0.08])\n",
    "ax_title.axis('off')\n",
    "ax_title.text(0.5, 0.7, 'Model Position-Aware F1 Comparison', \n",
    "              fontsize=20, fontweight='bold', ha='center', va='center')\n",
    "ax_title.text(0.5, 0.15, \n",
    "              f'{CURRENT_MODEL_NAME} vs {COMPETING_MODEL_NAME}  |  '\n",
    "              f'{metrics[\"n_fields_compared\"]} comparable fields  |  '\n",
    "              f'Mean improvement: {metrics[\"mean_difference\"]:+.1%}', \n",
    "              fontsize=11, ha='center', va='center', color='#555')\n",
    "\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [\n",
    "    Patch(facecolor=colors_current, alpha=0.85, label=CURRENT_MODEL_NAME),\n",
    "    Patch(facecolor=colors_competing, alpha=0.85, label=COMPETING_MODEL_NAME)\n",
    "]\n",
    "fig.legend(handles=legend_elements, loc='upper right', bbox_to_anchor=(0.98, 0.98),\n",
    "           ncol=1, fontsize=10, frameon=True, fancybox=True)\n",
    "\n",
    "# --- Panel A: Field-level comparison (left, spans all 3 rows) ---\n",
    "ax1 = fig.add_subplot(gs[:, 0])\n",
    "plot_sorted = plot_df.sort_values('current_accuracy', ascending=True)\n",
    "y_pos = np.arange(len(plot_sorted))\n",
    "\n",
    "ax1.barh(y_pos + 0.2, plot_sorted['current_accuracy'], 0.4, \n",
    "         label=CURRENT_MODEL_NAME, color=colors_current, alpha=0.85)\n",
    "ax1.barh(y_pos - 0.2, plot_sorted['competing_accuracy'], 0.4,\n",
    "         label=COMPETING_MODEL_NAME, color=colors_competing, alpha=0.85)\n",
    "\n",
    "ax1.set_yticks(y_pos)\n",
    "ax1.set_yticklabels(plot_sorted['field'], fontsize=9)\n",
    "ax1.set_xlabel('Position-Aware F1', fontsize=10)\n",
    "ax1.set_xlim(0, 1)\n",
    "ax1.set_title('A. Field-Level F1 (Comparable Fields)', fontsize=12, fontweight='bold', loc='left')\n",
    "ax1.axvline(x=0.5, color='gray', linestyle=':', alpha=0.5)\n",
    "sns.despine(ax=ax1)\n",
    "\n",
    "for i, (_, row) in enumerate(plot_sorted.iterrows()):\n",
    "    ax1.text(row['current_accuracy'] + 0.01, i + 0.2, f\"{row['current_accuracy']:.0%}\", \n",
    "             va='center', ha='left', fontsize=7, color=colors_current)\n",
    "    ax1.text(row['competing_accuracy'] + 0.01, i - 0.2, f\"{row['competing_accuracy']:.0%}\", \n",
    "             va='center', ha='left', fontsize=7, color=colors_competing)\n",
    "\n",
    "# --- Panel B: Radar Chart ---\n",
    "ax2 = fig.add_subplot(gs[0, 1], polar=True)\n",
    "\n",
    "cat_data = []\n",
    "for cat, fields in FIELD_CATEGORIES.items():\n",
    "    cat_df_temp = comparison_df[comparison_df['field'].isin(fields)]\n",
    "    if len(cat_df_temp) > 0:\n",
    "        cat_data.append({\n",
    "            'category': cat,\n",
    "            'current': cat_df_temp['current_accuracy'].mean(),\n",
    "            'competing': cat_df_temp['competing_accuracy'].mean()\n",
    "        })\n",
    "\n",
    "radar_df = pd.DataFrame(cat_data)\n",
    "categories_list = radar_df['category'].tolist()\n",
    "N = len(categories_list)\n",
    "\n",
    "angles = [n / float(N) * 2 * np.pi for n in range(N)]\n",
    "angles += angles[:1]\n",
    "\n",
    "values_current = radar_df['current'].tolist()\n",
    "values_current += values_current[:1]\n",
    "ax2.plot(angles, values_current, 'o-', linewidth=2, color=colors_current, markersize=4)\n",
    "ax2.fill(angles, values_current, alpha=0.25, color=colors_current)\n",
    "\n",
    "values_competing = radar_df['competing'].tolist()\n",
    "values_competing += values_competing[:1]\n",
    "ax2.plot(angles, values_competing, 'o-', linewidth=2, color=colors_competing, markersize=4)\n",
    "ax2.fill(angles, values_competing, alpha=0.25, color=colors_competing)\n",
    "\n",
    "ax2.set_xticks(angles[:-1])\n",
    "ax2.set_xticklabels(categories_list, fontsize=8)\n",
    "ax2.set_ylim(0, 1)\n",
    "ax2.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "ax2.set_yticklabels(['0.2', '', '0.6', '', '1.0'], fontsize=7)\n",
    "ax2.set_title('B. By Category', fontsize=12, fontweight='bold', loc='left', pad=15)\n",
    "\n",
    "# --- Panel C: Distribution ---\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "box_df = pd.DataFrame({\n",
    "    'Model': [CURRENT_MODEL_NAME] * len(plot_df) + [COMPETING_MODEL_NAME] * len(plot_df),\n",
    "    'F1': list(plot_df['current_accuracy']) + list(plot_df['competing_accuracy'])\n",
    "})\n",
    "\n",
    "sns.boxplot(data=box_df, x='Model', y='F1', ax=ax3, \n",
    "            palette=[colors_current, colors_competing], width=0.5)\n",
    "sns.stripplot(data=box_df, x='Model', y='F1', ax=ax3,\n",
    "              color='black', alpha=0.4, size=4, jitter=0.1)\n",
    "\n",
    "ax3.set_ylim(0, 1)\n",
    "ax3.set_xlabel('')\n",
    "ax3.set_ylabel('Position-Aware F1', fontsize=10)\n",
    "ax3.set_title('C. Distribution', fontsize=12, fontweight='bold', loc='left')\n",
    "ax3.tick_params(axis='x', labelsize=9)\n",
    "sns.despine(ax=ax3)\n",
    "\n",
    "# --- Panel D: Summary Statistics (top right) ---\n",
    "ax4 = fig.add_subplot(gs[0, 3])\n",
    "ax4.axis('off')\n",
    "\n",
    "current_short = CURRENT_MODEL_NAME.replace('InternVL', 'IVL').replace('-', '')\n",
    "competing_short = COMPETING_MODEL_NAME\n",
    "\n",
    "metrics_data = [\n",
    "    ['', current_short, competing_short],\n",
    "    ['Mean F1', f\"{metrics['current_mean']:.1%}\", f\"{metrics['competing_mean']:.1%}\"],\n",
    "    ['Median', f\"{metrics['current_median']:.1%}\", f\"{metrics['competing_median']:.1%}\"],\n",
    "    ['Critical', f\"{metrics['critical_current_mean']:.1%}\", f\"{metrics['critical_competing_mean']:.1%}\"],\n",
    "]\n",
    "\n",
    "stats_data = [\n",
    "    ['Test', 'p-value', 'Effect'],\n",
    "    ['t-test', f\"{stat_results['paired_ttest']['pvalue']:.4f}\", \n",
    "     '***' if stat_results['paired_ttest']['pvalue'] < 0.001 else ('**' if stat_results['paired_ttest']['pvalue'] < 0.01 else ('*' if stat_results['paired_ttest']['pvalue'] < 0.05 else 'ns'))],\n",
    "    [\"Cohen's d\", f\"{stat_results['cohens_d']:.2f}\", effect_interp],\n",
    "]\n",
    "\n",
    "table1 = ax4.table(cellText=metrics_data, loc='upper center', cellLoc='center',\n",
    "                   colWidths=[0.3, 0.35, 0.35], bbox=[0, 0.5, 1, 0.5])\n",
    "table1.auto_set_font_size(False)\n",
    "table1.set_fontsize(9)\n",
    "\n",
    "for j in range(3):\n",
    "    table1[(0, j)].set_facecolor(colors_current if j == 1 else (colors_competing if j == 2 else '#f5f5f5'))\n",
    "    table1[(0, j)].set_text_props(fontweight='bold', color='white' if j > 0 else 'black')\n",
    "for i in range(1, 4):\n",
    "    for j in range(3):\n",
    "        table1[(i, j)].set_facecolor('#f9f9f9' if i % 2 == 0 else 'white')\n",
    "\n",
    "table2 = ax4.table(cellText=stats_data, loc='lower center', cellLoc='center',\n",
    "                   colWidths=[0.3, 0.35, 0.35], bbox=[0, 0, 1, 0.45])\n",
    "table2.auto_set_font_size(False)\n",
    "table2.set_fontsize(9)\n",
    "\n",
    "for j in range(3):\n",
    "    table2[(0, j)].set_facecolor('#555555')\n",
    "    table2[(0, j)].set_text_props(fontweight='bold', color='white')\n",
    "for i in range(1, 3):\n",
    "    for j in range(3):\n",
    "        table2[(i, j)].set_facecolor('#f9f9f9' if i % 2 == 0 else 'white')\n",
    "\n",
    "ax4.set_title('D. Summary Statistics', fontsize=12, fontweight='bold', loc='left', pad=10)\n",
    "\n",
    "# --- Panel E: Improvement per Field (spans rows 1-2, cols 1-2) ---\n",
    "ax5 = fig.add_subplot(gs[1:3, 1:3])\n",
    "diff_sorted = plot_df.sort_values('difference')\n",
    "y_diff = np.arange(len(diff_sorted))\n",
    "diff_colors = [colors_current if d >= 0 else '#e74c3c' for d in diff_sorted['difference']]\n",
    "\n",
    "ax5.hlines(y=y_diff, xmin=0, xmax=diff_sorted['difference'], color=diff_colors, linewidth=2)\n",
    "ax5.scatter(diff_sorted['difference'], y_diff, color=diff_colors, s=50, zorder=3)\n",
    "ax5.axvline(x=0, color='black', linewidth=1)\n",
    "\n",
    "ax5.set_yticks(y_diff)\n",
    "ax5.set_yticklabels(diff_sorted['field'], fontsize=8)\n",
    "ax5.set_xlabel('Position-Aware F1 Difference', fontsize=10)\n",
    "ax5.set_title('E. Improvement per Field', fontsize=12, fontweight='bold', loc='left')\n",
    "ax5.set_xlim(min(diff_sorted['difference']) - 0.05, max(diff_sorted['difference']) + 0.05)\n",
    "sns.despine(ax=ax5)\n",
    "\n",
    "# --- Panel F: Additional Capabilities (row 1, col 3 - under D) ---\n",
    "ax6 = fig.add_subplot(gs[1, 3])\n",
    "ax6.axis('off')\n",
    "\n",
    "ax6.set_title('F. Additional Capabilities', fontsize=11, fontweight='bold', loc='left')\n",
    "\n",
    "model_short = CURRENT_MODEL_NAME.replace('InternVL', 'IVL')\n",
    "exc_lines = [f\"({model_short} only)\", \"\"]\n",
    "if len(exclusive_df) > 0:\n",
    "    for _, row in exclusive_df.iterrows():\n",
    "        acc_str = f\"{row['current_accuracy']:.1%}\" if pd.notna(row['current_accuracy']) else \"N/A\"\n",
    "        exc_lines.append(f\"{row['field']}: {acc_str}\")\n",
    "    if not np.isnan(metrics.get('exclusive_mean', np.nan)):\n",
    "        exc_lines.append(f\"Mean: {metrics['exclusive_mean']:.1%}\")\n",
    "else:\n",
    "    exc_lines.append(\"None\")\n",
    "exc_text = chr(10).join(exc_lines)\n",
    "\n",
    "ax6.text(0.5, 0.4, exc_text, transform=ax6.transAxes, fontsize=9,\n",
    "         ha='center', va='center',\n",
    "         bbox=dict(boxstyle='round,pad=0.3', facecolor='#e8f5e9', edgecolor='#2ecc71', linewidth=2))\n",
    "\n",
    "# --- Panel G: Throughput (row 2, col 3 - under F) ---\n",
    "ax7 = fig.add_subplot(gs[2, 3])\n",
    "\n",
    "throughput_models = ['LayoutLM', 'IVL3.5-8B']\n",
    "throughput_values = [3.5, 1.6]\n",
    "throughput_colors = [colors_competing, colors_current]\n",
    "\n",
    "bars = ax7.barh(throughput_models, throughput_values, color=throughput_colors, height=0.5, \n",
    "                edgecolor='black', linewidth=1)\n",
    "\n",
    "for bar, val in zip(bars, throughput_values):\n",
    "    ax7.text(val + 0.08, bar.get_y() + bar.get_height()/2, \n",
    "             f'{val:.1f}', va='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "ax7.set_xlabel('pages/min', fontsize=9)\n",
    "ax7.set_title('G. Throughput', fontsize=11, fontweight='bold', loc='left')\n",
    "ax7.set_xlim(0, 5)\n",
    "ax7.tick_params(axis='y', labelsize=9)\n",
    "ax7.xaxis.grid(True, linestyle='--', alpha=0.7)\n",
    "ax7.set_axisbelow(True)\n",
    "sns.despine(ax=ax7)\n",
    "\n",
    "plt.savefig(OUTPUT_DIR / 'comparison_dashboard_landscape.png', dpi=600, bbox_inches='tight', \n",
    "            facecolor='white', edgecolor='none')\n",
    "plt.savefig(EXPORTS_DIR / 'comparison_dashboard_landscape.svg', bbox_inches='tight')\n",
    "print(f\"Saved: {OUTPUT_DIR / 'comparison_dashboard_landscape.png'}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \ud83d\udcca Landscape Dashboard - Supporting Data Tables\n",
    "\n",
    "The tables below contain the exact values used to generate each panel in the Landscape dashboard above.\n",
    "Data Scientists can use these to verify the accuracy of all visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# A4 DASHBOARD - SUPPORTING DATA TABLES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"\ud83d\udcca A4 DASHBOARD - SUPPORTING DATA TABLES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# TABLE A: Field-Level F1 Scores\n",
    "print(\"\\n\ud83d\udcc8 TABLE A: Field-Level F1 Scores\")\n",
    "print(\"-\" * 80)\n",
    "field_table = plot_df[['field', 'current_accuracy', 'competing_accuracy', 'difference']].copy()\n",
    "field_table = field_table.sort_values('current_accuracy', ascending=False)\n",
    "field_display = field_table.copy()\n",
    "field_display['current_accuracy'] = field_display['current_accuracy'].apply(lambda x: f\"{x:.1%}\")\n",
    "field_display['competing_accuracy'] = field_display['competing_accuracy'].apply(lambda x: f\"{x:.1%}\")\n",
    "field_display['difference'] = field_display['difference'].apply(lambda x: f\"{x:+.1%}\")\n",
    "field_display.columns = ['Field', CURRENT_MODEL_NAME, COMPETING_MODEL_NAME, 'Difference']\n",
    "print(field_display.to_string(index=False))\n",
    "\n",
    "# TABLE B: Category Averages\n",
    "print(\"\\n\\n\ud83d\udcc8 TABLE B: Category Average F1 Scores\")\n",
    "print(\"-\" * 80)\n",
    "cat_table_data = []\n",
    "for cat, fields in FIELD_CATEGORIES.items():\n",
    "    cat_df_temp = comparison_df[comparison_df['field'].isin(fields)]\n",
    "    if len(cat_df_temp) > 0:\n",
    "        cat_table_data.append({\n",
    "            'Category': cat,\n",
    "            CURRENT_MODEL_NAME: f\"{cat_df_temp['current_accuracy'].mean():.1%}\",\n",
    "            COMPETING_MODEL_NAME: f\"{cat_df_temp['competing_accuracy'].mean():.1%}\",\n",
    "            'Difference': f\"{cat_df_temp['current_accuracy'].mean() - cat_df_temp['competing_accuracy'].mean():+.1%}\",\n",
    "        })\n",
    "print(pd.DataFrame(cat_table_data).to_string(index=False))\n",
    "\n",
    "# TABLE C: Distribution Statistics\n",
    "print(\"\\n\\n\ud83d\udcc8 TABLE C: Distribution Statistics\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Statistic':<12} {CURRENT_MODEL_NAME:<20} {COMPETING_MODEL_NAME:<20}\")\n",
    "print(f\"{'Mean':<12} {plot_df['current_accuracy'].mean():<20.1%} {plot_df['competing_accuracy'].mean():<20.1%}\")\n",
    "print(f\"{'Median':<12} {plot_df['current_accuracy'].median():<20.1%} {plot_df['competing_accuracy'].median():<20.1%}\")\n",
    "print(f\"{'Std Dev':<12} {plot_df['current_accuracy'].std():<20.1%} {plot_df['competing_accuracy'].std():<20.1%}\")\n",
    "print(f\"{'Min':<12} {plot_df['current_accuracy'].min():<20.1%} {plot_df['competing_accuracy'].min():<20.1%}\")\n",
    "print(f\"{'Max':<12} {plot_df['current_accuracy'].max():<20.1%} {plot_df['competing_accuracy'].max():<20.1%}\")\n",
    "\n",
    "# TABLE E: Summary Metrics\n",
    "print(\"\\n\\n\ud83d\udcc8 TABLE E: Summary Metrics\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Mean F1: {CURRENT_MODEL_NAME}={metrics['current_mean']:.1%}, {COMPETING_MODEL_NAME}={metrics['competing_mean']:.1%}\")\n",
    "print(f\"Paired t-test p-value: {stat_results['paired_ttest']['pvalue']:.4f}\")\n",
    "print(f\"Cohen's d: {stat_results['cohens_d']:.2f} ({effect_interp})\")\n",
    "print(f\"95% CI: {stat_results['bootstrap_ci_95'][0]:+.1%} to {stat_results['bootstrap_ci_95'][1]:+.1%}\")\n",
    "\n",
    "# TABLE F: Additional Capabilities\n",
    "print(\"\\n\\n\ud83d\udcc8 TABLE F: Additional Capabilities\")\n",
    "print(\"-\" * 80)\n",
    "if len(exclusive_df) > 0:\n",
    "    exc_display = exclusive_df[['field', 'current_accuracy']].copy()\n",
    "    exc_display['current_accuracy'] = exc_display['current_accuracy'].apply(lambda x: f\"{x:.1%}\" if pd.notna(x) else \"N/A\")\n",
    "    exc_display.columns = ['Field', 'F1 Score']\n",
    "    print(exc_display.to_string(index=False))\n",
    "else:\n",
    "    print(\"No exclusive capabilities.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary report with Additional Capabilities and F1 section\n",
    "report = f\"\"\"# Model F1 Comparison Report\n",
    "\n",
    "## Overview\n",
    "- **Current Model**: {CURRENT_MODEL_NAME}\n",
    "- **Competing Model**: {COMPETING_MODEL_NAME}\n",
    "- **Comparable Fields**: {metrics['n_fields_compared']}\n",
    "- **Fields with F1 Comparison**: {metrics['n_fields_with_f1']}\n",
    "- **Additional Capabilities** (our model only): {metrics['n_exclusive_fields']}\n",
    "- **Document Types**: Invoices, Receipts, Bank Statements (195 documents)\n",
    "\n",
    "## Summary Statistics (Comparable Fields Only)\n",
    "\n",
    "### Accuracy Metrics\n",
    "\n",
    "| Metric | {CURRENT_MODEL_NAME} | {COMPETING_MODEL_NAME} |\n",
    "|--------|-----------|----------|\n",
    "| Mean Accuracy | {metrics['current_mean']:.1%} | {metrics['competing_mean']:.1%} |\n",
    "| Median Accuracy | {metrics['current_median']:.1%} | {metrics['competing_median']:.1%} |\n",
    "| Std Dev | {metrics['current_std']:.1%} | {metrics['competing_std']:.1%} |\n",
    "| Min | {metrics['current_min']:.1%} | {metrics['competing_min']:.1%} |\n",
    "| Max | {metrics['current_max']:.1%} | {metrics['competing_max']:.1%} |\n",
    "\"\"\"\n",
    "\n",
    "# Add F1 metrics if available\n",
    "if metrics['n_fields_with_f1'] > 0:\n",
    "    report += f\"\"\"\n",
    "### F1 Score Metrics (Position-Aware)\n",
    "\n",
    "| Metric | {CURRENT_MODEL_NAME} | {COMPETING_MODEL_NAME} |\n",
    "|--------|-----------|----------|\n",
    "| Mean F1 | {metrics['current_f1_mean']:.1%} | {metrics['competing_f1_mean']:.1%} |\n",
    "| Mean F1 Difference | {metrics['f1_mean_difference']:+.1%} | - |\n",
    "| Fields {CURRENT_MODEL_NAME} Better | {metrics['f1_fields_current_better']} | - |\n",
    "| Fields {COMPETING_MODEL_NAME} Better | {metrics['f1_fields_competing_better']} | - |\n",
    "\n",
    "*Note: F1 scores use position-aware matching where items must match both in value AND position.*\n",
    "\"\"\"\n",
    "\n",
    "report += f\"\"\"\n",
    "## Comparison Results\n",
    "\n",
    "- **Mean Accuracy Difference**: {metrics['mean_difference']:+.1%} (positive = {CURRENT_MODEL_NAME} better)\n",
    "- **Fields where {CURRENT_MODEL_NAME} better**: {metrics['fields_current_better']}\n",
    "- **Fields where {COMPETING_MODEL_NAME} better**: {metrics['fields_competing_better']}\n",
    "- **Fields equal**: {metrics['fields_equal']}\n",
    "\n",
    "## Critical Fields Analysis\n",
    "\n",
    "Critical fields: {', '.join(CRITICAL_FIELDS)}\n",
    "\n",
    "- **{CURRENT_MODEL_NAME} Mean**: {metrics['critical_current_mean']:.1%}\n",
    "- **{COMPETING_MODEL_NAME} Mean**: {metrics['critical_competing_mean']:.1%}\n",
    "\n",
    "## Statistical Significance\n",
    "\n",
    "- **Paired t-test p-value**: {stat_results['paired_ttest']['pvalue']:.4f} ({'Significant' if stat_results['paired_ttest']['pvalue'] < 0.05 else 'Not significant'} at \u03b1=0.05)\n",
    "- **Wilcoxon test p-value**: {stat_results['wilcoxon']['pvalue']:.4f} ({'Significant' if stat_results['wilcoxon']['pvalue'] < 0.05 else 'Not significant'} at \u03b1=0.05)\n",
    "- **Cohen's d effect size**: {stat_results['cohens_d']:.4f} ({effect_interp})\n",
    "- **95% CI for difference**: [{stat_results['bootstrap_ci_95'][0]:+.1%}, {stat_results['bootstrap_ci_95'][1]:+.1%}]\n",
    "\n",
    "## Field-Level Details (Comparable Fields)\n",
    "\n",
    "### Accuracy\n",
    "| Field | {CURRENT_MODEL_NAME} | {COMPETING_MODEL_NAME} | Difference | Critical |\n",
    "|-------|-----------|----------|------------|----------|\n",
    "\"\"\"\n",
    "\n",
    "for _, row in comparable_df.iterrows():\n",
    "    curr = f\"{row['current_accuracy']:.1%}\" if pd.notna(row['current_accuracy']) else \"N/A\"\n",
    "    comp = f\"{row['competing_accuracy']:.1%}\" if pd.notna(row['competing_accuracy']) else \"N/A\"\n",
    "    diff = f\"{row['difference']:+.1%}\" if pd.notna(row['difference']) else \"N/A\"\n",
    "    critical = \"Yes\" if row['is_critical'] else \"\"\n",
    "    report += f\"| {row['field']} | {curr} | {comp} | {diff} | {critical} |\\n\"\n",
    "\n",
    "# Add F1 table if available\n",
    "f1_data = comparable_df.dropna(subset=['current_f1', 'competing_f1'])\n",
    "if len(f1_data) > 0:\n",
    "    report += f\"\"\"\n",
    "### F1 Scores (Position-Aware)\n",
    "| Field | {CURRENT_MODEL_NAME} F1 | {COMPETING_MODEL_NAME} F1 | Difference |\n",
    "|-------|-----------|----------|------------|\n",
    "\"\"\"\n",
    "    for _, row in f1_data.iterrows():\n",
    "        curr_f1 = f\"{row['current_f1']:.1%}\" if pd.notna(row['current_f1']) else \"N/A\"\n",
    "        comp_f1 = f\"{row['competing_f1']:.1%}\" if pd.notna(row['competing_f1']) else \"N/A\"\n",
    "        diff_f1 = f\"{row['f1_difference']:+.1%}\" if pd.notna(row['f1_difference']) else \"N/A\"\n",
    "        report += f\"| {row['field']} | {curr_f1} | {comp_f1} | {diff_f1} |\\n\"\n",
    "\n",
    "# Add Additional Capabilities section\n",
    "if metrics['n_exclusive_fields'] > 0:\n",
    "    report += f\"\"\"\n",
    "## Additional Capabilities ({CURRENT_MODEL_NAME} Only)\n",
    "\n",
    "The following fields are **unique to our model** and not available in the competing model:\n",
    "\n",
    "| Field | {CURRENT_MODEL_NAME} Accuracy | Category |\n",
    "|-------|------------------|----------|\n",
    "\"\"\"\n",
    "    for _, row in exclusive_df.iterrows():\n",
    "        acc = f\"{row['current_accuracy']:.1%}\" if pd.notna(row['current_accuracy']) else \"N/A\"\n",
    "        report += f\"| {row['field']} | {acc} | {row['category']} |\\n\"\n",
    "    \n",
    "    if not np.isnan(metrics['exclusive_mean']):\n",
    "        report += f\"\\n**Mean F1 of exclusive capabilities**: {metrics['exclusive_mean']:.1%}\\n\"\n",
    "\n",
    "report += f\"\"\"\n",
    "## Output Files\n",
    "\n",
    "- `comparison_dashboard_a4.png` - A4 dashboard summary (300 dpi)\n",
    "- `comparison_dashboard_a4.pdf` - A4 dashboard PDF\n",
    "- `comparison_report.md` - This report\n",
    "- `accuracy_comparison_bars.png` - Side-by-side bar chart\n",
    "- `accuracy_difference_lollipop.png` - Difference visualization\n",
    "- `accuracy_radar_chart.png` - Category radar chart\n",
    "- `accuracy_heatmap.png` - Heatmap comparison\n",
    "- `accuracy_boxplot.png` - Distribution comparison\n",
    "- `accuracy_scatter.png` - Scatter plot with identity line\n",
    "- `accuracy_by_category.png` - Category comparison chart\n",
    "- `f1_comparison.png` - F1 score comparison (if F1 data available)\n",
    "\"\"\"\n",
    "\n",
    "# Save report\n",
    "report_path = OUTPUT_DIR / 'comparison_report.md'\n",
    "with open(report_path, 'w') as f:\n",
    "    f.write(report)\n",
    "\n",
    "print(f\"Saved: {report_path}\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"COMPARISON COMPLETE\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Quick Reference: Data Format Requirements\n",
    "\n",
    "### CSV File Formats\n",
    "\n",
    "**competing_model_accuracy.csv:**\n",
    "```csv\n",
    "Subset,Field,F1,Exclusive\n",
    "Invoices & receipts,BUSINESS_ABN,0.204081633,FALSE\n",
    "Invoices & receipts,SUPPLIER_NAME,0.587926509,FALSE\n",
    "Bank statements,TRANSACTION_DATES,0.56918239,FALSE\n",
    "...\n",
    "```\n",
    "\n",
    "**current_model_accuracy.csv:**\n",
    "```csv\n",
    "Subset,Field,F1,Precision,Recall,Exclusive\n",
    "Invoices & receipts,DOCUMENT_TYPE,0.728205128,0.728205128,0.728205,TRUE\n",
    "Invoices & receipts,BUSINESS_ABN,0.851282051,0.851282051,0.851282,FALSE\n",
    "Bank statements,TRANSACTION_DATES,0.925574861,0.94022792,0.928886,FALSE\n",
    "...\n",
    "```\n",
    "\n",
    "### Column Descriptions\n",
    "\n",
    "| Column | Description |\n",
    "|--------|-------------|\n",
    "| Subset | Document category: \"Invoices & receipts\" or \"Bank statements\" |\n",
    "| Field | Schema field name (e.g., BUSINESS_ABN, TOTAL_AMOUNT) |\n",
    "| F1 | F1 score (primary comparison metric) |\n",
    "| Precision | Precision score (optional) |\n",
    "| Recall | Recall score (optional) |\n",
    "| Exclusive | TRUE if field is exclusive to our model (not in competing model) |\n",
    "\n",
    "### F1 Calculation Method\n",
    "\n",
    "All F1 scores are computed using `calculate_field_accuracy_f1()` from `common/evaluation_metrics.py`.\n",
    "\n",
    "#### Single-Value Fields\n",
    "- **Text fields** (SUPPLIER_NAME, BUSINESS_ADDRESS): Levenshtein distance with ANLS-style 0.5 threshold\n",
    "- **ID fields** (BUSINESS_ABN): Exact match after normalization (remove spaces, dashes, prefixes)\n",
    "- **Monetary fields** (TOTAL_AMOUNT, GST_AMOUNT): Numeric comparison with 1% tolerance\n",
    "- **Boolean fields** (IS_GST_INCLUDED): Case-insensitive boolean parsing\n",
    "- **Date fields** (INVOICE_DATE, STATEMENT_DATE_RANGE): Semantic date parsing supporting ranges and format variations\n",
    "\n",
    "#### List Fields (Pipe-Separated Values)\n",
    "Uses **POSITION-AWARE (order-aware) F1 matching**:\n",
    "\n",
    "```\n",
    "Extracted:    [\"apple\", \"banana\", \"cherry\"]\n",
    "Ground Truth: [\"banana\", \"apple\", \"cherry\"]\n",
    "Result:       33.3% F1 (only position 2 matches)\n",
    "```\n",
    "\n",
    "**Position-by-position comparison:**\n",
    "- Position i: extracted[i] vs ground_truth[i]\n",
    "- Match at same position \u2192 TP (True Positive)\n",
    "- Mismatch at same position \u2192 FN (False Negative) \n",
    "- Extra extracted items \u2192 FP (False Positive)\n",
    "- Missing ground truth items \u2192 FN (False Negative)\n",
    "\n",
    "**Metrics:**\n",
    "- Precision = TP / (TP + FP)\n",
    "- Recall = TP / (TP + FN)  \n",
    "- F1 = 2 \u00d7 (Precision \u00d7 Recall) / (Precision + Recall)\n",
    "\n",
    "**List field matching by type:**\n",
    "- **Monetary lists** (LINE_ITEM_PRICES): Numeric comparison with 1% tolerance\n",
    "- **Quantity lists** (LINE_ITEM_QUANTITIES): Integer comparison\n",
    "- **Text lists** (LINE_ITEM_DESCRIPTIONS): Fuzzy text matching (0.75 word overlap threshold)\n",
    "- **Transaction lists** (TRANSACTION_DATES, TRANSACTION_AMOUNTS_PAID): Type-specific matching\n",
    "\n",
    "### Data Sources\n",
    "\n",
    "- **per_field_metrics.csv** - Generated by `model_comparison_reporter_v2.ipynb`\n",
    "- **harvest_model_accuracy.ipynb** - Exports per-model CSV files from per_field_metrics.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \ud83d\udce4 Export Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXPORT NOTEBOOK TO HTML\n",
    "# =============================================================================\n",
    "# SAVE the notebook first (Ctrl+S) before running this cell!\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import json as json_lib\n",
    "\n",
    "NOTEBOOK_NAME = \"lmmpoc_layoutlm_comparison.ipynb\"\n",
    "EXPORT_DIR = Path(\"./exports\")\n",
    "EXPORT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "base_name = NOTEBOOK_NAME.replace(\".ipynb\", \"\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"\ud83d\udce4 EXPORTING NOTEBOOK WITH OUTPUTS\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n\u26a0\ufe0f  Make sure you SAVED the notebook before running this cell!\")\n",
    "\n",
    "print(f\"\\n\ud83d\udcc4 Reading {NOTEBOOK_NAME}...\")\n",
    "with open(NOTEBOOK_NAME, 'r', encoding='utf-8') as f:\n",
    "    notebook = json_lib.load(f)\n",
    "\n",
    "print(\"\ud83d\udcc4 Converting to HTML...\")\n",
    "\n",
    "html_parts = [f\"\"\"<!DOCTYPE html>\n",
    "<html><head><meta charset=\"utf-8\"><title>Model F1 Comparison Dashboard</title>\n",
    "<style>\n",
    "body {{ font-family: sans-serif; max-width: 1200px; margin: 0 auto; padding: 20px; }}\n",
    ".cell {{ background: white; margin: 10px 0; padding: 15px; border-radius: 8px; box-shadow: 0 1px 3px rgba(0,0,0,0.1); }}\n",
    ".input {{ background: #f8f8f8; padding: 10px; font-family: monospace; font-size: 13px; overflow-x: auto; white-space: pre-wrap; }}\n",
    ".output img {{ max-width: 100%; }}\n",
    ".output pre {{ background: #f0f0f0; padding: 10px; font-family: monospace; }}\n",
    "</style></head><body>\n",
    "<h1>Model F1 Comparison Dashboard</h1><p>Generated: {timestamp}</p><hr>\"\"\"]\n",
    "\n",
    "for cell in notebook['cells']:\n",
    "    source = ''.join(cell.get('source', []))\n",
    "    if cell['cell_type'] == 'markdown':\n",
    "        html_parts.append(f'<div class=\"cell\">{source}</div>')\n",
    "    elif cell['cell_type'] == 'code':\n",
    "        html_parts.append(f'<div class=\"cell\"><div class=\"input\"><pre>{source}</pre></div>')\n",
    "        for output in cell.get('outputs', []):\n",
    "            if output.get('output_type') == 'stream':\n",
    "                html_parts.append(f'<div class=\"output\"><pre>{\"\".join(output.get(\"text\", []))}</pre></div>')\n",
    "            elif 'data' in output:\n",
    "                data = output['data']\n",
    "                if 'image/png' in data:\n",
    "                    html_parts.append(f'<div class=\"output\"><img src=\"data:image/png;base64,{data[\"image/png\"]}\"></div>')\n",
    "                elif 'text/plain' in data:\n",
    "                    html_parts.append(f'<div class=\"output\"><pre>{\"\".join(data[\"text/plain\"])}</pre></div>')\n",
    "        html_parts.append('</div>')\n",
    "\n",
    "html_parts.append('</body></html>')\n",
    "\n",
    "html_output = EXPORT_DIR / f\"{base_name}_{timestamp}.html\"\n",
    "with open(html_output, 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(html_parts))\n",
    "\n",
    "print(f\"\\n\u2705 HTML exported: {html_output}\")\n",
    "print(f\"   Size: {html_output.stat().st_size / 1024:.1f} KB\")\n",
    "print(\"\\n\ud83d\udccb To create PDF: Open HTML in browser \u2192 Print \u2192 Save as PDF\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "du",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}