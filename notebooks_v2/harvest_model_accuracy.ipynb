{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Harvest Model Accuracy from Evaluation Results\n",
    "\n",
    "This notebook scans `output/csv/` for evaluation results and generates the `current_model_accuracy.csv` file for use in model comparison.\n",
    "\n",
    "## Data Sources\n",
    "1. **per_field_metrics.csv** - Pre-computed field-level metrics by model\n",
    "2. **Model batch results** - Per-image results that can be aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning: /Users/tod/Desktop/LMM_POC/output/csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# Paths\n",
    "CSV_DIR = Path(\"../output/csv\")\n",
    "OUTPUT_DIR = Path(\"../output\")\n",
    "\n",
    "print(f\"Scanning: {CSV_DIR.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Discover Available Result Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Result Files\n",
      "============================================================\n",
      "\n",
      "per_field_metrics.csv: Found\n",
      "\n",
      "Llama batch results: 2 files\n",
      "  - llama_batch_results_20251210_003155.csv\n",
      "  - llama_batch_results_20251210_001755.csv\n",
      "\n",
      "InternVL3-8B batch results: 0 files\n",
      "\n",
      "InternVL3 (generic, treated as 8B): 1 files\n",
      "  - internvl3_batch_results_20251210_005902.csv\n",
      "\n",
      "InternVL3-2B batch results: 1 files\n",
      "  - internvl3_2b_batch_results_20251210_013149.csv\n"
     ]
    }
   ],
   "source": [
    "def discover_result_files(csv_dir: Path) -> dict:\n",
    "    \"\"\"Discover all available result files in the CSV directory.\"\"\"\n",
    "    files = {\n",
    "        'per_field_metrics': None,\n",
    "        'llama_batch': [],\n",
    "        'internvl3_batch': [],  # Generic internvl3 (treated as 8B)\n",
    "        'internvl3_2b_batch': [],\n",
    "        'internvl3_8b_batch': [],\n",
    "        'batch_summary': [],\n",
    "    }\n",
    "    \n",
    "    for f in csv_dir.glob(\"*.csv\"):\n",
    "        name = f.name\n",
    "        \n",
    "        if name == 'per_field_metrics.csv':\n",
    "            files['per_field_metrics'] = f\n",
    "        elif name.startswith('llama_batch_results_'):\n",
    "            files['llama_batch'].append(f)\n",
    "        elif name.startswith('internvl3_2b_batch_results_'):\n",
    "            files['internvl3_2b_batch'].append(f)\n",
    "        elif name.startswith('internvl3_5_8b_batch_results_') or name.startswith('internvl3_8b_batch_results_'):\n",
    "            files['internvl3_8b_batch'].append(f)\n",
    "        elif name.startswith('internvl3_batch_results_'):\n",
    "            # Treat generic internvl3_batch_results as 8B (most common case)\n",
    "            files['internvl3_batch'].append(f)\n",
    "        elif '_summary.csv' in name:\n",
    "            files['batch_summary'].append(f)\n",
    "    \n",
    "    # Sort by timestamp (most recent first)\n",
    "    for key in ['llama_batch', 'internvl3_batch', 'internvl3_2b_batch', 'internvl3_8b_batch']:\n",
    "        files[key] = sorted(files[key], key=lambda x: x.name, reverse=True)\n",
    "    \n",
    "    return files\n",
    "\n",
    "result_files = discover_result_files(CSV_DIR)\n",
    "\n",
    "print(\"Available Result Files\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nper_field_metrics.csv: {'Found' if result_files['per_field_metrics'] else 'Not found'}\")\n",
    "print(f\"\\nLlama batch results: {len(result_files['llama_batch'])} files\")\n",
    "for f in result_files['llama_batch'][:3]:\n",
    "    print(f\"  - {f.name}\")\n",
    "\n",
    "print(f\"\\nInternVL3-8B batch results: {len(result_files['internvl3_8b_batch'])} files\")\n",
    "for f in result_files['internvl3_8b_batch'][:3]:\n",
    "    print(f\"  - {f.name}\")\n",
    "\n",
    "print(f\"\\nInternVL3 (generic, treated as 8B): {len(result_files['internvl3_batch'])} files\")\n",
    "for f in result_files['internvl3_batch'][:3]:\n",
    "    print(f\"  - {f.name}\")\n",
    "\n",
    "print(f\"\\nInternVL3-2B batch results: {len(result_files['internvl3_2b_batch'])} files\")\n",
    "for f in result_files['internvl3_2b_batch'][:3]:\n",
    "    print(f\"  - {f.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Option A: Use per_field_metrics.csv (Recommended)\n",
    "\n",
    "This file already contains pre-computed field-level accuracy for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: ../output/csv/per_field_metrics.csv\n",
      "Shape: (51, 6)\n",
      "Models: ['Llama-11B', 'InternVL3-8B', 'InternVL3-2B']\n",
      "Fields: 17\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>field</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>support</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DOCUMENT_TYPE</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9</td>\n",
       "      <td>Llama-11B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BUSINESS_ABN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9</td>\n",
       "      <td>Llama-11B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SUPPLIER_NAME</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>9</td>\n",
       "      <td>Llama-11B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BUSINESS_ADDRESS</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>9</td>\n",
       "      <td>Llama-11B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PAYER_NAME</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9</td>\n",
       "      <td>Llama-11B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>PAYER_ADDRESS</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9</td>\n",
       "      <td>Llama-11B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>INVOICE_DATE</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9</td>\n",
       "      <td>Llama-11B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LINE_ITEM_DESCRIPTIONS</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.909259</td>\n",
       "      <td>0.929502</td>\n",
       "      <td>9</td>\n",
       "      <td>Llama-11B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LINE_ITEM_QUANTITIES</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9</td>\n",
       "      <td>Llama-11B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LINE_ITEM_PRICES</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9</td>\n",
       "      <td>Llama-11B</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    field  precision    recall  f1_score  support      model\n",
       "0           DOCUMENT_TYPE   1.000000  1.000000  1.000000        9  Llama-11B\n",
       "1            BUSINESS_ABN   1.000000  1.000000  1.000000        9  Llama-11B\n",
       "2           SUPPLIER_NAME   0.888889  0.888889  0.888889        9  Llama-11B\n",
       "3        BUSINESS_ADDRESS   0.888889  0.888889  0.888889        9  Llama-11B\n",
       "4              PAYER_NAME   1.000000  1.000000  1.000000        9  Llama-11B\n",
       "5           PAYER_ADDRESS   1.000000  1.000000  1.000000        9  Llama-11B\n",
       "6            INVOICE_DATE   1.000000  1.000000  1.000000        9  Llama-11B\n",
       "7  LINE_ITEM_DESCRIPTIONS   1.000000  0.909259  0.929502        9  Llama-11B\n",
       "8    LINE_ITEM_QUANTITIES   1.000000  1.000000  1.000000        9  Llama-11B\n",
       "9        LINE_ITEM_PRICES   1.000000  1.000000  1.000000        9  Llama-11B"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_per_field_metrics(csv_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Load and display per-field metrics.\"\"\"\n",
    "    if not csv_path or not csv_path.exists():\n",
    "        print(\"per_field_metrics.csv not found\")\n",
    "        return None\n",
    "    \n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"Loaded: {csv_path}\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Models: {df['model'].unique().tolist()}\")\n",
    "    print(f\"Fields: {df['field'].nunique()}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "per_field_df = load_per_field_metrics(result_files['per_field_metrics'])\n",
    "if per_field_df is not None:\n",
    "    display(per_field_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Available models in per_field_metrics.csv:\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'accuracy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/du/lib/python3.11/site-packages/pandas/core/indexes/base.py:3805\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3804\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3805\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3806\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'accuracy'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 61\u001b[39m\n\u001b[32m     59\u001b[39m model_data = per_field_df[per_field_df[\u001b[33m'\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m'\u001b[39m] == model]\n\u001b[32m     60\u001b[39m count = \u001b[38;5;28mlen\u001b[39m(model_data)\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m mean_acc = \u001b[43mmodel_data\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43maccuracy\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m.mean()\n\u001b[32m     62\u001b[39m mean_f1 = model_data[\u001b[33m'\u001b[39m\u001b[33mf1_score\u001b[39m\u001b[33m'\u001b[39m].mean() \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mf1_score\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model_data.columns \u001b[38;5;28;01melse\u001b[39;00m np.nan\n\u001b[32m     63\u001b[39m f1_str = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m, mean F1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmean_f1\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1%\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np.isnan(mean_f1) \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/du/lib/python3.11/site-packages/pandas/core/frame.py:4102\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4101\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4102\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4104\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/du/lib/python3.11/site-packages/pandas/core/indexes/base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3807\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3808\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3809\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3810\u001b[39m     ):\n\u001b[32m   3811\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3814\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3815\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3816\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3817\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'accuracy'"
     ]
    }
   ],
   "source": [
    "# Fields that are EXCLUSIVE to our model (competing model doesn't have these)\n",
    "EXCLUSIVE_FIELDS = [\"DOCUMENT_TYPE\"]\n",
    "\n",
    "# List fields where F1 scores are meaningful (for comparison)\n",
    "LIST_FIELDS = [\n",
    "    \"LINE_ITEM_DESCRIPTIONS\", \"LINE_ITEM_QUANTITIES\", \"LINE_ITEM_PRICES\",\n",
    "    \"LINE_ITEM_TOTAL_PRICES\", \"TRANSACTION_DATES\", \"TRANSACTION_AMOUNTS_PAID\",\n",
    "]\n",
    "\n",
    "def extract_model_accuracy_from_per_field(df: pd.DataFrame, model_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract field-level F1 scores for a specific model from per_field_metrics.csv.\n",
    "\n",
    "    The F1 scores are computed using calculate_field_accuracy_f1 for consistency\n",
    "    with model_comparison_reporter_v2.ipynb.\n",
    "    \"\"\"\n",
    "    model_df = df[df['model'] == model_name].copy()\n",
    "\n",
    "    if len(model_df) == 0:\n",
    "        print(f\"Model '{model_name}' not found. Available: {df['model'].unique().tolist()}\")\n",
    "        return None\n",
    "\n",
    "    # Determine subset based on field\n",
    "    bank_fields = ['STATEMENT_DATE_RANGE', 'TRANSACTION_DATES', 'TRANSACTION_AMOUNTS_PAID']\n",
    "\n",
    "    result = []\n",
    "    for _, row in model_df.iterrows():\n",
    "        subset = 'Bank statements' if row['field'] in bank_fields else 'Invoices & receipts'\n",
    "\n",
    "        result.append({\n",
    "            'Subset': subset,\n",
    "            'Field': row['field'],\n",
    "            'F1': row.get('f1_score', np.nan),\n",
    "            'Precision': row.get('precision', np.nan),\n",
    "            'Recall': row.get('recall', np.nan),\n",
    "            'Exclusive': row['field'] in EXCLUSIVE_FIELDS,\n",
    "        })\n",
    "\n",
    "    result_df = pd.DataFrame(result)\n",
    "\n",
    "    # Summary\n",
    "    exclusive_count = result_df['Exclusive'].sum()\n",
    "    comparable_count = len(result_df) - exclusive_count\n",
    "\n",
    "    print(f\"  Comparable fields: {comparable_count}\")\n",
    "    print(f\"  Exclusive fields (not in competing model): {exclusive_count}\")\n",
    "    print(f\"  Mean F1: {result_df['F1'].mean():.1%}\")\n",
    "\n",
    "    return result_df\n",
    "\n",
    "# Show available models\n",
    "if per_field_df is not None:\n",
    "    print(\"\\nAvailable models in per_field_metrics.csv:\")\n",
    "    for model in per_field_df['model'].unique():\n",
    "        model_data = per_field_df[per_field_df['model'] == model]\n",
    "        count = len(model_data)\n",
    "        mean_f1 = model_data['f1_score'].mean() if 'f1_score' in model_data.columns else np.nan\n",
    "        mean_prec = model_data['precision'].mean() if 'precision' in model_data.columns else np.nan\n",
    "        mean_rec = model_data['recall'].mean() if 'recall' in model_data.columns else np.nan\n",
    "        print(f\"  - {model}: {count} fields, F1: {mean_f1:.1%}, P: {mean_prec:.1%}, R: {mean_rec:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Option B: Aggregate from Batch Results\n",
    "\n",
    "If per_field_metrics.csv is not available or you want fresh results from a specific batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema fields by document type\n",
    "INVOICE_RECEIPT_FIELDS = [\n",
    "    \"DOCUMENT_TYPE\", \"BUSINESS_ABN\", \"SUPPLIER_NAME\", \"BUSINESS_ADDRESS\",\n",
    "    \"PAYER_NAME\", \"PAYER_ADDRESS\", \"INVOICE_DATE\", \"LINE_ITEM_DESCRIPTIONS\",\n",
    "    \"LINE_ITEM_QUANTITIES\", \"LINE_ITEM_PRICES\", \"LINE_ITEM_TOTAL_PRICES\",\n",
    "    \"IS_GST_INCLUDED\", \"GST_AMOUNT\", \"TOTAL_AMOUNT\",\n",
    "]\n",
    "\n",
    "BANK_STATEMENT_FIELDS = [\n",
    "    \"STATEMENT_DATE_RANGE\", \"LINE_ITEM_DESCRIPTIONS\",  # LINE_ITEM_DESCRIPTIONS = transaction descriptions\n",
    "    \"TRANSACTION_DATES\", \"TRANSACTION_AMOUNTS_PAID\",\n",
    "]\n",
    "\n",
    "# List fields where F1 is meaningful (multiple items)\n",
    "LIST_FIELDS = [\n",
    "    \"LINE_ITEM_DESCRIPTIONS\", \"LINE_ITEM_QUANTITIES\", \"LINE_ITEM_PRICES\",\n",
    "    \"LINE_ITEM_TOTAL_PRICES\", \"TRANSACTION_DATES\", \"TRANSACTION_AMOUNTS_PAID\",\n",
    "]\n",
    "\n",
    "# Fields that are EXCLUSIVE to our model (competing model doesn't have these)\n",
    "EXCLUSIVE_FIELDS = [\"DOCUMENT_TYPE\"]\n",
    "\n",
    "# Document type mapping (normalize variations)\n",
    "DOC_TYPE_MAP = {\n",
    "    'receipt': 'invoice_receipt',\n",
    "    'invoice': 'invoice_receipt', \n",
    "    'bank_statement': 'bank_statement',\n",
    "    'RECEIPT': 'invoice_receipt',\n",
    "    'INVOICE': 'invoice_receipt',\n",
    "    'BANK_STATEMENT': 'bank_statement',\n",
    "}\n",
    "\n",
    "# Ground truth paths\n",
    "GROUND_TRUTH_PATHS = {\n",
    "    'inv_rec': Path(\"../evaluation_data/inv_rec/ground_truth_inv_rec.csv\"),\n",
    "    'bank': Path(\"../evaluation_data/bank/ground_truth_bank.csv\"),\n",
    "    'synthetic': Path(\"../evaluation_data/synthetic/ground_truth_synthetic.csv\"),\n",
    "}\n",
    "\n",
    "def load_ground_truth() -> pd.DataFrame:\n",
    "    \"\"\"Load and combine all ground truth files.\"\"\"\n",
    "    dfs = []\n",
    "    for name, path in GROUND_TRUTH_PATHS.items():\n",
    "        if path.exists():\n",
    "            df = pd.read_csv(path)\n",
    "            df['source'] = name\n",
    "            dfs.append(df)\n",
    "            print(f\"  Loaded {name}: {len(df)} rows\")\n",
    "    \n",
    "    if dfs:\n",
    "        combined = pd.concat(dfs, ignore_index=True)\n",
    "        # Normalize image_file to stem for matching\n",
    "        combined['image_stem'] = combined['image_file'].apply(lambda x: Path(str(x)).stem)\n",
    "        print(f\"  Total ground truth: {len(combined)} rows\")\n",
    "        return combined\n",
    "    return None\n",
    "\n",
    "print(\"Loading ground truth for F1 computation...\")\n",
    "ground_truth_df = load_ground_truth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path for imports\n",
    "PROJECT_ROOT = Path(\"..\").resolve()\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from common.evaluation_metrics import calculate_field_accuracy_f1\n",
    "\n",
    "def load_batch_results(csv_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Load a batch results file.\"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"Loaded: {csv_path.name}\")\n",
    "    print(f\"  Images: {len(df)}\")\n",
    "    print(f\"  Document types: {df['document_type'].value_counts().to_dict()}\")\n",
    "    if 'overall_accuracy' in df.columns:\n",
    "        print(f\"  Mean overall accuracy: {df['overall_accuracy'].mean():.1%}\")\n",
    "\n",
    "    # Add image_stem for ground truth matching\n",
    "    if 'image_file' in df.columns:\n",
    "        df['image_stem'] = df['image_file'].apply(lambda x: Path(str(x)).stem)\n",
    "    elif 'image_name' in df.columns:\n",
    "        df['image_stem'] = df['image_name'].apply(lambda x: Path(str(x)).stem)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Schema fields by document type\n",
    "INVOICE_RECEIPT_FIELDS = [\n",
    "    \"DOCUMENT_TYPE\", \"BUSINESS_ABN\", \"SUPPLIER_NAME\", \"BUSINESS_ADDRESS\",\n",
    "    \"PAYER_NAME\", \"PAYER_ADDRESS\", \"INVOICE_DATE\", \"LINE_ITEM_DESCRIPTIONS\",\n",
    "    \"LINE_ITEM_QUANTITIES\", \"LINE_ITEM_PRICES\", \"LINE_ITEM_TOTAL_PRICES\",\n",
    "    \"IS_GST_INCLUDED\", \"GST_AMOUNT\", \"TOTAL_AMOUNT\",\n",
    "]\n",
    "\n",
    "BANK_STATEMENT_FIELDS = [\n",
    "    \"STATEMENT_DATE_RANGE\", \"LINE_ITEM_DESCRIPTIONS\",\n",
    "    \"TRANSACTION_DATES\", \"TRANSACTION_AMOUNTS_PAID\",\n",
    "]\n",
    "\n",
    "# Document type mapping\n",
    "DOC_TYPE_MAP = {\n",
    "    'receipt': 'invoice_receipt',\n",
    "    'invoice': 'invoice_receipt',\n",
    "    'bank_statement': 'bank_statement',\n",
    "    'RECEIPT': 'invoice_receipt',\n",
    "    'INVOICE': 'invoice_receipt',\n",
    "    'BANK_STATEMENT': 'bank_statement',\n",
    "}\n",
    "\n",
    "\n",
    "def compute_field_accuracy_from_batch(df: pd.DataFrame, ground_truth: pd.DataFrame = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute field-level F1 metrics from batch results using calculate_field_accuracy_f1.\n",
    "\n",
    "    This function uses the SAME F1 calculation as model_comparison_reporter_v2.ipynb\n",
    "    to ensure consistent metrics across all notebooks.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    # Normalize document types\n",
    "    df = df.copy()\n",
    "    df['doc_type_normalized'] = df['document_type'].map(DOC_TYPE_MAP).fillna('unknown')\n",
    "\n",
    "    # Split by document type\n",
    "    inv_rec_df = df[df['doc_type_normalized'] == 'invoice_receipt']\n",
    "    bank_df = df[df['doc_type_normalized'] == 'bank_statement']\n",
    "\n",
    "    print(f\"  Invoice/Receipt images: {len(inv_rec_df)}\")\n",
    "    print(f\"  Bank statement images: {len(bank_df)}\")\n",
    "\n",
    "    # Merge with ground truth if available\n",
    "    has_gt = False\n",
    "    if ground_truth is not None and 'image_stem' in df.columns:\n",
    "        df = df.merge(ground_truth, on='image_stem', how='left', suffixes=('', '_gt'))\n",
    "        has_gt = True\n",
    "        matched = df['image_file_gt'].notna().sum()\n",
    "        print(f\"  Matched with ground truth: {matched}/{len(df)}\")\n",
    "    else:\n",
    "        print(\"  No ground truth - using extraction rate as proxy\")\n",
    "\n",
    "    # Re-split after merge\n",
    "    inv_rec_df = df[df['doc_type_normalized'] == 'invoice_receipt']\n",
    "    bank_df = df[df['doc_type_normalized'] == 'bank_statement']\n",
    "\n",
    "    def compute_f1_for_field(subset_df, field):\n",
    "        \"\"\"Compute F1 using calculate_field_accuracy_f1 for consistency.\"\"\"\n",
    "        if not has_gt or f'{field}_gt' not in subset_df.columns:\n",
    "            return np.nan, np.nan, np.nan\n",
    "\n",
    "        f1_scores = []\n",
    "        precision_scores = []\n",
    "        recall_scores = []\n",
    "\n",
    "        for _, row in subset_df.iterrows():\n",
    "            if pd.notna(row.get(f'{field}_gt')):\n",
    "                ext = str(row[field]) if pd.notna(row[field]) else 'NOT_FOUND'\n",
    "                gt = str(row[f'{field}_gt']) if pd.notna(row[f'{field}_gt']) else 'NOT_FOUND'\n",
    "\n",
    "                # Use calculate_field_accuracy_f1 for consistent F1 calculation\n",
    "                metrics = calculate_field_accuracy_f1(ext, gt, field, debug=False)\n",
    "\n",
    "                f1_scores.append(metrics['f1_score'])\n",
    "                precision_scores.append(metrics['precision'])\n",
    "                recall_scores.append(metrics['recall'])\n",
    "\n",
    "        if f1_scores:\n",
    "            return np.mean(f1_scores), np.mean(precision_scores), np.mean(recall_scores)\n",
    "        return np.nan, np.nan, np.nan\n",
    "\n",
    "    # Process Invoice & Receipt fields\n",
    "    for field in INVOICE_RECEIPT_FIELDS:\n",
    "        if field not in df.columns:\n",
    "            continue\n",
    "        if len(inv_rec_df) == 0:\n",
    "            continue\n",
    "\n",
    "        valid = inv_rec_df[field].notna() & (inv_rec_df[field] != 'NOT_FOUND') & (inv_rec_df[field] != '')\n",
    "        coverage = valid.mean() if len(inv_rec_df) > 0 else np.nan\n",
    "\n",
    "        f1, precision, recall = compute_f1_for_field(inv_rec_df, field)\n",
    "\n",
    "        results.append({\n",
    "            'Subset': 'Invoices & receipts',\n",
    "            'Field': field,\n",
    "            'Accuracy': coverage,\n",
    "            'F1': f1,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'Exclusive': field in EXCLUSIVE_FIELDS,\n",
    "            'extracted_count': int(valid.sum()),\n",
    "            'total_count': len(inv_rec_df),\n",
    "        })\n",
    "\n",
    "    # Process Bank Statement fields\n",
    "    for field in BANK_STATEMENT_FIELDS:\n",
    "        if field not in df.columns:\n",
    "            continue\n",
    "        if len(bank_df) == 0:\n",
    "            continue\n",
    "\n",
    "        valid = bank_df[field].notna() & (bank_df[field] != 'NOT_FOUND') & (bank_df[field] != '')\n",
    "        coverage = valid.mean() if len(bank_df) > 0 else np.nan\n",
    "\n",
    "        f1, precision, recall = compute_f1_for_field(bank_df, field)\n",
    "\n",
    "        results.append({\n",
    "            'Subset': 'Bank statements',\n",
    "            'Field': field,\n",
    "            'Accuracy': coverage,\n",
    "            'F1': f1,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'Exclusive': field in EXCLUSIVE_FIELDS,\n",
    "            'extracted_count': int(valid.sum()),\n",
    "            'total_count': len(bank_df),\n",
    "        })\n",
    "\n",
    "    result_df = pd.DataFrame(results)\n",
    "\n",
    "    # Summary\n",
    "    exclusive_count = result_df['Exclusive'].sum()\n",
    "    comparable_count = len(result_df) - exclusive_count\n",
    "\n",
    "    print(f\"\\n  F1 Score (using calculate_field_accuracy_f1): {result_df['F1'].mean():.1%}\")\n",
    "    print(f\"  Precision: {result_df['Precision'].mean():.1%}\")\n",
    "    print(f\"  Recall: {result_df['Recall'].mean():.1%}\")\n",
    "    print(f\"\\n  Comparable fields: {comparable_count}\")\n",
    "    print(f\"  Exclusive fields: {exclusive_count}\")\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load most recent batch results for each model\n",
    "print(\"Most recent batch results by model:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "batch_dfs = {}\n",
    "\n",
    "if result_files['llama_batch']:\n",
    "    latest = result_files['llama_batch'][0]\n",
    "    batch_dfs['Llama-11B'] = load_batch_results(latest)\n",
    "    print()\n",
    "\n",
    "# InternVL3-8B: Check both explicit 8B files and generic internvl3 files\n",
    "if result_files['internvl3_8b_batch']:\n",
    "    latest = result_files['internvl3_8b_batch'][0]\n",
    "    batch_dfs['InternVL3-8B'] = load_batch_results(latest)\n",
    "    print()\n",
    "elif result_files['internvl3_batch']:\n",
    "    # Use generic internvl3_batch_results as 8B fallback\n",
    "    latest = result_files['internvl3_batch'][0]\n",
    "    print(\"Using generic internvl3_batch_results as InternVL3-8B:\")\n",
    "    batch_dfs['InternVL3-8B'] = load_batch_results(latest)\n",
    "    print()\n",
    "\n",
    "if result_files['internvl3_2b_batch']:\n",
    "    latest = result_files['internvl3_2b_batch'][0]\n",
    "    batch_dfs['InternVL3-2B'] = load_batch_results(latest)\n",
    "    print()\n",
    "\n",
    "print(f\"\\nModels loaded: {list(batch_dfs.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Select Model and Export\n",
    "\n",
    "Choose which model's accuracy to export as `current_model_accuracy.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION - Select model and data source\n",
    "# =============================================================================\n",
    "\n",
    "# Choose model to export\n",
    "SELECTED_MODEL = \"Llama-11B\"  # Options: \"Llama-11B\", \"InternVL3-8B\", \"InternVL3-2B\"\n",
    "\n",
    "# Choose data source\n",
    "USE_PER_FIELD_METRICS = False  # True = use per_field_metrics.csv (recommended)\n",
    "                               # False = aggregate from batch results (coverage only)\n",
    "\n",
    "# Output filename\n",
    "OUTPUT_FILENAME = \"current_model_accuracy.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_model_accuracy(model_name: str, use_per_field: bool,\n",
    "                          per_field_df: pd.DataFrame, batch_dfs: dict,\n",
    "                          ground_truth_df: pd.DataFrame,\n",
    "                          output_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Export field-level accuracy and F1 for selected model.\"\"\"\n",
    "\n",
    "    if use_per_field and per_field_df is not None:\n",
    "        print(f\"Using per_field_metrics.csv for {model_name}\")\n",
    "        result_df = extract_model_accuracy_from_per_field(per_field_df, model_name)\n",
    "        if result_df is None:\n",
    "            return None\n",
    "    elif model_name in batch_dfs:\n",
    "        print(f\"Aggregating from batch results for {model_name}\")\n",
    "        if ground_truth_df is not None:\n",
    "            print(\"Computing F1 against ground truth (using calculate_field_accuracy_f1)...\")\n",
    "        result_df = compute_field_accuracy_from_batch(batch_dfs[model_name], ground_truth_df)\n",
    "    else:\n",
    "        print(f\"No data available for {model_name}\")\n",
    "        return None\n",
    "\n",
    "    # Determine which columns to save\n",
    "    save_cols = ['Subset', 'Field', 'Accuracy']\n",
    "\n",
    "    # Add F1/Precision/Recall if available\n",
    "    for col in ['F1', 'Precision', 'Recall', 'F1_Agnostic']:\n",
    "        if col in result_df.columns and result_df[col].notna().any():\n",
    "            save_cols.append(col)\n",
    "\n",
    "    if 'Exclusive' in result_df.columns:\n",
    "        save_cols.append('Exclusive')\n",
    "\n",
    "    result_df[save_cols].to_csv(output_path, index=False)\n",
    "    print(f\"\\nSaved: {output_path}\")\n",
    "    print(f\"Fields: {len(result_df)}\")\n",
    "    print(f\"Mean accuracy: {result_df['Accuracy'].mean():.1%}\")\n",
    "\n",
    "    # Print F1 summary\n",
    "    if 'F1' in save_cols:\n",
    "        print(f\"Mean F1: {result_df['F1'].mean():.1%}\")\n",
    "    if 'Precision' in save_cols:\n",
    "        print(f\"Mean Precision: {result_df['Precision'].mean():.1%}\")\n",
    "    if 'Recall' in save_cols:\n",
    "        print(f\"Mean Recall: {result_df['Recall'].mean():.1%}\")\n",
    "\n",
    "    return result_df\n",
    "\n",
    "# Export\n",
    "output_path = OUTPUT_DIR / OUTPUT_FILENAME\n",
    "exported_df = export_model_accuracy(\n",
    "    SELECTED_MODEL,\n",
    "    USE_PER_FIELD_METRICS,\n",
    "    per_field_df,\n",
    "    batch_dfs,\n",
    "    ground_truth_df,\n",
    "    output_path\n",
    ")\n",
    "\n",
    "if exported_df is not None:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Exported Data:\")\n",
    "    print(\"=\" * 60)\n",
    "    display(exported_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Compare Multiple Models (Optional)\n",
    "\n",
    "Quick comparison of all available models from per_field_metrics.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if per_field_df is not None:\n",
    "    # Pivot to compare models by F1 score\n",
    "    comparison = per_field_df.pivot(index='field', columns='model', values='f1_score')\n",
    "    comparison['best_model'] = comparison.idxmax(axis=1)\n",
    "\n",
    "    print(\"\\nField-Level F1 Comparison (from per_field_metrics.csv)\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Format as percentages\n",
    "    display_df = comparison.copy()\n",
    "    for col in display_df.columns:\n",
    "        if col != 'best_model':\n",
    "            display_df[col] = display_df[col].apply(lambda x: f\"{x:.1%}\" if pd.notna(x) else \"N/A\")\n",
    "\n",
    "    display(display_df)\n",
    "\n",
    "    # Summary\n",
    "    print(\"\\nModel Summary:\")\n",
    "    for model in per_field_df['model'].unique():\n",
    "        model_data = per_field_df[per_field_df['model'] == model]\n",
    "        wins = (comparison['best_model'] == model).sum()\n",
    "        print(f\"  {model}: mean F1={model_data['f1_score'].mean():.1%}, best on {wins} fields\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Export All Models for Comparison\n",
    "\n",
    "Export accuracy files for all available models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_all_models(per_field_df: pd.DataFrame, output_dir: Path):\n",
    "    \"\"\"Export F1 CSV for each model in per_field_metrics.csv\"\"\"\n",
    "    if per_field_df is None:\n",
    "        print(\"No per_field_metrics.csv available\")\n",
    "        return\n",
    "\n",
    "    exported = []\n",
    "    for model in per_field_df['model'].unique():\n",
    "        # Create safe filename\n",
    "        safe_name = model.lower().replace('-', '_').replace(' ', '_')\n",
    "        filename = f\"{safe_name}_accuracy.csv\"\n",
    "        output_path = output_dir / filename\n",
    "\n",
    "        result_df = extract_model_accuracy_from_per_field(per_field_df, model)\n",
    "        if result_df is not None:\n",
    "            result_df.to_csv(output_path, index=False)\n",
    "            mean_f1 = result_df['F1'].mean()\n",
    "            exported.append((model, output_path, mean_f1))\n",
    "            print(f\"Exported: {filename} (mean F1: {mean_f1:.1%})\")\n",
    "\n",
    "    return exported\n",
    "\n",
    "print(\"Exporting all models...\")\n",
    "print(\"=\" * 60)\n",
    "all_exports = export_all_models(per_field_df, OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "Files created:\n",
    "- `current_model_accuracy.csv` - Selected model for comparison notebook\n",
    "- `{model}_accuracy.csv` - Individual files for each model\n",
    "\n",
    "To use in comparison notebook:\n",
    "```python\n",
    "CURRENT_MODEL_CSV = Path(\"../output/current_model_accuracy.csv\")\n",
    "# Or use specific model file:\n",
    "CURRENT_MODEL_CSV = Path(\"../output/internvl3_8b_accuracy.csv\")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "du",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}