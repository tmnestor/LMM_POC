{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Harvest Model Accuracy from Evaluation Results\n",
    "\n",
    "This notebook scans `output/csv/` for evaluation results and generates the `current_model_accuracy.csv` file for use in model comparison.\n",
    "\n",
    "## Data Sources\n",
    "1. **per_field_metrics.csv** - Pre-computed field-level metrics by model\n",
    "2. **Model batch results** - Per-image results that can be aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# Paths\n",
    "CSV_DIR = Path(\"../output/csv\")\n",
    "OUTPUT_DIR = Path(\"../output\")\n",
    "\n",
    "print(f\"Scanning: {CSV_DIR.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Discover Available Result Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discover_result_files(csv_dir: Path) -> dict:\n",
    "    \"\"\"Discover all available result files in the CSV directory.\"\"\"\n",
    "    files = {\n",
    "        'per_field_metrics': None,\n",
    "        'llama_batch': [],\n",
    "        'internvl3_batch': [],\n",
    "        'internvl3_2b_batch': [],\n",
    "        'internvl3_8b_batch': [],\n",
    "        'batch_summary': [],\n",
    "    }\n",
    "    \n",
    "    for f in csv_dir.glob(\"*.csv\"):\n",
    "        name = f.name\n",
    "        \n",
    "        if name == 'per_field_metrics.csv':\n",
    "            files['per_field_metrics'] = f\n",
    "        elif name.startswith('llama_batch_results_'):\n",
    "            files['llama_batch'].append(f)\n",
    "        elif name.startswith('internvl3_2b_batch_results_'):\n",
    "            files['internvl3_2b_batch'].append(f)\n",
    "        elif name.startswith('internvl3_5_8b_batch_results_') or name.startswith('internvl3_8b_batch_results_'):\n",
    "            files['internvl3_8b_batch'].append(f)\n",
    "        elif name.startswith('internvl3_batch_results_'):\n",
    "            files['internvl3_batch'].append(f)\n",
    "        elif '_summary.csv' in name:\n",
    "            files['batch_summary'].append(f)\n",
    "    \n",
    "    # Sort by timestamp (most recent first)\n",
    "    for key in ['llama_batch', 'internvl3_batch', 'internvl3_2b_batch', 'internvl3_8b_batch']:\n",
    "        files[key] = sorted(files[key], key=lambda x: x.name, reverse=True)\n",
    "    \n",
    "    return files\n",
    "\n",
    "result_files = discover_result_files(CSV_DIR)\n",
    "\n",
    "print(\"Available Result Files\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nper_field_metrics.csv: {'Found' if result_files['per_field_metrics'] else 'Not found'}\")\n",
    "print(f\"\\nLlama batch results: {len(result_files['llama_batch'])} files\")\n",
    "for f in result_files['llama_batch'][:5]:\n",
    "    print(f\"  - {f.name}\")\n",
    "if len(result_files['llama_batch']) > 5:\n",
    "    print(f\"  ... and {len(result_files['llama_batch']) - 5} more\")\n",
    "\n",
    "print(f\"\\nInternVL3-8B batch results: {len(result_files['internvl3_8b_batch'])} files\")\n",
    "for f in result_files['internvl3_8b_batch'][:5]:\n",
    "    print(f\"  - {f.name}\")\n",
    "\n",
    "print(f\"\\nInternVL3-2B batch results: {len(result_files['internvl3_2b_batch'])} files\")\n",
    "for f in result_files['internvl3_2b_batch'][:5]:\n",
    "    print(f\"  - {f.name}\")\n",
    "\n",
    "print(f\"\\nInternVL3 (unversioned) batch results: {len(result_files['internvl3_batch'])} files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Option A: Use per_field_metrics.csv (Recommended)\n",
    "\n",
    "This file already contains pre-computed field-level accuracy for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_per_field_metrics(csv_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Load and display per-field metrics.\"\"\"\n",
    "    if not csv_path or not csv_path.exists():\n",
    "        print(\"per_field_metrics.csv not found\")\n",
    "        return None\n",
    "    \n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"Loaded: {csv_path}\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Models: {df['model'].unique().tolist()}\")\n",
    "    print(f\"Fields: {df['field'].nunique()}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "per_field_df = load_per_field_metrics(result_files['per_field_metrics'])\n",
    "if per_field_df is not None:\n",
    "    display(per_field_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Fields that are EXCLUSIVE to our model (competing model doesn't have these)\nEXCLUSIVE_FIELDS = [\"DOCUMENT_TYPE\"]\n\n# List fields where F1 scores are meaningful (for comparison)\n# These are fields with multiple items where precision/recall matter\nLIST_FIELDS = [\n    \"LINE_ITEM_DESCRIPTIONS\", \"LINE_ITEM_QUANTITIES\", \"LINE_ITEM_PRICES\",\n    \"LINE_ITEM_TOTAL_PRICES\", \"TRANSACTION_DATES\", \"TRANSACTION_AMOUNTS_PAID\",\n]\n\ndef extract_model_accuracy_from_per_field(df: pd.DataFrame, model_name: str) -> pd.DataFrame:\n    \"\"\"\n    Extract field-level accuracy AND F1 scores for a specific model.\n    \n    The F1 scores in per_field_metrics.csv are POSITION-AGNOSTIC (set-based).\n    This allows fair comparison with competing models using the same methodology.\n    \"\"\"\n    model_df = df[df['model'] == model_name].copy()\n    \n    if len(model_df) == 0:\n        print(f\"Model '{model_name}' not found. Available: {df['model'].unique().tolist()}\")\n        return None\n    \n    # Determine subset based on field\n    bank_fields = ['STATEMENT_DATE_RANGE', 'TRANSACTION_DATES', 'TRANSACTION_AMOUNTS_PAID']\n    \n    result = []\n    for _, row in model_df.iterrows():\n        subset = 'Bank statements' if row['field'] in bank_fields else 'Invoices & receipts'\n        \n        # Extract F1 score if available (position-agnostic from sklearn)\n        f1_agnostic = row.get('f1_score', np.nan) if 'f1_score' in row.index else np.nan\n        \n        result.append({\n            'Subset': subset,\n            'Field': row['field'],\n            'Accuracy': row['accuracy'],\n            'F1_Agnostic': f1_agnostic,  # Position-agnostic F1 for fair comparison\n            'Exclusive': row['field'] in EXCLUSIVE_FIELDS,\n        })\n    \n    result_df = pd.DataFrame(result)\n    \n    # Summary\n    exclusive_count = result_df['Exclusive'].sum()\n    comparable_count = len(result_df) - exclusive_count\n    f1_available = result_df['F1_Agnostic'].notna().sum()\n    \n    print(f\"  Comparable fields: {comparable_count}\")\n    print(f\"  Exclusive fields (not in competing model): {exclusive_count}\")\n    print(f\"  Fields with F1 scores: {f1_available}\")\n    \n    return result_df\n\n# Show available models\nif per_field_df is not None:\n    print(\"\\nAvailable models in per_field_metrics.csv:\")\n    for model in per_field_df['model'].unique():\n        model_data = per_field_df[per_field_df['model'] == model]\n        count = len(model_data)\n        mean_acc = model_data['accuracy'].mean()\n        mean_f1 = model_data['f1_score'].mean() if 'f1_score' in model_data.columns else np.nan\n        f1_str = f\", mean F1: {mean_f1:.1%}\" if not np.isnan(mean_f1) else \"\"\n        print(f\"  - {model}: {count} fields, mean accuracy: {mean_acc:.1%}{f1_str}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Option B: Aggregate from Batch Results\n",
    "\n",
    "If per_field_metrics.csv is not available or you want fresh results from a specific batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Schema fields by document type\nINVOICE_RECEIPT_FIELDS = [\n    \"DOCUMENT_TYPE\", \"BUSINESS_ABN\", \"SUPPLIER_NAME\", \"BUSINESS_ADDRESS\",\n    \"PAYER_NAME\", \"PAYER_ADDRESS\", \"INVOICE_DATE\", \"LINE_ITEM_DESCRIPTIONS\",\n    \"LINE_ITEM_QUANTITIES\", \"LINE_ITEM_PRICES\", \"LINE_ITEM_TOTAL_PRICES\",\n    \"IS_GST_INCLUDED\", \"GST_AMOUNT\", \"TOTAL_AMOUNT\",\n]\n\nBANK_STATEMENT_FIELDS = [\n    \"STATEMENT_DATE_RANGE\", \"LINE_ITEM_DESCRIPTIONS\",  # LINE_ITEM_DESCRIPTIONS = transaction descriptions\n    \"TRANSACTION_DATES\", \"TRANSACTION_AMOUNTS_PAID\",\n]\n\n# Fields that are EXCLUSIVE to our model (competing model doesn't have these)\n# These will be flagged in the output for separate handling in comparison\nEXCLUSIVE_FIELDS = [\"DOCUMENT_TYPE\"]\n\n# Document type mapping (normalize variations)\nDOC_TYPE_MAP = {\n    'receipt': 'invoice_receipt',\n    'invoice': 'invoice_receipt', \n    'bank_statement': 'bank_statement',\n    'RECEIPT': 'invoice_receipt',\n    'INVOICE': 'invoice_receipt',\n    'BANK_STATEMENT': 'bank_statement',\n}"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def load_batch_results(csv_path: Path) -> pd.DataFrame:\n    \"\"\"Load a batch results file.\"\"\"\n    df = pd.read_csv(csv_path)\n    print(f\"Loaded: {csv_path.name}\")\n    print(f\"  Images: {len(df)}\")\n    print(f\"  Document types: {df['document_type'].value_counts().to_dict()}\")\n    if 'overall_accuracy' in df.columns:\n        print(f\"  Mean overall accuracy: {df['overall_accuracy'].mean():.1%}\")\n    return df\n\n\ndef compute_field_accuracy_from_batch(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Compute field-level accuracy from batch results, split by document type.\n    \n    LINE_ITEM_DESCRIPTIONS appears in both subsets:\n    - Invoices & receipts: product/service descriptions\n    - Bank statements: transaction descriptions\n    \n    Adds 'Exclusive' column to flag fields not present in competing model.\n    \n    Uses extraction rate as proxy for accuracy (without ground truth comparison).\n    \"\"\"\n    results = []\n    \n    # Normalize document types\n    df = df.copy()\n    df['doc_type_normalized'] = df['document_type'].map(DOC_TYPE_MAP).fillna('unknown')\n    \n    # Split by document type\n    inv_rec_df = df[df['doc_type_normalized'] == 'invoice_receipt']\n    bank_df = df[df['doc_type_normalized'] == 'bank_statement']\n    \n    print(f\"  Invoice/Receipt images: {len(inv_rec_df)}\")\n    print(f\"  Bank statement images: {len(bank_df)}\")\n    \n    # Process Invoice & Receipt fields\n    for field in INVOICE_RECEIPT_FIELDS:\n        if field not in df.columns:\n            continue\n        if len(inv_rec_df) == 0:\n            continue\n            \n        valid = inv_rec_df[field].notna() & (inv_rec_df[field] != 'NOT_FOUND') & (inv_rec_df[field] != '')\n        coverage = valid.mean() if len(inv_rec_df) > 0 else np.nan\n        \n        results.append({\n            'Subset': 'Invoices & receipts',\n            'Field': field,\n            'Accuracy': coverage,\n            'Exclusive': field in EXCLUSIVE_FIELDS,\n            'extracted_count': valid.sum(),\n            'total_count': len(inv_rec_df),\n        })\n    \n    # Process Bank Statement fields\n    for field in BANK_STATEMENT_FIELDS:\n        if field not in df.columns:\n            continue\n        if len(bank_df) == 0:\n            continue\n            \n        valid = bank_df[field].notna() & (bank_df[field] != 'NOT_FOUND') & (bank_df[field] != '')\n        coverage = valid.mean() if len(bank_df) > 0 else np.nan\n        \n        results.append({\n            'Subset': 'Bank statements',\n            'Field': field,\n            'Accuracy': coverage,\n            'Exclusive': field in EXCLUSIVE_FIELDS,\n            'extracted_count': valid.sum(),\n            'total_count': len(bank_df),\n        })\n    \n    result_df = pd.DataFrame(results)\n    \n    # Summary\n    exclusive_count = result_df['Exclusive'].sum()\n    comparable_count = len(result_df) - exclusive_count\n    print(f\"  Comparable fields: {comparable_count}\")\n    print(f\"  Exclusive fields (not in competing model): {exclusive_count}\")\n    \n    return result_df"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load most recent batch results for each model\n",
    "print(\"Most recent batch results by model:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "batch_dfs = {}\n",
    "\n",
    "if result_files['llama_batch']:\n",
    "    latest = result_files['llama_batch'][0]\n",
    "    batch_dfs['Llama-11B'] = load_batch_results(latest)\n",
    "    print()\n",
    "\n",
    "if result_files['internvl3_8b_batch']:\n",
    "    latest = result_files['internvl3_8b_batch'][0]\n",
    "    batch_dfs['InternVL3-8B'] = load_batch_results(latest)\n",
    "    print()\n",
    "\n",
    "if result_files['internvl3_2b_batch']:\n",
    "    latest = result_files['internvl3_2b_batch'][0]\n",
    "    batch_dfs['InternVL3-2B'] = load_batch_results(latest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Select Model and Export\n",
    "\n",
    "Choose which model's accuracy to export as `current_model_accuracy.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION - Select model and data source\n",
    "# =============================================================================\n",
    "\n",
    "# Choose model to export\n",
    "SELECTED_MODEL = \"InternVL3-8B\"  # Options: \"Llama-11B\", \"InternVL3-8B\", \"InternVL3-2B\"\n",
    "\n",
    "# Choose data source\n",
    "USE_PER_FIELD_METRICS = True  # True = use per_field_metrics.csv (recommended)\n",
    "                               # False = aggregate from batch results (coverage only)\n",
    "\n",
    "# Output filename\n",
    "OUTPUT_FILENAME = \"current_model_accuracy.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_model_accuracy(model_name: str, use_per_field: bool, \n",
    "                          per_field_df: pd.DataFrame, batch_dfs: dict,\n",
    "                          output_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Export field-level accuracy for selected model.\"\"\"\n",
    "    \n",
    "    if use_per_field and per_field_df is not None:\n",
    "        print(f\"Using per_field_metrics.csv for {model_name}\")\n",
    "        result_df = extract_model_accuracy_from_per_field(per_field_df, model_name)\n",
    "        if result_df is None:\n",
    "            return None\n",
    "    elif model_name in batch_dfs:\n",
    "        print(f\"Aggregating from batch results for {model_name}\")\n",
    "        print(\"WARNING: This gives extraction coverage, not true accuracy!\")\n",
    "        result_df = compute_field_accuracy_from_batch(batch_dfs[model_name])\n",
    "        result_df = result_df[['Subset', 'Field', 'Accuracy']]\n",
    "    else:\n",
    "        print(f\"No data available for {model_name}\")\n",
    "        return None\n",
    "    \n",
    "    # Save\n",
    "    result_df.to_csv(output_path, index=False)\n",
    "    print(f\"\\nSaved: {output_path}\")\n",
    "    print(f\"Fields: {len(result_df)}\")\n",
    "    print(f\"Mean accuracy: {result_df['Accuracy'].mean():.1%}\")\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "# Export\n",
    "output_path = OUTPUT_DIR / OUTPUT_FILENAME\n",
    "exported_df = export_model_accuracy(\n",
    "    SELECTED_MODEL, \n",
    "    USE_PER_FIELD_METRICS, \n",
    "    per_field_df, \n",
    "    batch_dfs,\n",
    "    output_path\n",
    ")\n",
    "\n",
    "if exported_df is not None:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Exported Data:\")\n",
    "    print(\"=\" * 60)\n",
    "    display(exported_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Compare Multiple Models (Optional)\n",
    "\n",
    "Quick comparison of all available models from per_field_metrics.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if per_field_df is not None:\n",
    "    # Pivot to compare models\n",
    "    comparison = per_field_df.pivot(index='field', columns='model', values='accuracy')\n",
    "    comparison['best_model'] = comparison.idxmax(axis=1)\n",
    "    \n",
    "    print(\"\\nField-Level Accuracy Comparison (from per_field_metrics.csv)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Format as percentages\n",
    "    display_df = comparison.copy()\n",
    "    for col in display_df.columns:\n",
    "        if col != 'best_model':\n",
    "            display_df[col] = display_df[col].apply(lambda x: f\"{x:.1%}\" if pd.notna(x) else \"N/A\")\n",
    "    \n",
    "    display(display_df)\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\nModel Summary:\")\n",
    "    for model in per_field_df['model'].unique():\n",
    "        model_data = per_field_df[per_field_df['model'] == model]\n",
    "        wins = (comparison['best_model'] == model).sum()\n",
    "        print(f\"  {model}: mean={model_data['accuracy'].mean():.1%}, best on {wins} fields\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Export All Models for Comparison\n",
    "\n",
    "Export accuracy files for all available models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_all_models(per_field_df: pd.DataFrame, output_dir: Path):\n",
    "    \"\"\"Export accuracy CSV for each model in per_field_metrics.csv\"\"\"\n",
    "    if per_field_df is None:\n",
    "        print(\"No per_field_metrics.csv available\")\n",
    "        return\n",
    "    \n",
    "    exported = []\n",
    "    for model in per_field_df['model'].unique():\n",
    "        # Create safe filename\n",
    "        safe_name = model.lower().replace('-', '_').replace(' ', '_')\n",
    "        filename = f\"{safe_name}_accuracy.csv\"\n",
    "        output_path = output_dir / filename\n",
    "        \n",
    "        result_df = extract_model_accuracy_from_per_field(per_field_df, model)\n",
    "        if result_df is not None:\n",
    "            result_df.to_csv(output_path, index=False)\n",
    "            exported.append((model, output_path, result_df['Accuracy'].mean()))\n",
    "            print(f\"Exported: {filename} (mean accuracy: {result_df['Accuracy'].mean():.1%})\")\n",
    "    \n",
    "    return exported\n",
    "\n",
    "print(\"Exporting all models...\")\n",
    "print(\"=\" * 60)\n",
    "all_exports = export_all_models(per_field_df, OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "Files created:\n",
    "- `current_model_accuracy.csv` - Selected model for comparison notebook\n",
    "- `{model}_accuracy.csv` - Individual files for each model\n",
    "\n",
    "To use in comparison notebook:\n",
    "```python\n",
    "CURRENT_MODEL_CSV = Path(\"../output/current_model_accuracy.csv\")\n",
    "# Or use specific model file:\n",
    "CURRENT_MODEL_CSV = Path(\"../output/internvl3_8b_accuracy.csv\")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}