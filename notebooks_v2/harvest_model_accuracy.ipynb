{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Harvest Model Accuracy from Evaluation Results\n",
    "\n",
    "This notebook scans `output/csv/` for evaluation results and generates the `current_model_accuracy.csv` file for use in model comparison.\n",
    "\n",
    "## Data Sources\n",
    "1. **per_field_metrics.csv** - Pre-computed field-level metrics by model\n",
    "2. **Model batch results** - Per-image results that can be aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning: /Users/tod/Desktop/LMM_POC/output/csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# Paths\n",
    "CSV_DIR = Path(\"../output/csv\")\n",
    "OUTPUT_DIR = Path(\"../output\")\n",
    "\n",
    "print(f\"Scanning: {CSV_DIR.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Discover Available Result Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Result Files\n",
      "============================================================\n",
      "\n",
      "per_field_metrics.csv: Not found\n",
      "\n",
      "Llama batch results: 2 files\n",
      "  - llama_batch_results_20251210_003155.csv\n",
      "  - llama_batch_results_20251210_001755.csv\n",
      "\n",
      "InternVL3-8B batch results: 0 files\n",
      "\n",
      "InternVL3 (generic, treated as 8B): 1 files\n",
      "  - internvl3_batch_results_20251210_005902.csv\n",
      "\n",
      "InternVL3-2B batch results: 1 files\n",
      "  - internvl3_2b_batch_results_20251210_013149.csv\n"
     ]
    }
   ],
   "source": [
    "def discover_result_files(csv_dir: Path) -> dict:\n",
    "    \"\"\"Discover all available result files in the CSV directory.\"\"\"\n",
    "    files = {\n",
    "        'per_field_metrics': None,\n",
    "        'llama_batch': [],\n",
    "        'internvl3_batch': [],  # Generic internvl3 (treated as 8B)\n",
    "        'internvl3_2b_batch': [],\n",
    "        'internvl3_8b_batch': [],\n",
    "        'batch_summary': [],\n",
    "    }\n",
    "    \n",
    "    for f in csv_dir.glob(\"*.csv\"):\n",
    "        name = f.name\n",
    "        \n",
    "        if name == 'per_field_metrics.csv':\n",
    "            files['per_field_metrics'] = f\n",
    "        elif name.startswith('llama_batch_results_'):\n",
    "            files['llama_batch'].append(f)\n",
    "        elif name.startswith('internvl3_2b_batch_results_'):\n",
    "            files['internvl3_2b_batch'].append(f)\n",
    "        elif name.startswith('internvl3_5_8b_batch_results_') or name.startswith('internvl3_8b_batch_results_'):\n",
    "            files['internvl3_8b_batch'].append(f)\n",
    "        elif name.startswith('internvl3_batch_results_'):\n",
    "            # Treat generic internvl3_batch_results as 8B (most common case)\n",
    "            files['internvl3_batch'].append(f)\n",
    "        elif '_summary.csv' in name:\n",
    "            files['batch_summary'].append(f)\n",
    "    \n",
    "    # Sort by timestamp (most recent first)\n",
    "    for key in ['llama_batch', 'internvl3_batch', 'internvl3_2b_batch', 'internvl3_8b_batch']:\n",
    "        files[key] = sorted(files[key], key=lambda x: x.name, reverse=True)\n",
    "    \n",
    "    return files\n",
    "\n",
    "result_files = discover_result_files(CSV_DIR)\n",
    "\n",
    "print(\"Available Result Files\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nper_field_metrics.csv: {'Found' if result_files['per_field_metrics'] else 'Not found'}\")\n",
    "print(f\"\\nLlama batch results: {len(result_files['llama_batch'])} files\")\n",
    "for f in result_files['llama_batch'][:3]:\n",
    "    print(f\"  - {f.name}\")\n",
    "\n",
    "print(f\"\\nInternVL3-8B batch results: {len(result_files['internvl3_8b_batch'])} files\")\n",
    "for f in result_files['internvl3_8b_batch'][:3]:\n",
    "    print(f\"  - {f.name}\")\n",
    "\n",
    "print(f\"\\nInternVL3 (generic, treated as 8B): {len(result_files['internvl3_batch'])} files\")\n",
    "for f in result_files['internvl3_batch'][:3]:\n",
    "    print(f\"  - {f.name}\")\n",
    "\n",
    "print(f\"\\nInternVL3-2B batch results: {len(result_files['internvl3_2b_batch'])} files\")\n",
    "for f in result_files['internvl3_2b_batch'][:3]:\n",
    "    print(f\"  - {f.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Option A: Use per_field_metrics.csv (Recommended)\n",
    "\n",
    "This file already contains pre-computed field-level accuracy for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "per_field_metrics.csv not found\n"
     ]
    }
   ],
   "source": [
    "def load_per_field_metrics(csv_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Load and display per-field metrics.\"\"\"\n",
    "    if not csv_path or not csv_path.exists():\n",
    "        print(\"per_field_metrics.csv not found\")\n",
    "        return None\n",
    "    \n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"Loaded: {csv_path}\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Models: {df['model'].unique().tolist()}\")\n",
    "    print(f\"Fields: {df['field'].nunique()}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "per_field_df = load_per_field_metrics(result_files['per_field_metrics'])\n",
    "if per_field_df is not None:\n",
    "    display(per_field_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fields that are EXCLUSIVE to our model (competing model doesn't have these)\n",
    "EXCLUSIVE_FIELDS = [\"DOCUMENT_TYPE\"]\n",
    "\n",
    "# List fields where F1 scores are meaningful (for comparison)\n",
    "# These are fields with multiple items where precision/recall matter\n",
    "LIST_FIELDS = [\n",
    "    \"LINE_ITEM_DESCRIPTIONS\", \"LINE_ITEM_QUANTITIES\", \"LINE_ITEM_PRICES\",\n",
    "    \"LINE_ITEM_TOTAL_PRICES\", \"TRANSACTION_DATES\", \"TRANSACTION_AMOUNTS_PAID\",\n",
    "]\n",
    "\n",
    "def extract_model_accuracy_from_per_field(df: pd.DataFrame, model_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract field-level accuracy AND F1 scores for a specific model.\n",
    "    \n",
    "    The F1 scores in per_field_metrics.csv are POSITION-AGNOSTIC (set-based).\n",
    "    This allows fair comparison with competing models using the same methodology.\n",
    "    \"\"\"\n",
    "    model_df = df[df['model'] == model_name].copy()\n",
    "    \n",
    "    if len(model_df) == 0:\n",
    "        print(f\"Model '{model_name}' not found. Available: {df['model'].unique().tolist()}\")\n",
    "        return None\n",
    "    \n",
    "    # Determine subset based on field\n",
    "    bank_fields = ['STATEMENT_DATE_RANGE', 'TRANSACTION_DATES', 'TRANSACTION_AMOUNTS_PAID']\n",
    "    \n",
    "    result = []\n",
    "    for _, row in model_df.iterrows():\n",
    "        subset = 'Bank statements' if row['field'] in bank_fields else 'Invoices & receipts'\n",
    "        \n",
    "        # Extract F1 score if available (position-agnostic from sklearn)\n",
    "        f1_agnostic = row.get('f1_score', np.nan) if 'f1_score' in row.index else np.nan\n",
    "        \n",
    "        result.append({\n",
    "            'Subset': subset,\n",
    "            'Field': row['field'],\n",
    "            'Accuracy': row['accuracy'],\n",
    "            'F1_Agnostic': f1_agnostic,  # Position-agnostic F1 for fair comparison\n",
    "            'Exclusive': row['field'] in EXCLUSIVE_FIELDS,\n",
    "        })\n",
    "    \n",
    "    result_df = pd.DataFrame(result)\n",
    "    \n",
    "    # Summary\n",
    "    exclusive_count = result_df['Exclusive'].sum()\n",
    "    comparable_count = len(result_df) - exclusive_count\n",
    "    f1_available = result_df['F1_Agnostic'].notna().sum()\n",
    "    \n",
    "    print(f\"  Comparable fields: {comparable_count}\")\n",
    "    print(f\"  Exclusive fields (not in competing model): {exclusive_count}\")\n",
    "    print(f\"  Fields with F1 scores: {f1_available}\")\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "# Show available models\n",
    "if per_field_df is not None:\n",
    "    print(\"\\nAvailable models in per_field_metrics.csv:\")\n",
    "    for model in per_field_df['model'].unique():\n",
    "        model_data = per_field_df[per_field_df['model'] == model]\n",
    "        count = len(model_data)\n",
    "        mean_acc = model_data['accuracy'].mean()\n",
    "        mean_f1 = model_data['f1_score'].mean() if 'f1_score' in model_data.columns else np.nan\n",
    "        f1_str = f\", mean F1: {mean_f1:.1%}\" if not np.isnan(mean_f1) else \"\"\n",
    "        print(f\"  - {model}: {count} fields, mean accuracy: {mean_acc:.1%}{f1_str}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Option B: Aggregate from Batch Results\n",
    "\n",
    "If per_field_metrics.csv is not available or you want fresh results from a specific batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ground truth for F1 computation...\n",
      "  Loaded inv_rec: 6 rows\n",
      "  Loaded bank: 15 rows\n",
      "  Loaded synthetic: 9 rows\n",
      "  Total ground truth: 30 rows\n"
     ]
    }
   ],
   "source": [
    "# Schema fields by document type\n",
    "INVOICE_RECEIPT_FIELDS = [\n",
    "    \"DOCUMENT_TYPE\", \"BUSINESS_ABN\", \"SUPPLIER_NAME\", \"BUSINESS_ADDRESS\",\n",
    "    \"PAYER_NAME\", \"PAYER_ADDRESS\", \"INVOICE_DATE\", \"LINE_ITEM_DESCRIPTIONS\",\n",
    "    \"LINE_ITEM_QUANTITIES\", \"LINE_ITEM_PRICES\", \"LINE_ITEM_TOTAL_PRICES\",\n",
    "    \"IS_GST_INCLUDED\", \"GST_AMOUNT\", \"TOTAL_AMOUNT\",\n",
    "]\n",
    "\n",
    "BANK_STATEMENT_FIELDS = [\n",
    "    \"STATEMENT_DATE_RANGE\", \"LINE_ITEM_DESCRIPTIONS\",  # LINE_ITEM_DESCRIPTIONS = transaction descriptions\n",
    "    \"TRANSACTION_DATES\", \"TRANSACTION_AMOUNTS_PAID\",\n",
    "]\n",
    "\n",
    "# List fields where F1 is meaningful (multiple items)\n",
    "LIST_FIELDS = [\n",
    "    \"LINE_ITEM_DESCRIPTIONS\", \"LINE_ITEM_QUANTITIES\", \"LINE_ITEM_PRICES\",\n",
    "    \"LINE_ITEM_TOTAL_PRICES\", \"TRANSACTION_DATES\", \"TRANSACTION_AMOUNTS_PAID\",\n",
    "]\n",
    "\n",
    "# Fields that are EXCLUSIVE to our model (competing model doesn't have these)\n",
    "EXCLUSIVE_FIELDS = [\"DOCUMENT_TYPE\"]\n",
    "\n",
    "# Document type mapping (normalize variations)\n",
    "DOC_TYPE_MAP = {\n",
    "    'receipt': 'invoice_receipt',\n",
    "    'invoice': 'invoice_receipt', \n",
    "    'bank_statement': 'bank_statement',\n",
    "    'RECEIPT': 'invoice_receipt',\n",
    "    'INVOICE': 'invoice_receipt',\n",
    "    'BANK_STATEMENT': 'bank_statement',\n",
    "}\n",
    "\n",
    "# Ground truth paths\n",
    "GROUND_TRUTH_PATHS = {\n",
    "    'inv_rec': Path(\"../evaluation_data/inv_rec/ground_truth_inv_rec.csv\"),\n",
    "    'bank': Path(\"../evaluation_data/bank/ground_truth_bank.csv\"),\n",
    "    'synthetic': Path(\"../evaluation_data/synthetic/ground_truth_synthetic.csv\"),\n",
    "}\n",
    "\n",
    "def load_ground_truth() -> pd.DataFrame:\n",
    "    \"\"\"Load and combine all ground truth files.\"\"\"\n",
    "    dfs = []\n",
    "    for name, path in GROUND_TRUTH_PATHS.items():\n",
    "        if path.exists():\n",
    "            df = pd.read_csv(path)\n",
    "            df['source'] = name\n",
    "            dfs.append(df)\n",
    "            print(f\"  Loaded {name}: {len(df)} rows\")\n",
    "    \n",
    "    if dfs:\n",
    "        combined = pd.concat(dfs, ignore_index=True)\n",
    "        # Normalize image_file to stem for matching\n",
    "        combined['image_stem'] = combined['image_file'].apply(lambda x: Path(str(x)).stem)\n",
    "        print(f\"  Total ground truth: {len(combined)} rows\")\n",
    "        return combined\n",
    "    return None\n",
    "\n",
    "print(\"Loading ground truth for F1 computation...\")\n",
    "ground_truth_df = load_ground_truth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def load_batch_results(csv_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Load a batch results file.\"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"Loaded: {csv_path.name}\")\n",
    "    print(f\"  Images: {len(df)}\")\n",
    "    print(f\"  Document types: {df['document_type'].value_counts().to_dict()}\")\n",
    "    if 'overall_accuracy' in df.columns:\n",
    "        print(f\"  Mean overall accuracy: {df['overall_accuracy'].mean():.1%}\")\n",
    "    \n",
    "    # Add image_stem for ground truth matching\n",
    "    if 'image_file' in df.columns:\n",
    "        df['image_stem'] = df['image_file'].apply(lambda x: Path(str(x)).stem)\n",
    "    elif 'image_name' in df.columns:\n",
    "        df['image_stem'] = df['image_name'].apply(lambda x: Path(str(x)).stem)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def normalize_value(value: str, field_name: str = \"\") -> str:\n",
    "    \"\"\"Normalize a value for comparison.\"\"\"\n",
    "    if pd.isna(value):\n",
    "        return \"\"\n",
    "    s = str(value).strip().lower()\n",
    "    if s == 'not_found':\n",
    "        return 'NOT_FOUND'\n",
    "    \n",
    "    # For monetary fields, normalize to just digits and decimal\n",
    "    if any(x in field_name.upper() for x in ['AMOUNT', 'PRICE', 'TOTAL', 'GST']):\n",
    "        s = re.sub(r'[^\\d.]', '', s)\n",
    "    \n",
    "    # For ABN/numeric IDs, keep only digits\n",
    "    if any(x in field_name.upper() for x in ['ABN', 'BSB', 'NUMBER']):\n",
    "        s = re.sub(r'\\D', '', s)\n",
    "    \n",
    "    # General normalization: remove extra spaces\n",
    "    s = re.sub(r'\\s+', ' ', s)\n",
    "    s = s.strip()\n",
    "    \n",
    "    return s\n",
    "\n",
    "\n",
    "def compute_token_similarity(s1: str, s2: str) -> float:\n",
    "    \"\"\"Compute token-level F1 similarity between two strings.\"\"\"\n",
    "    t1 = set(s1.lower().split())\n",
    "    t2 = set(s2.lower().split())\n",
    "    \n",
    "    if not t1 or not t2:\n",
    "        return 0.0\n",
    "    \n",
    "    overlap = len(t1 & t2)\n",
    "    precision = overlap / len(t1)\n",
    "    recall = overlap / len(t2)\n",
    "    \n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    return 2 * precision * recall / (precision + recall)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Field Type Categories for Smart F1 Selection\n",
    "# =============================================================================\n",
    "\n",
    "# Fields requiring strict binary matching (exact full value)\n",
    "BINARY_FIELDS = [\n",
    "    \"BUSINESS_ABN\", \"GST_AMOUNT\", \"TOTAL_AMOUNT\", \n",
    "    \"IS_GST_INCLUDED\", \"DOCUMENT_TYPE\",\n",
    "]\n",
    "\n",
    "# Fields requiring position-aware matching (ordered lists)\n",
    "POSITION_AWARE_FIELDS = [\n",
    "    \"LINE_ITEM_QUANTITIES\", \"LINE_ITEM_PRICES\", \"LINE_ITEM_TOTAL_PRICES\",\n",
    "    \"TRANSACTION_AMOUNTS_PAID\",\n",
    "]\n",
    "\n",
    "# Fields using position-agnostic matching (unordered sets)\n",
    "POSITION_AGNOSTIC_FIELDS = [\n",
    "    \"LINE_ITEM_DESCRIPTIONS\", \"TRANSACTION_DATES\",\n",
    "]\n",
    "\n",
    "# Fields using fuzzy matching (names, addresses, free text)\n",
    "FUZZY_FIELDS = [\n",
    "    \"SUPPLIER_NAME\", \"BUSINESS_ADDRESS\", \"PAYER_NAME\", \"PAYER_ADDRESS\",\n",
    "    \"INVOICE_DATE\", \"STATEMENT_DATE_RANGE\",\n",
    "]\n",
    "\n",
    "\n",
    "def get_smart_f1_method(field_name: str) -> str:\n",
    "    \"\"\"Determine the most appropriate F1 method for a field.\"\"\"\n",
    "    if field_name in BINARY_FIELDS:\n",
    "        return \"binary\"\n",
    "    elif field_name in POSITION_AWARE_FIELDS:\n",
    "        return \"aware\"\n",
    "    elif field_name in POSITION_AGNOSTIC_FIELDS:\n",
    "        return \"agnostic\"\n",
    "    elif field_name in FUZZY_FIELDS:\n",
    "        return \"fuzzy\"\n",
    "    else:\n",
    "        # Default to position-agnostic for unknown fields\n",
    "        return \"agnostic\"\n",
    "\n",
    "\n",
    "def compute_f1_binary(extracted: str, ground_truth: str, field_name: str = \"\") -> float:\n",
    "    \"\"\"\n",
    "    Binary F1 - entire field value must match exactly.\n",
    "    \n",
    "    - Strictest possible matching\n",
    "    - No item-level comparison for lists\n",
    "    - Entire string must match after normalization\n",
    "    - Returns 1.0 or 0.0 only\n",
    "    \"\"\"\n",
    "    if pd.isna(extracted) or pd.isna(ground_truth):\n",
    "        return np.nan\n",
    "    \n",
    "    ext_str = str(extracted).strip()\n",
    "    gt_str = str(ground_truth).strip()\n",
    "    \n",
    "    # Handle NOT_FOUND\n",
    "    if ext_str.upper() == 'NOT_FOUND' and gt_str.upper() == 'NOT_FOUND':\n",
    "        return 1.0\n",
    "    if ext_str.upper() == 'NOT_FOUND' or gt_str.upper() == 'NOT_FOUND':\n",
    "        return 0.0\n",
    "    \n",
    "    # Normalize entire string and compare\n",
    "    ext_norm = normalize_value(ext_str, field_name)\n",
    "    gt_norm = normalize_value(gt_str, field_name)\n",
    "    \n",
    "    return 1.0 if ext_norm == gt_norm else 0.0\n",
    "\n",
    "\n",
    "def compute_f1_position_agnostic(extracted: str, ground_truth: str, field_name: str = \"\") -> float:\n",
    "    \"\"\"\n",
    "    Position-Agnostic F1 with EXACT item matching.\n",
    "    \n",
    "    - Order doesn't matter (set-based)\n",
    "    - Items must match exactly after normalization\n",
    "    - No partial credit\n",
    "    \"\"\"\n",
    "    if pd.isna(extracted) or pd.isna(ground_truth):\n",
    "        return np.nan\n",
    "    \n",
    "    ext_str = str(extracted).strip()\n",
    "    gt_str = str(ground_truth).strip()\n",
    "    \n",
    "    # Handle NOT_FOUND\n",
    "    if ext_str.upper() == 'NOT_FOUND' and gt_str.upper() == 'NOT_FOUND':\n",
    "        return 1.0\n",
    "    if ext_str.upper() == 'NOT_FOUND' or gt_str.upper() == 'NOT_FOUND':\n",
    "        return 0.0\n",
    "    \n",
    "    # Check if list field\n",
    "    is_list = '|' in ext_str or '|' in gt_str or field_name in LIST_FIELDS\n",
    "    \n",
    "    if is_list:\n",
    "        ext_items = {normalize_value(item.strip(), field_name)\n",
    "                     for item in ext_str.split('|') if item.strip()}\n",
    "        gt_items = {normalize_value(item.strip(), field_name)\n",
    "                    for item in gt_str.split('|') if item.strip()}\n",
    "        \n",
    "        if not ext_items and not gt_items:\n",
    "            return 1.0\n",
    "        if not ext_items or not gt_items:\n",
    "            return 0.0\n",
    "        \n",
    "        # Set intersection = exact matches\n",
    "        tp = len(ext_items & gt_items)\n",
    "        precision = tp / len(ext_items)\n",
    "        recall = tp / len(gt_items)\n",
    "        \n",
    "        if precision + recall == 0:\n",
    "            return 0.0\n",
    "        return 2 * precision * recall / (precision + recall)\n",
    "    else:\n",
    "        # Scalar: exact match\n",
    "        return 1.0 if normalize_value(ext_str, field_name) == normalize_value(gt_str, field_name) else 0.0\n",
    "\n",
    "\n",
    "def compute_f1_fuzzy_agnostic(extracted: str, ground_truth: str, field_name: str = \"\") -> float:\n",
    "    \"\"\"\n",
    "    Fuzzy Position-Agnostic F1 with token-level matching.\n",
    "    \n",
    "    - Order doesn't matter (set-based)\n",
    "    - Items can partially match using token overlap\n",
    "    - Partial credit for similar items (threshold 0.5)\n",
    "    \"\"\"\n",
    "    if pd.isna(extracted) or pd.isna(ground_truth):\n",
    "        return np.nan\n",
    "    \n",
    "    ext_str = str(extracted).strip()\n",
    "    gt_str = str(ground_truth).strip()\n",
    "    \n",
    "    # Handle NOT_FOUND\n",
    "    if ext_str.upper() == 'NOT_FOUND' and gt_str.upper() == 'NOT_FOUND':\n",
    "        return 1.0\n",
    "    if ext_str.upper() == 'NOT_FOUND' or gt_str.upper() == 'NOT_FOUND':\n",
    "        return 0.0\n",
    "    \n",
    "    # Check if list field\n",
    "    is_list = '|' in ext_str or '|' in gt_str or field_name in LIST_FIELDS\n",
    "    \n",
    "    if is_list:\n",
    "        ext_items = [normalize_value(item.strip(), field_name)\n",
    "                     for item in ext_str.split('|') if item.strip()]\n",
    "        gt_items = [normalize_value(item.strip(), field_name)\n",
    "                    for item in gt_str.split('|') if item.strip()]\n",
    "        \n",
    "        if not ext_items and not gt_items:\n",
    "            return 1.0\n",
    "        if not ext_items or not gt_items:\n",
    "            return 0.0\n",
    "        \n",
    "        # Greedy matching: for each extracted, find best GT match\n",
    "        matched_gt = set()\n",
    "        tp = 0.0\n",
    "        \n",
    "        for ext_item in ext_items:\n",
    "            best_score = 0.0\n",
    "            best_idx = -1\n",
    "            for i, gt_item in enumerate(gt_items):\n",
    "                if i in matched_gt:\n",
    "                    continue\n",
    "                # Exact match = 1.0, else token similarity\n",
    "                if ext_item == gt_item:\n",
    "                    score = 1.0\n",
    "                else:\n",
    "                    score = compute_token_similarity(ext_item, gt_item)\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_idx = i\n",
    "            \n",
    "            if best_score >= 0.5:  # Threshold for match\n",
    "                tp += best_score  # Partial credit\n",
    "                if best_idx >= 0:\n",
    "                    matched_gt.add(best_idx)\n",
    "        \n",
    "        precision = tp / len(ext_items)\n",
    "        recall = tp / len(gt_items)\n",
    "        \n",
    "        if precision + recall == 0:\n",
    "            return 0.0\n",
    "        return 2 * precision * recall / (precision + recall)\n",
    "    else:\n",
    "        # Scalar: token similarity\n",
    "        ext_norm = normalize_value(ext_str, field_name)\n",
    "        gt_norm = normalize_value(gt_str, field_name)\n",
    "        if ext_norm == gt_norm:\n",
    "            return 1.0\n",
    "        return compute_token_similarity(ext_norm, gt_norm)\n",
    "\n",
    "\n",
    "def compute_f1_position_aware(extracted: str, ground_truth: str, field_name: str = \"\") -> float:\n",
    "    \"\"\"\n",
    "    Position-Aware F1 with exact positional matching.\n",
    "    \n",
    "    - Items must match at the SAME position\n",
    "    - Strict positional matching\n",
    "    - No partial credit\n",
    "    \"\"\"\n",
    "    if pd.isna(extracted) or pd.isna(ground_truth):\n",
    "        return np.nan\n",
    "    \n",
    "    ext_str = str(extracted).strip()\n",
    "    gt_str = str(ground_truth).strip()\n",
    "    \n",
    "    # Handle NOT_FOUND\n",
    "    if ext_str.upper() == 'NOT_FOUND' and gt_str.upper() == 'NOT_FOUND':\n",
    "        return 1.0\n",
    "    if ext_str.upper() == 'NOT_FOUND' or gt_str.upper() == 'NOT_FOUND':\n",
    "        return 0.0\n",
    "    \n",
    "    # Check if list field\n",
    "    is_list = '|' in ext_str or '|' in gt_str or field_name in LIST_FIELDS\n",
    "    \n",
    "    if is_list:\n",
    "        ext_items = [normalize_value(item.strip(), field_name)\n",
    "                     for item in ext_str.split('|') if item.strip()]\n",
    "        gt_items = [normalize_value(item.strip(), field_name)\n",
    "                    for item in gt_str.split('|') if item.strip()]\n",
    "        \n",
    "        if not ext_items and not gt_items:\n",
    "            return 1.0\n",
    "        if not ext_items or not gt_items:\n",
    "            return 0.0\n",
    "        \n",
    "        # Position-aware: compare at same index\n",
    "        tp = 0\n",
    "        for i in range(min(len(ext_items), len(gt_items))):\n",
    "            if ext_items[i] == gt_items[i]:\n",
    "                tp += 1\n",
    "        \n",
    "        precision = tp / len(ext_items)\n",
    "        recall = tp / len(gt_items)\n",
    "        \n",
    "        if precision + recall == 0:\n",
    "            return 0.0\n",
    "        return 2 * precision * recall / (precision + recall)\n",
    "    else:\n",
    "        # Scalar: exact match\n",
    "        return 1.0 if normalize_value(ext_str, field_name) == normalize_value(gt_str, field_name) else 0.0\n",
    "\n",
    "\n",
    "def compute_f1_smart(extracted: str, ground_truth: str, field_name: str) -> tuple[float, str]:\n",
    "    \"\"\"\n",
    "    Smart F1 - automatically selects the most appropriate F1 method based on field type.\n",
    "    \n",
    "    Returns: (f1_score, method_used)\n",
    "    \n",
    "    Field type mappings:\n",
    "    - Binary: ABN, amounts, GST, document type (strict validation)\n",
    "    - Position-Aware: quantities, prices, amounts (ordered lists)\n",
    "    - Position-Agnostic: descriptions, dates (unordered sets)\n",
    "    - Fuzzy: names, addresses (free text)\n",
    "    \"\"\"\n",
    "    method = get_smart_f1_method(field_name)\n",
    "    \n",
    "    if method == \"binary\":\n",
    "        return compute_f1_binary(extracted, ground_truth, field_name), \"binary\"\n",
    "    elif method == \"aware\":\n",
    "        return compute_f1_position_aware(extracted, ground_truth, field_name), \"aware\"\n",
    "    elif method == \"agnostic\":\n",
    "        return compute_f1_position_agnostic(extracted, ground_truth, field_name), \"agnostic\"\n",
    "    else:  # fuzzy\n",
    "        return compute_f1_fuzzy_agnostic(extracted, ground_truth, field_name), \"fuzzy\"\n",
    "\n",
    "\n",
    "def compute_field_accuracy_from_batch(df: pd.DataFrame, ground_truth: pd.DataFrame = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute field-level accuracy AND five F1 variants from batch results.\n",
    "    \n",
    "    F1 Variants (from strictest to most lenient):\n",
    "    - F1_Binary: Entire field must match exactly (strictest)\n",
    "    - F1_Aware: Position-aware with exact item matching at same position\n",
    "    - F1_Agnostic: Position-agnostic with exact item matching (set-based)\n",
    "    - F1_Fuzzy: Position-agnostic with fuzzy/token matching (most lenient)\n",
    "    - F1_Smart: Automatically selects best method per field type (RECOMMENDED)\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Normalize document types\n",
    "    df = df.copy()\n",
    "    df['doc_type_normalized'] = df['document_type'].map(DOC_TYPE_MAP).fillna('unknown')\n",
    "    \n",
    "    # Split by document type\n",
    "    inv_rec_df = df[df['doc_type_normalized'] == 'invoice_receipt']\n",
    "    bank_df = df[df['doc_type_normalized'] == 'bank_statement']\n",
    "    \n",
    "    print(f\"  Invoice/Receipt images: {len(inv_rec_df)}\")\n",
    "    print(f\"  Bank statement images: {len(bank_df)}\")\n",
    "    \n",
    "    # Merge with ground truth if available\n",
    "    if ground_truth is not None and 'image_stem' in df.columns:\n",
    "        df = df.merge(ground_truth, on='image_stem', how='left', suffixes=('', '_gt'))\n",
    "        has_gt = True\n",
    "        matched = df['image_file_gt'].notna().sum()\n",
    "        print(f\"  Matched with ground truth: {matched}/{len(df)}\")\n",
    "    else:\n",
    "        has_gt = False\n",
    "        print(\"  No ground truth - using extraction rate as proxy\")\n",
    "    \n",
    "    # Re-split after merge\n",
    "    inv_rec_df = df[df['doc_type_normalized'] == 'invoice_receipt']\n",
    "    bank_df = df[df['doc_type_normalized'] == 'bank_statement']\n",
    "    \n",
    "    def compute_all_f1(subset_df, field):\n",
    "        \"\"\"Compute all five F1 variants for a field.\"\"\"\n",
    "        f1_binary, f1_agnostic, f1_fuzzy, f1_aware, f1_smart = np.nan, np.nan, np.nan, np.nan, np.nan\n",
    "        smart_method = get_smart_f1_method(field)\n",
    "        \n",
    "        if has_gt and f'{field}_gt' in subset_df.columns:\n",
    "            scores_binary, scores_agnostic, scores_fuzzy, scores_aware, scores_smart = [], [], [], [], []\n",
    "            \n",
    "            for _, row in subset_df.iterrows():\n",
    "                if pd.notna(row.get(f'{field}_gt')):\n",
    "                    ext = row[field]\n",
    "                    gt = row[f'{field}_gt']\n",
    "                    \n",
    "                    s0 = compute_f1_binary(ext, gt, field)\n",
    "                    s1 = compute_f1_position_agnostic(ext, gt, field)\n",
    "                    s2 = compute_f1_fuzzy_agnostic(ext, gt, field)\n",
    "                    s3 = compute_f1_position_aware(ext, gt, field)\n",
    "                    s4, _ = compute_f1_smart(ext, gt, field)\n",
    "                    \n",
    "                    if not np.isnan(s0): scores_binary.append(s0)\n",
    "                    if not np.isnan(s1): scores_agnostic.append(s1)\n",
    "                    if not np.isnan(s2): scores_fuzzy.append(s2)\n",
    "                    if not np.isnan(s3): scores_aware.append(s3)\n",
    "                    if not np.isnan(s4): scores_smart.append(s4)\n",
    "            \n",
    "            if scores_binary: f1_binary = np.mean(scores_binary)\n",
    "            if scores_agnostic: f1_agnostic = np.mean(scores_agnostic)\n",
    "            if scores_fuzzy: f1_fuzzy = np.mean(scores_fuzzy)\n",
    "            if scores_aware: f1_aware = np.mean(scores_aware)\n",
    "            if scores_smart: f1_smart = np.mean(scores_smart)\n",
    "        \n",
    "        return f1_binary, f1_agnostic, f1_fuzzy, f1_aware, f1_smart, smart_method\n",
    "    \n",
    "    # Process Invoice & Receipt fields\n",
    "    for field in INVOICE_RECEIPT_FIELDS:\n",
    "        if field not in df.columns:\n",
    "            continue\n",
    "        if len(inv_rec_df) == 0:\n",
    "            continue\n",
    "        \n",
    "        valid = inv_rec_df[field].notna() & (inv_rec_df[field] != 'NOT_FOUND') & (inv_rec_df[field] != '')\n",
    "        coverage = valid.mean() if len(inv_rec_df) > 0 else np.nan\n",
    "        \n",
    "        f1_binary, f1_agnostic, f1_fuzzy, f1_aware, f1_smart, smart_method = compute_all_f1(inv_rec_df, field)\n",
    "        \n",
    "        results.append({\n",
    "            'Subset': 'Invoices & receipts',\n",
    "            'Field': field,\n",
    "            'Accuracy': coverage,\n",
    "            'F1_Binary': f1_binary,\n",
    "            'F1_Aware': f1_aware,\n",
    "            'F1_Agnostic': f1_agnostic,\n",
    "            'F1_Fuzzy': f1_fuzzy,\n",
    "            'F1_Smart': f1_smart,\n",
    "            'Smart_Method': smart_method,\n",
    "            'Exclusive': field in EXCLUSIVE_FIELDS,\n",
    "            'extracted_count': int(valid.sum()),\n",
    "            'total_count': len(inv_rec_df),\n",
    "        })\n",
    "    \n",
    "    # Process Bank Statement fields\n",
    "    for field in BANK_STATEMENT_FIELDS:\n",
    "        if field not in df.columns:\n",
    "            continue\n",
    "        if len(bank_df) == 0:\n",
    "            continue\n",
    "        \n",
    "        valid = bank_df[field].notna() & (bank_df[field] != 'NOT_FOUND') & (bank_df[field] != '')\n",
    "        coverage = valid.mean() if len(bank_df) > 0 else np.nan\n",
    "        \n",
    "        f1_binary, f1_agnostic, f1_fuzzy, f1_aware, f1_smart, smart_method = compute_all_f1(bank_df, field)\n",
    "        \n",
    "        results.append({\n",
    "            'Subset': 'Bank statements',\n",
    "            'Field': field,\n",
    "            'Accuracy': coverage,\n",
    "            'F1_Binary': f1_binary,\n",
    "            'F1_Aware': f1_aware,\n",
    "            'F1_Agnostic': f1_agnostic,\n",
    "            'F1_Fuzzy': f1_fuzzy,\n",
    "            'F1_Smart': f1_smart,\n",
    "            'Smart_Method': smart_method,\n",
    "            'Exclusive': field in EXCLUSIVE_FIELDS,\n",
    "            'extracted_count': int(valid.sum()),\n",
    "            'total_count': len(bank_df),\n",
    "        })\n",
    "    \n",
    "    result_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Summary\n",
    "    exclusive_count = result_df['Exclusive'].sum()\n",
    "    comparable_count = len(result_df) - exclusive_count\n",
    "    \n",
    "    print(f\"\\n  F1 Scores Summary:\")\n",
    "    print(f\"    F1_Smart (field-appropriate): {result_df['F1_Smart'].mean():.1%}  ← RECOMMENDED\")\n",
    "    print(f\"    ----------------------------------------\")\n",
    "    print(f\"    Binary (exact full match):    {result_df['F1_Binary'].mean():.1%}\")\n",
    "    print(f\"    Position-Aware (positional):  {result_df['F1_Aware'].mean():.1%}\")\n",
    "    print(f\"    Position-Agnostic (set):      {result_df['F1_Agnostic'].mean():.1%}\")\n",
    "    print(f\"    Fuzzy Agnostic (lenient):     {result_df['F1_Fuzzy'].mean():.1%}\")\n",
    "    print(f\"\\n  Smart F1 Method Selection:\")\n",
    "    for method in ['binary', 'aware', 'agnostic', 'fuzzy']:\n",
    "        fields = result_df[result_df['Smart_Method'] == method]['Field'].tolist()\n",
    "        if fields:\n",
    "            print(f\"    {method}: {', '.join(fields)}\")\n",
    "    print(f\"\\n  Comparable fields: {comparable_count}\")\n",
    "    print(f\"  Exclusive fields: {exclusive_count}\")\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most recent batch results by model:\n",
      "============================================================\n",
      "Loaded: llama_batch_results_20251210_003155.csv\n",
      "  Images: 9\n",
      "  Document types: {'bank_statement': 3, 'receipt': 3, 'invoice': 3}\n",
      "  Mean overall accuracy: 9764.3%\n",
      "\n",
      "Using generic internvl3_batch_results as InternVL3-8B:\n",
      "Loaded: internvl3_batch_results_20251210_005902.csv\n",
      "  Images: 9\n",
      "  Document types: {'bank_statement': 3, 'receipt': 3, 'invoice': 3}\n",
      "  Mean overall accuracy: 8485.7%\n",
      "\n",
      "Loaded: internvl3_2b_batch_results_20251210_013149.csv\n",
      "  Images: 9\n",
      "  Document types: {'bank_statement': 3, 'receipt': 3, 'invoice': 3}\n",
      "  Mean overall accuracy: 7136.5%\n",
      "\n",
      "\n",
      "Models loaded: ['Llama-11B', 'InternVL3-8B', 'InternVL3-2B']\n"
     ]
    }
   ],
   "source": [
    "# Example: Load most recent batch results for each model\n",
    "print(\"Most recent batch results by model:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "batch_dfs = {}\n",
    "\n",
    "if result_files['llama_batch']:\n",
    "    latest = result_files['llama_batch'][0]\n",
    "    batch_dfs['Llama-11B'] = load_batch_results(latest)\n",
    "    print()\n",
    "\n",
    "# InternVL3-8B: Check both explicit 8B files and generic internvl3 files\n",
    "if result_files['internvl3_8b_batch']:\n",
    "    latest = result_files['internvl3_8b_batch'][0]\n",
    "    batch_dfs['InternVL3-8B'] = load_batch_results(latest)\n",
    "    print()\n",
    "elif result_files['internvl3_batch']:\n",
    "    # Use generic internvl3_batch_results as 8B fallback\n",
    "    latest = result_files['internvl3_batch'][0]\n",
    "    print(\"Using generic internvl3_batch_results as InternVL3-8B:\")\n",
    "    batch_dfs['InternVL3-8B'] = load_batch_results(latest)\n",
    "    print()\n",
    "\n",
    "if result_files['internvl3_2b_batch']:\n",
    "    latest = result_files['internvl3_2b_batch'][0]\n",
    "    batch_dfs['InternVL3-2B'] = load_batch_results(latest)\n",
    "    print()\n",
    "\n",
    "print(f\"\\nModels loaded: {list(batch_dfs.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Select Model and Export\n",
    "\n",
    "Choose which model's accuracy to export as `current_model_accuracy.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION - Select model and data source\n",
    "# =============================================================================\n",
    "\n",
    "# Choose model to export\n",
    "SELECTED_MODEL = \"Llama-11B\"  # Options: \"Llama-11B\", \"InternVL3-8B\", \"InternVL3-2B\"\n",
    "\n",
    "# Choose data source\n",
    "USE_PER_FIELD_METRICS = False  # True = use per_field_metrics.csv (recommended)\n",
    "                               # False = aggregate from batch results (coverage only)\n",
    "\n",
    "# Output filename\n",
    "OUTPUT_FILENAME = \"current_model_accuracy.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregating from batch results for Llama-11B\n",
      "Computing F1 against ground truth...\n",
      "  Invoice/Receipt images: 6\n",
      "  Bank statement images: 3\n",
      "  Matched with ground truth: 18/18\n",
      "\n",
      "  F1 Scores Summary:\n",
      "    F1_Smart (field-appropriate): 96.0%  ← RECOMMENDED\n",
      "    ----------------------------------------\n",
      "    Binary (exact full match):    90.7%\n",
      "    Position-Aware (positional):  96.1%\n",
      "    Position-Agnostic (set):      96.0%\n",
      "    Fuzzy Agnostic (lenient):     97.6%\n",
      "\n",
      "  Smart F1 Method Selection:\n",
      "    binary: DOCUMENT_TYPE, BUSINESS_ABN, IS_GST_INCLUDED, GST_AMOUNT, TOTAL_AMOUNT\n",
      "    aware: LINE_ITEM_QUANTITIES, LINE_ITEM_PRICES, LINE_ITEM_TOTAL_PRICES, TRANSACTION_AMOUNTS_PAID\n",
      "    agnostic: LINE_ITEM_DESCRIPTIONS, LINE_ITEM_DESCRIPTIONS, TRANSACTION_DATES\n",
      "    fuzzy: SUPPLIER_NAME, BUSINESS_ADDRESS, PAYER_NAME, PAYER_ADDRESS, INVOICE_DATE, STATEMENT_DATE_RANGE\n",
      "\n",
      "  Comparable fields: 17\n",
      "  Exclusive fields: 1\n",
      "\n",
      "Saved: ../output/current_model_accuracy.csv\n",
      "Fields: 18\n",
      "Mean accuracy: 100.0%\n",
      "\n",
      "F1 Scores:\n",
      "  F1_Smart (field-appropriate): 96.0%  ← RECOMMENDED\n",
      "  ----------------------------------------\n",
      "  Binary (exact full match):    90.7%\n",
      "  Position-Aware (positional):  96.1%\n",
      "  Position-Agnostic (set):      96.0%\n",
      "  Fuzzy Agnostic (lenient):     97.6%\n",
      "\n",
      "============================================================\n",
      "Exported Data:\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subset</th>\n",
       "      <th>Field</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1_Binary</th>\n",
       "      <th>F1_Aware</th>\n",
       "      <th>F1_Agnostic</th>\n",
       "      <th>F1_Fuzzy</th>\n",
       "      <th>F1_Smart</th>\n",
       "      <th>Smart_Method</th>\n",
       "      <th>Exclusive</th>\n",
       "      <th>extracted_count</th>\n",
       "      <th>total_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Invoices &amp; receipts</td>\n",
       "      <td>DOCUMENT_TYPE</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>binary</td>\n",
       "      <td>True</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Invoices &amp; receipts</td>\n",
       "      <td>BUSINESS_ABN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>binary</td>\n",
       "      <td>False</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Invoices &amp; receipts</td>\n",
       "      <td>SUPPLIER_NAME</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>fuzzy</td>\n",
       "      <td>False</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Invoices &amp; receipts</td>\n",
       "      <td>BUSINESS_ADDRESS</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>fuzzy</td>\n",
       "      <td>False</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Invoices &amp; receipts</td>\n",
       "      <td>PAYER_NAME</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>fuzzy</td>\n",
       "      <td>False</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Invoices &amp; receipts</td>\n",
       "      <td>PAYER_ADDRESS</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>fuzzy</td>\n",
       "      <td>False</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Invoices &amp; receipts</td>\n",
       "      <td>INVOICE_DATE</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>fuzzy</td>\n",
       "      <td>False</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Invoices &amp; receipts</td>\n",
       "      <td>LINE_ITEM_DESCRIPTIONS</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>agnostic</td>\n",
       "      <td>False</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Invoices &amp; receipts</td>\n",
       "      <td>LINE_ITEM_QUANTITIES</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>aware</td>\n",
       "      <td>False</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Invoices &amp; receipts</td>\n",
       "      <td>LINE_ITEM_PRICES</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>aware</td>\n",
       "      <td>False</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Invoices &amp; receipts</td>\n",
       "      <td>LINE_ITEM_TOTAL_PRICES</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>aware</td>\n",
       "      <td>False</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Invoices &amp; receipts</td>\n",
       "      <td>IS_GST_INCLUDED</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>binary</td>\n",
       "      <td>False</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Invoices &amp; receipts</td>\n",
       "      <td>GST_AMOUNT</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>binary</td>\n",
       "      <td>False</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Invoices &amp; receipts</td>\n",
       "      <td>TOTAL_AMOUNT</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>binary</td>\n",
       "      <td>False</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Bank statements</td>\n",
       "      <td>STATEMENT_DATE_RANGE</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>fuzzy</td>\n",
       "      <td>False</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Bank statements</td>\n",
       "      <td>LINE_ITEM_DESCRIPTIONS</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.655172</td>\n",
       "      <td>0.653333</td>\n",
       "      <td>0.933082</td>\n",
       "      <td>0.653333</td>\n",
       "      <td>agnostic</td>\n",
       "      <td>False</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Bank statements</td>\n",
       "      <td>TRANSACTION_DATES</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.988506</td>\n",
       "      <td>0.962963</td>\n",
       "      <td>0.988506</td>\n",
       "      <td>0.962963</td>\n",
       "      <td>agnostic</td>\n",
       "      <td>False</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Bank statements</td>\n",
       "      <td>TRANSACTION_AMOUNTS_PAID</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.988506</td>\n",
       "      <td>0.988506</td>\n",
       "      <td>0.988506</td>\n",
       "      <td>0.988506</td>\n",
       "      <td>aware</td>\n",
       "      <td>False</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Subset                     Field  Accuracy  F1_Binary  \\\n",
       "0   Invoices & receipts             DOCUMENT_TYPE       1.0   1.000000   \n",
       "1   Invoices & receipts              BUSINESS_ABN       1.0   1.000000   \n",
       "2   Invoices & receipts             SUPPLIER_NAME       1.0   1.000000   \n",
       "3   Invoices & receipts          BUSINESS_ADDRESS       1.0   0.833333   \n",
       "4   Invoices & receipts                PAYER_NAME       1.0   1.000000   \n",
       "5   Invoices & receipts             PAYER_ADDRESS       1.0   1.000000   \n",
       "6   Invoices & receipts              INVOICE_DATE       1.0   0.833333   \n",
       "7   Invoices & receipts    LINE_ITEM_DESCRIPTIONS       1.0   1.000000   \n",
       "8   Invoices & receipts      LINE_ITEM_QUANTITIES       1.0   1.000000   \n",
       "9   Invoices & receipts          LINE_ITEM_PRICES       1.0   1.000000   \n",
       "10  Invoices & receipts    LINE_ITEM_TOTAL_PRICES       1.0   1.000000   \n",
       "11  Invoices & receipts           IS_GST_INCLUDED       1.0   1.000000   \n",
       "12  Invoices & receipts                GST_AMOUNT       1.0   1.000000   \n",
       "13  Invoices & receipts              TOTAL_AMOUNT       1.0   1.000000   \n",
       "14      Bank statements      STATEMENT_DATE_RANGE       1.0   1.000000   \n",
       "15      Bank statements    LINE_ITEM_DESCRIPTIONS       1.0   0.333333   \n",
       "16      Bank statements         TRANSACTION_DATES       1.0   0.666667   \n",
       "17      Bank statements  TRANSACTION_AMOUNTS_PAID       1.0   0.666667   \n",
       "\n",
       "    F1_Aware  F1_Agnostic  F1_Fuzzy  F1_Smart Smart_Method  Exclusive  \\\n",
       "0   1.000000     1.000000  1.000000  1.000000       binary       True   \n",
       "1   1.000000     1.000000  1.000000  1.000000       binary      False   \n",
       "2   1.000000     1.000000  1.000000  1.000000        fuzzy      False   \n",
       "3   0.833333     0.833333  0.833333  0.833333        fuzzy      False   \n",
       "4   1.000000     1.000000  1.000000  1.000000        fuzzy      False   \n",
       "5   1.000000     1.000000  1.000000  1.000000        fuzzy      False   \n",
       "6   0.833333     0.833333  0.833333  0.833333        fuzzy      False   \n",
       "7   1.000000     1.000000  1.000000  1.000000     agnostic      False   \n",
       "8   1.000000     1.000000  1.000000  1.000000        aware      False   \n",
       "9   1.000000     1.000000  1.000000  1.000000        aware      False   \n",
       "10  1.000000     1.000000  1.000000  1.000000        aware      False   \n",
       "11  1.000000     1.000000  1.000000  1.000000       binary      False   \n",
       "12  1.000000     1.000000  1.000000  1.000000       binary      False   \n",
       "13  1.000000     1.000000  1.000000  1.000000       binary      False   \n",
       "14  1.000000     1.000000  1.000000  1.000000        fuzzy      False   \n",
       "15  0.655172     0.653333  0.933082  0.653333     agnostic      False   \n",
       "16  0.988506     0.962963  0.988506  0.962963     agnostic      False   \n",
       "17  0.988506     0.988506  0.988506  0.988506        aware      False   \n",
       "\n",
       "    extracted_count  total_count  \n",
       "0                12           12  \n",
       "1                12           12  \n",
       "2                12           12  \n",
       "3                12           12  \n",
       "4                12           12  \n",
       "5                12           12  \n",
       "6                12           12  \n",
       "7                12           12  \n",
       "8                12           12  \n",
       "9                12           12  \n",
       "10               12           12  \n",
       "11               12           12  \n",
       "12               12           12  \n",
       "13               12           12  \n",
       "14                6            6  \n",
       "15                6            6  \n",
       "16                6            6  \n",
       "17                6            6  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def export_model_accuracy(model_name: str, use_per_field: bool, \n",
    "                          per_field_df: pd.DataFrame, batch_dfs: dict,\n",
    "                          ground_truth_df: pd.DataFrame,\n",
    "                          output_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Export field-level accuracy AND all F1 variants for selected model.\"\"\"\n",
    "    \n",
    "    if use_per_field and per_field_df is not None:\n",
    "        print(f\"Using per_field_metrics.csv for {model_name}\")\n",
    "        result_df = extract_model_accuracy_from_per_field(per_field_df, model_name)\n",
    "        if result_df is None:\n",
    "            return None\n",
    "    elif model_name in batch_dfs:\n",
    "        print(f\"Aggregating from batch results for {model_name}\")\n",
    "        if ground_truth_df is not None:\n",
    "            print(\"Computing F1 against ground truth...\")\n",
    "        result_df = compute_field_accuracy_from_batch(batch_dfs[model_name], ground_truth_df)\n",
    "    else:\n",
    "        print(f\"No data available for {model_name}\")\n",
    "        return None\n",
    "    \n",
    "    # Determine which columns to save\n",
    "    save_cols = ['Subset', 'Field', 'Accuracy']\n",
    "    \n",
    "    # Add F1 columns if they have data (F1_Smart first as recommended)\n",
    "    for f1_col in ['F1_Smart', 'F1_Binary', 'F1_Aware', 'F1_Agnostic', 'F1_Fuzzy']:\n",
    "        if f1_col in result_df.columns and result_df[f1_col].notna().any():\n",
    "            save_cols.append(f1_col)\n",
    "    \n",
    "    # Add Smart_Method to show which method was used\n",
    "    if 'Smart_Method' in result_df.columns:\n",
    "        save_cols.append('Smart_Method')\n",
    "    \n",
    "    if 'Exclusive' in result_df.columns:\n",
    "        save_cols.append('Exclusive')\n",
    "    \n",
    "    result_df[save_cols].to_csv(output_path, index=False)\n",
    "    print(f\"\\nSaved: {output_path}\")\n",
    "    print(f\"Fields: {len(result_df)}\")\n",
    "    print(f\"Mean accuracy: {result_df['Accuracy'].mean():.1%}\")\n",
    "    \n",
    "    # Print F1 summaries\n",
    "    print(f\"\\nF1 Scores:\")\n",
    "    if 'F1_Smart' in save_cols:\n",
    "        print(f\"  F1_Smart (field-appropriate): {result_df['F1_Smart'].mean():.1%}  ← RECOMMENDED\")\n",
    "        print(f\"  ----------------------------------------\")\n",
    "    if 'F1_Binary' in save_cols:\n",
    "        print(f\"  Binary (exact full match):    {result_df['F1_Binary'].mean():.1%}\")\n",
    "    if 'F1_Aware' in save_cols:\n",
    "        print(f\"  Position-Aware (positional):  {result_df['F1_Aware'].mean():.1%}\")\n",
    "    if 'F1_Agnostic' in save_cols:\n",
    "        print(f\"  Position-Agnostic (set):      {result_df['F1_Agnostic'].mean():.1%}\")\n",
    "    if 'F1_Fuzzy' in save_cols:\n",
    "        print(f\"  Fuzzy Agnostic (lenient):     {result_df['F1_Fuzzy'].mean():.1%}\")\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "# Export\n",
    "output_path = OUTPUT_DIR / OUTPUT_FILENAME\n",
    "exported_df = export_model_accuracy(\n",
    "    SELECTED_MODEL, \n",
    "    USE_PER_FIELD_METRICS, \n",
    "    per_field_df, \n",
    "    batch_dfs,\n",
    "    ground_truth_df,  # Pass ground truth for F1 computation\n",
    "    output_path\n",
    ")\n",
    "\n",
    "if exported_df is not None:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Exported Data:\")\n",
    "    print(\"=\" * 60)\n",
    "    display(exported_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Compare Multiple Models (Optional)\n",
    "\n",
    "Quick comparison of all available models from per_field_metrics.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if per_field_df is not None:\n",
    "    # Pivot to compare models\n",
    "    comparison = per_field_df.pivot(index='field', columns='model', values='accuracy')\n",
    "    comparison['best_model'] = comparison.idxmax(axis=1)\n",
    "    \n",
    "    print(\"\\nField-Level Accuracy Comparison (from per_field_metrics.csv)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Format as percentages\n",
    "    display_df = comparison.copy()\n",
    "    for col in display_df.columns:\n",
    "        if col != 'best_model':\n",
    "            display_df[col] = display_df[col].apply(lambda x: f\"{x:.1%}\" if pd.notna(x) else \"N/A\")\n",
    "    \n",
    "    display(display_df)\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\nModel Summary:\")\n",
    "    for model in per_field_df['model'].unique():\n",
    "        model_data = per_field_df[per_field_df['model'] == model]\n",
    "        wins = (comparison['best_model'] == model).sum()\n",
    "        print(f\"  {model}: mean={model_data['accuracy'].mean():.1%}, best on {wins} fields\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Export All Models for Comparison\n",
    "\n",
    "Export accuracy files for all available models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting all models...\n",
      "============================================================\n",
      "No per_field_metrics.csv available\n"
     ]
    }
   ],
   "source": [
    "def export_all_models(per_field_df: pd.DataFrame, output_dir: Path):\n",
    "    \"\"\"Export accuracy CSV for each model in per_field_metrics.csv\"\"\"\n",
    "    if per_field_df is None:\n",
    "        print(\"No per_field_metrics.csv available\")\n",
    "        return\n",
    "    \n",
    "    exported = []\n",
    "    for model in per_field_df['model'].unique():\n",
    "        # Create safe filename\n",
    "        safe_name = model.lower().replace('-', '_').replace(' ', '_')\n",
    "        filename = f\"{safe_name}_accuracy.csv\"\n",
    "        output_path = output_dir / filename\n",
    "        \n",
    "        result_df = extract_model_accuracy_from_per_field(per_field_df, model)\n",
    "        if result_df is not None:\n",
    "            result_df.to_csv(output_path, index=False)\n",
    "            exported.append((model, output_path, result_df['Accuracy'].mean()))\n",
    "            print(f\"Exported: {filename} (mean accuracy: {result_df['Accuracy'].mean():.1%})\")\n",
    "    \n",
    "    return exported\n",
    "\n",
    "print(\"Exporting all models...\")\n",
    "print(\"=\" * 60)\n",
    "all_exports = export_all_models(per_field_df, OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "Files created:\n",
    "- `current_model_accuracy.csv` - Selected model for comparison notebook\n",
    "- `{model}_accuracy.csv` - Individual files for each model\n",
    "\n",
    "To use in comparison notebook:\n",
    "```python\n",
    "CURRENT_MODEL_CSV = Path(\"../output/current_model_accuracy.csv\")\n",
    "# Or use specific model file:\n",
    "CURRENT_MODEL_CSV = Path(\"../output/internvl3_8b_accuracy.csv\")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "du",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
