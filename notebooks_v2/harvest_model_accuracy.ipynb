{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Harvest Model Accuracy from Evaluation Results\n",
    "\n",
    "This notebook scans `output/csv/` for evaluation results and generates the `current_model_accuracy.csv` file for use in model comparison.\n",
    "\n",
    "## Data Sources\n",
    "1. **per_field_metrics.csv** - Pre-computed field-level metrics by model\n",
    "2. **Model batch results** - Per-image results that can be aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning: /Users/tod/Desktop/LMM_POC/output/csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# Paths\n",
    "CSV_DIR = Path(\"../output/csv\")\n",
    "OUTPUT_DIR = Path(\"../output\")\n",
    "\n",
    "print(f\"Scanning: {CSV_DIR.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Discover Available Result Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Result Files\n",
      "============================================================\n",
      "\n",
      "per_field_metrics.csv: Not found\n",
      "\n",
      "Llama batch results: 2 files\n",
      "  - llama_batch_results_20251210_003155.csv\n",
      "  - llama_batch_results_20251210_001755.csv\n",
      "\n",
      "InternVL3-8B batch results: 0 files\n",
      "\n",
      "InternVL3 (generic, treated as 8B): 1 files\n",
      "  - internvl3_batch_results_20251210_005902.csv\n",
      "\n",
      "InternVL3-2B batch results: 1 files\n",
      "  - internvl3_2b_batch_results_20251210_013149.csv\n"
     ]
    }
   ],
   "source": [
    "def discover_result_files(csv_dir: Path) -> dict:\n",
    "    \"\"\"Discover all available result files in the CSV directory.\"\"\"\n",
    "    files = {\n",
    "        'per_field_metrics': None,\n",
    "        'llama_batch': [],\n",
    "        'internvl3_batch': [],  # Generic internvl3 (treated as 8B)\n",
    "        'internvl3_2b_batch': [],\n",
    "        'internvl3_8b_batch': [],\n",
    "        'batch_summary': [],\n",
    "    }\n",
    "    \n",
    "    for f in csv_dir.glob(\"*.csv\"):\n",
    "        name = f.name\n",
    "        \n",
    "        if name == 'per_field_metrics.csv':\n",
    "            files['per_field_metrics'] = f\n",
    "        elif name.startswith('llama_batch_results_'):\n",
    "            files['llama_batch'].append(f)\n",
    "        elif name.startswith('internvl3_2b_batch_results_'):\n",
    "            files['internvl3_2b_batch'].append(f)\n",
    "        elif name.startswith('internvl3_5_8b_batch_results_') or name.startswith('internvl3_8b_batch_results_'):\n",
    "            files['internvl3_8b_batch'].append(f)\n",
    "        elif name.startswith('internvl3_batch_results_'):\n",
    "            # Treat generic internvl3_batch_results as 8B (most common case)\n",
    "            files['internvl3_batch'].append(f)\n",
    "        elif '_summary.csv' in name:\n",
    "            files['batch_summary'].append(f)\n",
    "    \n",
    "    # Sort by timestamp (most recent first)\n",
    "    for key in ['llama_batch', 'internvl3_batch', 'internvl3_2b_batch', 'internvl3_8b_batch']:\n",
    "        files[key] = sorted(files[key], key=lambda x: x.name, reverse=True)\n",
    "    \n",
    "    return files\n",
    "\n",
    "result_files = discover_result_files(CSV_DIR)\n",
    "\n",
    "print(\"Available Result Files\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nper_field_metrics.csv: {'Found' if result_files['per_field_metrics'] else 'Not found'}\")\n",
    "print(f\"\\nLlama batch results: {len(result_files['llama_batch'])} files\")\n",
    "for f in result_files['llama_batch'][:3]:\n",
    "    print(f\"  - {f.name}\")\n",
    "\n",
    "print(f\"\\nInternVL3-8B batch results: {len(result_files['internvl3_8b_batch'])} files\")\n",
    "for f in result_files['internvl3_8b_batch'][:3]:\n",
    "    print(f\"  - {f.name}\")\n",
    "\n",
    "print(f\"\\nInternVL3 (generic, treated as 8B): {len(result_files['internvl3_batch'])} files\")\n",
    "for f in result_files['internvl3_batch'][:3]:\n",
    "    print(f\"  - {f.name}\")\n",
    "\n",
    "print(f\"\\nInternVL3-2B batch results: {len(result_files['internvl3_2b_batch'])} files\")\n",
    "for f in result_files['internvl3_2b_batch'][:3]:\n",
    "    print(f\"  - {f.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Option A: Use per_field_metrics.csv (Recommended)\n",
    "\n",
    "This file already contains pre-computed field-level accuracy for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "per_field_metrics.csv not found\n"
     ]
    }
   ],
   "source": [
    "def load_per_field_metrics(csv_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Load and display per-field metrics.\"\"\"\n",
    "    if not csv_path or not csv_path.exists():\n",
    "        print(\"per_field_metrics.csv not found\")\n",
    "        return None\n",
    "    \n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"Loaded: {csv_path}\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Models: {df['model'].unique().tolist()}\")\n",
    "    print(f\"Fields: {df['field'].nunique()}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "per_field_df = load_per_field_metrics(result_files['per_field_metrics'])\n",
    "if per_field_df is not None:\n",
    "    display(per_field_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fields that are EXCLUSIVE to our model (competing model doesn't have these)\n",
    "EXCLUSIVE_FIELDS = [\"DOCUMENT_TYPE\"]\n",
    "\n",
    "# List fields where F1 scores are meaningful (for comparison)\n",
    "# These are fields with multiple items where precision/recall matter\n",
    "LIST_FIELDS = [\n",
    "    \"LINE_ITEM_DESCRIPTIONS\", \"LINE_ITEM_QUANTITIES\", \"LINE_ITEM_PRICES\",\n",
    "    \"LINE_ITEM_TOTAL_PRICES\", \"TRANSACTION_DATES\", \"TRANSACTION_AMOUNTS_PAID\",\n",
    "]\n",
    "\n",
    "def extract_model_accuracy_from_per_field(df: pd.DataFrame, model_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract field-level accuracy AND F1 scores for a specific model.\n",
    "    \n",
    "    The F1 scores in per_field_metrics.csv are POSITION-AGNOSTIC (set-based).\n",
    "    This allows fair comparison with competing models using the same methodology.\n",
    "    \"\"\"\n",
    "    model_df = df[df['model'] == model_name].copy()\n",
    "    \n",
    "    if len(model_df) == 0:\n",
    "        print(f\"Model '{model_name}' not found. Available: {df['model'].unique().tolist()}\")\n",
    "        return None\n",
    "    \n",
    "    # Determine subset based on field\n",
    "    bank_fields = ['STATEMENT_DATE_RANGE', 'TRANSACTION_DATES', 'TRANSACTION_AMOUNTS_PAID']\n",
    "    \n",
    "    result = []\n",
    "    for _, row in model_df.iterrows():\n",
    "        subset = 'Bank statements' if row['field'] in bank_fields else 'Invoices & receipts'\n",
    "        \n",
    "        # Extract F1 score if available (position-agnostic from sklearn)\n",
    "        f1_agnostic = row.get('f1_score', np.nan) if 'f1_score' in row.index else np.nan\n",
    "        \n",
    "        result.append({\n",
    "            'Subset': subset,\n",
    "            'Field': row['field'],\n",
    "            'Accuracy': row['accuracy'],\n",
    "            'F1_Agnostic': f1_agnostic,  # Position-agnostic F1 for fair comparison\n",
    "            'Exclusive': row['field'] in EXCLUSIVE_FIELDS,\n",
    "        })\n",
    "    \n",
    "    result_df = pd.DataFrame(result)\n",
    "    \n",
    "    # Summary\n",
    "    exclusive_count = result_df['Exclusive'].sum()\n",
    "    comparable_count = len(result_df) - exclusive_count\n",
    "    f1_available = result_df['F1_Agnostic'].notna().sum()\n",
    "    \n",
    "    print(f\"  Comparable fields: {comparable_count}\")\n",
    "    print(f\"  Exclusive fields (not in competing model): {exclusive_count}\")\n",
    "    print(f\"  Fields with F1 scores: {f1_available}\")\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "# Show available models\n",
    "if per_field_df is not None:\n",
    "    print(\"\\nAvailable models in per_field_metrics.csv:\")\n",
    "    for model in per_field_df['model'].unique():\n",
    "        model_data = per_field_df[per_field_df['model'] == model]\n",
    "        count = len(model_data)\n",
    "        mean_acc = model_data['accuracy'].mean()\n",
    "        mean_f1 = model_data['f1_score'].mean() if 'f1_score' in model_data.columns else np.nan\n",
    "        f1_str = f\", mean F1: {mean_f1:.1%}\" if not np.isnan(mean_f1) else \"\"\n",
    "        print(f\"  - {model}: {count} fields, mean accuracy: {mean_acc:.1%}{f1_str}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Option B: Aggregate from Batch Results\n",
    "\n",
    "If per_field_metrics.csv is not available or you want fresh results from a specific batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ground truth for F1 computation...\n",
      "  Loaded inv_rec: 6 rows\n",
      "  Loaded bank: 15 rows\n",
      "  Loaded synthetic: 9 rows\n",
      "  Total ground truth: 30 rows\n"
     ]
    }
   ],
   "source": [
    "# Schema fields by document type\n",
    "INVOICE_RECEIPT_FIELDS = [\n",
    "    \"DOCUMENT_TYPE\", \"BUSINESS_ABN\", \"SUPPLIER_NAME\", \"BUSINESS_ADDRESS\",\n",
    "    \"PAYER_NAME\", \"PAYER_ADDRESS\", \"INVOICE_DATE\", \"LINE_ITEM_DESCRIPTIONS\",\n",
    "    \"LINE_ITEM_QUANTITIES\", \"LINE_ITEM_PRICES\", \"LINE_ITEM_TOTAL_PRICES\",\n",
    "    \"IS_GST_INCLUDED\", \"GST_AMOUNT\", \"TOTAL_AMOUNT\",\n",
    "]\n",
    "\n",
    "BANK_STATEMENT_FIELDS = [\n",
    "    \"STATEMENT_DATE_RANGE\", \"LINE_ITEM_DESCRIPTIONS\",  # LINE_ITEM_DESCRIPTIONS = transaction descriptions\n",
    "    \"TRANSACTION_DATES\", \"TRANSACTION_AMOUNTS_PAID\",\n",
    "]\n",
    "\n",
    "# List fields where F1 is meaningful (multiple items)\n",
    "LIST_FIELDS = [\n",
    "    \"LINE_ITEM_DESCRIPTIONS\", \"LINE_ITEM_QUANTITIES\", \"LINE_ITEM_PRICES\",\n",
    "    \"LINE_ITEM_TOTAL_PRICES\", \"TRANSACTION_DATES\", \"TRANSACTION_AMOUNTS_PAID\",\n",
    "]\n",
    "\n",
    "# Fields that are EXCLUSIVE to our model (competing model doesn't have these)\n",
    "EXCLUSIVE_FIELDS = [\"DOCUMENT_TYPE\"]\n",
    "\n",
    "# Document type mapping (normalize variations)\n",
    "DOC_TYPE_MAP = {\n",
    "    'receipt': 'invoice_receipt',\n",
    "    'invoice': 'invoice_receipt', \n",
    "    'bank_statement': 'bank_statement',\n",
    "    'RECEIPT': 'invoice_receipt',\n",
    "    'INVOICE': 'invoice_receipt',\n",
    "    'BANK_STATEMENT': 'bank_statement',\n",
    "}\n",
    "\n",
    "# Ground truth paths\n",
    "GROUND_TRUTH_PATHS = {\n",
    "    'inv_rec': Path(\"../evaluation_data/inv_rec/ground_truth_inv_rec.csv\"),\n",
    "    'bank': Path(\"../evaluation_data/bank/ground_truth_bank.csv\"),\n",
    "    'synthetic': Path(\"../evaluation_data/synthetic/ground_truth_synthetic.csv\"),\n",
    "}\n",
    "\n",
    "def load_ground_truth() -> pd.DataFrame:\n",
    "    \"\"\"Load and combine all ground truth files.\"\"\"\n",
    "    dfs = []\n",
    "    for name, path in GROUND_TRUTH_PATHS.items():\n",
    "        if path.exists():\n",
    "            df = pd.read_csv(path)\n",
    "            df['source'] = name\n",
    "            dfs.append(df)\n",
    "            print(f\"  Loaded {name}: {len(df)} rows\")\n",
    "    \n",
    "    if dfs:\n",
    "        combined = pd.concat(dfs, ignore_index=True)\n",
    "        # Normalize image_file to stem for matching\n",
    "        combined['image_stem'] = combined['image_file'].apply(lambda x: Path(str(x)).stem)\n",
    "        print(f\"  Total ground truth: {len(combined)} rows\")\n",
    "        return combined\n",
    "    return None\n",
    "\n",
    "print(\"Loading ground truth for F1 computation...\")\n",
    "ground_truth_df = load_ground_truth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path for imports\n",
    "PROJECT_ROOT = Path(\"..\").resolve()\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from common.evaluation_metrics import calculate_field_accuracy_f1\n",
    "\n",
    "def load_batch_results(csv_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Load a batch results file.\"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"Loaded: {csv_path.name}\")\n",
    "    print(f\"  Images: {len(df)}\")\n",
    "    print(f\"  Document types: {df['document_type'].value_counts().to_dict()}\")\n",
    "    if 'overall_accuracy' in df.columns:\n",
    "        print(f\"  Mean overall accuracy: {df['overall_accuracy'].mean():.1%}\")\n",
    "\n",
    "    # Add image_stem for ground truth matching\n",
    "    if 'image_file' in df.columns:\n",
    "        df['image_stem'] = df['image_file'].apply(lambda x: Path(str(x)).stem)\n",
    "    elif 'image_name' in df.columns:\n",
    "        df['image_stem'] = df['image_name'].apply(lambda x: Path(str(x)).stem)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Schema fields by document type\n",
    "INVOICE_RECEIPT_FIELDS = [\n",
    "    \"DOCUMENT_TYPE\", \"BUSINESS_ABN\", \"SUPPLIER_NAME\", \"BUSINESS_ADDRESS\",\n",
    "    \"PAYER_NAME\", \"PAYER_ADDRESS\", \"INVOICE_DATE\", \"LINE_ITEM_DESCRIPTIONS\",\n",
    "    \"LINE_ITEM_QUANTITIES\", \"LINE_ITEM_PRICES\", \"LINE_ITEM_TOTAL_PRICES\",\n",
    "    \"IS_GST_INCLUDED\", \"GST_AMOUNT\", \"TOTAL_AMOUNT\",\n",
    "]\n",
    "\n",
    "BANK_STATEMENT_FIELDS = [\n",
    "    \"STATEMENT_DATE_RANGE\", \"LINE_ITEM_DESCRIPTIONS\",\n",
    "    \"TRANSACTION_DATES\", \"TRANSACTION_AMOUNTS_PAID\",\n",
    "]\n",
    "\n",
    "# Document type mapping\n",
    "DOC_TYPE_MAP = {\n",
    "    'receipt': 'invoice_receipt',\n",
    "    'invoice': 'invoice_receipt',\n",
    "    'bank_statement': 'bank_statement',\n",
    "    'RECEIPT': 'invoice_receipt',\n",
    "    'INVOICE': 'invoice_receipt',\n",
    "    'BANK_STATEMENT': 'bank_statement',\n",
    "}\n",
    "\n",
    "\n",
    "def compute_field_accuracy_from_batch(df: pd.DataFrame, ground_truth: pd.DataFrame = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute field-level F1 metrics from batch results using calculate_field_accuracy_f1.\n",
    "\n",
    "    This function uses the SAME F1 calculation as model_comparison_reporter_v2.ipynb\n",
    "    to ensure consistent metrics across all notebooks.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    # Normalize document types\n",
    "    df = df.copy()\n",
    "    df['doc_type_normalized'] = df['document_type'].map(DOC_TYPE_MAP).fillna('unknown')\n",
    "\n",
    "    # Split by document type\n",
    "    inv_rec_df = df[df['doc_type_normalized'] == 'invoice_receipt']\n",
    "    bank_df = df[df['doc_type_normalized'] == 'bank_statement']\n",
    "\n",
    "    print(f\"  Invoice/Receipt images: {len(inv_rec_df)}\")\n",
    "    print(f\"  Bank statement images: {len(bank_df)}\")\n",
    "\n",
    "    # Merge with ground truth if available\n",
    "    has_gt = False\n",
    "    if ground_truth is not None and 'image_stem' in df.columns:\n",
    "        df = df.merge(ground_truth, on='image_stem', how='left', suffixes=('', '_gt'))\n",
    "        has_gt = True\n",
    "        matched = df['image_file_gt'].notna().sum()\n",
    "        print(f\"  Matched with ground truth: {matched}/{len(df)}\")\n",
    "    else:\n",
    "        print(\"  No ground truth - using extraction rate as proxy\")\n",
    "\n",
    "    # Re-split after merge\n",
    "    inv_rec_df = df[df['doc_type_normalized'] == 'invoice_receipt']\n",
    "    bank_df = df[df['doc_type_normalized'] == 'bank_statement']\n",
    "\n",
    "    def compute_f1_for_field(subset_df, field):\n",
    "        \"\"\"Compute F1 using calculate_field_accuracy_f1 for consistency.\"\"\"\n",
    "        if not has_gt or f'{field}_gt' not in subset_df.columns:\n",
    "            return np.nan, np.nan, np.nan\n",
    "\n",
    "        f1_scores = []\n",
    "        precision_scores = []\n",
    "        recall_scores = []\n",
    "\n",
    "        for _, row in subset_df.iterrows():\n",
    "            if pd.notna(row.get(f'{field}_gt')):\n",
    "                ext = str(row[field]) if pd.notna(row[field]) else 'NOT_FOUND'\n",
    "                gt = str(row[f'{field}_gt']) if pd.notna(row[f'{field}_gt']) else 'NOT_FOUND'\n",
    "\n",
    "                # Use calculate_field_accuracy_f1 for consistent F1 calculation\n",
    "                metrics = calculate_field_accuracy_f1(ext, gt, field, debug=False)\n",
    "\n",
    "                f1_scores.append(metrics['f1_score'])\n",
    "                precision_scores.append(metrics['precision'])\n",
    "                recall_scores.append(metrics['recall'])\n",
    "\n",
    "        if f1_scores:\n",
    "            return np.mean(f1_scores), np.mean(precision_scores), np.mean(recall_scores)\n",
    "        return np.nan, np.nan, np.nan\n",
    "\n",
    "    # Process Invoice & Receipt fields\n",
    "    for field in INVOICE_RECEIPT_FIELDS:\n",
    "        if field not in df.columns:\n",
    "            continue\n",
    "        if len(inv_rec_df) == 0:\n",
    "            continue\n",
    "\n",
    "        valid = inv_rec_df[field].notna() & (inv_rec_df[field] != 'NOT_FOUND') & (inv_rec_df[field] != '')\n",
    "        coverage = valid.mean() if len(inv_rec_df) > 0 else np.nan\n",
    "\n",
    "        f1, precision, recall = compute_f1_for_field(inv_rec_df, field)\n",
    "\n",
    "        results.append({\n",
    "            'Subset': 'Invoices & receipts',\n",
    "            'Field': field,\n",
    "            'Accuracy': coverage,\n",
    "            'F1': f1,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'Exclusive': field in EXCLUSIVE_FIELDS,\n",
    "            'extracted_count': int(valid.sum()),\n",
    "            'total_count': len(inv_rec_df),\n",
    "        })\n",
    "\n",
    "    # Process Bank Statement fields\n",
    "    for field in BANK_STATEMENT_FIELDS:\n",
    "        if field not in df.columns:\n",
    "            continue\n",
    "        if len(bank_df) == 0:\n",
    "            continue\n",
    "\n",
    "        valid = bank_df[field].notna() & (bank_df[field] != 'NOT_FOUND') & (bank_df[field] != '')\n",
    "        coverage = valid.mean() if len(bank_df) > 0 else np.nan\n",
    "\n",
    "        f1, precision, recall = compute_f1_for_field(bank_df, field)\n",
    "\n",
    "        results.append({\n",
    "            'Subset': 'Bank statements',\n",
    "            'Field': field,\n",
    "            'Accuracy': coverage,\n",
    "            'F1': f1,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'Exclusive': field in EXCLUSIVE_FIELDS,\n",
    "            'extracted_count': int(valid.sum()),\n",
    "            'total_count': len(bank_df),\n",
    "        })\n",
    "\n",
    "    result_df = pd.DataFrame(results)\n",
    "\n",
    "    # Summary\n",
    "    exclusive_count = result_df['Exclusive'].sum()\n",
    "    comparable_count = len(result_df) - exclusive_count\n",
    "\n",
    "    print(f\"\\n  F1 Score (using calculate_field_accuracy_f1): {result_df['F1'].mean():.1%}\")\n",
    "    print(f\"  Precision: {result_df['Precision'].mean():.1%}\")\n",
    "    print(f\"  Recall: {result_df['Recall'].mean():.1%}\")\n",
    "    print(f\"\\n  Comparable fields: {comparable_count}\")\n",
    "    print(f\"  Exclusive fields: {exclusive_count}\")\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most recent batch results by model:\n",
      "============================================================\n",
      "Loaded: llama_batch_results_20251210_003155.csv\n",
      "  Images: 9\n",
      "  Document types: {'bank_statement': 3, 'receipt': 3, 'invoice': 3}\n",
      "  Mean overall accuracy: 9764.3%\n",
      "\n",
      "Using generic internvl3_batch_results as InternVL3-8B:\n",
      "Loaded: internvl3_batch_results_20251210_005902.csv\n",
      "  Images: 9\n",
      "  Document types: {'bank_statement': 3, 'receipt': 3, 'invoice': 3}\n",
      "  Mean overall accuracy: 8485.7%\n",
      "\n",
      "Loaded: internvl3_2b_batch_results_20251210_013149.csv\n",
      "  Images: 9\n",
      "  Document types: {'bank_statement': 3, 'receipt': 3, 'invoice': 3}\n",
      "  Mean overall accuracy: 7136.5%\n",
      "\n",
      "\n",
      "Models loaded: ['Llama-11B', 'InternVL3-8B', 'InternVL3-2B']\n"
     ]
    }
   ],
   "source": [
    "# Example: Load most recent batch results for each model\n",
    "print(\"Most recent batch results by model:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "batch_dfs = {}\n",
    "\n",
    "if result_files['llama_batch']:\n",
    "    latest = result_files['llama_batch'][0]\n",
    "    batch_dfs['Llama-11B'] = load_batch_results(latest)\n",
    "    print()\n",
    "\n",
    "# InternVL3-8B: Check both explicit 8B files and generic internvl3 files\n",
    "if result_files['internvl3_8b_batch']:\n",
    "    latest = result_files['internvl3_8b_batch'][0]\n",
    "    batch_dfs['InternVL3-8B'] = load_batch_results(latest)\n",
    "    print()\n",
    "elif result_files['internvl3_batch']:\n",
    "    # Use generic internvl3_batch_results as 8B fallback\n",
    "    latest = result_files['internvl3_batch'][0]\n",
    "    print(\"Using generic internvl3_batch_results as InternVL3-8B:\")\n",
    "    batch_dfs['InternVL3-8B'] = load_batch_results(latest)\n",
    "    print()\n",
    "\n",
    "if result_files['internvl3_2b_batch']:\n",
    "    latest = result_files['internvl3_2b_batch'][0]\n",
    "    batch_dfs['InternVL3-2B'] = load_batch_results(latest)\n",
    "    print()\n",
    "\n",
    "print(f\"\\nModels loaded: {list(batch_dfs.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Select Model and Export\n",
    "\n",
    "Choose which model's accuracy to export as `current_model_accuracy.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION - Select model and data source\n",
    "# =============================================================================\n",
    "\n",
    "# Choose model to export\n",
    "SELECTED_MODEL = \"Llama-11B\"  # Options: \"Llama-11B\", \"InternVL3-8B\", \"InternVL3-2B\"\n",
    "\n",
    "# Choose data source\n",
    "USE_PER_FIELD_METRICS = False  # True = use per_field_metrics.csv (recommended)\n",
    "                               # False = aggregate from batch results (coverage only)\n",
    "\n",
    "# Output filename\n",
    "OUTPUT_FILENAME = \"current_model_accuracy.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregating from batch results for Llama-11B\n",
      "Computing F1 against ground truth...\n",
      "  Invoice/Receipt images: 6\n",
      "  Bank statement images: 3\n",
      "  Matched with ground truth: 18/18\n",
      "\n",
      "  F1 Scores Summary:\n",
      "    F1_Smart (field-appropriate): 96.0%  \u2190 RECOMMENDED\n",
      "    ----------------------------------------\n",
      "    Binary (exact full match):    90.7%\n",
      "    Position-Aware (positional):  96.1%\n",
      "    Position-Agnostic (set):      96.0%\n",
      "    Fuzzy Agnostic (lenient):     97.6%\n",
      "\n",
      "  Smart F1 Method Selection:\n",
      "    binary: DOCUMENT_TYPE, BUSINESS_ABN, IS_GST_INCLUDED, GST_AMOUNT, TOTAL_AMOUNT\n",
      "    aware: LINE_ITEM_QUANTITIES, LINE_ITEM_PRICES, LINE_ITEM_TOTAL_PRICES, TRANSACTION_AMOUNTS_PAID\n",
      "    agnostic: LINE_ITEM_DESCRIPTIONS, LINE_ITEM_DESCRIPTIONS, TRANSACTION_DATES\n",
      "    fuzzy: SUPPLIER_NAME, BUSINESS_ADDRESS, PAYER_NAME, PAYER_ADDRESS, INVOICE_DATE, STATEMENT_DATE_RANGE\n",
      "\n",
      "  Comparable fields: 17\n",
      "  Exclusive fields: 1\n",
      "\n",
      "Saved: ../output/current_model_accuracy.csv\n",
      "Fields: 18\n",
      "Mean accuracy: 100.0%\n",
      "\n",
      "F1 Scores:\n",
      "  F1_Smart (field-appropriate): 96.0%  \u2190 RECOMMENDED\n",
      "  ----------------------------------------\n",
      "  Binary (exact full match):    90.7%\n",
      "  Position-Aware (positional):  96.1%\n",
      "  Position-Agnostic (set):      96.0%\n",
      "  Fuzzy Agnostic (lenient):     97.6%\n",
      "\n",
      "============================================================\n",
      "Exported Data:\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subset</th>\n",
       "      <th>Field</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1_Binary</th>\n",
       "      <th>F1_Aware</th>\n",
       "      <th>F1_Agnostic</th>\n",
       "      <th>F1_Fuzzy</th>\n",
       "      <th>F1_Smart</th>\n",
       "      <th>Smart_Method</th>\n",
       "      <th>Exclusive</th>\n",
       "      <th>extracted_count</th>\n",
       "      <th>total_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Invoices &amp; receipts</td>\n",
       "      <td>DOCUMENT_TYPE</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>binary</td>\n",
       "      <td>True</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Invoices &amp; receipts</td>\n",
       "      <td>BUSINESS_ABN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>binary</td>\n",
       "      <td>False</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Invoices &amp; receipts</td>\n",
       "      <td>SUPPLIER_NAME</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>fuzzy</td>\n",
       "      <td>False</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Invoices &amp; receipts</td>\n",
       "      <td>BUSINESS_ADDRESS</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>fuzzy</td>\n",
       "      <td>False</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Invoices &amp; receipts</td>\n",
       "      <td>PAYER_NAME</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>fuzzy</td>\n",
       "      <td>False</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Invoices &amp; receipts</td>\n",
       "      <td>PAYER_ADDRESS</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>fuzzy</td>\n",
       "      <td>False</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Invoices &amp; receipts</td>\n",
       "      <td>INVOICE_DATE</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>fuzzy</td>\n",
       "      <td>False</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Invoices &amp; receipts</td>\n",
       "      <td>LINE_ITEM_DESCRIPTIONS</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>agnostic</td>\n",
       "      <td>False</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Invoices &amp; receipts</td>\n",
       "      <td>LINE_ITEM_QUANTITIES</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>aware</td>\n",
       "      <td>False</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Invoices &amp; receipts</td>\n",
       "      <td>LINE_ITEM_PRICES</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>aware</td>\n",
       "      <td>False</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Invoices &amp; receipts</td>\n",
       "      <td>LINE_ITEM_TOTAL_PRICES</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>aware</td>\n",
       "      <td>False</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Invoices &amp; receipts</td>\n",
       "      <td>IS_GST_INCLUDED</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>binary</td>\n",
       "      <td>False</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Invoices &amp; receipts</td>\n",
       "      <td>GST_AMOUNT</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>binary</td>\n",
       "      <td>False</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Invoices &amp; receipts</td>\n",
       "      <td>TOTAL_AMOUNT</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>binary</td>\n",
       "      <td>False</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Bank statements</td>\n",
       "      <td>STATEMENT_DATE_RANGE</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>fuzzy</td>\n",
       "      <td>False</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Bank statements</td>\n",
       "      <td>LINE_ITEM_DESCRIPTIONS</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.655172</td>\n",
       "      <td>0.653333</td>\n",
       "      <td>0.933082</td>\n",
       "      <td>0.653333</td>\n",
       "      <td>agnostic</td>\n",
       "      <td>False</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Bank statements</td>\n",
       "      <td>TRANSACTION_DATES</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.988506</td>\n",
       "      <td>0.962963</td>\n",
       "      <td>0.988506</td>\n",
       "      <td>0.962963</td>\n",
       "      <td>agnostic</td>\n",
       "      <td>False</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Bank statements</td>\n",
       "      <td>TRANSACTION_AMOUNTS_PAID</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.988506</td>\n",
       "      <td>0.988506</td>\n",
       "      <td>0.988506</td>\n",
       "      <td>0.988506</td>\n",
       "      <td>aware</td>\n",
       "      <td>False</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Subset                     Field  Accuracy  F1_Binary  \\\n",
       "0   Invoices & receipts             DOCUMENT_TYPE       1.0   1.000000   \n",
       "1   Invoices & receipts              BUSINESS_ABN       1.0   1.000000   \n",
       "2   Invoices & receipts             SUPPLIER_NAME       1.0   1.000000   \n",
       "3   Invoices & receipts          BUSINESS_ADDRESS       1.0   0.833333   \n",
       "4   Invoices & receipts                PAYER_NAME       1.0   1.000000   \n",
       "5   Invoices & receipts             PAYER_ADDRESS       1.0   1.000000   \n",
       "6   Invoices & receipts              INVOICE_DATE       1.0   0.833333   \n",
       "7   Invoices & receipts    LINE_ITEM_DESCRIPTIONS       1.0   1.000000   \n",
       "8   Invoices & receipts      LINE_ITEM_QUANTITIES       1.0   1.000000   \n",
       "9   Invoices & receipts          LINE_ITEM_PRICES       1.0   1.000000   \n",
       "10  Invoices & receipts    LINE_ITEM_TOTAL_PRICES       1.0   1.000000   \n",
       "11  Invoices & receipts           IS_GST_INCLUDED       1.0   1.000000   \n",
       "12  Invoices & receipts                GST_AMOUNT       1.0   1.000000   \n",
       "13  Invoices & receipts              TOTAL_AMOUNT       1.0   1.000000   \n",
       "14      Bank statements      STATEMENT_DATE_RANGE       1.0   1.000000   \n",
       "15      Bank statements    LINE_ITEM_DESCRIPTIONS       1.0   0.333333   \n",
       "16      Bank statements         TRANSACTION_DATES       1.0   0.666667   \n",
       "17      Bank statements  TRANSACTION_AMOUNTS_PAID       1.0   0.666667   \n",
       "\n",
       "    F1_Aware  F1_Agnostic  F1_Fuzzy  F1_Smart Smart_Method  Exclusive  \\\n",
       "0   1.000000     1.000000  1.000000  1.000000       binary       True   \n",
       "1   1.000000     1.000000  1.000000  1.000000       binary      False   \n",
       "2   1.000000     1.000000  1.000000  1.000000        fuzzy      False   \n",
       "3   0.833333     0.833333  0.833333  0.833333        fuzzy      False   \n",
       "4   1.000000     1.000000  1.000000  1.000000        fuzzy      False   \n",
       "5   1.000000     1.000000  1.000000  1.000000        fuzzy      False   \n",
       "6   0.833333     0.833333  0.833333  0.833333        fuzzy      False   \n",
       "7   1.000000     1.000000  1.000000  1.000000     agnostic      False   \n",
       "8   1.000000     1.000000  1.000000  1.000000        aware      False   \n",
       "9   1.000000     1.000000  1.000000  1.000000        aware      False   \n",
       "10  1.000000     1.000000  1.000000  1.000000        aware      False   \n",
       "11  1.000000     1.000000  1.000000  1.000000       binary      False   \n",
       "12  1.000000     1.000000  1.000000  1.000000       binary      False   \n",
       "13  1.000000     1.000000  1.000000  1.000000       binary      False   \n",
       "14  1.000000     1.000000  1.000000  1.000000        fuzzy      False   \n",
       "15  0.655172     0.653333  0.933082  0.653333     agnostic      False   \n",
       "16  0.988506     0.962963  0.988506  0.962963     agnostic      False   \n",
       "17  0.988506     0.988506  0.988506  0.988506        aware      False   \n",
       "\n",
       "    extracted_count  total_count  \n",
       "0                12           12  \n",
       "1                12           12  \n",
       "2                12           12  \n",
       "3                12           12  \n",
       "4                12           12  \n",
       "5                12           12  \n",
       "6                12           12  \n",
       "7                12           12  \n",
       "8                12           12  \n",
       "9                12           12  \n",
       "10               12           12  \n",
       "11               12           12  \n",
       "12               12           12  \n",
       "13               12           12  \n",
       "14                6            6  \n",
       "15                6            6  \n",
       "16                6            6  \n",
       "17                6            6  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def export_model_accuracy(model_name: str, use_per_field: bool,\n",
    "                          per_field_df: pd.DataFrame, batch_dfs: dict,\n",
    "                          ground_truth_df: pd.DataFrame,\n",
    "                          output_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Export field-level accuracy and F1 for selected model.\"\"\"\n",
    "\n",
    "    if use_per_field and per_field_df is not None:\n",
    "        print(f\"Using per_field_metrics.csv for {model_name}\")\n",
    "        result_df = extract_model_accuracy_from_per_field(per_field_df, model_name)\n",
    "        if result_df is None:\n",
    "            return None\n",
    "    elif model_name in batch_dfs:\n",
    "        print(f\"Aggregating from batch results for {model_name}\")\n",
    "        if ground_truth_df is not None:\n",
    "            print(\"Computing F1 against ground truth (using calculate_field_accuracy_f1)...\")\n",
    "        result_df = compute_field_accuracy_from_batch(batch_dfs[model_name], ground_truth_df)\n",
    "    else:\n",
    "        print(f\"No data available for {model_name}\")\n",
    "        return None\n",
    "\n",
    "    # Determine which columns to save\n",
    "    save_cols = ['Subset', 'Field', 'Accuracy']\n",
    "\n",
    "    # Add F1/Precision/Recall if available\n",
    "    for col in ['F1', 'Precision', 'Recall', 'F1_Agnostic']:\n",
    "        if col in result_df.columns and result_df[col].notna().any():\n",
    "            save_cols.append(col)\n",
    "\n",
    "    if 'Exclusive' in result_df.columns:\n",
    "        save_cols.append('Exclusive')\n",
    "\n",
    "    result_df[save_cols].to_csv(output_path, index=False)\n",
    "    print(f\"\\nSaved: {output_path}\")\n",
    "    print(f\"Fields: {len(result_df)}\")\n",
    "    print(f\"Mean accuracy: {result_df['Accuracy'].mean():.1%}\")\n",
    "\n",
    "    # Print F1 summary\n",
    "    if 'F1' in save_cols:\n",
    "        print(f\"Mean F1: {result_df['F1'].mean():.1%}\")\n",
    "    if 'Precision' in save_cols:\n",
    "        print(f\"Mean Precision: {result_df['Precision'].mean():.1%}\")\n",
    "    if 'Recall' in save_cols:\n",
    "        print(f\"Mean Recall: {result_df['Recall'].mean():.1%}\")\n",
    "\n",
    "    return result_df\n",
    "\n",
    "# Export\n",
    "output_path = OUTPUT_DIR / OUTPUT_FILENAME\n",
    "exported_df = export_model_accuracy(\n",
    "    SELECTED_MODEL,\n",
    "    USE_PER_FIELD_METRICS,\n",
    "    per_field_df,\n",
    "    batch_dfs,\n",
    "    ground_truth_df,\n",
    "    output_path\n",
    ")\n",
    "\n",
    "if exported_df is not None:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Exported Data:\")\n",
    "    print(\"=\" * 60)\n",
    "    display(exported_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Compare Multiple Models (Optional)\n",
    "\n",
    "Quick comparison of all available models from per_field_metrics.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if per_field_df is not None:\n",
    "    # Pivot to compare models\n",
    "    comparison = per_field_df.pivot(index='field', columns='model', values='accuracy')\n",
    "    comparison['best_model'] = comparison.idxmax(axis=1)\n",
    "    \n",
    "    print(\"\\nField-Level Accuracy Comparison (from per_field_metrics.csv)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Format as percentages\n",
    "    display_df = comparison.copy()\n",
    "    for col in display_df.columns:\n",
    "        if col != 'best_model':\n",
    "            display_df[col] = display_df[col].apply(lambda x: f\"{x:.1%}\" if pd.notna(x) else \"N/A\")\n",
    "    \n",
    "    display(display_df)\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\nModel Summary:\")\n",
    "    for model in per_field_df['model'].unique():\n",
    "        model_data = per_field_df[per_field_df['model'] == model]\n",
    "        wins = (comparison['best_model'] == model).sum()\n",
    "        print(f\"  {model}: mean={model_data['accuracy'].mean():.1%}, best on {wins} fields\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Export All Models for Comparison\n",
    "\n",
    "Export accuracy files for all available models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting all models...\n",
      "============================================================\n",
      "No per_field_metrics.csv available\n"
     ]
    }
   ],
   "source": [
    "def export_all_models(per_field_df: pd.DataFrame, output_dir: Path):\n",
    "    \"\"\"Export accuracy CSV for each model in per_field_metrics.csv\"\"\"\n",
    "    if per_field_df is None:\n",
    "        print(\"No per_field_metrics.csv available\")\n",
    "        return\n",
    "    \n",
    "    exported = []\n",
    "    for model in per_field_df['model'].unique():\n",
    "        # Create safe filename\n",
    "        safe_name = model.lower().replace('-', '_').replace(' ', '_')\n",
    "        filename = f\"{safe_name}_accuracy.csv\"\n",
    "        output_path = output_dir / filename\n",
    "        \n",
    "        result_df = extract_model_accuracy_from_per_field(per_field_df, model)\n",
    "        if result_df is not None:\n",
    "            result_df.to_csv(output_path, index=False)\n",
    "            exported.append((model, output_path, result_df['Accuracy'].mean()))\n",
    "            print(f\"Exported: {filename} (mean accuracy: {result_df['Accuracy'].mean():.1%})\")\n",
    "    \n",
    "    return exported\n",
    "\n",
    "print(\"Exporting all models...\")\n",
    "print(\"=\" * 60)\n",
    "all_exports = export_all_models(per_field_df, OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "Files created:\n",
    "- `current_model_accuracy.csv` - Selected model for comparison notebook\n",
    "- `{model}_accuracy.csv` - Individual files for each model\n",
    "\n",
    "To use in comparison notebook:\n",
    "```python\n",
    "CURRENT_MODEL_CSV = Path(\"../output/current_model_accuracy.csv\")\n",
    "# Or use specific model file:\n",
    "CURRENT_MODEL_CSV = Path(\"../output/internvl3_8b_accuracy.csv\")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "du",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}