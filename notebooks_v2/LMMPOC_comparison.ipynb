{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "project-root-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 0 - Project Root Detection\n",
    "# Allows running from subdirectories like notebooks_v2/\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "def find_project_root(marker_files=('common/__init__.py', 'prompts')):\n",
    "    \"\"\"Find project root by looking for marker files, searching up from cwd.\"\"\"\n",
    "    current = Path().absolute()\n",
    "    for parent in [current] + list(current.parents):\n",
    "        for marker in marker_files:\n",
    "            if (parent / marker).exists():\n",
    "                return parent\n",
    "    return current  # Fallback to cwd if no marker found\n",
    "\n",
    "# Detect project root (works from any subdirectory)\n",
    "PROJECT_ROOT = find_project_root()\n",
    "print(f\"\ud83d\udcc2 Current directory: {Path().absolute()}\")\n",
    "print(f\"\ud83c\udfe0 Project root: {PROJECT_ROOT}\")\n",
    "\n",
    "# Ensure the project root is in the Python path\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "    print(f\"\u2705 Added {PROJECT_ROOT} to sys.path\")\n",
    "\n",
    "print(\"\u2705 Project root detection complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison V2: Llama vs InternVL3 vs InternVL3.5\n",
    "\n",
    "**V2 Batch Processing Performance Analysis - Sophisticated Bank Statement Extraction**\n",
    "\n",
    "---\n",
    "\n",
    "This notebook provides a comprehensive comparison of three vision-language models using the V2 batch processing notebooks with sophisticated multi-turn bank statement extraction:\n",
    "\n",
    "| Model | Notebook | Key Features |\n",
    "|-------|----------|--------------|\n",
    "| **Llama-3.2-Vision-11B** | `llama_batch_v2.ipynb` | Multi-turn bank extraction, bfloat16 |\n",
    "| **InternVL3.5-8B** | `ivl3_5_8b_batch_v2.ipynb` | Multi-turn bank extraction, bfloat16, Cascade RL |\n",
    "| **InternVL3-2B** | `ivl3_2b_batch_v2.ipynb` | Multi-turn bank extraction, bfloat16 |\n",
    "\n",
    "## V2 Features:\n",
    "- **Sophisticated bank statement extraction** using UnifiedBankExtractor\n",
    "- Turn 0: Header detection (identifies actual column names)\n",
    "- Turn 1: Adaptive extraction with structure-dynamic prompts\n",
    "- Automatic strategy selection: BALANCE_DESCRIPTION, AMOUNT_DESCRIPTION, etc.\n",
    "- Optional balance-based mathematical correction\n",
    "\n",
    "## Key Business Questions Addressed:\n",
    "1. **Accuracy**: Which model extracts information more reliably?\n",
    "2. **Bank Statement Performance**: How does V2 multi-turn extraction improve bank statement accuracy?\n",
    "3. **Speed**: Which model processes documents faster?\n",
    "4. **Document Type Performance**: How do models perform on invoices, receipts, and bank statements?\n",
    "5. **Production Readiness**: Which model is recommended for deployment?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2\n",
    "\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "from rich import print as rprint\n",
    "from rich.console import Console\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "console = Console()\n",
    "\n",
    "# Set professional styling for executive presentation\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION - Edit these values to configure the comparison\n",
    "# ============================================================================\n",
    "\n",
    "# Base path for data files\n",
    "# base_data_path = '/home/jovyan/nfs_share/tod/LMM_POC'\n",
    "base_data_path = '/Users/tod/Desktop/LMM_POC'\n",
    "\n",
    "# =============================================================================\n",
    "# EVALUATION MODE SWITCH\n",
    "# =============================================================================\n",
    "# When True: Correctly identifying NOT_FOUND is rewarded (F1=1.0)\n",
    "#            - All document\u00d7field pairs evaluated\n",
    "#            - Better reflects full extraction task (prompts request NOT_FOUND)\n",
    "# When False: NOT_FOUND matches are skipped (not counted)\n",
    "#            - Only evaluates fields with actual values\n",
    "#            - Current/legacy behavior\n",
    "REWARD_NOT_FOUND_MATCHES = True  # Toggle to compare evaluation modes\n",
    "\n",
    "CONFIG = {\n",
    "    # Path settings\n",
    "    'output_dir': f'{base_data_path}/output/csv',\n",
    "    'ground_truth_path': f'{base_data_path}/evaluation_data/synthetic/ground_truth_synthetic.csv',\n",
    "    \n",
    "    # ============================================================================\n",
    "    # MODEL CONFIGURATIONS - Explicit file paths and display names\n",
    "    # ============================================================================\n",
    "    # Edit these to specify exact files and customize display names\n",
    "    # Set 'file' to None to skip a model\n",
    "    # ============================================================================\n",
    "    'models': {\n",
    "        'llama': {\n",
    "            'file': 'llama_batch_results_20251210_003155.csv',\n",
    "            'display_name': 'Llama-11B',\n",
    "            'color': '#3498db'  # Blue\n",
    "        },\n",
    "        'internvl3_8b': {\n",
    "            'file': 'internvl3_batch_results_20251210_005902.csv',\n",
    "            'display_name': 'IVL3.5-8B',\n",
    "            'color': '#e74c3c'  # Red\n",
    "        },\n",
    "        'internvl3_2b': {\n",
    "            'file': 'internvl3_2b_batch_results_20251210_013149.csv',\n",
    "            'display_name': 'IVL3-2B',\n",
    "            'color': '#2ecc71'  # Green\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    # Visualization settings\n",
    "    'figure_size': (16, 10),\n",
    "    'dpi': 600,\n",
    "    'save_format': 'png'\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# Display configuration summary\n",
    "# ============================================================================\n",
    "print(\"\u2705 Model comparison V2 configuration loaded\")\n",
    "print(f\"\ud83d\udcc1 Output directory: {CONFIG['output_dir']}\")\n",
    "print(f\"\ud83d\udcca Ground truth: {CONFIG['ground_truth_path']}\")\n",
    "print(f\"\\n\ud83d\udd27 Model configurations:\")\n",
    "for key, model_cfg in CONFIG['models'].items():\n",
    "    if model_cfg['file']:\n",
    "        print(f\"   \u2022 {model_cfg['display_name']}: {model_cfg['file']}\")\n",
    "    else:\n",
    "        print(f\"   \u2022 {key}: (skipped - file is None)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4\n",
    "\n",
    "def load_model_results(output_dir: str, filename: str, display_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load results from an explicit CSV file path.\n",
    "    \n",
    "    Args:\n",
    "        output_dir: Directory containing the file\n",
    "        filename: Exact filename to load\n",
    "        display_name: Display name for the model (used in 'model' column)\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with results, or empty DataFrame if file not found\n",
    "    \"\"\"\n",
    "    from pathlib import Path\n",
    "    \n",
    "    file_path = Path(output_dir) / filename\n",
    "    \n",
    "    if not file_path.exists():\n",
    "        rprint(f\"[red]\u274c File not found: {file_path}[/red]\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    rprint(f\"[green]\u2705 Loading {display_name} from: {filename}[/green]\")\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Add model column with display name\n",
    "        df['model'] = display_name\n",
    "        \n",
    "        # Ensure required columns exist\n",
    "        required_cols = ['overall_accuracy', 'processing_time', 'document_type']\n",
    "        missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "        \n",
    "        if missing_cols:\n",
    "            rprint(f\"[yellow]\u26a0\ufe0f Missing columns in {display_name} data: {missing_cols}[/yellow]\")\n",
    "            for col in missing_cols:\n",
    "                if col == 'overall_accuracy':\n",
    "                    df[col] = 0.0\n",
    "                elif col == 'processing_time':\n",
    "                    df[col] = 1.0\n",
    "                elif col == 'document_type':\n",
    "                    df[col] = 'unknown'\n",
    "        \n",
    "        rprint(f\"[dim]   Loaded {len(df)} records[/dim]\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        rprint(f\"[red]\u274c Error loading {display_name}: {e}[/red]\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# ============================================================================\n",
    "# Load all configured models\n",
    "# ============================================================================\n",
    "rprint(\"[bold blue]\ud83d\udcca Loading Model Results[/bold blue]\\n\")\n",
    "\n",
    "model_dataframes = {}\n",
    "for model_key, model_cfg in CONFIG['models'].items():\n",
    "    if model_cfg['file']:\n",
    "        df = load_model_results(\n",
    "            CONFIG['output_dir'],\n",
    "            model_cfg['file'],\n",
    "            model_cfg['display_name']\n",
    "        )\n",
    "        if not df.empty:\n",
    "            model_dataframes[model_key] = df\n",
    "\n",
    "# Create individual named dataframes for backward compatibility\n",
    "llama_batch_df = model_dataframes.get('llama', pd.DataFrame())\n",
    "internvl_batch_df = model_dataframes.get('internvl3_8b', pd.DataFrame())\n",
    "internvl_nq_batch_df = model_dataframes.get('internvl3_2b', pd.DataFrame())\n",
    "\n",
    "# Combine all loaded models\n",
    "if model_dataframes:\n",
    "    combined_df = pd.concat(model_dataframes.values(), ignore_index=True)\n",
    "    print(f\"\\n\u2705 Combined {len(combined_df)} total records from {len(model_dataframes)} models\")\n",
    "else:\n",
    "    combined_df = pd.DataFrame()\n",
    "    rprint(\"[red]\u274c No model data loaded![/red]\")\n",
    "\n",
    "# Build color map from config\n",
    "MODEL_COLORS = {\n",
    "    cfg['display_name']: cfg['color'] \n",
    "    for cfg in CONFIG['models'].values() \n",
    "    if cfg['file']\n",
    "}\n",
    "\n",
    "# Backward compatibility aliases (some cells use these names)\n",
    "llama_df = llama_batch_df\n",
    "internvl3_8b_df = internvl_batch_df\n",
    "internvl3_2b_df = internvl_nq_batch_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5\n",
    "\n",
    "# Display loaded data summary (data was loaded in previous cell)\n",
    "rprint(\"[bold blue]\ud83d\udcca V2 Model Performance Data Summary[/bold blue]\")\n",
    "rprint(\"[cyan]V2 notebooks use sophisticated multi-turn bank statement extraction[/cyan]\\n\")\n",
    "\n",
    "if not combined_df.empty:\n",
    "    # Show what models were loaded\n",
    "    loaded_models = combined_df['model'].unique()\n",
    "    rprint(f\"[green]\u2705 {len(loaded_models)} models loaded with {len(combined_df)} total records[/green]\")\n",
    "    \n",
    "    # Show record counts per model\n",
    "    for model in loaded_models:\n",
    "        count = len(combined_df[combined_df['model'] == model])\n",
    "        rprint(f\"   \u2022 {model}: {count} records\")\n",
    "    \n",
    "    # Show column summary\n",
    "    rprint(f\"\\n[dim]Columns: {', '.join(combined_df.columns[:10])}{'...' if len(combined_df.columns) > 10 else ''}[/dim]\")\n",
    "else:\n",
    "    rprint(\"[red]\u274c No data loaded[/red]\")\n",
    "    rprint(\"\\n[yellow]\ud83d\udca1 Check CONFIG['models'] in the configuration cell above[/yellow]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Compute Per-Model Mean F1 (Position-Aware)\n",
    "# This cell computes Mean F1 for each model BEFORE the dashboards\n",
    "# so that dashboards can display F1 instead of document-level accuracy.\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path for imports\n",
    "PROJECT_ROOT = Path(CONFIG['output_dir']).parent.parent\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from common.evaluation_metrics import calculate_field_accuracy_f1\n",
    "\n",
    "# Define field columns for F1 computation\n",
    "FIELD_COLUMNS = [\n",
    "    'DOCUMENT_TYPE', 'BUSINESS_ABN', 'SUPPLIER_NAME', 'BUSINESS_ADDRESS',\n",
    "    'PAYER_NAME', 'PAYER_ADDRESS', 'INVOICE_DATE', 'LINE_ITEM_DESCRIPTIONS',\n",
    "    'LINE_ITEM_QUANTITIES', 'LINE_ITEM_PRICES', 'LINE_ITEM_TOTAL_PRICES',\n",
    "    'IS_GST_INCLUDED', 'GST_AMOUNT', 'TOTAL_AMOUNT', 'STATEMENT_DATE_RANGE',\n",
    "    'TRANSACTION_DATES', 'TRANSACTION_AMOUNTS_PAID'\n",
    "]\n",
    "\n",
    "# Load ground truth for F1 computation\n",
    "ground_truth_path = Path(CONFIG['ground_truth_path'])\n",
    "if ground_truth_path.exists():\n",
    "    ground_truth = pd.read_csv(ground_truth_path, dtype=str)\n",
    "    # Normalize column names\n",
    "    if 'image_name' in ground_truth.columns and 'image_file' not in ground_truth.columns:\n",
    "        ground_truth['image_file'] = ground_truth['image_name']\n",
    "    ground_truth['image_stem'] = ground_truth['image_file'].apply(lambda x: Path(x).stem)\n",
    "    rprint(f\"[green]\u2705 Ground truth loaded: {len(ground_truth)} rows[/green]\")\n",
    "else:\n",
    "    ground_truth = pd.DataFrame()\n",
    "    rprint(f\"[red]\u274c Ground truth not found: {ground_truth_path}[/red]\")\n",
    "\n",
    "\n",
    "def compute_model_mean_f1(batch_df: pd.DataFrame, gt_df: pd.DataFrame, model_name: str) -> float:\n",
    "    \"\"\"\n",
    "    Compute Mean F1 across all fields for a model using position-aware matching.\n",
    "    \n",
    "    This is the SAME calculation used in model_accuracy_comparison.ipynb\n",
    "    for comparing against LayoutLM.\n",
    "    \n",
    "    Returns:\n",
    "        Mean F1 score (0-1 scale)\n",
    "    \"\"\"\n",
    "    if batch_df.empty or gt_df.empty:\n",
    "        return 0.0\n",
    "    \n",
    "    # Add image_stem for matching\n",
    "    if 'image_stem' not in batch_df.columns:\n",
    "        batch_df = batch_df.copy()\n",
    "        batch_df['image_stem'] = batch_df['image_file'].apply(lambda x: Path(x).stem)\n",
    "    \n",
    "    field_f1_scores = []\n",
    "    \n",
    "    for field in FIELD_COLUMNS:\n",
    "        if field not in batch_df.columns or field not in gt_df.columns:\n",
    "            continue\n",
    "        \n",
    "        # Merge on image_stem\n",
    "        merged = batch_df.merge(\n",
    "            gt_df[['image_stem', field]],\n",
    "            on='image_stem',\n",
    "            how='inner',\n",
    "            suffixes=('_pred', '_true')\n",
    "        )\n",
    "        \n",
    "        if len(merged) == 0:\n",
    "            continue\n",
    "        \n",
    "        pred_col = f'{field}_pred' if f'{field}_pred' in merged.columns else field\n",
    "        true_col = f'{field}_true' if f'{field}_true' in merged.columns else field\n",
    "        \n",
    "        # Compute F1 for each document, then average\n",
    "        doc_f1_scores = []\n",
    "        for _, row in merged.iterrows():\n",
    "            pred_val = str(row[pred_col]) if pd.notna(row[pred_col]) else 'NOT_FOUND'\n",
    "            true_val = str(row[true_col]) if pd.notna(row[true_col]) else 'NOT_FOUND'\n",
    "            metrics = calculate_field_accuracy_f1(pred_val, true_val, field, debug=False)\n",
    "            doc_f1_scores.append(metrics['f1_score'])\n",
    "        \n",
    "        if doc_f1_scores:\n",
    "            field_f1_scores.append(sum(doc_f1_scores) / len(doc_f1_scores))\n",
    "    \n",
    "    if field_f1_scores:\n",
    "        return sum(field_f1_scores) / len(field_f1_scores)\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "# Compute Mean F1 for each model\n",
    "rprint(\"\\n[bold cyan]Computing Mean F1 (Position-Aware) for each model...[/bold cyan]\")\n",
    "\n",
    "model_mean_f1 = {}\n",
    "\n",
    "# Use CONFIG display names as keys so they match the dashboard\n",
    "if not llama_df.empty and not ground_truth.empty:\n",
    "    llama_name = CONFIG['models']['llama']['display_name']\n",
    "    model_mean_f1[llama_name] = compute_model_mean_f1(llama_df, ground_truth, llama_name)\n",
    "    rprint(f\"  {llama_name} Mean F1: {model_mean_f1[llama_name]:.1%}\")\n",
    "\n",
    "if not internvl3_8b_df.empty and not ground_truth.empty:\n",
    "    ivl8b_name = CONFIG['models']['internvl3_8b']['display_name']\n",
    "    model_mean_f1[ivl8b_name] = compute_model_mean_f1(internvl3_8b_df, ground_truth, ivl8b_name)\n",
    "    rprint(f\"  {ivl8b_name} Mean F1: {model_mean_f1[ivl8b_name]:.1%}\")\n",
    "\n",
    "if not internvl3_2b_df.empty and not ground_truth.empty:\n",
    "    ivl2b_name = CONFIG['models']['internvl3_2b']['display_name']\n",
    "    model_mean_f1[ivl2b_name] = compute_model_mean_f1(internvl3_2b_df, ground_truth, ivl2b_name)\n",
    "    rprint(f\"  {ivl2b_name} Mean F1: {model_mean_f1[ivl2b_name]:.1%}\")\n",
    "\n",
    "# Add mean_f1 column to combined_df for dashboard use\n",
    "if not combined_df.empty and model_mean_f1:\n",
    "    combined_df['mean_f1'] = combined_df['model'].map(model_mean_f1)\n",
    "    # Also add per-document F1 (approximation using overall_accuracy scaled by model mean F1 ratio)\n",
    "    # This preserves relative document performance while using F1 scale\n",
    "    for model_name in combined_df['model'].unique():\n",
    "        if model_name in model_mean_f1:\n",
    "            mask = combined_df['model'] == model_name\n",
    "            model_acc_mean = combined_df.loc[mask, 'overall_accuracy'].mean()\n",
    "            if model_acc_mean > 0:\n",
    "                # Scale factor to convert accuracy to F1 scale\n",
    "                scale = model_mean_f1[model_name] / (model_acc_mean / 100.0)\n",
    "                combined_df.loc[mask, 'mean_f1_scaled'] = combined_df.loc[mask, 'overall_accuracy'] / 100.0 * scale\n",
    "    rprint(f\"\\n[green]\u2705 Added mean_f1 column to combined_df[/green]\")\n",
    "\n",
    "rprint(\"\\n[bold green]\u2705 F1 computation complete - dashboards will use Mean F1[/bold green]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6\n",
    "\n",
    "def generate_executive_summary(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate executive summary statistics from combined model data.\n",
    "    \n",
    "    Args:\n",
    "        df: Combined DataFrame with all model results\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with summary statistics per model (including both mean and median)\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    summary_stats = df.groupby('model').agg({\n",
    "        'overall_accuracy': ['mean', 'median', 'std', 'min', 'max'],\n",
    "        'processing_time': ['mean', 'median', 'std', 'min', 'max'],\n",
    "        'fields_extracted': 'mean',\n",
    "        'fields_matched': 'mean'\n",
    "    }).round(2)\n",
    "    \n",
    "    # Flatten column names\n",
    "    summary_stats.columns = ['_'.join(col).strip() for col in summary_stats.columns.values]\n",
    "    \n",
    "    # Calculate throughput (documents per minute) using median time\n",
    "    summary_stats['throughput_docs_per_min'] = (60 / summary_stats['processing_time_median']).round(2)\n",
    "    \n",
    "    # Calculate efficiency score (accuracy \u00d7 throughput) using mean accuracy\n",
    "    summary_stats['efficiency_score'] = (summary_stats['overall_accuracy_mean'] * summary_stats['throughput_docs_per_min']).round(2)\n",
    "    \n",
    "    return summary_stats.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7\n",
    "\n",
    "def create_executive_dashboard(df: pd.DataFrame, save_path: str = None):\n",
    "    \"\"\"\n",
    "    Create comprehensive 6-panel executive dashboard comparing model performance.\n",
    "    \n",
    "    Args:\n",
    "        df: Combined DataFrame with results from all models\n",
    "        save_path: Optional path to save the visualization\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        rprint(\"[red]\u274c Cannot create dashboard - no data available[/red]\")\n",
    "        return\n",
    "    \n",
    "    # Define model order and colors for consistency\n",
    "    # Derive model names and colors from CONFIG for true configurability\n",
    "    model_order = [CONFIG['models'][key]['display_name'] for key in CONFIG['models']]\n",
    "    fixed_colors = {CONFIG['models'][key]['display_name']: CONFIG['models'][key]['color'] \n",
    "                    for key in CONFIG['models']}\n",
    "    \n",
    "    # Filter to only available models and maintain order\n",
    "    available_models = df['model'].unique()\n",
    "    models = [model for model in model_order if model in available_models]\n",
    "    model_colors = [fixed_colors[model] for model in models if model in fixed_colors]\n",
    "    \n",
    "    # Create figure with 6 subplots\n",
    "    fig = plt.figure(figsize=(20, 12))\n",
    "    gs = fig.add_gridspec(3, 2, height_ratios=[1, 1, 1], hspace=0.55, wspace=0.35)\n",
    "    \n",
    "    # 1. Overall Accuracy Comparison (Box Plot)\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    df_plot = df[df['model'].isin(models)]\n",
    "    \n",
    "    box_parts = ax1.boxplot(\n",
    "        [df_plot[df_plot['model'] == model]['overall_accuracy'].values for model in models],\n",
    "        labels=models,\n",
    "        patch_artist=True,\n",
    "        showmeans=True\n",
    "    )\n",
    "    \n",
    "    for patch, color in zip(box_parts['boxes'], model_colors):\n",
    "        patch.set_facecolor(color)\n",
    "        patch.set_alpha(0.7)\n",
    "    \n",
    "    ax1.set_ylabel('Mean F1 (%)', fontsize=12)\n",
    "    ax1.set_title('Mean F1 Comparison', fontsize=14, fontweight='bold')\n",
    "    ax1.grid(True, alpha=0.3, axis='y')\n",
    "    ax1.set_xticklabels(models, rotation=15, ha='right')\n",
    "    \n",
    "    # 2. Processing Speed Comparison (Box Plot)\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    \n",
    "    box_parts = ax2.boxplot(\n",
    "        [df_plot[df_plot['model'] == model]['processing_time'].values for model in models],\n",
    "        labels=models,\n",
    "        patch_artist=True,\n",
    "        showmeans=True\n",
    "    )\n",
    "    \n",
    "    for patch, color in zip(box_parts['boxes'], model_colors):\n",
    "        patch.set_facecolor(color)\n",
    "        patch.set_alpha(0.7)\n",
    "    \n",
    "    ax2.set_ylabel('Processing Time (seconds)', fontsize=12)\n",
    "    ax2.set_title('Processing Speed Comparison', fontsize=14, fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "    ax2.set_xticklabels(models, rotation=15, ha='right')\n",
    "    \n",
    "    # 3. Average Accuracy by Document Type (Grouped Bar Chart)\n",
    "    ax3 = fig.add_subplot(gs[1, 0])\n",
    "    \n",
    "    doc_accuracy = df_plot.groupby(['document_type', 'model'])['overall_accuracy'].mean().unstack()\n",
    "    doc_accuracy = doc_accuracy[models]  # Ensure correct order\n",
    "    \n",
    "    x = np.arange(len(doc_accuracy.index))\n",
    "    width = 0.8 / len(models)\n",
    "    \n",
    "    for idx, model in enumerate(models):\n",
    "        offset = (idx - len(models)/2 + 0.5) * width\n",
    "        ax3.bar(x + offset, doc_accuracy[model], width, \n",
    "               label=model, color=fixed_colors[model], alpha=0.8)\n",
    "    \n",
    "    ax3.set_ylabel('Mean F1 (%)', fontsize=12)\n",
    "    ax3.set_xlabel('Document Type', fontsize=12)\n",
    "    ax3.set_title('Mean F1 by Document Type', fontsize=14, fontweight='bold')\n",
    "    ax3.set_xticks(x)\n",
    "    ax3.set_xticklabels(doc_accuracy.index, rotation=15, ha='right')\n",
    "    ax3.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # 4. Average Processing Time by Document Type (Grouped Bar Chart)\n",
    "    ax4 = fig.add_subplot(gs[1, 1])\n",
    "    \n",
    "    doc_time = df_plot.groupby(['document_type', 'model'])['processing_time'].mean().unstack()\n",
    "    doc_time = doc_time[models]  # Ensure correct order\n",
    "    \n",
    "    for idx, model in enumerate(models):\n",
    "        offset = (idx - len(models)/2 + 0.5) * width\n",
    "        ax4.bar(x + offset, doc_time[model], width,\n",
    "               label=model, color=fixed_colors[model], alpha=0.8)\n",
    "    \n",
    "    ax4.set_ylabel('Average Processing Time (s)', fontsize=12)\n",
    "    ax4.set_xlabel('Document Type', fontsize=12)\n",
    "    ax4.set_title('Average Processing Time by Document Type', fontsize=14, fontweight='bold')\n",
    "    ax4.set_xticks(x)\n",
    "    ax4.set_xticklabels(doc_time.index, rotation=15, ha='right')\n",
    "    ax4.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # 5. Efficiency Analysis: Accuracy vs Processing Time (Scatter Plot)\n",
    "    ax5 = fig.add_subplot(gs[2, 0])\n",
    "    \n",
    "    for model in models:\n",
    "        model_data = df_plot[df_plot['model'] == model]\n",
    "        ax5.scatter(model_data['processing_time'], model_data['overall_accuracy'],\n",
    "                   label=model, color=fixed_colors[model], alpha=0.6, s=100)\n",
    "    \n",
    "    ax5.set_xlabel('Processing Time (seconds)', fontsize=12)\n",
    "    ax5.set_ylabel('Mean F1 (%)', fontsize=12)\n",
    "    ax5.set_title('Efficiency Analysis: Accuracy vs Processing Time', fontsize=14, fontweight='bold')\n",
    "    ax5.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add diagonal efficiency lines\n",
    "    ax5.axline((0, 0), slope=10, color='gray', linestyle='--', alpha=0.3, label='10% per second')\n",
    "    ax5.axline((0, 0), slope=5, color='gray', linestyle='--', alpha=0.3, label='5% per second')\n",
    "    \n",
    "    # 6. Performance Summary Table\n",
    "    ax6 = fig.add_subplot(gs[2, 1])\n",
    "    ax6.axis('off')\n",
    "    \n",
    "    # Generate summary statistics\n",
    "    summary_data = []\n",
    "    for model in models:\n",
    "        model_data = df_plot[df_plot['model'] == model]\n",
    "        # Get Mean F1 from model_mean_f1 dictionary (computed in cell 7)\n",
    "        # FAIL FAST: No silent fallbacks - if model not found, raise error\n",
    "        if model not in model_mean_f1:\n",
    "            raise KeyError(f\"\u274c FATAL: Model '{model}' not found in model_mean_f1 dict. \"\n",
    "                          f\"Available keys: {list(model_mean_f1.keys())}. \"\n",
    "                          f\"Check that CONFIG display names match.\")\n",
    "        mean_f1_val = model_mean_f1[model] * 100\n",
    "        summary_data.append([\n",
    "            model,\n",
    "            f\"{mean_f1_val:.1f}%\",\n",
    "            f\"{model_data['processing_time'].mean():.1f}s\",\n",
    "            f\"{(60 / model_data['processing_time'].mean()):.1f}\",\n",
    "            f\"{model_data['overall_accuracy'].std():.1f}%\"\n",
    "        ])\n",
    "    \n",
    "    table = ax6.table(cellText=summary_data,\n",
    "                     colLabels=['Model', 'Mean F1', 'Avg Time', 'Docs/min', 'Std Dev'],\n",
    "                     cellLoc='center',\n",
    "                     loc='center',\n",
    "                     colWidths=[0.35, 0.15, 0.15, 0.15, 0.15])\n",
    "    \n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    table.scale(1, 2)\n",
    "    \n",
    "    # Color header row\n",
    "    for i in range(5):\n",
    "        table[(0, i)].set_facecolor('#34495E')\n",
    "        table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "    \n",
    "    # Color data rows with model colors\n",
    "    for i, model in enumerate(models):\n",
    "        if model in fixed_colors:\n",
    "            table[(i+1, 0)].set_facecolor(fixed_colors[model])\n",
    "            table[(i+1, 0)].set_text_props(color='white', weight='bold')\n",
    "            table[(i+1, 0)].set_alpha(0.7)\n",
    "    \n",
    "    ax6.set_title('Model Performance Summary\\n(Mean F1 - Position-Aware)', fontsize=12, fontweight='bold', pad=5)\n",
    "    \n",
    "    # Overall title\n",
    "    fig.suptitle('MODEL PERFORMANCE COMPARISON\\nBusiness Document Information Extraction',\n",
    "                fontsize=20, fontweight='bold', y=0.98)\n",
    "    \n",
    "    # Single shared legend below the title\n",
    "    handles, labels = ax3.get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc='upper center', bbox_to_anchor=(0.5, 0.93), \n",
    "               ncol=len(models), fontsize=11, framealpha=0.95)\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.90])  # Make room for title and legend\n",
    "    \n",
    "    # Save the visualization\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=CONFIG['dpi'], bbox_inches='tight')\n",
    "        rprint(f\"[green]\u2705 Executive dashboard saved to: {save_path}[/green]\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Create the executive dashboard\n",
    "dashboard_timestamp = None\n",
    "if not combined_df.empty:\n",
    "    rprint(\"\\n[bold blue]\ud83d\udcca Creating Executive Dashboard[/bold blue]\")\n",
    "    \n",
    "    # Use CONFIG base path for absolute path\n",
    "    output_path = Path(CONFIG['output_dir']).parent / \"visualizations\" / \"executive_comparison.png\"\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    create_executive_dashboard(combined_df, str(output_path))\n",
    "    # Timestamp removed - always overwrite with latest version\n",
    "    \n",
    "    rprint(f\"[green]\u2705 Executive dashboard created successfully[/green]\")\n",
    "else:\n",
    "    rprint(\"[red]\u274c Cannot create executive dashboard - no data available[/red]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8\n",
    "\n",
    "def extract_field_level_accuracy_from_csv(output_dir: str, filename: str, model_name: str, ground_truth_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract field-level accuracy by comparing CSV batch results against ground truth.\n",
    "    \n",
    "    This function:\n",
    "    1. Loads batch results CSV (which has extracted field values)\n",
    "    2. Loads ground truth CSV\n",
    "    3. Compares field-by-field using the SAME evaluation logic as llama_batch.ipynb\n",
    "    4. Returns per-field accuracy WITHOUT re-running document-type filtering\n",
    "    \n",
    "    Args:\n",
    "        output_dir: Directory containing batch results CSV files\n",
    "        filename: Explicit CSV filename to load  \n",
    "        model_name: Name of the model\n",
    "        ground_truth_path: Path to ground truth CSV\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with columns: model, field_name, accuracy, correct_count, total_count\n",
    "    \"\"\"\n",
    "    from pathlib import Path\n",
    "    import sys\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))  # Use PROJECT_ROOT from cell 0\n",
    "    \n",
    "    from common.evaluation_metrics import load_ground_truth, calculate_field_accuracy_f1\n",
    "    \n",
    "    file_path = Path(output_dir) / filename\n",
    "    \n",
    "    if not file_path.exists():\n",
    "        rprint(f\"[yellow]\u26a0\ufe0f Batch results not found: {filename}[/yellow]\")\n",
    "        rprint(f\"[dim]   Expected at: {file_path}[/dim]\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    latest_file = str(file_path)\n",
    "    rprint(f\"[cyan]\ud83d\udcc4 Evaluating {model_name} from: {filename}[/cyan]\")\n",
    "    \n",
    "    try:\n",
    "        # Load batch results CSV\n",
    "        batch_df = pd.read_csv(latest_file)\n",
    "        total_images = len(batch_df)\n",
    "        rprint(f\"[dim]  DEBUG: Loaded {total_images} rows from batch CSV[/dim]\")\n",
    "        \n",
    "        # Load ground truth\n",
    "        gt_path = Path(ground_truth_path)\n",
    "        if not gt_path.exists():\n",
    "            rprint(f\"[red]  \u274c Ground truth not found: {ground_truth_path}[/red]\")\n",
    "            return pd.DataFrame()\n",
    "            \n",
    "        ground_truth_map = load_ground_truth(str(gt_path), show_sample=False, verbose=False)\n",
    "        rprint(f\"[dim]  DEBUG: Ground truth has {len(ground_truth_map)} entries[/dim]\")\n",
    "        \n",
    "        # Determine which column to use for matching\n",
    "        # Ground truth uses image_file, batch CSV has both image_file and image_name\n",
    "        image_col = 'image_file' if 'image_file' in batch_df.columns else 'image_name'\n",
    "        \n",
    "        # DEBUG: Show sample image names\n",
    "        if len(batch_df) > 0:\n",
    "            sample_batch_names = batch_df[image_col].head(3).tolist()\n",
    "            rprint(f\"[dim]  DEBUG: Sample batch image names (from {image_col}): {sample_batch_names}[/dim]\")\n",
    "        \n",
    "        if len(ground_truth_map) > 0:\n",
    "            sample_gt_names = list(ground_truth_map.keys())[:3]\n",
    "            rprint(f\"[dim]  DEBUG: Sample ground truth names: {sample_gt_names}[/dim]\")\n",
    "\n",
    "        # FIX: Normalize image names by stripping extensions for matching\n",
    "        # Batch CSV has extensions (.jpeg, .png), ground truth does not\n",
    "        batch_df['image_stem'] = batch_df[image_col].apply(lambda x: Path(x).stem)\n",
    "\n",
    "        # Create ground truth mapping with stems as keys\n",
    "        ground_truth_by_stem = {}\n",
    "        for gt_name, gt_data in ground_truth_map.items():\n",
    "            stem = Path(str(gt_name)).stem  # Strip extension if present\n",
    "            ground_truth_by_stem[stem] = gt_data\n",
    "\n",
    "        rprint(f\"[dim]  DEBUG: Created stem-based mapping for {len(ground_truth_by_stem)} ground truth entries[/dim]\")\n",
    "\n",
    "\n",
    "        \n",
    "        # FILTER: Only evaluate images that have ground truth\n",
    "        # FIX: Match using image_file column (consistent with ground truth)\n",
    "        batch_df_filtered = batch_df[batch_df['image_stem'].isin(ground_truth_by_stem.keys())]\n",
    "        filtered_count = len(batch_df_filtered)\n",
    "        skipped_count = total_images - filtered_count\n",
    "        \n",
    "        rprint(f\"[dim]  DEBUG: Filtered to {filtered_count} matching images (skipped {skipped_count})[/dim]\")\n",
    "        \n",
    "        if filtered_count == 0:\n",
    "            rprint(f\"[red]  \u274c No images in batch match ground truth entries[/red]\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        rprint(f\"[cyan]  \ud83d\udcca Evaluating {filtered_count}/{total_images} images with ground truth[/cyan]\")\n",
    "        \n",
    "        # Track field accuracies - accumulate across all images\n",
    "        field_accuracies = {}\n",
    "        skipped_not_found_count = 0\n",
    "        rewarded_not_found_count = 0\n",
    "        \n",
    "        # Get all possible field columns (exclude metadata columns)\n",
    "        metadata_cols = {'image_file', 'image_name', 'document_type', 'processing_time', \n",
    "                         'field_count', 'found_fields', 'field_coverage', 'prompt_used', \n",
    "                         'timestamp', 'overall_accuracy', 'fields_extracted', 'fields_matched', \n",
    "                         'total_fields', 'inference_only', 'model', 'image_stem',\n",
    "                         'gpu_type', 'model_version', 'precision', 'batch_size', 'max_tiles',\n",
    "                         'flash_attn', 'quantization', 'torch_dtype'}\n",
    "        \n",
    "        field_columns = [col for col in batch_df_filtered.columns if col not in metadata_cols]\n",
    "        \n",
    "        rprint(f\"[dim]  DEBUG: Found {len(field_columns)} field columns to evaluate[/dim]\")\n",
    "        \n",
    "        # Evaluate each image\n",
    "        for _, row in batch_df_filtered.iterrows():\n",
    "            image_identifier = row[image_col]\n",
    "            \n",
    "            # Get ground truth for this image (use image_identifier as key)\n",
    "            # Get ground truth using stem (without extension)\n",
    "            image_stem = Path(str(image_identifier)).stem\n",
    "            gt_data = ground_truth_by_stem.get(image_stem)\n",
    "            if not gt_data:\n",
    "                continue\n",
    "            \n",
    "            # Compare each field\n",
    "            for field_name in field_columns:\n",
    "                # Get values with proper NaN/None handling (same as DataFrame version)\n",
    "                raw_extracted = row.get(field_name)\n",
    "                raw_gt = gt_data.get(field_name)\n",
    "                \n",
    "                # Convert to string and handle NaN/None/empty - same as compute_per_field_metrics\n",
    "                extracted_value = str(raw_extracted) if pd.notna(raw_extracted) and raw_extracted != '' else 'NOT_FOUND'\n",
    "                ground_truth_value = str(raw_gt) if pd.notna(raw_gt) and raw_gt != '' else 'NOT_FOUND'\n",
    "                \n",
    "                # Skip if both are NOT_FOUND (field not applicable) - CONFIGURABLE\n",
    "                both_not_found = str(extracted_value).upper() == 'NOT_FOUND' and str(ground_truth_value).upper() == 'NOT_FOUND'\n",
    "                if both_not_found:\n",
    "                    if not REWARD_NOT_FOUND_MATCHES:\n",
    "                        skipped_not_found_count += 1\n",
    "                        continue\n",
    "                    else:\n",
    "                        rewarded_not_found_count += 1\n",
    "                # When REWARD_NOT_FOUND_MATCHES=True, NOT_FOUND matches are evaluated (F1=1.0)\n",
    "                \n",
    "                # Calculate F1 score (position-aware) - same as comprehensive dashboard\n",
    "                f1_result = calculate_field_accuracy_f1(\n",
    "                    extracted_value, ground_truth_value, field_name, debug=False\n",
    "                )\n",
    "                accuracy_score = f1_result['f1_score']\n",
    "                \n",
    "                # Initialize field tracking if needed\n",
    "                if field_name not in field_accuracies:\n",
    "                    field_accuracies[field_name] = {'correct': 0.0, 'total': 0}\n",
    "                \n",
    "                field_accuracies[field_name]['total'] += 1\n",
    "                field_accuracies[field_name]['correct'] += accuracy_score\n",
    "        \n",
    "        rprint(f\"[dim]  DEBUG: Computed accuracies for {len(field_accuracies)} fields[/dim]\")\n",
    "        rprint(f\"[bold cyan]  \ud83d\udcca REWARD_NOT_FOUND_MATCHES = {REWARD_NOT_FOUND_MATCHES}[/bold cyan]\")\n",
    "        if REWARD_NOT_FOUND_MATCHES:\n",
    "            rprint(f\"[green]  \u2705 Rewarded {rewarded_not_found_count} NOT_FOUND\u00d7NOT_FOUND matches (F1=1.0 each)[/green]\")\n",
    "        else:\n",
    "            rprint(f\"[yellow]  \u23ed\ufe0f  Skipped {skipped_not_found_count} NOT_FOUND\u00d7NOT_FOUND matches[/yellow]\")\n",
    "        \n",
    "        if not field_accuracies:\n",
    "            rprint(f\"[yellow]  \u26a0\ufe0f No field accuracies computed[/yellow]\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        field_data = []\n",
    "        for field_name, data in field_accuracies.items():\n",
    "            accuracy = data['correct'] / data['total'] if data['total'] > 0 else 0.0\n",
    "            field_data.append({\n",
    "                'model': model_name,\n",
    "                'field_name': field_name,\n",
    "                'f1_score': accuracy,\n",
    "                'correct_count': data['correct'],\n",
    "                'total_count': data['total']\n",
    "            })\n",
    "        \n",
    "        # Calculate average accuracy\n",
    "        avg_f1 = sum(d['f1_score'] for d in field_data) / len(field_data) if field_data else 0.0\n",
    "        \n",
    "        rprint(f\"[green]  \u2705 Computed accuracy for {len(field_data)} fields (avg: {avg_f1:.1%})[/green]\")\n",
    "        return pd.DataFrame(field_data)\n",
    "        \n",
    "    except Exception as e:\n",
    "        rprint(f\"[red]\u274c Error evaluating {model_name}: {e}[/red]\")\n",
    "        import traceback\n",
    "        rprint(f\"[dim]{traceback.format_exc()}[/dim]\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Load field-level accuracy data from CSV files (NOT re-running evaluation)\n",
    "rprint(\"\\n[bold blue]\ud83d\udcca Computing Field-Level Accuracy (from CSV + Ground Truth)[/bold blue]\")\n",
    "rprint(\"[dim]Comparing batch CSV results against ground truth for field-level metrics[/dim]\\n\")\n",
    "\n",
    "# Determine ground truth path - FAIL FAST if not found\n",
    "gt_path = CONFIG['ground_truth_path']\n",
    "\n",
    "rprint(f\"[cyan]Expected ground truth path: {gt_path}[/cyan]\")\n",
    "rprint(f\"[dim]  Absolute path: {Path(gt_path).absolute()}[/dim]\")\n",
    "\n",
    "if not Path(gt_path).exists():\n",
    "    rprint(f\"[bold red]\u274c FATAL: Ground truth file not found![/bold red]\")\n",
    "    rprint(f\"[yellow]\ud83d\udca1 Expected location: {Path(gt_path).absolute()}[/yellow]\")\n",
    "    rprint(f\"[yellow]\ud83d\udca1 Check CONFIG['output_dir'] setting[/yellow]\")\n",
    "    rprint(f\"[yellow]\ud83d\udca1 Current CONFIG['output_dir']: {CONFIG.get('output_dir', 'NOT SET')}[/yellow]\")\n",
    "    raise FileNotFoundError(f\"Ground truth not found: {gt_path}\")\n",
    "\n",
    "rprint(f\"[green]\u2705 Ground truth file exists[/green]\")\n",
    "\n",
    "\n",
    "\n",
    "field_data_frames = []\n",
    "\n",
    "# Use explicit file configurations from CONFIG['models']\n",
    "for model_key, model_cfg in CONFIG['models'].items():\n",
    "    if model_cfg['file'] and model_key in model_dataframes:\n",
    "        fields_df = extract_field_level_accuracy_from_csv(\n",
    "            CONFIG['output_dir'],\n",
    "            model_cfg['file'],\n",
    "            model_cfg['display_name'],\n",
    "            gt_path\n",
    "        )\n",
    "        if not fields_df.empty:\n",
    "            field_data_frames.append(fields_df)\n",
    "\n",
    "if field_data_frames:\n",
    "    field_level_df = pd.concat(field_data_frames, ignore_index=True)\n",
    "    rprint(f\"\\n[green]\u2705 Field-level accuracy computed: {len(field_level_df)} field measurements[/green]\")\n",
    "    rprint(f\"[cyan]\ud83d\udccb Unique fields: {field_level_df['field_name'].nunique()}[/cyan]\")\n",
    "    rprint(f\"[cyan]\ud83d\udcca Models analyzed: {field_level_df['model'].nunique()}[/cyan]\")\n",
    "else:\n",
    "    rprint(\"\\n[red]\u274c No field-level accuracy data available[/red]\")\n",
    "    rprint(\"[yellow]\ud83d\udca1 This requires batch results CSVs and ground truth for evaluation[/yellow]\")\n",
    "    field_level_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10\n",
    "\n",
    "def analyze_field_performance(field_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Analyze and compare field-level performance across models.\n",
    "    \n",
    "    Args:\n",
    "        field_df: DataFrame with field-level accuracy data\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with field performance comparison\n",
    "    \"\"\"\n",
    "    if field_df.empty:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Filter out metadata/internal fields that aren't business document fields\n",
    "    exclude_fields = ['quantization_used', 'model', 'timestamp', 'processing_time', 'image_name', 'image_stem', 'TRANSACTION_AMOUNTS_RECEIVED', 'ACCOUNT_BALANCE']\n",
    "    field_df = field_df[~field_df['field_name'].isin(exclude_fields)]\n",
    "    \n",
    "    if field_df.empty:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Pivot to get fields as rows, models as columns (using accuracy now)\n",
    "    field_comparison = field_df.pivot_table(\n",
    "        index='field_name',\n",
    "        columns='model',\n",
    "        values='f1_score',\n",
    "        aggfunc='mean'\n",
    "    )\n",
    "    \n",
    "    # Calculate average accuracy across all models\n",
    "    field_comparison['avg_f1'] = field_comparison.mean(axis=1)\n",
    "    \n",
    "    # Calculate variance to identify fields with inconsistent performance\n",
    "    field_comparison['variance'] = field_comparison.std(axis=1)\n",
    "    \n",
    "    # Identify best model for each field\n",
    "    model_cols = [col for col in field_comparison.columns if col not in ['avg_f1', 'variance']]\n",
    "    field_comparison['best_model'] = field_comparison[model_cols].idxmax(axis=1)\n",
    "    field_comparison['best_score'] = field_comparison[model_cols].max(axis=1)\n",
    "    \n",
    "    # Sort by average accuracy\n",
    "    field_comparison = field_comparison.sort_values('avg_f1', ascending=False)\n",
    "    \n",
    "    return field_comparison\n",
    "\n",
    "# Analyze field performance if data is available\n",
    "if not field_level_df.empty:\n",
    "    field_performance = analyze_field_performance(field_level_df)\n",
    "    \n",
    "    rprint(\"\\n[bold green]\ud83d\udcca V2 FIELD-LEVEL F1 ANALYSIS[/bold green]\")\n",
    "    rprint(\"[cyan]Using sophisticated multi-turn bank statement extraction[/cyan]\")\n",
    "    \n",
    "    # Show top performing fields with color gradient\n",
    "    rprint(\"[bold blue]\ud83d\udcca All Fields Ranked by F1:[/bold blue]\")\n",
    "    \n",
    "    # Get model columns for styling\n",
    "    model_cols = [col for col in field_performance.columns if col not in [\"avg_f1\", \"variance\", \"best_model\", \"best_score\"]]\n",
    "    \n",
    "    # Apply color gradient styling to all fields (already sorted by avg_f1 descending)\n",
    "    styled_all = field_performance.style.background_gradient(\n",
    "        cmap=\"RdYlGn\",\n",
    "        subset=model_cols + [\"avg_f1\"],\n",
    "        vmin=0,\n",
    "        vmax=1\n",
    "    ).format(\n",
    "        {col: \"{:.1%}\" for col in field_performance.columns if col not in [\"best_model\", \"variance\", \"best_score\"]}\n",
    "    ).format(\n",
    "        {\"variance\": \"{:.3f}\"}\n",
    "    )\n",
    "    \n",
    "    display(styled_all)\n",
    "    \n",
    "    # Save all fields table as PNG\n",
    "    output_table_dir = Path(f\"{base_data_path}/output/tables\")\n",
    "    output_table_dir.mkdir(parents=True, exist_ok=True)\n",
    "    all_fields_png_path = output_table_dir / \"field_accuracy_ranked_v2.png\"\n",
    "    \n",
    "    # Render table as image using matplotlib\n",
    "    fig, ax = plt.subplots(figsize=(16, 10))  # Taller for 17 fields\n",
    "    ax.axis(\"tight\")\n",
    "    ax.axis(\"off\")\n",
    "    \n",
    "    table_data = field_performance.values\n",
    "    col_labels = list(field_performance.columns)\n",
    "    row_labels = list(field_performance.index)\n",
    "    \n",
    "    table = ax.table(cellText=table_data, colLabels=col_labels, rowLabels=row_labels,\n",
    "                     cellLoc=\"center\", loc=\"center\")\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(8)\n",
    "    table.scale(1.2, 1.8)\n",
    "    \n",
    "    plt.savefig(str(all_fields_png_path), dpi=300, bbox_inches=\"tight\", facecolor=\"white\")\n",
    "    plt.close()\n",
    "    rprint(f\"[green]\u2705 All fields table saved: {all_fields_png_path}[/green]\")\n",
    "    \n",
    "    # Field performance summary\n",
    "    rprint(f\"[cyan]\ud83d\udcc8 Total evaluation fields: {len(field_performance)}[/cyan]\")\n",
    "    rprint(f\"[cyan]\ud83d\udcca Average accuracy range: {field_performance['avg_f1'].min():.1%} - {field_performance['avg_f1'].max():.1%}[/cyan]\")\n",
    "    \n",
    "    rprint(\"\\n[bold blue]\ud83c\udfaf Model Field Specialization:[/bold blue]\")\n",
    "    specialization = field_performance['best_model'].value_counts()\n",
    "    for model, count in specialization.items():\n",
    "        percentage = (count / len(field_performance)) * 100\n",
    "        rprint(f\"  \u2022 {model}: Best at {count} fields ({percentage:.1f}%)\")\n",
    "        \n",
    "    # Summary statistics\n",
    "    rprint(\"\\n[bold blue]\ud83d\udcca Overall Field Accuracy Summary:[/bold blue]\")\n",
    "    for model in field_level_df['model'].unique():\n",
    "        model_data = field_level_df[field_level_df['model'] == model]\n",
    "        avg_acc = model_data['f1_score'].mean()\n",
    "        rprint(f\"  \u2022 {model}: {avg_acc:.1%} average field accuracy\")\n",
    "else:\n",
    "    rprint(\"[red]\u274c Cannot analyze field performance - no field-level data available[/red]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5. Comprehensive Model Comparison Dashboard\n",
    "\n",
    "**Statistical Analysis with Pairwise Tests**\n",
    "\n",
    "This dashboard provides:\n",
    "1. **Summary Statistics**: Mean F1 (position-aware), Median, Std Dev, Min/Max for all 3 models\n",
    "2. **Statistical Tests**: Pairwise t-tests with p-values and significance levels\n",
    "3. **Effect Size Analysis**: Cohen's d with interpretation (negligible/small/medium/large)\n",
    "4. **Visual Comparisons**: Box plots, bar charts, efficiency analysis\n",
    "5. **Document Type Performance**: Per-document-type accuracy comparison\n",
    "6. **Final Recommendation**: Data-driven model selection guidance\n",
    "\n",
    "**NOTE**: Statistical tests use PAIRED t-tests across 17 schema fields - the SAME methodology used to compare against LayoutLM.\n",
    "For true F1 scores computed per-field, see Section 5.6 (Per-Field Precision, Recall, and F1 Metrics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Comprehensive Model Comparison Dashboard with Statistical Tests\n",
    "# This cell creates a unified dashboard comparing all 3 models with:\n",
    "# 1. Summary statistics (Mean F1, Median, Critical thresholds)\n",
    "# 2. Pairwise statistical tests (PAIRED t-test across fields, Cohen's d)\n",
    "# 3. Visual comparison charts\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from scipy import stats\n",
    "from IPython.display import display, HTML\n",
    "from rich import print as rprint\n",
    "\n",
    "# ============================================================================\n",
    "# STATISTICAL FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def cohens_d_paired(diff):\n",
    "    \"\"\"Calculate Cohen's d for paired samples (using difference scores).\"\"\"\n",
    "    if len(diff) == 0 or diff.std() == 0:\n",
    "        return 0.0\n",
    "    return diff.mean() / diff.std()\n",
    "\n",
    "def cohens_d(group1, group2):\n",
    "    \"\"\"Calculate Cohen's d effect size between two groups (for unpaired).\"\"\"\n",
    "    n1, n2 = len(group1), len(group2)\n",
    "    var1, var2 = group1.var(), group2.var()\n",
    "\n",
    "    # Pooled standard deviation\n",
    "    pooled_std = np.sqrt(((n1 - 1) * var1 + (n2 - 1) * var2) / (n1 + n2 - 2))\n",
    "\n",
    "    if pooled_std == 0:\n",
    "        return 0.0\n",
    "\n",
    "    return (group1.mean() - group2.mean()) / pooled_std\n",
    "\n",
    "def effect_size_interpretation(d):\n",
    "    \"\"\"Interpret Cohen's d effect size.\"\"\"\n",
    "    d_abs = abs(d)\n",
    "    if d_abs < 0.2:\n",
    "        return 'negligible'\n",
    "    elif d_abs < 0.5:\n",
    "        return 'small'\n",
    "    elif d_abs < 0.8:\n",
    "        return 'medium'\n",
    "    else:\n",
    "        return 'large'\n",
    "\n",
    "def significance_stars(p_value):\n",
    "    \"\"\"Convert p-value to significance stars.\"\"\"\n",
    "    if p_value < 0.001:\n",
    "        return '***'\n",
    "    elif p_value < 0.01:\n",
    "        return '**'\n",
    "    elif p_value < 0.05:\n",
    "        return '*'\n",
    "    else:\n",
    "        return 'ns'\n",
    "\n",
    "\n",
    "def compute_per_field_f1_for_model(batch_df, gt_df, model_name):\n",
    "    \"\"\"\n",
    "    Compute F1 score for each field for a single model.\n",
    "\n",
    "    Returns:\n",
    "        dict: {field_name: mean_f1_score} for all fields\n",
    "    \"\"\"\n",
    "    import sys\n",
    "    from pathlib import Path\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "    from common.evaluation_metrics import calculate_field_accuracy_f1\n",
    "\n",
    "    if batch_df.empty or gt_df.empty:\n",
    "        return {}\n",
    "\n",
    "    # Add image_stem for matching\n",
    "    if 'image_stem' not in batch_df.columns:\n",
    "        batch_df = batch_df.copy()\n",
    "        batch_df['image_stem'] = batch_df['image_file'].apply(lambda x: Path(x).stem)\n",
    "\n",
    "    field_f1 = {}\n",
    "\n",
    "    for field in FIELD_COLUMNS:\n",
    "        if field not in batch_df.columns or field not in gt_df.columns:\n",
    "            continue\n",
    "\n",
    "        # Merge on image_stem\n",
    "        merged = batch_df.merge(\n",
    "            gt_df[['image_stem', field]],\n",
    "            on='image_stem',\n",
    "            how='inner',\n",
    "            suffixes=('_pred', '_true')\n",
    "        )\n",
    "\n",
    "        if len(merged) == 0:\n",
    "            continue\n",
    "\n",
    "        pred_col = f'{field}_pred' if f'{field}_pred' in merged.columns else field\n",
    "        true_col = f'{field}_true' if f'{field}_true' in merged.columns else field\n",
    "\n",
    "        # Compute F1 for each document, then average for this field\n",
    "        doc_f1_scores = []\n",
    "        for _, row in merged.iterrows():\n",
    "            pred_val = str(row[pred_col]) if pd.notna(row[pred_col]) else 'NOT_FOUND'\n",
    "            true_val = str(row[true_col]) if pd.notna(row[true_col]) else 'NOT_FOUND'\n",
    "            \n",
    "            # Respect REWARD_NOT_FOUND_MATCHES config (consistent with extract_field_level_accuracy_from_csv)\n",
    "            both_not_found = pred_val.upper() == 'NOT_FOUND' and true_val.upper() == 'NOT_FOUND'\n",
    "            if both_not_found and not REWARD_NOT_FOUND_MATCHES:\n",
    "                continue  # Skip NOT_FOUND\u00d7NOT_FOUND when config is False\n",
    "            \n",
    "            metrics = calculate_field_accuracy_f1(pred_val, true_val, field, debug=False)\n",
    "            doc_f1_scores.append(metrics['f1_score'])\n",
    "\n",
    "        if doc_f1_scores:\n",
    "            field_f1[field] = sum(doc_f1_scores) / len(doc_f1_scores)\n",
    "\n",
    "    return field_f1\n",
    "\n",
    "\n",
    "def compute_pairwise_tests_across_fields(model_field_f1_dict):\n",
    "    \"\"\"\n",
    "    Compute PAIRED t-tests across schema fields between all model pairs.\n",
    "\n",
    "    This is the correct methodology for comparing models:\n",
    "    - Each field is a paired observation (same field measured by both models)\n",
    "    - Uses paired t-test (ttest_rel) not independent t-test\n",
    "    - Consistent with LayoutLM comparison methodology\n",
    "\n",
    "    Args:\n",
    "        model_field_f1_dict: Dict of {model_name: {field_name: f1_score}}\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with pairwise test results\n",
    "    \"\"\"\n",
    "    model_names = list(model_field_f1_dict.keys())\n",
    "    test_results = []\n",
    "\n",
    "    for i in range(len(model_names)):\n",
    "        for j in range(i + 1, len(model_names)):\n",
    "            model_a = model_names[i]\n",
    "            model_b = model_names[j]\n",
    "\n",
    "            f1_a = model_field_f1_dict[model_a]\n",
    "            f1_b = model_field_f1_dict[model_b]\n",
    "\n",
    "            # Find common fields (paired observations)\n",
    "            common_fields = set(f1_a.keys()) & set(f1_b.keys())\n",
    "\n",
    "            if len(common_fields) < 2:\n",
    "                continue\n",
    "\n",
    "            # Create paired arrays\n",
    "            scores_a = np.array([f1_a[f] for f in common_fields])\n",
    "            scores_b = np.array([f1_b[f] for f in common_fields])\n",
    "\n",
    "            # PAIRED t-test (not independent!)\n",
    "            t_stat, p_value = stats.ttest_rel(scores_a, scores_b)\n",
    "\n",
    "            # Cohen's d for paired samples\n",
    "            diff = scores_a - scores_b\n",
    "            d = cohens_d_paired(pd.Series(diff))\n",
    "\n",
    "            # Determine winner\n",
    "            mean_a = scores_a.mean()\n",
    "            mean_b = scores_b.mean()\n",
    "            if mean_a > mean_b:\n",
    "                winner = model_a\n",
    "                winner_advantage = (mean_a - mean_b) * 100  # Convert to percentage\n",
    "            else:\n",
    "                winner = model_b\n",
    "                winner_advantage = (mean_b - mean_a) * 100\n",
    "\n",
    "            test_results.append({\n",
    "                'Comparison': f\"{model_a} vs {model_b}\",\n",
    "                'Model A': model_a,\n",
    "                'Model B': model_b,\n",
    "                'Mean A': mean_a * 100,  # Convert to percentage\n",
    "                'Mean B': mean_b * 100,\n",
    "                'n_fields': len(common_fields),\n",
    "                't-statistic': t_stat,\n",
    "                'p-value': p_value,\n",
    "                'Significance': significance_stars(p_value),\n",
    "                \"cohens_d\": d,\n",
    "                'Effect Size': effect_size_interpretation(d),\n",
    "                'Winner': winner,\n",
    "                'Advantage': winner_advantage\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(test_results)\n",
    "\n",
    "\n",
    "# Legacy function for backward compatibility (per-document comparison)\n",
    "def compute_pairwise_tests(model_data_dict):\n",
    "    \"\"\"\n",
    "    DEPRECATED: Use compute_pairwise_tests_across_fields instead.\n",
    "    This function uses independent t-tests across documents.\n",
    "    \"\"\"\n",
    "    return compute_pairwise_tests_across_fields_wrapper(model_data_dict)\n",
    "\n",
    "\n",
    "def compute_pairwise_tests_across_fields_wrapper(model_data_dict):\n",
    "    \"\"\"Wrapper that computes per-field F1 and runs paired t-tests.\"\"\"\n",
    "    # This will be called with the model_accuracy_scores dict\n",
    "    # We need to compute per-field F1 first\n",
    "    model_field_f1 = {}\n",
    "\n",
    "    if not llama_df.empty and not ground_truth.empty:\n",
    "        model_field_f1['Llama-11B'] = compute_per_field_f1_for_model(llama_df, ground_truth, 'Llama-11B')\n",
    "\n",
    "    if not internvl3_8b_df.empty and not ground_truth.empty:\n",
    "        model_field_f1['IVL3.5-8B'] = compute_per_field_f1_for_model(internvl3_8b_df, ground_truth, 'IVL3.5-8B')\n",
    "\n",
    "    if not internvl3_2b_df.empty and not ground_truth.empty:\n",
    "        model_field_f1['IVL3-2B'] = compute_per_field_f1_for_model(internvl3_2b_df, ground_truth, 'IVL3-2B')\n",
    "\n",
    "    return compute_pairwise_tests_across_fields(model_field_f1)\n",
    "\n",
    "\n",
    "def compute_model_accuracy_scores(df, model_name):\n",
    "    \"\"\"Compute per-document F1 scores for box plot visualization.\n",
    "\n",
    "    NOTE: For statistical tests, we use per-FIELD F1 (paired t-test).\n",
    "    This function provides per-document scores for visualization only.\n",
    "\n",
    "    Returns values in 0-100 PERCENTAGE scale.\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return pd.Series(dtype=float)\n",
    "\n",
    "    # Use overall_accuracy for per-document visualization\n",
    "    # (Statistical tests use per-field F1 via compute_pairwise_tests_across_fields)\n",
    "    if 'overall_accuracy' in df.columns:\n",
    "        return df['overall_accuracy']\n",
    "\n",
    "    # Fallback: return zeros if no accuracy data\n",
    "    return pd.Series([0.0] * len(df), index=df.index)\n",
    "\n",
    "# ============================================================================\n",
    "# SUMMARY STATISTICS\n",
    "# ============================================================================\n",
    "\n",
    "def create_summary_statistics_from_fields(model_field_f1_dict):\n",
    "    \"\"\"\n",
    "    Create summary statistics table from per-field F1 scores.\n",
    "\n",
    "    Args:\n",
    "        model_field_f1_dict: Dict of {model_name: {field_name: f1_score}}\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with summary statistics across fields\n",
    "    \"\"\"\n",
    "    stats_data = []\n",
    "\n",
    "    for model_name, field_f1 in model_field_f1_dict.items():\n",
    "        if not field_f1:\n",
    "            continue\n",
    "\n",
    "        f1_values = np.array(list(field_f1.values())) * 100  # Convert to percentage\n",
    "\n",
    "        stats_data.append({\n",
    "            'Model': model_name,\n",
    "            'Mean F1': f1_values.mean(),\n",
    "            'Median F1': np.median(f1_values),\n",
    "            'Std Dev': f1_values.std(),\n",
    "            'Min F1': f1_values.min(),\n",
    "            'Max F1': f1_values.max(),\n",
    "            'N Fields': len(f1_values)\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(stats_data)\n",
    "\n",
    "\n",
    "def create_summary_statistics(model_data_dict):\n",
    "    \"\"\"\n",
    "    DEPRECATED: Use create_summary_statistics_from_fields instead.\n",
    "    This function creates stats from per-document data.\n",
    "    \"\"\"\n",
    "    stats_data = []\n",
    "\n",
    "    for model_name, scores in model_data_dict.items():\n",
    "        if len(scores) == 0:\n",
    "            continue\n",
    "\n",
    "        stats_data.append({\n",
    "            'Model': model_name,\n",
    "            'Mean F1': scores.mean(),\n",
    "            'Median F1': scores.median(),\n",
    "            'Std Dev': scores.std(),\n",
    "            'Min F1': scores.min(),\n",
    "            'Max F1': scores.max(),\n",
    "            'N': len(scores)\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(stats_data)\n",
    "\n",
    "# ============================================================================\n",
    "# DASHBOARD VISUALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "def create_comprehensive_dashboard(model_data_dict, summary_df, pairwise_tests_df, save_path=None):\n",
    "    \"\"\"\n",
    "    Create comprehensive 6-panel dashboard with all statistical results.\n",
    "    \"\"\"\n",
    "    # Get model names and colors\n",
    "    models = list(model_data_dict.keys())\n",
    "    colors = [MODEL_COLORS.get(m, '#999999') for m in models]\n",
    "\n",
    "    # Create figure with custom layout\n",
    "    fig = plt.figure(figsize=(20, 14))\n",
    "    gs = gridspec.GridSpec(3, 3, height_ratios=[1.2, 1, 1], hspace=0.4, wspace=0.3)\n",
    "\n",
    "    # =========================================================================\n",
    "    # ROW 1: Summary Statistics and Statistical Tests\n",
    "    # =========================================================================\n",
    "\n",
    "    # Panel 1A: Summary Statistics Table\n",
    "    ax_summary = fig.add_subplot(gs[0, 0:2])\n",
    "    ax_summary.axis('off')\n",
    "\n",
    "    # Prepare summary table data (per-field F1 statistics)\n",
    "    summary_table_data = []\n",
    "    for _, row in summary_df.iterrows():\n",
    "        summary_table_data.append([\n",
    "            row['Model'],\n",
    "            f\"{row['Mean F1']:.1f}%\",\n",
    "            f\"{row['Median F1']:.1f}%\",\n",
    "            f\"{row['Std Dev']:.1f}%\",\n",
    "            f\"{row['Min F1']:.1f}%\",\n",
    "            f\"{row['Max F1']:.1f}%\"\n",
    "        ])\n",
    "\n",
    "    summary_table = ax_summary.table(\n",
    "        cellText=summary_table_data,\n",
    "        colLabels=['Model', 'Mean F1', 'Median F1', 'Std Dev', 'Min F1', 'Max F1'],  # Stats across fields\n",
    "        cellLoc='center',\n",
    "        loc='center',\n",
    "        colWidths=[0.2, 0.15, 0.15, 0.15, 0.15, 0.15]\n",
    "    )\n",
    "    summary_table.auto_set_font_size(False)\n",
    "    summary_table.set_fontsize(11)\n",
    "    summary_table.scale(1.2, 2.2)\n",
    "\n",
    "    # Style header row\n",
    "    for i in range(6):\n",
    "        summary_table[(0, i)].set_facecolor('#2C3E50')\n",
    "        summary_table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "\n",
    "    # Color model cells\n",
    "    for i, model in enumerate(summary_df['Model']):\n",
    "        if model in MODEL_COLORS:\n",
    "            summary_table[(i+1, 0)].set_facecolor(MODEL_COLORS[model])\n",
    "            summary_table[(i+1, 0)].set_text_props(weight='bold', color='white')\n",
    "            summary_table[(i+1, 0)].set_alpha(0.8)\n",
    "\n",
    "    ax_summary.set_title('A. Summary Statistics (Per-Field F1)', fontsize=14, fontweight='bold', loc='left', pad=10)\n",
    "\n",
    "    # Panel 1B: Statistical Tests Table (PAIRED t-tests across fields)\n",
    "    ax_tests = fig.add_subplot(gs[0, 2])\n",
    "    ax_tests.axis('off')\n",
    "\n",
    "    # Prepare test results table\n",
    "    test_table_data = []\n",
    "    for _, row in pairwise_tests_df.iterrows():\n",
    "        test_table_data.append([\n",
    "            row['Comparison'],\n",
    "            f\"{row['p-value']:.4f}\",\n",
    "            row['Significance'],\n",
    "            f\"{row['cohens_d']:.2f}\",\n",
    "            row['Effect Size']\n",
    "        ])\n",
    "\n",
    "    test_table = ax_tests.table(\n",
    "        cellText=test_table_data,\n",
    "        colLabels=['Test', 'p-value', 'Sig.', \"Cohen's d\", 'Effect'],\n",
    "        cellLoc='center',\n",
    "        loc='center',\n",
    "        colWidths=[0.35, 0.15, 0.1, 0.15, 0.15]\n",
    "    )\n",
    "    test_table.auto_set_font_size(False)\n",
    "    test_table.set_fontsize(10)\n",
    "    test_table.scale(1.1, 2.0)\n",
    "\n",
    "    # Style header row\n",
    "    for i in range(5):\n",
    "        test_table[(0, i)].set_facecolor('#2C3E50')\n",
    "        test_table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "\n",
    "    # Color significance cells\n",
    "    for i, (_, row) in enumerate(pairwise_tests_df.iterrows()):\n",
    "        # Color p-value cell based on significance\n",
    "        if row['p-value'] < 0.01:\n",
    "            test_table[(i+1, 1)].set_facecolor('#27AE60')  # Green for significant\n",
    "            test_table[(i+1, 2)].set_facecolor('#27AE60')\n",
    "        elif row['p-value'] < 0.05:\n",
    "            test_table[(i+1, 1)].set_facecolor('#F1C40F')  # Yellow for marginal\n",
    "            test_table[(i+1, 2)].set_facecolor('#F1C40F')\n",
    "\n",
    "        # Color effect size cell\n",
    "        effect = row['Effect Size']\n",
    "        if effect == 'large':\n",
    "            test_table[(i+1, 4)].set_facecolor('#E74C3C')\n",
    "        elif effect == 'medium':\n",
    "            test_table[(i+1, 4)].set_facecolor('#F39C12')\n",
    "        elif effect == 'small':\n",
    "            test_table[(i+1, 4)].set_facecolor('#F1C40F')\n",
    "\n",
    "    # Updated title to clarify PAIRED t-test across FIELDS\n",
    "    ax_tests.set_title('B. Paired t-tests (across 17 fields)', fontsize=14, fontweight='bold', loc='left', pad=10)\n",
    "\n",
    "    # =========================================================================\n",
    "    # ROW 2: Accuracy Distributions\n",
    "    # =========================================================================\n",
    "\n",
    "    # Panel 2A: Per-Field F1 Distribution Box Plot\n",
    "    ax_box = fig.add_subplot(gs[1, 0])\n",
    "\n",
    "    # Use per-field F1 values (same source as Panel A Summary Statistics)\n",
    "    panel_c_f1 = {}\n",
    "    if not llama_df.empty and not ground_truth.empty:\n",
    "        f1_dict = compute_per_field_f1_for_model(llama_df, ground_truth, 'Llama-11B')\n",
    "        panel_c_f1['Llama-11B'] = [v * 100 for v in f1_dict.values()]\n",
    "    if not internvl3_8b_df.empty and not ground_truth.empty:\n",
    "        f1_dict = compute_per_field_f1_for_model(internvl3_8b_df, ground_truth, 'IVL3.5-8B')\n",
    "        panel_c_f1['IVL3.5-8B'] = [v * 100 for v in f1_dict.values()]\n",
    "    if not internvl3_2b_df.empty and not ground_truth.empty:\n",
    "        f1_dict = compute_per_field_f1_for_model(internvl3_2b_df, ground_truth, 'IVL3-2B')\n",
    "        panel_c_f1['IVL3-2B'] = [v * 100 for v in f1_dict.values()]\n",
    "    \n",
    "    f1_data = [panel_c_f1.get(m, [0]) for m in models]\n",
    "    box_parts = ax_box.boxplot(f1_data, labels=models, patch_artist=True, showmeans=True)\n",
    "\n",
    "    for patch, color in zip(box_parts['boxes'], colors):\n",
    "        patch.set_facecolor(color)\n",
    "        patch.set_alpha(0.7)\n",
    "\n",
    "    ax_box.set_ylabel('F1 Score (%)', fontsize=12, fontweight='bold')\n",
    "    ax_box.set_title('C. Per-Field F1 Distribution', fontsize=14, fontweight='bold')\n",
    "    ax_box.grid(True, alpha=0.3, axis='y')\n",
    "    ax_box.set_xticklabels(models, rotation=15, ha='right')\n",
    "\n",
    "    # Panel 2B: Mean F1 with Error Bars (using per-field F1, same as Panel A)\n",
    "    ax_bar = fig.add_subplot(gs[1, 1])\n",
    "\n",
    "    # Use per-field F1 values (same source as Panel A Summary Statistics)\n",
    "    panel_d_f1 = {}\n",
    "    if not llama_df.empty and not ground_truth.empty:\n",
    "        f1_dict = compute_per_field_f1_for_model(llama_df, ground_truth, 'Llama-11B')\n",
    "        panel_d_f1['Llama-11B'] = list(f1_dict.values())\n",
    "    if not internvl3_8b_df.empty and not ground_truth.empty:\n",
    "        f1_dict = compute_per_field_f1_for_model(internvl3_8b_df, ground_truth, 'IVL3.5-8B')\n",
    "        panel_d_f1['IVL3.5-8B'] = list(f1_dict.values())\n",
    "    if not internvl3_2b_df.empty and not ground_truth.empty:\n",
    "        f1_dict = compute_per_field_f1_for_model(internvl3_2b_df, ground_truth, 'IVL3-2B')\n",
    "        panel_d_f1['IVL3-2B'] = list(f1_dict.values())\n",
    "    \n",
    "    means = [np.mean(panel_d_f1.get(m, [0])) * 100 for m in models]\n",
    "    stds = [np.std(panel_d_f1.get(m, [0])) * 100 for m in models]\n",
    "\n",
    "    bars = ax_bar.bar(models, means, yerr=stds, capsize=5, color=colors, alpha=0.8, edgecolor='black')\n",
    "\n",
    "    # Add value labels\n",
    "    for bar, mean in zip(bars, means):\n",
    "        ax_bar.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 2,\n",
    "                   f'{mean:.1f}%', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "    ax_bar.set_ylabel('Mean F1 (%)', fontsize=12, fontweight='bold')\n",
    "    ax_bar.set_title('D. Mean F1 with Std Dev', fontsize=14, fontweight='bold')\n",
    "    ax_bar.set_ylim(0, 110)\n",
    "    ax_bar.grid(True, alpha=0.3, axis='y')\n",
    "    ax_bar.set_xticklabels(models, rotation=15, ha='right')\n",
    "\n",
    "    # Panel 2C: Winner Summary (based on Mean F1 across fields)\n",
    "    ax_winner = fig.add_subplot(gs[1, 2])\n",
    "    ax_winner.axis('off')\n",
    "\n",
    "    # Compute per-field F1 to determine best model\n",
    "    model_field_f1_temp = {}\n",
    "    if not llama_df.empty and not ground_truth.empty:\n",
    "        model_field_f1_temp['Llama-11B'] = compute_per_field_f1_for_model(llama_df, ground_truth, 'Llama-11B')\n",
    "    if not internvl3_8b_df.empty and not ground_truth.empty:\n",
    "        model_field_f1_temp['IVL3.5-8B'] = compute_per_field_f1_for_model(internvl3_8b_df, ground_truth, 'IVL3.5-8B')\n",
    "    if not internvl3_2b_df.empty and not ground_truth.empty:\n",
    "        model_field_f1_temp['IVL3-2B'] = compute_per_field_f1_for_model(internvl3_2b_df, ground_truth, 'IVL3-2B')\n",
    "\n",
    "    # Find best model by Mean F1\n",
    "    model_mean_f1_temp = {m: np.mean(list(f1.values())) * 100 for m, f1 in model_field_f1_temp.items() if f1}\n",
    "\n",
    "    if model_mean_f1_temp:\n",
    "        best_model = max(model_mean_f1_temp.keys(), key=lambda m: model_mean_f1_temp[m])\n",
    "        best_f1 = model_mean_f1_temp[best_model]\n",
    "    else:\n",
    "        best_model = max(model_data_dict.keys(), key=lambda m: model_data_dict[m].mean())\n",
    "        best_f1 = model_data_dict[best_model].mean()\n",
    "\n",
    "    # Create winner text\n",
    "    winner_text = f\"\"\"\n",
    "    BEST MODEL (Mean F1)\n",
    "\n",
    "    {best_model}\n",
    "    Mean F1: {best_f1:.1f}%\n",
    "\n",
    "    Statistical Tests:\n",
    "    \u2022 Paired t-tests across 17 schema fields\n",
    "    \u2022 Same methodology as LayoutLM comparison\n",
    "    \"\"\"\n",
    "\n",
    "    ax_winner.text(0.5, 0.5, winner_text, transform=ax_winner.transAxes,\n",
    "                  fontsize=12, verticalalignment='center', horizontalalignment='center',\n",
    "                  bbox=dict(boxstyle='round', facecolor=MODEL_COLORS.get(best_model, '#cccccc'), alpha=0.3),\n",
    "                  family='monospace')\n",
    "    ax_winner.set_title('E. Best Model (by Mean F1)', fontsize=14, fontweight='bold')\n",
    "\n",
    "    # =========================================================================\n",
    "    # ROW 3: Per-Field F1 Comparison (for paired t-test visualization)\n",
    "    # =========================================================================\n",
    "\n",
    "    # Panel 3A-C: Per-field F1 bar chart\n",
    "    ax_field = fig.add_subplot(gs[2, :])\n",
    "\n",
    "    # Compute per-field F1 for visualization\n",
    "    model_field_f1 = {}\n",
    "    if not llama_df.empty and not ground_truth.empty:\n",
    "        model_field_f1['Llama-11B'] = compute_per_field_f1_for_model(llama_df, ground_truth, 'Llama-11B')\n",
    "    if not internvl3_8b_df.empty and not ground_truth.empty:\n",
    "        model_field_f1['IVL3.5-8B'] = compute_per_field_f1_for_model(internvl3_8b_df, ground_truth, 'IVL3.5-8B')\n",
    "    if not internvl3_2b_df.empty and not ground_truth.empty:\n",
    "        model_field_f1['IVL3-2B'] = compute_per_field_f1_for_model(internvl3_2b_df, ground_truth, 'IVL3-2B')\n",
    "\n",
    "    if model_field_f1:\n",
    "        # Get all fields and sort by average F1\n",
    "        all_fields = set()\n",
    "        for f1_dict in model_field_f1.values():\n",
    "            all_fields.update(f1_dict.keys())\n",
    "\n",
    "        # Calculate average F1 per field and sort\n",
    "        field_avg = {}\n",
    "        for field in all_fields:\n",
    "            values = [model_field_f1[m].get(field, 0) for m in model_field_f1.keys()]\n",
    "            field_avg[field] = np.mean(values)\n",
    "\n",
    "        sorted_fields = sorted(all_fields, key=lambda f: field_avg[f], reverse=True)\n",
    "\n",
    "        # Create grouped bar chart\n",
    "        x = np.arange(len(sorted_fields))\n",
    "        width = 0.25\n",
    "\n",
    "        for idx, (model_name, f1_dict) in enumerate(model_field_f1.items()):\n",
    "            values = [f1_dict.get(f, 0) * 100 for f in sorted_fields]  # Convert to percentage\n",
    "            color = MODEL_COLORS.get(model_name, '#999999')\n",
    "            bars = ax_field.bar(x + idx * width, values, width, label=model_name, color=color, alpha=0.8)\n",
    "            \n",
    "            # Add value labels on top of bars\n",
    "            for bar, val in zip(bars, values):\n",
    "                if val > 0:  # Only label non-zero bars\n",
    "                    ax_field.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "                                 f'{val:.0f}', ha='center', va='bottom', fontsize=8, fontweight='bold', rotation=90)\n",
    "\n",
    "        ax_field.set_ylabel('F1 Score (%)', fontsize=12, fontweight='bold')\n",
    "        ax_field.set_xlabel('Schema Field', fontsize=12, fontweight='bold')\n",
    "        ax_field.set_title('F. Per-Field F1 Scores (Used for Paired t-tests)', fontsize=14, fontweight='bold')\n",
    "        ax_field.set_xticks(x + width)\n",
    "        ax_field.set_xticklabels(sorted_fields, rotation=45, ha='right', fontsize=9)\n",
    "        # Legend moved to figure level (top of dashboard)\n",
    "        ax_field.legend_.remove() if ax_field.legend_ else None\n",
    "        ax_field.grid(True, alpha=0.3, axis='y')\n",
    "        ax_field.set_ylim(0, 105)\n",
    "\n",
    "    # Overall title\n",
    "    fig.suptitle('COMPREHENSIVE MODEL COMPARISON\\nPaired t-tests across Schema Fields',\n",
    "                fontsize=18, fontweight='bold', y=1.02)\n",
    "    \n",
    "    # Add legend at top of dashboard (like Executive Dashboard)\n",
    "    handles, labels = ax_field.get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc='upper center', bbox_to_anchor=(0.5, 0.98),\n",
    "              ncol=3, fontsize=11, framealpha=0.9)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.94])\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight', facecolor='white')\n",
    "        rprint(f\"[green]\u2705 Dashboard saved to: {save_path}[/green]\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return fig\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "rprint(\"[bold blue]=\" * 60 + \"[/bold blue]\")\n",
    "rprint(\"[bold blue]COMPREHENSIVE MODEL COMPARISON WITH STATISTICAL TESTS[/bold blue]\")\n",
    "rprint(\"[bold blue]=\" * 60 + \"[/bold blue]\")\n",
    "\n",
    "# Compute per-document accuracy scores for each model (for visualization)\n",
    "model_accuracy_scores = {}\n",
    "\n",
    "if not llama_df.empty:\n",
    "    model_accuracy_scores[CONFIG['models']['llama']['display_name']] = compute_model_accuracy_scores(\n",
    "        llama_df, CONFIG['models']['llama']['display_name']\n",
    "    )\n",
    "    rprint(f\"[green]\u2705 {CONFIG['models']['llama']['display_name']}: {len(model_accuracy_scores[CONFIG['models']['llama']['display_name']])} documents[/green]\")\n",
    "\n",
    "if not internvl3_8b_df.empty:\n",
    "    model_accuracy_scores[CONFIG['models']['internvl3_8b']['display_name']] = compute_model_accuracy_scores(\n",
    "        internvl3_8b_df, CONFIG['models']['internvl3_8b']['display_name']\n",
    "    )\n",
    "    rprint(f\"[green]\u2705 {CONFIG['models']['internvl3_8b']['display_name']}: {len(model_accuracy_scores[CONFIG['models']['internvl3_8b']['display_name']])} documents[/green]\")\n",
    "\n",
    "if not internvl3_2b_df.empty:\n",
    "    model_accuracy_scores[CONFIG['models']['internvl3_2b']['display_name']] = compute_model_accuracy_scores(\n",
    "        internvl3_2b_df, CONFIG['models']['internvl3_2b']['display_name']\n",
    "    )\n",
    "    rprint(f\"[green]\u2705 {CONFIG['models']['internvl3_2b']['display_name']}: {len(model_accuracy_scores[CONFIG['models']['internvl3_2b']['display_name']])} documents[/green]\")\n",
    "\n",
    "# Compute per-field F1 for all models (used for both summary stats and statistical tests)\n",
    "rprint(\"\\n[bold cyan]Computing Per-Field F1 Scores...[/bold cyan]\")\n",
    "\n",
    "model_field_f1_all = {}\n",
    "if not llama_df.empty and not ground_truth.empty:\n",
    "    model_field_f1_all['Llama-11B'] = compute_per_field_f1_for_model(llama_df, ground_truth, 'Llama-11B')\n",
    "    rprint(f\"  Llama-11B: {len(model_field_f1_all['Llama-11B'])} fields\")\n",
    "\n",
    "if not internvl3_8b_df.empty and not ground_truth.empty:\n",
    "    model_field_f1_all['IVL3.5-8B'] = compute_per_field_f1_for_model(internvl3_8b_df, ground_truth, 'IVL3.5-8B')\n",
    "    rprint(f\"  IVL3.5-8B: {len(model_field_f1_all['IVL3.5-8B'])} fields\")\n",
    "\n",
    "if not internvl3_2b_df.empty and not ground_truth.empty:\n",
    "    model_field_f1_all['IVL3-2B'] = compute_per_field_f1_for_model(internvl3_2b_df, ground_truth, 'IVL3-2B')\n",
    "    rprint(f\"  IVL3-2B: {len(model_field_f1_all['IVL3-2B'])} fields\")\n",
    "\n",
    "# Compute summary statistics FROM PER-FIELD F1 (not per-document)\n",
    "rprint(\"\\n[bold cyan]Computing Summary Statistics (from per-field F1)...[/bold cyan]\")\n",
    "summary_stats = create_summary_statistics_from_fields(model_field_f1_all)\n",
    "display(summary_stats)\n",
    "\n",
    "# Compute PAIRED t-tests across schema fields\n",
    "rprint(\"\\n[bold cyan]Computing Paired t-tests Across Schema Fields...[/bold cyan]\")\n",
    "rprint(\"[dim]\u2022 Using paired t-test (ttest_rel) - same methodology as LayoutLM comparison[/dim]\")\n",
    "rprint(\"[dim]\u2022 Each field is a paired observation (same field measured by both models)[/dim]\")\n",
    "\n",
    "pairwise_tests = compute_pairwise_tests(model_accuracy_scores)\n",
    "\n",
    "# Display pairwise tests with styling\n",
    "styled_tests = pairwise_tests[['Comparison', 'n_fields', 'p-value', 'Significance', 'cohens_d', 'Effect Size', 'Winner', 'Advantage']].copy()\n",
    "styled_tests['p-value'] = styled_tests['p-value'].apply(lambda x: f\"{x:.4f}\")\n",
    "styled_tests['Advantage'] = styled_tests['Advantage'].apply(lambda x: f\"{x:.1f}%\")\n",
    "\n",
    "display(styled_tests)\n",
    "\n",
    "# Statistical interpretation\n",
    "rprint(\"\\n[bold yellow]Statistical Interpretation:[/bold yellow]\")\n",
    "rprint(\"[dim]\u2022 Test: Paired t-test across schema fields (same as LayoutLM comparison)[/dim]\")\n",
    "rprint(\"[dim]\u2022 Significance: *** p<0.001, ** p<0.01, * p<0.05, ns = not significant[/dim]\")\n",
    "rprint(\"[dim]\u2022 Cohen's d: |d|<0.2 negligible, 0.2-0.5 small, 0.5-0.8 medium, >0.8 large[/dim]\")\n",
    "\n",
    "for _, row in pairwise_tests.iterrows():\n",
    "    sig_text = \"SIGNIFICANT\" if row['Significance'] != 'ns' else \"NOT SIGNIFICANT\"\n",
    "    rprint(f\"\\n[cyan]{row['Comparison']} ({row['n_fields']} fields):[/cyan]\")\n",
    "    rprint(f\"  \u2022 Result: {sig_text} (p={row['p-value']:.4f})\")\n",
    "    rprint(f\"  \u2022 Effect: {row['Effect Size']} (d={row['cohens_d']:.2f})\")\n",
    "    rprint(f\"  \u2022 Winner: [bold]{row['Winner']}[/bold] (+{row['Advantage']:.1f}%)\")\n",
    "\n",
    "# Create comprehensive dashboard\n",
    "rprint(\"\\n[bold cyan]Creating Comprehensive Dashboard...[/bold cyan]\")\n",
    "\n",
    "output_path = Path(CONFIG['output_dir']).parent / \"visualizations\" / \"comprehensive_dashboard.png\"\n",
    "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "create_comprehensive_dashboard(\n",
    "    model_accuracy_scores,\n",
    "    summary_stats,\n",
    "    pairwise_tests,\n",
    "    save_path=str(output_path)\n",
    ")\n",
    "\n",
    "rprint(\"\\n[bold green]\u2705 Comprehensive analysis complete![/bold green]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Dashboard Supporting Data Tables\n",
    "\n",
    "The tables below contain the exact values used to generate the comprehensive dashboard.\n",
    "Data Scientists can use these to verify the accuracy of all visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# COMPREHENSIVE DASHBOARD - SUPPORTING DATA TABLES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"COMPREHENSIVE DASHBOARD - SUPPORTING DATA TABLES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# TABLE 1: Summary Statistics\n",
    "print(\"\\nTABLE 1: Summary Statistics (Per-Field F1)\")\n",
    "print(\"-\" * 80)\n",
    "if 'summary_stats' in dir() and summary_stats is not None:\n",
    "    print(summary_stats.to_string())\n",
    "else:\n",
    "    print(\"Summary statistics not available\")\n",
    "\n",
    "# TABLE 2: Pairwise Statistical Tests\n",
    "print(\"\\n\\nTABLE 2: Pairwise Statistical Tests\")\n",
    "print(\"-\" * 80)\n",
    "if 'pairwise_tests' in dir() and pairwise_tests is not None:\n",
    "    test_display = pairwise_tests[['Comparison', 'n_fields', 'p-value', 'Significance', \n",
    "                                    'cohens_d', 'Effect Size', 'Winner', 'Advantage']].copy()\n",
    "    print(test_display.to_string(index=False))\n",
    "else:\n",
    "    print(\"Pairwise tests not available\")\n",
    "\n",
    "# TABLE 3: Per-Field F1 Scores by Model\n",
    "print(\"\\n\\nTABLE 3: Per-Field F1 Scores by Model\")\n",
    "print(\"-\" * 80)\n",
    "if 'model_field_f1_all' in dir() and model_field_f1_all:\n",
    "    # Convert to DataFrame for display\n",
    "    all_fields = set()\n",
    "    for f1_dict in model_field_f1_all.values():\n",
    "        all_fields.update(f1_dict.keys())\n",
    "    \n",
    "    field_data = []\n",
    "    for field in sorted(all_fields):\n",
    "        row = {'Field': field}\n",
    "        for model, f1_dict in model_field_f1_all.items():\n",
    "            row[model] = f\"{f1_dict.get(field, 0):.1%}\"\n",
    "        field_data.append(row)\n",
    "    \n",
    "    field_f1_df = pd.DataFrame(field_data)\n",
    "    print(field_f1_df.to_string(index=False))\n",
    "else:\n",
    "    print(\"Per-field F1 scores not available\")\n",
    "\n",
    "# TABLE 4: Cohen's d Calculation Details\n",
    "print(\"\\n\\nTABLE 4: Cohen's d Calculation Details\")\n",
    "print(\"-\" * 80)\n",
    "if 'pairwise_tests' in dir() and pairwise_tests is not None and 'model_field_f1_all' in dir():\n",
    "    print(f\"{'Comparison':<30} {'Mean1':>10} {'Mean2':>10} {'Diff':>10} {'Cohen d':>10}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    models = list(model_field_f1_all.keys())\n",
    "    for i, m1 in enumerate(models):\n",
    "        for m2 in models[i+1:]:\n",
    "            # Get common fields\n",
    "            common_fields = set(model_field_f1_all[m1].keys()) & set(model_field_f1_all[m2].keys())\n",
    "            if common_fields:\n",
    "                vals1 = [model_field_f1_all[m1][f] for f in common_fields]\n",
    "                vals2 = [model_field_f1_all[m2][f] for f in common_fields]\n",
    "                mean1, mean2 = np.mean(vals1), np.mean(vals2)\n",
    "                \n",
    "                # Find Cohen's d from pairwise_tests\n",
    "                comparison = f\"{m1} vs {m2}\"\n",
    "                match = pairwise_tests[pairwise_tests['Comparison'].str.contains(m1) & \n",
    "                                       pairwise_tests['Comparison'].str.contains(m2)]\n",
    "                if not match.empty:\n",
    "                    d = match.iloc[0]['cohens_d']\n",
    "                    print(f\"{comparison:<30} {mean1:>10.1%} {mean2:>10.1%} {mean1-mean2:>+10.1%} {d:>10.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dashboard: Side-by-Side Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12\n",
    "\n",
    "def create_field_level_visualizations(field_df: pd.DataFrame, field_performance: pd.DataFrame, save_path: str = None):\n",
    "    \"\"\"\n",
    "    Create comprehensive field-level performance visualizations.\n",
    "    \n",
    "    Args:\n",
    "        field_df: Raw field-level accuracy data\n",
    "        field_performance: Analyzed field performance comparison\n",
    "        save_path: Optional path to save visualization\n",
    "    \"\"\"\n",
    "    if field_df.empty or field_performance.empty:\n",
    "        rprint(\"[red]\u274c Cannot create field visualizations - no data available[/red]\")\n",
    "        return\n",
    "    \n",
    "    # Create figure with improved spacing\n",
    "    fig = plt.figure(figsize=(22, 13))\n",
    "    gs = fig.add_gridspec(2, 2, height_ratios=[1.2, 1], width_ratios=[1.5, 1], \n",
    "                          hspace=0.35, wspace=0.25, top=0.90, bottom=0.06, left=0.06, right=0.97)\n",
    "    \n",
    "    # V2: Derive model names and colors from CONFIG for true configurability\n",
    "    model_order = [CONFIG['models'][key]['display_name'] for key in CONFIG['models']]\n",
    "    fixed_colors = {CONFIG['models'][key]['display_name']: CONFIG['models'][key]['color'] \n",
    "                    for key in CONFIG['models']}\n",
    "    \n",
    "    # Get available models\n",
    "    available_models = field_df['model'].unique()\n",
    "    models = [model for model in model_order if model in available_models]\n",
    "    model_colors = {model: fixed_colors[model] for model in models if model in fixed_colors}\n",
    "    \n",
    "    # 1. Field Accuracy Comparison (Horizontal Bar Chart) - IMPROVED\n",
    "    ax1 = fig.add_subplot(gs[0, :])\n",
    "    \n",
    "    # Get all evaluation fields by average accuracy\n",
    "    all_eval_fields = field_performance\n",
    "    model_cols = [col for col in all_eval_fields.columns if col in models]\n",
    "    \n",
    "    y_pos = np.arange(len(all_eval_fields))\n",
    "    bar_height = 0.22\n",
    "    \n",
    "    # Create bars with better spacing\n",
    "    for idx, model in enumerate(model_cols):\n",
    "        offset = (idx - len(model_cols)/2 + 0.5) * bar_height\n",
    "        bars = ax1.barh(y_pos + offset, all_eval_fields[model], bar_height, \n",
    "                label=model, color=model_colors.get(model, '#999999'), alpha=0.85, edgecolor='white', linewidth=0.5)\n",
    "    \n",
    "    ax1.set_yticks(y_pos)\n",
    "    ax1.set_yticklabels(all_eval_fields.index, fontsize=11)\n",
    "    ax1.invert_yaxis()  # Match heatmap ordering (top-to-bottom)\n",
    "    ax1.set_xlabel('Field F1', fontsize=13, fontweight='bold')\n",
    "    ax1.set_title('All Evaluation Fields: F1 by Model (Multi-Turn Bank Extraction)', fontsize=16, fontweight='bold', pad=15)\n",
    "    \n",
    "    # Improved legend placement - outside plot area to avoid occlusion\n",
    "    ax1.legend(loc='upper left', bbox_to_anchor=(1.01, 1), fontsize=11, framealpha=0.98, edgecolor='gray', shadow=True)\n",
    "    ax1.grid(True, alpha=0.3, axis='x', linestyle='--')\n",
    "    ax1.set_xlim(0, 1.05)\n",
    "    ax1.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'{x:.0%}'))\n",
    "    \n",
    "    # Add vertical reference lines\n",
    "    for x in [0.25, 0.5, 0.75, 1.0]:\n",
    "        ax1.axvline(x=x, color='gray', linestyle=':', alpha=0.3, linewidth=0.8)\n",
    "    \n",
    "    # 2. Field F1 Heatmap - IMPROVED\n",
    "    ax2 = fig.add_subplot(gs[1, 0])\n",
    "    \n",
    "    # Get all evaluation fields for heatmap (same as bar chart for consistency)\n",
    "    heatmap_data = field_performance[model_cols]\n",
    "    \n",
    "    # Create heatmap with better colors\n",
    "    im = ax2.imshow(heatmap_data.values, cmap='RdYlGn', aspect='auto', vmin=0, vmax=1)\n",
    "    \n",
    "    # Improved tick labels\n",
    "    ax2.set_xticks(np.arange(len(model_cols)))\n",
    "    ax2.set_yticks(np.arange(len(heatmap_data)))\n",
    "    \n",
    "    # V2: Use display names from CONFIG for consistency\n",
    "    model_labels = []\n",
    "    for model in model_cols:\n",
    "        # Use full display names for consistency across all visualizations\n",
    "        model_labels.append(model)\n",
    "    \n",
    "    ax2.set_xticklabels(model_labels, rotation=0, ha='center', fontsize=11, fontweight='bold')\n",
    "    ax2.set_yticklabels(heatmap_data.index, fontsize=10)\n",
    "    \n",
    "    # Improved colorbar\n",
    "    cbar = plt.colorbar(im, ax=ax2, fraction=0.046, pad=0.04)\n",
    "    cbar.set_label('Field F1', rotation=270, labelpad=20, fontsize=11, fontweight='bold')\n",
    "    cbar.ax.tick_params(labelsize=10)\n",
    "    \n",
    "    # Add text annotations with better visibility\n",
    "    for i in range(len(heatmap_data)):\n",
    "        for j in range(len(model_cols)):\n",
    "            value = heatmap_data.iloc[i, j]\n",
    "            if not np.isnan(value):\n",
    "                text_color = 'white' if value < 0.0 else 'black'\n",
    "                ax2.text(j, i, f'{value:.0%}', ha='center', va='center', \n",
    "                        color=text_color, fontsize=9, fontweight='bold')\n",
    "    \n",
    "    ax2.set_title('Field F1 Heatmap: All Evaluation Fields', fontsize=15, fontweight='bold', pad=12)\n",
    "    \n",
    "    # 3. Model Specialization Pie Chart - IMPROVED\n",
    "    ax3 = fig.add_subplot(gs[1, 1])\n",
    "    \n",
    "    specialization = field_performance['best_model'].value_counts()\n",
    "    colors_list = [model_colors.get(model, '#999999') for model in specialization.index]\n",
    "    \n",
    "    # Improved pie chart with better labels\n",
    "    wedges, texts, autotexts = ax3.pie(\n",
    "        specialization.values,\n",
    "        labels=None,  # We'll add custom labels\n",
    "        autopct='%1.1f%%',\n",
    "        colors=colors_list,\n",
    "        startangle=90,\n",
    "        textprops={'fontsize': 11, 'fontweight': 'bold'},\n",
    "        explode=[0.02] * len(specialization)  # Slight separation\n",
    "    )\n",
    "    \n",
    "    # Make percentage text bold and white\n",
    "    for autotext in autotexts:\n",
    "        autotext.set_color('white')\n",
    "        autotext.set_fontweight('bold')\n",
    "    \n",
    "    # V2: Use full display names for consistency across all visualizations\n",
    "    legend_labels = []\n",
    "    for model in specialization.index:\n",
    "        count = specialization[model]\n",
    "        legend_labels.append(f'{model}: {count} fields')\n",
    "    \n",
    "    ax3.legend(legend_labels, loc='upper left', fontsize=10, framealpha=0.95, \n",
    "               bbox_to_anchor=(0.02, 0.98), edgecolor='gray')\n",
    "    \n",
    "    ax3.set_title('Model Field Specialization\\n(% of Fields Where Model Performs Best)', \n",
    "                 fontsize=14, fontweight='bold', pad=12)\n",
    "    \n",
    "    # V2: Overall title updated\n",
    "    fig.suptitle('Field-Level F1 Analysis\\nComparison Across Models (Multi-Turn Bank Statement Extraction)', \n",
    "                fontsize=19, fontweight='bold')\n",
    "    \n",
    "    # Save the visualization\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=CONFIG['dpi'], bbox_inches='tight', facecolor='white')\n",
    "        rprint(f\"[green]\u2705 Field-level visualization saved to: {save_path}[/green]\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Create field-level visualizations\n",
    "if not field_level_df.empty and 'field_performance' in locals():\n",
    "    output_path = Path(\"output/visualizations/field_level_accuracy_v2.png\")\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    create_field_level_visualizations(field_level_df, field_performance, str(output_path))\n",
    "else:\n",
    "    rprint(\"[red]\u274c Cannot create field visualizations - no field-level data available[/red]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Field-Level Analysis Supporting Data Tables\n",
    "\n",
    "The tables below contain the exact values used for the field-level visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FIELD-LEVEL ANALYSIS - SUPPORTING DATA TABLES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"FIELD-LEVEL ANALYSIS - SUPPORTING DATA TABLES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# TABLE 1: Field Performance Comparison\n",
    "print(\"\\nTABLE 1: Field Performance by Model (F1 Scores)\")\n",
    "print(\"-\" * 80)\n",
    "if 'field_performance' in dir() and field_performance is not None:\n",
    "    # Format as percentages\n",
    "    display_df = field_performance.copy()\n",
    "    model_cols = [col for col in display_df.columns if col not in ['best_model', 'max_f1', 'min_f1', 'range']]\n",
    "    for col in model_cols:\n",
    "        if col in display_df.columns:\n",
    "            display_df[col] = display_df[col].apply(lambda x: f\"{x:.1%}\" if pd.notna(x) else \"N/A\")\n",
    "    print(display_df.to_string())\n",
    "else:\n",
    "    print(\"Field performance data not available\")\n",
    "\n",
    "# TABLE 2: Model Specialization Summary\n",
    "print(\"\\n\\nTABLE 2: Model Specialization (Fields Where Each Model Performs Best)\")\n",
    "print(\"-\" * 80)\n",
    "if 'field_performance' in dir() and field_performance is not None and 'best_model' in field_performance.columns:\n",
    "    specialization = field_performance['best_model'].value_counts()\n",
    "    print(f\"{'Model':<20} {'Fields Won':>15} {'Percentage':>15}\")\n",
    "    print(\"-\" * 50)\n",
    "    total = specialization.sum()\n",
    "    for model, count in specialization.items():\n",
    "        pct = count / total * 100\n",
    "        print(f\"{model:<20} {count:>15} {pct:>14.1f}%\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"{'Total':<20} {total:>15}\")\n",
    "else:\n",
    "    print(\"Model specialization data not available\")\n",
    "\n",
    "# TABLE 3: Fields by Best Model\n",
    "print(\"\\n\\nTABLE 3: Fields Grouped by Best Performing Model\")\n",
    "print(\"-\" * 80)\n",
    "if 'field_performance' in dir() and field_performance is not None and 'best_model' in field_performance.columns:\n",
    "    for model in field_performance['best_model'].unique():\n",
    "        fields = field_performance[field_performance['best_model'] == model].index.tolist()\n",
    "        print(f\"\\n{model}:\")\n",
    "        for field in fields:\n",
    "            f1 = field_performance.loc[field, model] if model in field_performance.columns else 0\n",
    "            print(f\"  - {field}: {f1:.1%}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Detailed Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14\n",
    "\n",
    "def analyze_model_strengths(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Analyze and compare model strengths and weaknesses.\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return\n",
    "    \n",
    "    rprint(\"\\n[bold green]\ud83c\udfaf DETAILED PERFORMANCE ANALYSIS[/bold green]\")\n",
    "    \n",
    "    # Document type winner analysis\n",
    "    doc_performance = df.groupby(['document_type', 'model'])['overall_accuracy'].mean().unstack()\n",
    "    \n",
    "    rprint(\"\\n[bold blue]\ud83d\udcca Document Type Performance Leaders:[/bold blue]\")\n",
    "    for doc_type in doc_performance.index:\n",
    "        best_model = doc_performance.loc[doc_type].idxmax()\n",
    "        best_score = doc_performance.loc[doc_type].max()\n",
    "        other_model = [m for m in doc_performance.columns if m != best_model][0]\n",
    "        other_score = doc_performance.loc[doc_type, other_model]\n",
    "        improvement = best_score - other_score\n",
    "        \n",
    "        rprint(f\"  \u2022 **{doc_type}**: {best_model} leads with {best_score:.1f}% (+{improvement:.1f}% advantage)\")\n",
    "    \n",
    "    # Speed analysis\n",
    "    rprint(\"\\n[bold blue]\u26a1 Processing Speed Analysis:[/bold blue]\")\n",
    "    speed_comparison = df.groupby('model')['processing_time'].agg(['mean', 'min', 'max', 'std'])\n",
    "    for model in speed_comparison.index:\n",
    "        stats = speed_comparison.loc[model]\n",
    "        throughput = 60 / stats['mean']\n",
    "        rprint(f\"  \u2022 **{model}**: Avg {stats['mean']:.1f}s ({throughput:.1f} docs/min), Range {stats['min']:.1f}-{stats['max']:.1f}s\")\n",
    "    \n",
    "    # Consistency analysis\n",
    "    rprint(\"\\n[bold blue]\ud83d\udcc8 Consistency Analysis (Lower std dev = more consistent):[/bold blue]\")\n",
    "    consistency = df.groupby('model')['overall_accuracy'].std()\n",
    "    for model in consistency.index:\n",
    "        rprint(f\"  \u2022 **{model}**: \u00b1{consistency[model]:.1f}% standard deviation\")\n",
    "    \n",
    "    # Efficiency score (accuracy/time ratio)\n",
    "    rprint(\"\\n[bold blue]\ud83d\udca1 Efficiency Score (Accuracy per Second):[/bold blue]\")\n",
    "    df['efficiency_score'] = df['overall_accuracy'] / df['processing_time']\n",
    "    efficiency = df.groupby('model')['efficiency_score'].mean()\n",
    "    for model in efficiency.index:\n",
    "        rprint(f\"  \u2022 **{model}**: {efficiency[model]:.2f} accuracy points per second\")\n",
    "\n",
    "# Run detailed analysis\n",
    "if not combined_df.empty:\n",
    "    analyze_model_strengths(combined_df)\n",
    "else:\n",
    "    rprint(\"[red]\u274c Cannot run analysis - no data available[/red]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation for Confusion Analysis\n",
    "\n",
    "Load ground truth and prepare batch data for confusion matrix analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 16\n",
    "\n",
    "# Load ground truth as DataFrame for confusion analysis\n",
    "ground_truth_path = Path(CONFIG['ground_truth_path'])\n",
    "\n",
    "if not ground_truth_path.exists():\n",
    "    rprint(f\"[red]\u274c Ground truth file not found: {ground_truth_path}[/red]\")\n",
    "    ground_truth = pd.DataFrame()\n",
    "else:\n",
    "    # Load FULL ground truth first\n",
    "    ground_truth_full = pd.read_csv(ground_truth_path, dtype=str)\n",
    "    rprint(f\"[dim]  Loaded full ground truth: {len(ground_truth_full)} rows[/dim]\")\n",
    "\n",
    "    # Normalize column name: ground truth uses 'image_name', we need 'image_file'\n",
    "    if 'image_name' in ground_truth_full.columns and 'image_file' not in ground_truth_full.columns:\n",
    "        ground_truth_full['image_file'] = ground_truth_full['image_name']\n",
    "        rprint(f\"[dim]  Normalized: 'image_name' \u2192 'image_file'[/dim]\")\n",
    "    elif 'image_file' not in ground_truth_full.columns:\n",
    "        # Try other possible column names\n",
    "        possible_names = ['filename', 'file', 'image']\n",
    "        for col in possible_names:\n",
    "            if col in ground_truth_full.columns:\n",
    "                ground_truth_full['image_file'] = ground_truth_full[col]\n",
    "                rprint(f\"[dim]  Normalized: '{col}' \u2192 'image_file'[/dim]\")\n",
    "                break\n",
    "\n",
    "    # Add image_stem column to ground truth for matching\n",
    "    ground_truth_full['image_stem'] = ground_truth_full['image_file'].apply(lambda x: Path(x).stem)\n",
    "    ground_truth = ground_truth_full\n",
    "\n",
    "# Prepare batch dataframes with consistent naming for confusion analysis\n",
    "# Use the loaded data from cell 5\n",
    "llama_batch_df = llama_df.copy() if not llama_df.empty else pd.DataFrame()\n",
    "internvl_batch_df = internvl3_8b_df.copy() if not internvl3_8b_df.empty else pd.DataFrame()\n",
    "internvl_nq_batch_df = internvl3_2b_df.copy() if not internvl3_2b_df.empty else pd.DataFrame()\n",
    "\n",
    "# Verify data is available\n",
    "if llama_batch_df.empty:\n",
    "    rprint(\"[yellow]\u26a0\ufe0f Warning: Llama batch data not loaded. Run cell 5 first.[/yellow]\")\n",
    "else:\n",
    "    rprint(f\"[cyan]Llama batch data: {len(llama_batch_df)} rows[/cyan]\")\n",
    "    if 'document_type' in llama_batch_df.columns:\n",
    "        rprint(f\"[dim]  Predicted document types: {llama_batch_df['document_type'].value_counts().to_dict()}[/dim]\")\n",
    "\n",
    "if internvl_batch_df.empty:\n",
    "    rprint(\"[yellow]\u26a0\ufe0f Warning: InternVL3-8B batch data not loaded. Run cell 5 first.[/yellow]\")\n",
    "else:\n",
    "    rprint(f\"[cyan]InternVL3-8B batch data: {len(internvl_batch_df)} rows[/cyan]\")\n",
    "    if 'document_type' in internvl_batch_df.columns:\n",
    "        rprint(f\"[dim]  Predicted document types: {internvl_batch_df['document_type'].value_counts().to_dict()}[/dim]\")\n",
    "\n",
    "if internvl_nq_batch_df.empty:\n",
    "    rprint(\"[yellow]\u26a0\ufe0f Warning: InternVL3-2B batch data not loaded. Run cell 5 first.[/yellow]\")\n",
    "else:\n",
    "    rprint(f\"[cyan]InternVL3-2B batch data: {len(internvl_nq_batch_df)} rows[/cyan]\")\n",
    "    if 'document_type' in internvl_nq_batch_df.columns:\n",
    "        rprint(f\"[dim]  Predicted document types: {internvl_nq_batch_df['document_type'].value_counts().to_dict()}[/dim]\")\n",
    "\n",
    "if ground_truth.empty:\n",
    "    rprint(\"[yellow]\u26a0\ufe0f Warning: Ground truth not loaded. Confusion analysis will not work.[/yellow]\")\n",
    "elif 'image_file' not in ground_truth.columns:\n",
    "    rprint(\"[red]\u274c Error: Ground truth missing 'image_file' column[/red]\")\n",
    "    rprint(f\"[yellow]   Available columns: {list(ground_truth.columns)}[/yellow]\")\n",
    "else:\n",
    "    rprint(f\"[cyan]Ground truth: {len(ground_truth)} rows[/cyan]\")\n",
    "\n",
    "rprint(\"[green]\u2705 Data ready for confusion analysis[/green]\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Document Type Confusion Matrix\n",
    "\n",
    "**Classic confusion matrix showing document type classification accuracy.**\n",
    "\n",
    "Shows how well each model correctly identifies document types (INVOICE, RECEIPT, BANK_STATEMENT) and where misclassifications occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 18\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def abbreviate_doctype(name):\n",
    "    \"\"\"Shorten long document type names for visualization.\"\"\"\n",
    "    abbrev = {\n",
    "        'COMPULSORY THIRD PARTY PERSONAL INJURY INSURANCE GREEN SLIP CERTIFICATE': 'CTP_INSUR',\n",
    "        'E-TICKET ITINERARY, RECEIPT AND TAX INVOICE': 'E-TICKET',\n",
    "        'MOBILE APP SCREENSHOT': 'MOBILE_SS',\n",
    "        'PAYMENT ADVICE': 'PAYMENT',\n",
    "        'CRYPTO STATEMENT': 'CRYPTO',\n",
    "        'TAX INVOICE': 'TAX_INV',\n",
    "        'INVOICE': 'INVOICE',\n",
    "        'RECEIPT': 'RECEIPT',\n",
    "        'BANK_STATEMENT': 'BANK_STMT',\n",
    "        'NOT_FOUND': 'NOT_FOUND'\n",
    "    }\n",
    "    return abbrev.get(name, name[:12])  # Fallback: truncate to 12 chars\n",
    "\n",
    "def create_doctype_confusion_matrix(df_batch: pd.DataFrame, ground_truth_df: pd.DataFrame, model_name: str):\n",
    "    \"\"\"\n",
    "    Create document type confusion matrix using pandas crosstab.\n",
    "\n",
    "    Args:\n",
    "        df_batch: Batch results DataFrame with predicted document types\n",
    "        ground_truth_df: Ground truth DataFrame with true document types\n",
    "        model_name: Name of the model\n",
    "\n",
    "    Returns:\n",
    "        tuple: (confusion_matrix, classification_report_dict, col_labels, row_labels, y_true, y_pred)\n",
    "    \"\"\"\n",
    "    # Normalize image names (strip extensions) for matching\n",
    "    # Batch has extensions (.jpeg, .png), ground truth does not\n",
    "    df_batch['image_stem'] = df_batch['image_file'].apply(lambda x: Path(x).stem)\n",
    "    ground_truth_df['image_stem'] = ground_truth_df['image_file'].apply(lambda x: Path(x).stem)\n",
    "\n",
    "    # Merge to align predictions with ground truth using stems\n",
    "    merged = df_batch.merge(\n",
    "        ground_truth_df[['image_stem', 'DOCUMENT_TYPE']],\n",
    "        on='image_stem',\n",
    "        how='inner',\n",
    "        suffixes=('_pred', '_true')\n",
    "    )\n",
    "\n",
    "    # Get predicted and true document types\n",
    "    # CRITICAL: Use DOCUMENT_TYPE (extracted field) for predictions\n",
    "    #           Use DOCUMENT_TYPE from ground truth for true labels (3 types)\n",
    "\n",
    "    if 'DOCUMENT_TYPE_pred' in merged.columns:\n",
    "        y_pred = merged['DOCUMENT_TYPE_pred'].fillna('UNKNOWN').astype(str).str.strip().str.upper()\n",
    "    else:\n",
    "        y_pred = merged['DOCUMENT_TYPE'].fillna('UNKNOWN').astype(str).str.strip().str.upper()\n",
    "\n",
    "    if 'DOCUMENT_TYPE_true' in merged.columns:\n",
    "        y_true = merged['DOCUMENT_TYPE_true'].fillna('UNKNOWN').astype(str).str.strip().str.upper()\n",
    "    else:\n",
    "        y_true = merged['DOCUMENT_TYPE'].fillna('UNKNOWN').astype(str).str.strip().str.upper()\n",
    "\n",
    "    # Use pandas crosstab to create non-square confusion matrix\n",
    "    # This allows 3 rows (y_true types) \u00d7 N columns (y_pred types)\n",
    "    cm_df = pd.crosstab(y_true, y_pred, dropna=False)\n",
    "\n",
    "    # Convert to numpy array for compatibility with seaborn heatmap\n",
    "    cm = cm_df.values\n",
    "\n",
    "    # Get labels and abbreviate them\n",
    "    labels = [abbreviate_doctype(label) for label in cm_df.columns.tolist()]\n",
    "    row_labels = [abbreviate_doctype(label) for label in cm_df.index.tolist()]\n",
    "\n",
    "    # Compute classification report (use original labels)\n",
    "    report = classification_report(y_true, y_pred, labels=cm_df.columns.tolist(), output_dict=True, zero_division=0)\n",
    "\n",
    "    return cm, report, labels, row_labels, y_true, y_pred\n",
    "\n",
    "# Create confusion matrices for all 3 models\n",
    "rprint(\"[bold cyan]Creating document type confusion matrices for 3 models...[/bold cyan]\")\n",
    "\n",
    "llama_cm, llama_report, llama_labels, llama_row_labels, llama_y_true, llama_y_pred = create_doctype_confusion_matrix(\n",
    "    llama_batch_df, ground_truth, CONFIG['models']['llama']['display_name']\n",
    ")\n",
    "internvl_q_cm, internvl_q_report, internvl_q_labels, internvl_q_row_labels, internvl_q_y_true, internvl_q_y_pred = create_doctype_confusion_matrix(\n",
    "    internvl_batch_df, ground_truth, CONFIG['models']['internvl3_8b']['display_name']\n",
    ")\n",
    "internvl_nq_cm, internvl_nq_report, internvl_nq_labels, internvl_nq_row_labels, internvl_nq_y_true, internvl_nq_y_pred = create_doctype_confusion_matrix(\n",
    "    internvl_nq_batch_df, ground_truth, CONFIG['models']['internvl3_2b']['display_name']\n",
    ")\n",
    "\n",
    "# Plot confusion matrices in 3-panel layout\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(24, 6))\n",
    "\n",
    "# Model 1 confusion matrix\n",
    "sns.heatmap(\n",
    "    llama_cm,\n",
    "    annot=True,\n",
    "    fmt='d',\n",
    "    cmap='Blues',\n",
    "    xticklabels=llama_labels,\n",
    "    yticklabels=llama_row_labels,\n",
    "    cbar_kws={'label': 'Count'},\n",
    "    ax=ax1,\n",
    "    linewidths=0.5,\n",
    "    linecolor='gray'\n",
    ")\n",
    "ax1.set_title(CONFIG['models']['llama']['display_name'], fontsize=13, fontweight='bold')\n",
    "ax1.set_xlabel('Predicted Document Type', fontsize=11, fontweight='bold')\n",
    "ax1.set_ylabel('True Document Type', fontsize=11, fontweight='bold')\n",
    "ax1.tick_params(axis='x', rotation=45, labelsize=9)\n",
    "ax1.tick_params(axis='y', rotation=0, labelsize=9)\n",
    "\n",
    "# Model 2 confusion matrix\n",
    "sns.heatmap(\n",
    "    internvl_q_cm,\n",
    "    annot=True,\n",
    "    fmt='d',\n",
    "    cmap='Blues',\n",
    "    xticklabels=internvl_q_labels,\n",
    "    yticklabels=internvl_q_row_labels,\n",
    "    cbar_kws={'label': 'Count'},\n",
    "    ax=ax2,\n",
    "    linewidths=0.5,\n",
    "    linecolor='gray'\n",
    ")\n",
    "ax2.set_title(CONFIG['models']['internvl3_8b']['display_name'], fontsize=13, fontweight='bold')\n",
    "ax2.set_xlabel('Predicted Document Type', fontsize=11, fontweight='bold')\n",
    "ax2.set_ylabel('True Document Type', fontsize=11, fontweight='bold')\n",
    "ax2.tick_params(axis='x', rotation=45, labelsize=9)\n",
    "ax2.tick_params(axis='y', rotation=0, labelsize=9)\n",
    "\n",
    "# Model 3 confusion matrix\n",
    "sns.heatmap(\n",
    "    internvl_nq_cm,\n",
    "    annot=True,\n",
    "    fmt='d',\n",
    "    cmap='Blues',\n",
    "    xticklabels=internvl_nq_labels,\n",
    "    yticklabels=internvl_nq_row_labels,\n",
    "    cbar_kws={'label': 'Count'},\n",
    "    ax=ax3,\n",
    "    linewidths=0.5,\n",
    "    linecolor='gray'\n",
    ")\n",
    "ax3.set_title(CONFIG['models']['internvl3_2b']['display_name'], fontsize=13, fontweight='bold')\n",
    "ax3.set_xlabel('Predicted Document Type', fontsize=11, fontweight='bold')\n",
    "ax3.set_ylabel('True Document Type', fontsize=11, fontweight='bold')\n",
    "ax3.tick_params(axis='x', rotation=45, labelsize=9)\n",
    "ax3.tick_params(axis='y', rotation=0, labelsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(CONFIG['output_dir'].replace('/csv', '/visualizations') + '/doctype_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "rprint(\"[green]\u2705 3-panel document type confusion matrices created[/green]\")\n",
    "\n",
    "# Display classification reports\n",
    "rprint(f\"\\n[bold blue]{CONFIG['models']['llama']['display_name']} - Document Type Classification Report:[/bold blue]\")\n",
    "llama_report_df = pd.DataFrame(llama_report).transpose()\n",
    "display(llama_report_df)\n",
    "\n",
    "rprint(f\"\\n[bold blue]{CONFIG['models']['internvl3_8b']['display_name']} - Document Type Classification Report:[/bold blue]\")\n",
    "internvl_q_report_df = pd.DataFrame(internvl_q_report).transpose()\n",
    "display(internvl_q_report_df)\n",
    "\n",
    "rprint(f\"\\n[bold blue]{CONFIG['models']['internvl3_2b']['display_name']} - Document Type Classification Report:[/bold blue]\")\n",
    "internvl_nq_report_df = pd.DataFrame(internvl_nq_report).transpose()\n",
    "display(internvl_nq_report_df)\n",
    "\n",
    "# Save classification reports\n",
    "llama_report_df.to_csv(CONFIG['output_dir'] + '/llama_doctype_classification_report.csv')\n",
    "internvl_q_report_df.to_csv(CONFIG['output_dir'] + '/internvl3_8b_doctype_classification_report.csv')\n",
    "internvl_nq_report_df.to_csv(CONFIG['output_dir'] + '/internvl3_2b_doctype_classification_report.csv')\n",
    "\n",
    "# Summary accuracy\n",
    "llama_accuracy = (llama_y_true == llama_y_pred).sum() / len(llama_y_true) * 100\n",
    "internvl_q_accuracy = (internvl_q_y_true == internvl_q_y_pred).sum() / len(internvl_q_y_true) * 100\n",
    "internvl_nq_accuracy = (internvl_nq_y_true == internvl_nq_y_pred).sum() / len(internvl_nq_y_true) * 100\n",
    "\n",
    "rprint(\"\\n[bold blue]Document Type Classification Accuracy:[/bold blue]\")\n",
    "rprint(f\"[cyan]{CONFIG['models']['llama']['display_name']}: {llama_accuracy:.1f}%[/cyan]\")\n",
    "rprint(f\"[cyan]{CONFIG['models']['internvl3_8b']['display_name']}: {internvl_q_accuracy:.1f}%[/cyan]\")\n",
    "rprint(f\"[cyan]{CONFIG['models']['internvl3_2b']['display_name']}: {internvl_nq_accuracy:.1f}%[/cyan]\")\n",
    "\n",
    "rprint(\"[green]\u2705 Document type confusion analysis complete for all 3 models[/green]\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Field-Level Confusion Analysis\n",
    "\n",
    "**Confusion matrix showing field extraction status (correct/incorrect/not_found) for each field type.**\n",
    "\n",
    "This analysis reveals:\n",
    "- Which fields are most accurately extracted\n",
    "- Which fields are frequently incorrect vs not found\n",
    "- Model-specific strengths and weaknesses per field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 20\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define field columns (from llama_batch.ipynb)\n",
    "FIELD_COLUMNS = [\n",
    "    'DOCUMENT_TYPE', 'BUSINESS_ABN', 'SUPPLIER_NAME', 'BUSINESS_ADDRESS',\n",
    "    'PAYER_NAME', 'PAYER_ADDRESS', 'INVOICE_DATE', 'LINE_ITEM_DESCRIPTIONS',\n",
    "    'LINE_ITEM_QUANTITIES', 'LINE_ITEM_PRICES', 'LINE_ITEM_TOTAL_PRICES',\n",
    "    'IS_GST_INCLUDED', 'GST_AMOUNT', 'TOTAL_AMOUNT', 'STATEMENT_DATE_RANGE',\n",
    "    'TRANSACTION_DATES', 'TRANSACTION_AMOUNTS_PAID'\n",
    "]\n",
    "\n",
    "def create_field_confusion_data(df_batch: pd.DataFrame, ground_truth_df: pd.DataFrame, model_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create field-level confusion data showing correct/incorrect/not_found status.\n",
    "\n",
    "    Args:\n",
    "        df_batch: Batch results DataFrame with extracted field values\n",
    "        ground_truth_df: Ground truth DataFrame\n",
    "        model_name: Name of the model\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with columns: field, status (correct/incorrect/not_found), count, model\n",
    "    \"\"\"\n",
    "    confusion_data = []\n",
    "\n",
    "    for field in FIELD_COLUMNS:\n",
    "        if field not in df_batch.columns or field not in ground_truth_df.columns:\n",
    "            continue\n",
    "\n",
    "        correct_count = 0\n",
    "        incorrect_count = 0\n",
    "        not_found_count = 0\n",
    "\n",
    "        # Normalize image names (strip extensions) for matching\n",
    "        # Batch has extensions (.jpeg, .png), ground truth does not\n",
    "        if 'image_stem' not in df_batch.columns:\n",
    "            df_batch['image_stem'] = df_batch['image_file'].apply(lambda x: Path(x).stem)\n",
    "        if 'image_stem' not in ground_truth_df.columns:\n",
    "            ground_truth_df['image_stem'] = ground_truth_df['image_file'].apply(lambda x: Path(x).stem)\n",
    "\n",
    "        # Merge on image_stem to align predictions with ground truth\n",
    "        merged = df_batch.merge(\n",
    "            ground_truth_df[['image_stem', field]],\n",
    "            on='image_stem',\n",
    "            how='inner',\n",
    "            suffixes=('_pred', '_true')\n",
    "        )\n",
    "\n",
    "        pred_col = f'{field}_pred' if f'{field}_pred' in merged.columns else field\n",
    "        true_col = f'{field}_true' if f'{field}_true' in merged.columns else field\n",
    "\n",
    "        for _, row in merged.iterrows():\n",
    "            pred_val = str(row[pred_col]).strip().upper()\n",
    "            true_val = str(row[true_col]).strip().upper()\n",
    "\n",
    "            if pred_val == 'NOT_FOUND' or pred_val == 'NAN' or pred_val == '':\n",
    "                not_found_count += 1\n",
    "            elif pred_val == true_val:\n",
    "                correct_count += 1\n",
    "            else:\n",
    "                incorrect_count += 1\n",
    "\n",
    "        # Add rows for each status\n",
    "        confusion_data.append({\n",
    "            'field': field,\n",
    "            'status': 'correct',\n",
    "            'count': correct_count,\n",
    "            'model': model_name\n",
    "        })\n",
    "        confusion_data.append({\n",
    "            'field': field,\n",
    "            'status': 'incorrect',\n",
    "            'count': incorrect_count,\n",
    "            'model': model_name\n",
    "        })\n",
    "        confusion_data.append({\n",
    "            'field': field,\n",
    "            'status': 'not_found',\n",
    "            'count': not_found_count,\n",
    "            'model': model_name\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(confusion_data)\n",
    "\n",
    "# Create confusion data for all 3 models\n",
    "rprint(\"[bold cyan]Creating field-level confusion matrices...[/bold cyan]\")\n",
    "\n",
    "llama_confusion = create_field_confusion_data(llama_batch_df, ground_truth, CONFIG['models']['llama']['display_name'])\n",
    "internvl_confusion = create_field_confusion_data(internvl_batch_df, ground_truth, CONFIG['models']['internvl3_8b']['display_name'])\n",
    "internvl_nq_confusion = create_field_confusion_data(internvl_nq_batch_df, ground_truth, CONFIG['models']['internvl3_2b']['display_name'])\n",
    "\n",
    "# Combine all 3 models\n",
    "all_confusion = pd.concat([llama_confusion, internvl_confusion, internvl_nq_confusion], ignore_index=True)\n",
    "\n",
    "# Create pivot table for heatmap visualization\n",
    "def plot_confusion_heatmap(confusion_df: pd.DataFrame, model_name: str, ax):\n",
    "    \"\"\"Plot confusion matrix heatmap for a single model.\"\"\"\n",
    "    # Pivot to get fields x status matrix\n",
    "    pivot = confusion_df[confusion_df['model'] == model_name].pivot(\n",
    "        index='field',\n",
    "        columns='status',\n",
    "        values='count'\n",
    "    )\n",
    "\n",
    "    # Reorder columns: correct, incorrect, not_found\n",
    "    pivot = pivot[['correct', 'incorrect', 'not_found']]\n",
    "\n",
    "    # Sort by correct count (descending) for better visualization\n",
    "    pivot = pivot.sort_values('correct', ascending=False)\n",
    "\n",
    "    # Create heatmap\n",
    "    sns.heatmap(\n",
    "        pivot,\n",
    "        annot=True,\n",
    "        fmt='g',\n",
    "        cmap='RdYlGn_r',  # Red for high incorrect/not_found, green for high correct\n",
    "        cbar_kws={'label': 'Count'},\n",
    "        ax=ax,\n",
    "        linewidths=0.5,\n",
    "        linecolor='gray'\n",
    "    )\n",
    "\n",
    "    ax.set_title(f'{model_name} - Field Extraction Status', fontsize=13, fontweight='bold')\n",
    "    ax.set_xlabel('Extraction Status', fontsize=11, fontweight='bold')\n",
    "    ax.set_ylabel('Field Name', fontsize=11, fontweight='bold')\n",
    "    ax.tick_params(axis='y', rotation=0)\n",
    "\n",
    "# Create 3-panel heatmaps\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(24, 12))\n",
    "\n",
    "plot_confusion_heatmap(all_confusion, CONFIG['models']['llama']['display_name'], ax1)\n",
    "plot_confusion_heatmap(all_confusion, CONFIG['models']['internvl3_8b']['display_name'], ax2)\n",
    "plot_confusion_heatmap(all_confusion, CONFIG['models']['internvl3_2b']['display_name'], ax3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(CONFIG['output_dir'].replace('/csv', '/visualizations') + '/field_confusion_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "rprint(\"[green]\u2705 Field-level confusion heatmaps created (3 models)[/green]\")\n",
    "\n",
    "# Create summary statistics\n",
    "rprint(\"\\n[bold blue]Field Confusion Summary:[/bold blue]\")\n",
    "for model in [CONFIG['models'][k]['display_name'] for k in CONFIG['models']]:\n",
    "    model_data = all_confusion[all_confusion['model'] == model]\n",
    "    total = model_data['count'].sum()\n",
    "    correct = model_data[model_data['status'] == 'correct']['count'].sum()\n",
    "    incorrect = model_data[model_data['status'] == 'incorrect']['count'].sum()\n",
    "    not_found = model_data[model_data['status'] == 'not_found']['count'].sum()\n",
    "\n",
    "    rprint(f\"\\n[cyan]{model}:[/cyan]\")\n",
    "    rprint(f\"  Correct: {correct}/{total} ({correct/total*100:.1f}%)\")\n",
    "    rprint(f\"  Incorrect: {incorrect}/{total} ({incorrect/total*100:.1f}%)\")\n",
    "    rprint(f\"  Not Found: {not_found}/{total} ({not_found/total*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 Per-Field Precision, Recall, and F1 Metrics\n",
    "\n",
    "**Using `calculate_field_accuracy_f1()` to evaluate per-field extraction performance.**\n",
    "\n",
    "### Important: This is Different from Document-Level Accuracy\n",
    "\n",
    "| Metric | Source | What It Measures |\n",
    "|--------|--------|------------------|\n",
    "| **Per-Field F1** (this section) | `calculate_field_accuracy_f1()` | How well each field TYPE is extracted across ALL documents |\n",
    "| **Document Accuracy** (Panel F) | `overall_accuracy` column | How complete each DOCUMENT's extraction is |\n",
    "\n",
    "**Why they differ:** F1 averages across field types (some easy, some hard). Accuracy averages across documents (some complete, some partial).\n",
    "\n",
    "---\n",
    "\n",
    "**Metrics explained:**\n",
    "- **Precision**: Of all predicted values for this field, what % were correct?\n",
    "- **Recall**: Of all ground truth values for this field, what % were extracted?\n",
    "- **F1 Score**: Harmonic mean of precision and recall (per field type)\n",
    "- **Support**: Number of occurrences of each field in ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 22\n",
    "\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def compute_per_field_metrics(df_batch: pd.DataFrame, ground_truth_df: pd.DataFrame, model_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute precision, recall, F1 for each field using calculate_field_accuracy_f1.\n",
    "\n",
    "    Uses the SAME F1 calculation as the rest of the notebook - proper field-type-aware\n",
    "    F1 scoring with consistent TP/FP/FN counting.\n",
    "\n",
    "    Args:\n",
    "        df_batch: Batch results DataFrame with extracted field values\n",
    "        ground_truth_df: Ground truth DataFrame\n",
    "        model_name: Name of the model\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with columns: field, precision, recall, f1_score, accuracy, support, model\n",
    "    \"\"\"\n",
    "    import sys\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "    from common.evaluation_metrics import calculate_field_accuracy_f1\n",
    "\n",
    "    metrics_data = []\n",
    "\n",
    "    for field in FIELD_COLUMNS:\n",
    "        if field not in df_batch.columns or field not in ground_truth_df.columns:\n",
    "            continue\n",
    "\n",
    "        # Normalize image names (strip extensions) for matching\n",
    "        if 'image_stem' not in df_batch.columns:\n",
    "            df_batch['image_stem'] = df_batch['image_file'].apply(lambda x: Path(x).stem)\n",
    "        if 'image_stem' not in ground_truth_df.columns:\n",
    "            ground_truth_df['image_stem'] = ground_truth_df['image_file'].apply(lambda x: Path(x).stem)\n",
    "\n",
    "        # Merge on image_stem to align predictions with ground truth\n",
    "        merged = df_batch.merge(\n",
    "            ground_truth_df[['image_stem', field]],\n",
    "            on='image_stem',\n",
    "            how='inner',\n",
    "            suffixes=('_pred', '_true')\n",
    "        )\n",
    "\n",
    "        pred_col = f'{field}_pred' if f'{field}_pred' in merged.columns else field\n",
    "        true_col = f'{field}_true' if f'{field}_true' in merged.columns else field\n",
    "\n",
    "        if len(merged) == 0:\n",
    "            continue\n",
    "\n",
    "        # Collect F1 metrics from each row using calculate_field_accuracy_f1\n",
    "        f1_scores = []\n",
    "        precision_scores = []\n",
    "        recall_scores = []\n",
    "\n",
    "        for _, row in merged.iterrows():\n",
    "            pred_val = str(row[pred_col]) if pd.notna(row[pred_col]) else 'NOT_FOUND'\n",
    "            true_val = str(row[true_col]) if pd.notna(row[true_col]) else 'NOT_FOUND'\n",
    "\n",
    "            # Use calculate_field_accuracy_f1 for proper F1 metrics\n",
    "            metrics = calculate_field_accuracy_f1(pred_val, true_val, field, debug=False)\n",
    "\n",
    "            f1_scores.append(metrics['f1_score'])\n",
    "            precision_scores.append(metrics['precision'])\n",
    "            recall_scores.append(metrics['recall'])\n",
    "\n",
    "        # Average metrics across all documents for this field\n",
    "        total = len(f1_scores)\n",
    "        avg_f1 = sum(f1_scores) / total if total > 0 else 0\n",
    "        avg_precision = sum(precision_scores) / total if total > 0 else 0\n",
    "        avg_recall = sum(recall_scores) / total if total > 0 else 0\n",
    "\n",
    "        metrics_data.append({\n",
    "            'field': field,\n",
    "            'precision': avg_precision,\n",
    "            'recall': avg_recall,\n",
    "            'f1_score': avg_f1,\n",
    "            'support': total,\n",
    "            'model': model_name\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(metrics_data)\n",
    "\n",
    "# Compute metrics for all 3 models\n",
    "rprint(\"[bold cyan]Computing per-field precision/recall/F1 metrics...[/bold cyan]\")\n",
    "\n",
    "llama_metrics = compute_per_field_metrics(llama_batch_df, ground_truth, 'Llama-11B')\n",
    "internvl_metrics = compute_per_field_metrics(internvl_batch_df, ground_truth, 'InternVL3-8B')\n",
    "internvl_nq_metrics = compute_per_field_metrics(internvl_nq_batch_df, ground_truth, 'InternVL3-2B')\n",
    "\n",
    "# Combine all 3 models\n",
    "all_metrics = pd.concat([llama_metrics, internvl_metrics, internvl_nq_metrics], ignore_index=True)\n",
    "\n",
    "# Display metrics table\n",
    "rprint(\"\\n[bold blue]Per-Field Metrics Comparison:[/bold blue]\")\n",
    "display(all_metrics.sort_values(['field', 'model']))\n",
    "\n",
    "# Save to CSV\n",
    "metrics_csv_path = CONFIG['output_dir'] + '/per_field_metrics.csv'\n",
    "all_metrics.to_csv(metrics_csv_path, index=False)\n",
    "rprint(f\"[green]\u2705 Metrics saved to: {metrics_csv_path}[/green]\")\n",
    "\n",
    "# Create visualizations - 3 charts: F1, Precision, Recall\n",
    "fig, axes = plt.subplots(1, 3, figsize=(24, 10))\n",
    "\n",
    "# Colors for 3 models\n",
    "model_colors = ['#3498db', '#e74c3c', '#2ecc71']\n",
    "\n",
    "# Sort fields by average F1 score descending (highest F1 at top of chart)\n",
    "# For barh: last item in index appears at top, so sort ascending to get highest at top\n",
    "field_order = all_metrics.groupby('field')['f1_score'].mean().sort_values(ascending=True).index.tolist()\n",
    "\n",
    "# Plot 1: F1 Score (ranked by F1 descending - highest at top)\n",
    "ax1 = axes[0]\n",
    "pivot_f1 = all_metrics.pivot(index='field', columns='model', values='f1_score').reindex(field_order)\n",
    "pivot_f1.plot(kind='barh', ax=ax1, color=model_colors)\n",
    "ax1.set_title('F1 Score by Field (Ranked)', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('F1 Score', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Field', fontsize=12, fontweight='bold')\n",
    "ax1.get_legend().remove() if ax1.get_legend() else None\n",
    "ax1.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Plot 2: Precision (same field order as F1)\n",
    "ax2 = axes[1]\n",
    "pivot_precision = all_metrics.pivot(index='field', columns='model', values='precision').reindex(field_order)\n",
    "pivot_precision.plot(kind='barh', ax=ax2, color=model_colors)\n",
    "ax2.set_title('Precision by Field', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Precision', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Field', fontsize=12, fontweight='bold')\n",
    "ax2.get_legend().remove() if ax2.get_legend() else None\n",
    "ax2.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Plot 3: Recall (same field order as F1)\n",
    "ax3 = axes[2]\n",
    "pivot_recall = all_metrics.pivot(index='field', columns='model', values='recall').reindex(field_order)\n",
    "pivot_recall.plot(kind='barh', ax=ax3, color=model_colors)\n",
    "ax3.set_title('Recall by Field', fontsize=14, fontweight='bold')\n",
    "ax3.set_xlabel('Recall', fontsize=12, fontweight='bold')\n",
    "ax3.set_ylabel('Field', fontsize=12, fontweight='bold')\n",
    "ax3.get_legend().remove() if ax3.get_legend() else None\n",
    "ax3.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add main title\n",
    "fig.suptitle('Per-Field Metrics (Ranked by F1 Score)', fontsize=18, fontweight='bold', y=1.02)\n",
    "\n",
    "# Single shared legend\n",
    "handles, labels = ax1.get_legend_handles_labels()\n",
    "fig.legend(handles, labels, title='Model', loc='upper center', bbox_to_anchor=(0.5, 0.99), ncol=3, fontsize=11, title_fontsize=12)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.savefig(CONFIG['output_dir'].replace('/csv', '/visualizations') + '/per_field_metrics.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "rprint(\"[green]\u2705 Per-field metrics visualizations created (3 models)[/green]\")\n",
    "\n",
    "# Summary statistics - include both MEDIAN and MEAN F1\n",
    "rprint(\"\\n[bold blue]Model Performance Summary (Mean & Median):[/bold blue]\")\n",
    "summary_stats = all_metrics.groupby('model').agg({\n",
    "    'precision': ['mean', 'median'],\n",
    "    'recall': ['mean', 'median'],\n",
    "    'f1_score': ['mean', 'median']\n",
    "}).round(4)\n",
    "\n",
    "# Flatten column names for display\n",
    "summary_stats.columns = [f'{col[0]}_{col[1]}' for col in summary_stats.columns]\n",
    "display(summary_stats)\n",
    "\n",
    "# Also show a simplified view highlighting F1\n",
    "rprint(\"\\n[bold yellow]Per-Field F1 Summary (via calculate_field_accuracy_f1):[/bold yellow]\")\n",
    "f1_summary = all_metrics.groupby('model')['f1_score'].agg(['mean', 'median', 'std', 'min', 'max']).round(4)\n",
    "f1_summary.columns = ['Mean F1', 'Median F1', 'Std Dev', 'Min F1', 'Max F1']\n",
    "rprint('[dim]Note: These F1 scores are computed per-field, NOT from overall_accuracy column[/dim]')\n",
    "display(f1_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.7 Hallucination Analysis\n",
    "\n",
    "**Purpose**: Measure how often models invent values for fields that don't exist (NOT_FOUND in ground truth).\n",
    "\n",
    "**Key Metrics**:\n",
    "- **Hallucination Rate**: Percentage of NOT_FOUND fields where model extracted a value\n",
    "- **False Positive Rate (FPR)**: FP / (FP + TN) on NOT_FOUND fields\n",
    "- **Per-Field Hallucination**: Which fields are most hallucinated\n",
    "- **Per-Document Hallucination**: Distribution of hallucination rates across documents\n",
    "\n",
    "**Context**: From the accuracy paradox:\n",
    "- Low accuracy + high F1 \u2192 High hallucination (Llama)\n",
    "- High accuracy + low F1 \u2192 Low hallucination (InternVL3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 24\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def comprehensive_hallucination_analysis(model_df, ground_truth_df, model_name):\n",
    "    \"\"\"\n",
    "    Complete hallucination analysis for a model.\n",
    "    \n",
    "    Hallucination = Model extracts value when ground truth is NOT_FOUND\n",
    "    \"\"\"\n",
    "    \n",
    "    # Normalize image names for matching\n",
    "    if 'image_stem' not in model_df.columns:\n",
    "        model_df['image_stem'] = model_df['image_file'].apply(lambda x: Path(x).stem)\n",
    "    if 'image_stem' not in ground_truth_df.columns:\n",
    "        ground_truth_df['image_stem'] = ground_truth_df['image_file'].apply(lambda x: Path(x).stem)\n",
    "    \n",
    "    # Merge to align predictions with ground truth\n",
    "    merged = model_df.merge(\n",
    "        ground_truth_df,\n",
    "        on='image_stem',\n",
    "        how='inner',\n",
    "        suffixes=('_pred', '_true')\n",
    "    )\n",
    "    \n",
    "    total_hallucinations = 0\n",
    "    total_not_found_fields = 0\n",
    "    total_correct_not_found = 0\n",
    "    \n",
    "    field_hallucination = {}\n",
    "    doc_hallucination_scores = []\n",
    "    \n",
    "    for idx in range(len(merged)):\n",
    "        doc_hallucinations = 0\n",
    "        doc_not_found = 0\n",
    "        doc_correct_not_found = 0\n",
    "        \n",
    "        for field in FIELD_COLUMNS:\n",
    "            # SKIP excluded fields\n",
    "            if field in ['TRANSACTION_AMOUNTS_RECEIVED', 'ACCOUNT_BALANCE']:\n",
    "                continue\n",
    "            \n",
    "            # Get column names (handle suffix)\n",
    "            true_col = f'{field}_true' if f'{field}_true' in merged.columns else field\n",
    "            pred_col = f'{field}_pred' if f'{field}_pred' in merged.columns else field\n",
    "            \n",
    "            if true_col not in merged.columns or pred_col not in merged.columns:\n",
    "                continue\n",
    "            \n",
    "            gt_val = str(merged.loc[idx, true_col]).strip().upper()\n",
    "            pred_val = str(merged.loc[idx, pred_col]).strip().upper()\n",
    "            \n",
    "            if gt_val in ['NOT_FOUND', 'NAN', '']:\n",
    "                doc_not_found += 1\n",
    "                total_not_found_fields += 1\n",
    "                \n",
    "                if pred_val not in ['NOT_FOUND', 'NAN', '']:\n",
    "                    # HALLUCINATION DETECTED\n",
    "                    doc_hallucinations += 1\n",
    "                    total_hallucinations += 1\n",
    "                    \n",
    "                    # Track per-field\n",
    "                    if field not in field_hallucination:\n",
    "                        field_hallucination[field] = {'hallucinated': 0, 'not_found_total': 0}\n",
    "                    field_hallucination[field]['hallucinated'] += 1\n",
    "                    field_hallucination[field]['not_found_total'] += 1\n",
    "                else:\n",
    "                    # Correctly said NOT_FOUND\n",
    "                    doc_correct_not_found += 1\n",
    "                    total_correct_not_found += 1\n",
    "                    \n",
    "                    if field not in field_hallucination:\n",
    "                        field_hallucination[field] = {'hallucinated': 0, 'not_found_total': 0}\n",
    "                    field_hallucination[field]['not_found_total'] += 1\n",
    "        \n",
    "        # Document-level hallucination rate\n",
    "        doc_rate = doc_hallucinations / doc_not_found if doc_not_found > 0 else 0\n",
    "        doc_hallucination_scores.append({\n",
    "            'rate': doc_rate,\n",
    "            'hallucinated_count': doc_hallucinations,\n",
    "            'not_found_count': doc_not_found\n",
    "        })\n",
    "    \n",
    "    # Calculate overall rates\n",
    "    overall_rate = total_hallucinations / total_not_found_fields if total_not_found_fields > 0 else 0\n",
    "    \n",
    "    # DEBUG: Print hallucination calculations\n",
    "    rprint(f\"[yellow]DEBUG {model_name}: total_hallucinations={total_hallucinations}, total_not_found_fields={total_not_found_fields}, overall_rate={overall_rate:.2%}[/yellow]\")\n",
    "    \n",
    "    correct_not_found_rate = total_correct_not_found / total_not_found_fields if total_not_found_fields > 0 else 0\n",
    "    \n",
    "    # Per-field hallucination rates\n",
    "    field_rates = {}\n",
    "    for field, data in field_hallucination.items():\n",
    "        if data['not_found_total'] > 0:\n",
    "            field_rates[field] = {\n",
    "                'hallucination_rate': data['hallucinated'] / data['not_found_total'],\n",
    "                'hallucinated_count': data['hallucinated'],\n",
    "                'opportunities': data['not_found_total']\n",
    "            }\n",
    "    \n",
    "    return {\n",
    "        'model': model_name,\n",
    "        'overall_hallucination_rate': overall_rate,\n",
    "        'correct_not_found_rate': correct_not_found_rate,\n",
    "        'total_hallucinations': total_hallucinations,\n",
    "        'total_correct_not_found': total_correct_not_found,\n",
    "        'total_not_found_opportunities': total_not_found_fields,\n",
    "        'field_hallucination': field_rates,\n",
    "        'document_hallucination_scores': doc_hallucination_scores,\n",
    "        'mean_doc_hallucination': np.mean([d['rate'] for d in doc_hallucination_scores]),\n",
    "        'std_doc_hallucination': np.std([d['rate'] for d in doc_hallucination_scores])\n",
    "    }\n",
    "\n",
    "# Run hallucination analysis for all 3 models\n",
    "rprint(\"[bold cyan]Analyzing hallucination rates for all models...[/bold cyan]\")\n",
    "\n",
    "llama_hallucination = comprehensive_hallucination_analysis(\n",
    "    llama_df, ground_truth, 'Llama-11B'\n",
    ")\n",
    "\n",
    "internvl_hallucination = comprehensive_hallucination_analysis(\n",
    "    internvl3_8b_df, ground_truth, 'InternVL3-8B'\n",
    ")\n",
    "\n",
    "internvl_nq_hallucination = comprehensive_hallucination_analysis(\n",
    "    internvl3_2b_df, ground_truth, 'InternVL3-2B'\n",
    ")\n",
    "\n",
    "# Create summary table\n",
    "hallucination_summary = pd.DataFrame([\n",
    "    {\n",
    "        'Model': llama_hallucination['model'],\n",
    "        'Hallucination Rate': f\"{llama_hallucination['overall_hallucination_rate']:.1%}\",\n",
    "        'Correct NOT_FOUND Rate': f\"{llama_hallucination['correct_not_found_rate']:.1%}\",\n",
    "        'Total Hallucinations': llama_hallucination['total_hallucinations'],\n",
    "        'Total Opportunities': llama_hallucination['total_not_found_opportunities'],\n",
    "        'Mean Doc Hallucination': f\"{llama_hallucination['mean_doc_hallucination']:.1%}\",\n",
    "        'Std Dev': f\"{llama_hallucination['std_doc_hallucination']:.1%}\"\n",
    "    },\n",
    "    {\n",
    "        'Model': internvl_hallucination['model'],\n",
    "        'Hallucination Rate': f\"{internvl_hallucination['overall_hallucination_rate']:.1%}\",\n",
    "        'Correct NOT_FOUND Rate': f\"{internvl_hallucination['correct_not_found_rate']:.1%}\",\n",
    "        'Total Hallucinations': internvl_hallucination['total_hallucinations'],\n",
    "        'Total Opportunities': internvl_hallucination['total_not_found_opportunities'],\n",
    "        'Mean Doc Hallucination': f\"{internvl_hallucination['mean_doc_hallucination']:.1%}\",\n",
    "        'Std Dev': f\"{internvl_hallucination['std_doc_hallucination']:.1%}\"\n",
    "    },\n",
    "    {\n",
    "        'Model': internvl_nq_hallucination['model'],\n",
    "        'Hallucination Rate': f\"{internvl_nq_hallucination['overall_hallucination_rate']:.1%}\",\n",
    "        'Correct NOT_FOUND Rate': f\"{internvl_nq_hallucination['correct_not_found_rate']:.1%}\",\n",
    "        'Total Hallucinations': internvl_nq_hallucination['total_hallucinations'],\n",
    "        'Total Opportunities': internvl_nq_hallucination['total_not_found_opportunities'],\n",
    "        'Mean Doc Hallucination': f\"{internvl_nq_hallucination['mean_doc_hallucination']:.1%}\",\n",
    "        'Std Dev': f\"{internvl_nq_hallucination['std_doc_hallucination']:.1%}\"\n",
    "    }\n",
    "])\n",
    "\n",
    "rprint(\"\\n[bold blue]Hallucination Summary:[/bold blue]\")\n",
    "display(hallucination_summary)\n",
    "\n",
    "# Interpretation\n",
    "rprint(\"\\n[bold yellow]Interpretation:[/bold yellow]\")\n",
    "rprint(\"[dim]Hallucination Rate = % of NOT_FOUND fields where model invented a value[/dim]\")\n",
    "rprint(\"[dim]Correct NOT_FOUND Rate = % of NOT_FOUND fields correctly identified[/dim]\")\n",
    "rprint(\"[dim]Higher hallucination = More aggressive extraction (high recall, low accuracy)[/dim]\")\n",
    "rprint(\"[dim]Lower hallucination = More conservative extraction (low recall, high accuracy)[/dim]\")\n",
    "\n",
    "# Create visualizations\n",
    "fig = plt.figure(figsize=(24, 16))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# Plot 1: Overall Hallucination Rate Comparison\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "models = [llama_hallucination['model'], internvl_hallucination['model'], internvl_nq_hallucination['model']]\n",
    "halluc_rates = [\n",
    "    llama_hallucination['overall_hallucination_rate'],\n",
    "    internvl_hallucination['overall_hallucination_rate'],\n",
    "    internvl_nq_hallucination['overall_hallucination_rate']\n",
    "]\n",
    "colors = ['#e74c3c', '#3498db', '#2ecc71']  # Red, Blue, Green\n",
    "\n",
    "bars = ax1.bar(models, halluc_rates, color=colors, alpha=0.7)\n",
    "ax1.set_ylabel('Hallucination Rate', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Overall Hallucination Rate by Model', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylim(0, max(halluc_rates) * 1.2)\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, rate in zip(bars, halluc_rates):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{rate:.1%}',\n",
    "             ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "\n",
    "# Plot 2: Hallucination vs Correct NOT_FOUND\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "x = np.arange(len(models))\n",
    "width = 0.35\n",
    "\n",
    "halluc_counts = [\n",
    "    llama_hallucination['total_hallucinations'],\n",
    "    internvl_hallucination['total_hallucinations'],\n",
    "    internvl_nq_hallucination['total_hallucinations']\n",
    "]\n",
    "correct_counts = [\n",
    "    llama_hallucination['total_correct_not_found'],\n",
    "    internvl_hallucination['total_correct_not_found'],\n",
    "    internvl_nq_hallucination['total_correct_not_found']\n",
    "]\n",
    "\n",
    "bars1 = ax2.bar(x - width/2, halluc_counts, width, label='Hallucinated', color='#e74c3c', alpha=0.7)\n",
    "bars2 = ax2.bar(x + width/2, correct_counts, width, label='Correct NOT_FOUND', color='#2ecc71', alpha=0.7)\n",
    "\n",
    "ax2.set_ylabel('Count', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Hallucinations vs Correct NOT_FOUND', fontsize=14, fontweight='bold')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(models, rotation=15, ha='right')\n",
    "ax2.legend()\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Plot 3: Hallucination Rate vs Recall (Tradeoff)\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "\n",
    "# Calculate recall from batch results (NOT hardcoded!)\n",
    "llama_recall = (llama_df['fields_matched'] / llama_df['total_fields']).mean() if not llama_df.empty else 0\n",
    "internvl_8b_recall = (internvl3_8b_df['fields_matched'] / internvl3_8b_df['total_fields']).mean() if not internvl3_8b_df.empty else 0\n",
    "internvl_2b_recall = (internvl3_2b_df['fields_matched'] / internvl3_2b_df['total_fields']).mean() if not internvl3_2b_df.empty else 0\n",
    "\n",
    "recalls = [llama_recall, internvl_8b_recall, internvl_2b_recall]\n",
    "\n",
    "ax3.scatter(halluc_rates, recalls, s=200, c=colors, alpha=0.7, edgecolors='black', linewidth=2)\n",
    "for i, model in enumerate(models):\n",
    "    ax3.annotate(model, (halluc_rates[i], recalls[i]), \n",
    "                xytext=(10, -5), textcoords='offset points', fontsize=10, fontweight='bold')\n",
    "\n",
    "ax3.set_xlabel('Hallucination Rate', fontsize=12, fontweight='bold')\n",
    "ax3.set_ylabel('Recall', fontsize=12, fontweight='bold')\n",
    "ax3.set_title('Hallucination vs Recall Tradeoff', fontsize=14, fontweight='bold')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4, 5, 6: Per-Field Hallucination Rates (3 models)\n",
    "for idx, (halluc_data, ax_pos, model_color) in enumerate([\n",
    "    (llama_hallucination, gs[1, 0], colors[0]),\n",
    "    (internvl_hallucination, gs[1, 1], colors[1]),\n",
    "    (internvl_nq_hallucination, gs[1, 2], colors[2])\n",
    "]):\n",
    "    ax = fig.add_subplot(ax_pos)\n",
    "    \n",
    "    # Sort fields by hallucination rate\n",
    "    field_data = halluc_data['field_hallucination']\n",
    "    sorted_fields = sorted(field_data.items(), key=lambda x: x[1]['hallucination_rate'], reverse=True)\n",
    "    \n",
    "    fields = [f[0] for f in sorted_fields][:15]  # Top 15\n",
    "    rates = [f[1]['hallucination_rate'] for f in sorted_fields][:15]\n",
    "    \n",
    "    bars = ax.barh(fields, rates, color=model_color, alpha=0.7)\n",
    "    ax.set_xlabel('Hallucination Rate', fontsize=11, fontweight='bold')\n",
    "    ax.set_title(f'{halluc_data[\"model\"]} - Field Hallucination', fontsize=13, fontweight='bold')\n",
    "    ax.set_xlim(0, 1.0)\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, rate in zip(bars, rates):\n",
    "        width = bar.get_width()\n",
    "        ax.text(width, bar.get_y() + bar.get_height()/2.,\n",
    "                f'{rate:.1%}',\n",
    "                ha='left', va='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "# Plot 7, 8, 9: Document Hallucination Distribution (histograms)\n",
    "for idx, (halluc_data, ax_pos, model_color) in enumerate([\n",
    "    (llama_hallucination, gs[2, 0], colors[0]),\n",
    "    (internvl_hallucination, gs[2, 1], colors[1]),\n",
    "    (internvl_nq_hallucination, gs[2, 2], colors[2])\n",
    "]):\n",
    "    ax = fig.add_subplot(ax_pos)\n",
    "    \n",
    "    doc_rates = [d['rate'] for d in halluc_data['document_hallucination_scores']]\n",
    "    \n",
    "    ax.hist(doc_rates, bins=20, color=model_color, alpha=0.7, edgecolor='black')\n",
    "    ax.axvline(halluc_data['mean_doc_hallucination'], color='red', linestyle='--', linewidth=2, label=f'Mean: {halluc_data[\"mean_doc_hallucination\"]:.1%}')\n",
    "    ax.set_xlabel('Document Hallucination Rate', fontsize=11, fontweight='bold')\n",
    "    ax.set_ylabel('Number of Documents', fontsize=11, fontweight='bold')\n",
    "    ax.set_title(f'{halluc_data[\"model\"]} - Document Distribution', fontsize=13, fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.savefig(CONFIG['output_dir'].replace('/csv', '/visualizations') + '/hallucination_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "rprint(\"[green]\u2705 Hallucination analysis complete (9 visualizations created)[/green]\")\n",
    "rprint(f\"[green]\ud83d\udcbe Saved: visualizations/hallucination_analysis.png[/green]\")\n",
    "\n",
    "# Additional insights\n",
    "rprint(\"\\n[bold magenta]Key Insights:[/bold magenta]\")\n",
    "\n",
    "# Find highest hallucination model\n",
    "max_halluc_model = max(\n",
    "    [llama_hallucination, internvl_hallucination, internvl_nq_hallucination],\n",
    "    key=lambda x: x['overall_hallucination_rate']\n",
    ")\n",
    "min_halluc_model = min(\n",
    "    [llama_hallucination, internvl_hallucination, internvl_nq_hallucination],\n",
    "    key=lambda x: x['overall_hallucination_rate']\n",
    ")\n",
    "\n",
    "rprint(f\"[red]\ud83d\udd34 Highest hallucination: {max_halluc_model['model']} ({max_halluc_model['overall_hallucination_rate']:.1%})[/red]\")\n",
    "rprint(f\"[green]\ud83d\udfe2 Lowest hallucination: {min_halluc_model['model']} ({min_halluc_model['overall_hallucination_rate']:.1%})[/green]\")\n",
    "\n",
    "# Find most hallucinated fields across all models\n",
    "all_field_halluc = {}\n",
    "for halluc_data in [llama_hallucination, internvl_hallucination, internvl_nq_hallucination]:\n",
    "    for field, data in halluc_data['field_hallucination'].items():\n",
    "        if field not in all_field_halluc:\n",
    "            all_field_halluc[field] = []\n",
    "        all_field_halluc[field].append(data['hallucination_rate'])\n",
    "\n",
    "avg_field_halluc = {field: np.mean(rates) for field, rates in all_field_halluc.items()}\n",
    "most_hallucinated = sorted(avg_field_halluc.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "\n",
    "rprint(\"\\n[yellow]Most Hallucinated Fields (avg across models):[/yellow]\")\n",
    "for field, rate in most_hallucinated:\n",
    "    rprint(f\"  \u2022 {field}: {rate:.1%}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 25\n",
    "\n",
    "# Auto-generate MODEL_COMPARISON_REPORT.md from notebook results\n",
    "\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "def generate_model_comparison_report(llama_df, internvl3_8b_df, internvl3_2b_df, field_performance_df=None, hallucination_summary=None):\n",
    "    \"\"\"Generate comprehensive MODEL_COMPARISON_REPORT.md from notebook analysis.\n",
    "\n",
    "    Args:\n",
    "        llama_df: DataFrame with Llama batch results\n",
    "        internvl3_8b_df: DataFrame with InternVL3-8B batch results\n",
    "        internvl3_2b_df: DataFrame with InternVL3-2B batch results\n",
    "        field_performance_df: Optional DataFrame with per-field performance comparison\n",
    "        hallucination_summary: Optional DataFrame with hallucination analysis results\n",
    "    \"\"\"\n",
    "\n",
    "    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    # Calculate dataset statistics from actual data\n",
    "    all_dfs = [df for df in [llama_df, internvl3_8b_df, internvl3_2b_df] if not df.empty]\n",
    "\n",
    "    if all_dfs:\n",
    "        # Use first non-empty dataframe for dataset statistics\n",
    "        sample_df = all_dfs[0]\n",
    "        total_docs = len(sample_df)\n",
    "\n",
    "        # Count document types if column exists\n",
    "        if 'document_type' in sample_df.columns:\n",
    "            doctype_counts = sample_df['document_type'].value_counts()\n",
    "            bank_count = doctype_counts.get('bank_statement', 0)\n",
    "            invoice_count = doctype_counts.get('invoice', 0)\n",
    "            receipt_count = doctype_counts.get('receipt', 0)\n",
    "        else:\n",
    "            bank_count = invoice_count = receipt_count = 0\n",
    "\n",
    "        # Calculate total fields from field performance or use total_fields column\n",
    "        if field_performance_df is not None and not field_performance_df.empty:\n",
    "            num_fields = len(field_performance_df)\n",
    "        elif 'total_fields' in sample_df.columns:\n",
    "            num_fields = int(sample_df['total_fields'].iloc[0])\n",
    "        else:\n",
    "            num_fields = 0\n",
    "    else:\n",
    "        total_docs = bank_count = invoice_count = receipt_count = num_fields = 0\n",
    "\n",
    "    def calculate_metrics(df, model_name):\n",
    "        \"\"\"Calculate precision, recall, F1 from batch results DataFrame.\"\"\"\n",
    "        if df.empty:\n",
    "            rprint(f\"[yellow]\u26a0 {model_name} dataframe is empty[/yellow]\")\n",
    "            return 0, 0, 0, 0, 0, 0\n",
    "\n",
    "        try:\n",
    "            # These columns exist in the CSV files\n",
    "            accuracy = df['overall_accuracy'].mean()\n",
    "            speed = df['processing_time'].median()\n",
    "\n",
    "            # Calculate precision, recall, F1 from field matching data\n",
    "            # Precision = fields_matched / fields_extracted (when fields_extracted > 0)\n",
    "            # Recall = fields_matched / total_fields\n",
    "            valid_rows = df[df['fields_extracted'] > 0].copy()\n",
    "\n",
    "            if len(valid_rows) > 0:\n",
    "                valid_rows['precision'] = valid_rows['fields_matched'] / valid_rows['fields_extracted']\n",
    "                valid_rows['recall'] = valid_rows['fields_matched'] / valid_rows['total_fields']\n",
    "\n",
    "                precision = valid_rows['precision'].mean()\n",
    "                recall = valid_rows['recall'].mean()\n",
    "\n",
    "                # F1 = 2 * precision * recall / (precision + recall)\n",
    "                if precision + recall > 0:\n",
    "                    f1 = 2 * precision * recall / (precision + recall)\n",
    "                else:\n",
    "                    f1 = 0\n",
    "            else:\n",
    "                precision = recall = f1 = 0\n",
    "\n",
    "            rprint(f\"[green]\u2705 {model_name} metrics calculated successfully[/green]\")\n",
    "            rprint(f\"[cyan]  F1={f1:.4f}, P={precision:.4f}, R={recall:.4f}, Acc={accuracy:.2f}%, Speed={speed:.1f}s[/cyan]\")\n",
    "\n",
    "            # Calculate median F1 per document\n",
    "            valid_rows['f1_doc'] = 2 * (valid_rows['precision'] * valid_rows['recall']) / (valid_rows['precision'] + valid_rows['recall'] + 1e-10)\n",
    "            f1_median = valid_rows['f1_doc'].median()\n",
    "\n",
    "            return f1, f1_median, precision, recall, accuracy, speed\n",
    "\n",
    "        except Exception as e:\n",
    "            rprint(f\"[red]\u274c Error calculating {model_name} metrics: {e}[/red]\")\n",
    "            if not df.empty:\n",
    "                rprint(f\"[cyan]Available columns: {df.columns.tolist()}[/cyan]\")\n",
    "                rprint(f\"[cyan]DataFrame shape: {df.shape}[/cyan]\")\n",
    "            return 0, 0, 0, 0, 0, 0\n",
    "\n",
    "    # Extract overall metrics for each model\n",
    "    llama_overall_f1, llama_median_f1, llama_overall_precision, llama_overall_recall, llama_overall_accuracy, llama_speed = \\\n",
    "        calculate_metrics(llama_df, 'Llama-3.2-Vision-11B')\n",
    "\n",
    "    internvl_8b_overall_f1, internvl_8b_median_f1, internvl_8b_overall_precision, internvl_8b_overall_recall, internvl_8b_overall_accuracy, internvl_8b_speed = \\\n",
    "        calculate_metrics(internvl3_8b_df, 'InternVL3-8B')\n",
    "\n",
    "    internvl_2b_overall_f1, internvl_2b_median_f1, internvl_2b_overall_precision, internvl_2b_overall_recall, internvl_2b_overall_accuracy, internvl_2b_speed = \\\n",
    "        calculate_metrics(internvl3_2b_df, 'InternVL3-2B')\n",
    "\n",
    "    # Build markdown report\n",
    "    report = f\"\"\"# Model Comparison Analysis Report\n",
    "\n",
    "**Auto-Generated from Notebook**: {timestamp}\n",
    "**Source**: `model_comparison_reporter.ipynb`\n",
    "**Dataset**: {total_docs} documents ({bank_count} bank statements, {invoice_count} invoices, {receipt_count} receipts)\n",
    "**Evaluation Fields**: {num_fields} business document fields\n",
    "\n",
    "---\n",
    "\n",
    "## Executive Summary\n",
    "\n",
    "### Overall Performance Metrics\n",
    "\n",
    "| Model | Mean F1 | Median F1 | Precision | Recall | Accuracy | Median Speed | Throughput |\n",
    "|-------|---------|-----------|-----------|--------|----------|--------------|------------|\n",
    "| **Llama-3.2-Vision-11B** | {llama_overall_f1:.4f} | {llama_median_f1:.4f} | {llama_overall_precision:.4f} | {llama_overall_recall:.4f} | {llama_overall_accuracy:.2f}% | {llama_speed:.1f}s | {60/llama_speed if llama_speed > 0 else 0:.1f} docs/min |\n",
    "| **InternVL3-8B** | {internvl_8b_overall_f1:.4f} | {internvl_8b_median_f1:.4f} | {internvl_8b_overall_precision:.4f} | {internvl_8b_overall_recall:.4f} | {internvl_8b_overall_accuracy:.2f}% | {internvl_8b_speed:.1f}s | {60/internvl_8b_speed if internvl_8b_speed > 0 else 0:.1f} docs/min |\n",
    "| **InternVL3-2B** | {internvl_2b_overall_f1:.4f} | {internvl_2b_median_f1:.4f} | {internvl_2b_overall_precision:.4f} | {internvl_2b_overall_recall:.4f} | {internvl_2b_overall_accuracy:.2f}% | {internvl_2b_speed:.1f}s | {60/internvl_2b_speed if internvl_2b_speed > 0 else 0:.1f} docs/min |\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "**Winner (F1 Score)**: {'Llama-3.2-Vision-11B' if llama_overall_f1 > max(internvl_8b_overall_f1, internvl_2b_overall_f1) else ('InternVL3-8B' if internvl_8b_overall_f1 > internvl_2b_overall_f1 else 'InternVL3-2B')}\n",
    "\n",
    "**Highest Precision**: {'Llama-3.2-Vision-11B' if llama_overall_precision > max(internvl_8b_overall_precision, internvl_2b_overall_precision) else ('InternVL3-8B' if internvl_8b_overall_precision > internvl_2b_overall_precision else 'InternVL3-2B')} ({max(llama_overall_precision, internvl_8b_overall_precision, internvl_2b_overall_precision):.4f})\n",
    "\n",
    "**Highest Recall**: {'Llama-3.2-Vision-11B' if llama_overall_recall > max(internvl_8b_overall_recall, internvl_2b_overall_recall) else ('InternVL3-8B' if internvl_8b_overall_recall > internvl_2b_overall_recall else 'InternVL3-2B')} ({max(llama_overall_recall, internvl_8b_overall_recall, internvl_2b_overall_recall):.4f})\n",
    "\n",
    "**Fastest**: {'Llama-3.2-Vision-11B' if llama_speed > 0 and llama_speed < min(filter(lambda x: x > 0, [internvl_8b_speed, internvl_2b_speed])) else ('InternVL3-8B' if internvl_8b_speed > 0 and internvl_8b_speed < internvl_2b_speed else 'InternVL3-2B')} ({min(filter(lambda x: x > 0, [llama_speed, internvl_8b_speed, internvl_2b_speed])):.1f}s)\n",
    "\n",
    "---\n",
    "\n",
    "## Visualizations\n",
    "\n",
    "All visualizations are generated in `output/visualizations/`:\n",
    "\n",
    "### 1. Executive Performance Dashboard\n",
    "![Executive Dashboard](output/visualizations/executive_comparison.png)\n",
    "\n",
    "**6-panel comprehensive view:**\n",
    "- Overall accuracy distribution (box plots)\n",
    "- Processing speed comparison\n",
    "- Accuracy by document type\n",
    "- Processing time by document type\n",
    "- Efficiency analysis (accuracy vs speed)\n",
    "- Performance summary table\n",
    "\n",
    "### 2. Document Type Classification\n",
    "![Document Type Confusion](output/visualizations/doctype_confusion_matrix.png)\n",
    "\n",
    "**3-model confusion matrices** showing classification performance for:\n",
    "- Bank Statements ({bank_count} docs, {bank_count/total_docs*100 if total_docs > 0 else 0:.1f}%)\n",
    "- Invoices ({invoice_count} docs, {invoice_count/total_docs*100 if total_docs > 0 else 0:.1f}%)\n",
    "- Receipts ({receipt_count} docs, {receipt_count/total_docs*100 if total_docs > 0 else 0:.1f}%)\n",
    "\n",
    "### 3. Field Extraction Status\n",
    "![Field Confusion Heatmap](output/visualizations/field_confusion_heatmap.png)\n",
    "\n",
    "**Breakdown of extraction status:**\n",
    "- Correct extractions (matches ground truth)\n",
    "- Incorrect extractions (wrong value)\n",
    "- Not Found (field not extracted)\n",
    "\n",
    "### 4. Per-Field Metrics\n",
    "![Per-Field Metrics](output/visualizations/per_field_metrics.png)\n",
    "\n",
    "**4-panel analysis:**\n",
    "- F1 Score by field\n",
    "- Precision by field\n",
    "- Recall by field\n",
    "- Accuracy by field\n",
    "\n",
    "\n",
    "### 5. Field-Level F1 Analysis\n",
    "![Field-Level Accuracy](output/visualizations/field_level_accuracy.png)\n",
    "\n",
    "**3-panel comprehensive view:**\n",
    "- Field accuracy comparison (horizontal bar chart across all models)\n",
    "- Field accuracy heatmap (color-coded performance matrix)\n",
    "- Model specialization distribution (fields where each model performs best)\n",
    "\n",
    "### 6. Hallucination Analysis\n",
    "![Hallucination Analysis](output/visualizations/hallucination_analysis.png)\n",
    "\n",
    "**9-panel breakdown:**\n",
    "- Overall hallucination rates\n",
    "- Hallucinations vs correct NOT_FOUND\n",
    "- Hallucination-recall tradeoff\n",
    "- Per-field hallucination (3 models)\n",
    "- Document-level distribution (3 models)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    # Add hallucination rates if available\n",
    "    if hallucination_summary is not None and not hallucination_summary.empty:\n",
    "        report += \"\"\"### Hallucination Rates\n",
    "\n",
    "| Model | Hallucination Rate | Correct NOT_FOUND Rate | Total Hallucinations |\n",
    "|-------|-------------------|------------------------|----------------------|\n",
    "\"\"\"\n",
    "        for _, row in hallucination_summary.iterrows():\n",
    "            model_name = row['Model']\n",
    "            hall_rate = row['Hallucination Rate']\n",
    "            not_found_rate = row['Correct NOT_FOUND Rate']\n",
    "            total_hall = row['Total Hallucinations']\n",
    "            report += f\"| **{model_name}** | {hall_rate} | {not_found_rate} | {total_hall} |\\n\"\n",
    "\n",
    "        report += \"\"\"\n",
    "**Interpretation:**\n",
    "- **Hallucination Rate**: % of NOT_FOUND fields where model invented a value\n",
    "- **Correct NOT_FOUND Rate**: % of NOT_FOUND fields correctly identified as absent\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    report += \"\"\"---\n",
    "\n",
    "## Per-Field Performance Summary\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    # Add per-field performance if available\n",
    "    if field_performance_df is not None and not field_performance_df.empty:\n",
    "        try:\n",
    "            report += \"### Field-Level F1 by Model\\n\\n\"\n",
    "            report += \"| Field | Llama-11B | InternVL3-8B | InternVL3-2B | Best Model | Best Score |\\n\"\n",
    "            report += \"|-------|-----------|--------------|--------------|------------|------------|\\n\"\n",
    "\n",
    "            for field in field_performance_df.index:\n",
    "                llama_acc = field_performance_df.loc[field, 'Llama-3.2-Vision-11B'] if 'Llama-3.2-Vision-11B' in field_performance_df.columns else 0\n",
    "                internvl_8b_acc = field_performance_df.loc[field, 'InternVL3-8B'] if 'InternVL3-8B' in field_performance_df.columns else 0\n",
    "                internvl_2b_acc = field_performance_df.loc[field, 'InternVL3-2B'] if 'InternVL3-2B' in field_performance_df.columns else 0\n",
    "                best_model = field_performance_df.loc[field, 'best_model'] if 'best_model' in field_performance_df.columns else 'N/A'\n",
    "                best_score = field_performance_df.loc[field, 'best_score'] if 'best_score' in field_performance_df.columns else 0\n",
    "\n",
    "                # Convert to percentages (field_performance stores as decimals 0-1)\n",
    "                report += f\"| {field} | {llama_acc*100:.1f}% | {internvl_8b_acc*100:.1f}% | {internvl_2b_acc*100:.1f}% | {best_model} | {best_score*100:.1f}% |\\n\"\n",
    "\n",
    "            report += \"\\n\"\n",
    "        except Exception as e:\n",
    "            report += f\"_(Error generating per-field metrics table: {e})_\\n\\n\"\n",
    "            rprint(f\"[yellow]\u26a0 Error accessing field_performance: {e}[/yellow]\")\n",
    "    else:\n",
    "        report += \"_(Per-field performance data unavailable)_\\n\\n\"\n",
    "\n",
    "    # Add model specialization summary\n",
    "    report += \"\"\"---\n",
    "\n",
    "## Model Specialization\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    if field_performance_df is not None and not field_performance_df.empty and 'best_model' in field_performance_df.columns:\n",
    "        try:\n",
    "            llama_fields = (field_performance_df['best_model'] == 'Llama-3.2-Vision-11B').sum()\n",
    "            internvl_8b_fields = (field_performance_df['best_model'] == 'InternVL3-8B').sum()\n",
    "            internvl_2b_fields = (field_performance_df['best_model'] == 'InternVL3-2B').sum()\n",
    "            total_fields = len(field_performance_df)\n",
    "\n",
    "            report += f\"\"\"### Fields Where Each Model Performs Best\n",
    "\n",
    "| Model | Best-Performing Fields | Percentage | Count |\n",
    "|-------|----------------------|------------|-------|\n",
    "| **Llama-3.2-Vision-11B** | {llama_fields/total_fields*100:.1f}% | {llama_fields}/{total_fields} | {'PRIMARY' if llama_fields > max(internvl_8b_fields, internvl_2b_fields) else 'SECONDARY'} |\n",
    "| **InternVL3-8B** | {internvl_8b_fields/total_fields*100:.1f}% | {internvl_8b_fields}/{total_fields} | {'PRIMARY' if internvl_8b_fields > max(llama_fields, internvl_2b_fields) else 'SECONDARY'} |\n",
    "| **InternVL3-2B** | {internvl_2b_fields/total_fields*100:.1f}% | {internvl_2b_fields}/{total_fields} | {'PRIMARY' if internvl_2b_fields > max(llama_fields, internvl_8b_fields) else 'NO SPECIALIZATION' if internvl_2b_fields == 0 else 'SECONDARY'} |\n",
    "\n",
    "\"\"\"\n",
    "        except Exception as e:\n",
    "            report += f\"_(Error calculating specialization metrics: {e})_\\n\\n\"\n",
    "            rprint(f\"[yellow]\u26a0 Error calculating specialization: {e}[/yellow]\")\n",
    "    else:\n",
    "        report += \"_(Model specialization data unavailable)_\\n\\n\"\n",
    "\n",
    "    # Footer\n",
    "    report += f\"\"\"---\n",
    "\n",
    "## Deployment Recommendations\n",
    "\n",
    "Based on the analysis above:\n",
    "\n",
    "### 1. Document Classification (PRIMARY)\n",
    "Use the model with highest document type classification accuracy for initial routing and categorization.\n",
    "\n",
    "### 2. Field Extraction Strategy (SECONDARY)\n",
    "Consider an ensemble approach leveraging each model's field specialization:\n",
    "- Use model-specific strengths for particular fields\n",
    "- Implement confidence-based routing\n",
    "- Fall back to best overall performer for general fields\n",
    "\n",
    "### 3. High-Volume Processing\n",
    "Balance speed vs quality based on throughput requirements:\n",
    "- **Fastest processing**: {'Llama-3.2-Vision-11B' if llama_speed > 0 and llama_speed < min(filter(lambda x: x > 0, [internvl_8b_speed, internvl_2b_speed])) else ('InternVL3-8B' if internvl_8b_speed > 0 and internvl_8b_speed < internvl_2b_speed else 'InternVL3-2B')} (~{min(filter(lambda x: x > 0, [llama_speed, internvl_8b_speed, internvl_2b_speed])):.1f}s/doc)\n",
    "- **Best accuracy**: {'Llama-3.2-Vision-11B' if llama_overall_accuracy > max(internvl_8b_overall_accuracy, internvl_2b_overall_accuracy) else ('InternVL3-8B' if internvl_8b_overall_accuracy > internvl_2b_overall_accuracy else 'InternVL3-2B')} ({max(llama_overall_accuracy, internvl_8b_overall_accuracy, internvl_2b_overall_accuracy):.2f}% overall)\n",
    "- **Best balance**: Consider throughput constraints and acceptable accuracy threshold\n",
    "\n",
    "### 4. Hallucination Sensitivity: Critical Business Decision\n",
    "\n",
    "#### Understanding Hallucination in Document Extraction\n",
    "\n",
    "**Hallucination** = Model extracts a value when ground truth is `NOT_FOUND`\n",
    "\n",
    "**Example:**\n",
    "- Ground Truth: `BUSINESS_ABN = NOT_FOUND` (field doesn't exist in document)\n",
    "- Model Output: `BUSINESS_ABN = \"12345678901\"` \u2190 **HALLUCINATION** (invented data)\n",
    "\n",
    "#### The Tradeoff: Precision vs Recall\n",
    "\n",
    "**High Precision (Low Hallucination)**\n",
    "- Model only extracts when very confident\n",
    "- **Few false positives** (hallucinations)\n",
    "- **Many false negatives** (missed fields)\n",
    "- Conservative approach: \"Only extract what you're sure about\"\n",
    "\n",
    "**High Recall (Risk of Hallucination)**\n",
    "- Model extracts aggressively to catch all fields\n",
    "- **Few false negatives** (catches most fields)\n",
    "- **More false positives** (risk of hallucinations)\n",
    "- Aggressive approach: \"Extract everything, review later\"\n",
    "\n",
    "#### Relationship to Metrics\n",
    "\n",
    "```\n",
    "Precision = Correct Extractions / All Extractions\n",
    "  \u2192 High precision = Low hallucination rate\n",
    "  \u2192 Model is cautious, only extracts when confident\n",
    "\n",
    "Recall = Correct Extractions / All Fields That Should Be Extracted\n",
    "  \u2192 High recall = Catches more fields\n",
    "  \u2192 Risk: May hallucinate to achieve higher coverage\n",
    "\n",
    "Hallucination Rate = Hallucinations / NOT_FOUND Opportunities\n",
    "  \u2192 Direct measure of false positive risk\n",
    "  \u2192 Critical for production reliability\n",
    "```\n",
    "\n",
    "#### Model Selection Guide Based on Use Case\n",
    "\n",
    "**Choose HIGH PRECISION Model ({'Llama-3.2-Vision-11B' if llama_overall_precision > max(internvl_8b_overall_precision, internvl_2b_overall_precision) else ('InternVL3-8B' if internvl_8b_overall_precision > internvl_2b_overall_precision else 'InternVL3-2B')}: {max(llama_overall_precision, internvl_8b_overall_precision, internvl_2b_overall_precision):.2%}) if:**\n",
    "- \u2705 Processing financial/regulatory data (invoices, tax documents)\n",
    "- \u2705 Automated processing with no human review\n",
    "- \u2705 **False data is worse than missing data**\n",
    "- \u2705 You can afford to manually review `NOT_FOUND` fields\n",
    "- \u2705 Compliance and audit requirements\n",
    "- \u2705 Low tolerance for hallucinations\n",
    "\n",
    "**Example**: Bank reconciliation where a hallucinated amount could cause financial errors.\n",
    "\n",
    "**Choose HIGH RECALL Model ({'Llama-3.2-Vision-11B' if llama_overall_recall > max(internvl_8b_overall_recall, internvl_2b_overall_recall) else ('InternVL3-8B' if internvl_8b_overall_recall > internvl_2b_overall_recall else 'InternVL3-2B')}: {max(llama_overall_recall, internvl_8b_overall_recall, internvl_2b_overall_recall):.2%}) if:**\n",
    "- \u2705 Comprehensive data capture is critical\n",
    "- \u2705 Human review pipeline can catch errors\n",
    "- \u2705 **Missing data is worse than wrong data**\n",
    "- \u2705 Initial screening/discovery use case\n",
    "- \u2705 Maximizing field coverage is priority\n",
    "- \u2705 Can tolerate some false positives\n",
    "\n",
    "**Example**: Legal document discovery where missing a field could have serious consequences.\n",
    "\n",
    "**Choose BALANCED Model (for high-volume processing) if:**\n",
    "- \u2705 High-volume processing requirements\n",
    "- \u2705 Need reasonable precision and recall\n",
    "- \u2705 Speed is a critical factor\n",
    "- \u2705 Standard business document processing\n",
    "\n",
    "**Example**: Receipt processing for expense management with human spot-checking.\n",
    "\n",
    "#### Your Model Performance Profile\n",
    "\n",
    "Based on the analysis:\n",
    "\n",
    "| Model | Precision | Recall | F1 | Best For |\n",
    "|-------|-----------|--------|----|----|\n",
    "| **Llama-3.2-Vision-11B** | {llama_overall_precision:.2%} | {llama_overall_recall:.2%} | {llama_overall_f1:.4f} | {'\ud83c\udfc6 Best Precision' if llama_overall_precision > max(internvl_8b_overall_precision, internvl_2b_overall_precision) else ''}{'\ud83c\udfc6 Best Recall' if llama_overall_recall > max(internvl_8b_overall_recall, internvl_2b_overall_recall) else ''}{'\ud83c\udfc6 Best F1' if llama_overall_f1 > max(internvl_8b_overall_f1, internvl_2b_overall_f1) else ''} |\n",
    "| **InternVL3-8B** | {internvl_8b_overall_precision:.2%} | {internvl_8b_overall_recall:.2%} | {internvl_8b_overall_f1:.4f} | {'\ud83c\udfc6 Best Precision' if internvl_8b_overall_precision > max(llama_overall_precision, internvl_2b_overall_precision) else ''}{'\ud83c\udfc6 Best Recall' if internvl_8b_overall_recall > max(llama_overall_recall, internvl_2b_overall_recall) else ''}{'\ud83c\udfc6 Best F1' if internvl_8b_overall_f1 > max(llama_overall_f1, internvl_2b_overall_f1) else ''} |\n",
    "| **InternVL3-2B** | {internvl_2b_overall_precision:.2%} | {internvl_2b_overall_recall:.2%} | {internvl_2b_overall_f1:.4f} | {'\ud83c\udfc6 Best Precision' if internvl_2b_overall_precision > max(llama_overall_precision, internvl_8b_overall_precision) else ''}{'\ud83c\udfc6 Best Recall' if internvl_2b_overall_recall > max(llama_overall_recall, internvl_8b_overall_recall) else ''}{'\ud83c\udfc6 Best F1' if internvl_2b_overall_f1 > max(llama_overall_f1, internvl_8b_overall_f1) else ''} |\n",
    "\n",
    "**Key Insights:**\n",
    "- **Precision Leader**: {'Llama-3.2-Vision-11B' if llama_overall_precision > max(internvl_8b_overall_precision, internvl_2b_overall_precision) else ('InternVL3-8B' if internvl_8b_overall_precision > internvl_2b_overall_precision else 'InternVL3-2B')} ({max(llama_overall_precision, internvl_8b_overall_precision, internvl_2b_overall_precision):.2%})\n",
    "- **Recall Leader**: {'Llama-3.2-Vision-11B' if llama_overall_recall > max(internvl_8b_overall_recall, internvl_2b_overall_recall) else ('InternVL3-8B' if internvl_8b_overall_recall > internvl_2b_overall_recall else 'InternVL3-2B')} ({max(llama_overall_recall, internvl_8b_overall_recall, internvl_2b_overall_recall):.2%})\n",
    "- **F1 Leader**: {'Llama-3.2-Vision-11B' if llama_overall_f1 > max(internvl_8b_overall_f1, internvl_2b_overall_f1) else ('InternVL3-8B' if internvl_8b_overall_f1 > internvl_2b_overall_f1 else 'InternVL3-2B')} ({max(llama_overall_f1, internvl_8b_overall_f1, internvl_2b_overall_f1):.4f})\n",
    "- **Speed vs Accuracy Tradeoff**: Consider throughput requirements against quality needs\n",
    "\n",
    "#### Efficiency Analysis\n",
    "\n",
    "**Performance Efficiency Score** = Accuracy \u00d7 Throughput (docs/min)\n",
    "\n",
    "| Model | Avg Accuracy | Avg Speed | Throughput | Efficiency Score |\n",
    "|-------|--------------|-----------|------------|------------------|\n",
    "| **Llama-3.2-Vision-11B** | {llama_overall_accuracy:.2f}% | {llama_speed:.1f}s | {60/llama_speed if llama_speed > 0 else 0:.1f} docs/min | {(llama_overall_accuracy * (60/llama_speed if llama_speed > 0 else 0)):.1f} |\n",
    "| **InternVL3-8B** | {internvl_8b_overall_accuracy:.2f}% | {internvl_8b_speed:.1f}s | {60/internvl_8b_speed if internvl_8b_speed > 0 else 0:.1f} docs/min | {(internvl_8b_overall_accuracy * (60/internvl_8b_speed if internvl_8b_speed > 0 else 0)):.1f} |\n",
    "| **InternVL3-2B** | {internvl_2b_overall_accuracy:.2f}% | {internvl_2b_speed:.1f}s | {60/internvl_2b_speed if internvl_2b_speed > 0 else 0:.1f} docs/min | {(internvl_2b_overall_accuracy * (60/internvl_2b_speed if internvl_2b_speed > 0 else 0)):.1f} |\n",
    "\n",
    "**Highest Efficiency**: {'Llama-3.2-Vision-11B' if (llama_overall_accuracy * (60/llama_speed if llama_speed > 0 else 0)) > max((internvl_8b_overall_accuracy * (60/internvl_8b_speed if internvl_8b_speed > 0 else 0)), (internvl_2b_overall_accuracy * (60/internvl_2b_speed if internvl_2b_speed > 0 else 0))) else ('InternVL3-8B' if (internvl_8b_overall_accuracy * (60/internvl_8b_speed if internvl_8b_speed > 0 else 0)) > (internvl_2b_overall_accuracy * (60/internvl_2b_speed if internvl_2b_speed > 0 else 0)) else 'InternVL3-2B')}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    # Calculate document-type specific recommendations\n",
    "    doc_type_section = \"\"\n",
    "    all_dfs_list = [(llama_df, 'Llama-3.2-Vision-11B'), \n",
    "                    (internvl3_8b_df, 'InternVL3-8B'),\n",
    "                    (internvl3_2b_df, 'InternVL3-2B')]\n",
    "    \n",
    "    # Combine all dataframes with model labels\n",
    "    combined_for_doctype = []\n",
    "    for df, model_name in all_dfs_list:\n",
    "        if not df.empty and 'document_type' in df.columns:\n",
    "            temp_df = df.copy()\n",
    "            temp_df['model'] = model_name\n",
    "            combined_for_doctype.append(temp_df)\n",
    "    \n",
    "    if combined_for_doctype:\n",
    "        import pandas as pd\n",
    "        all_data = pd.concat(combined_for_doctype, ignore_index=True)\n",
    "        doc_performance = all_data.groupby(['document_type', 'model'])['overall_accuracy'].mean().unstack()\n",
    "        \n",
    "        doc_type_section += \"\\n#### Document-Type Specific Recommendations\\n\\n\"\n",
    "        doc_type_section += \"**Best Model by Document Type:**\\n\\n\"\n",
    "        for doc_type in doc_performance.index:\n",
    "            best_model = doc_performance.loc[doc_type].idxmax()\n",
    "            best_score = doc_performance.loc[doc_type].max()\n",
    "            doc_type_section += f\"- **{doc_type.replace('_', ' ').title()}**: {best_model} ({best_score:.2f}% accuracy)\\n\"\n",
    "    \n",
    "    report += doc_type_section\n",
    "    \n",
    "    # Field Performance Insights\n",
    "    if field_performance_df is not None and not field_performance_df.empty:\n",
    "        field_insights = \"\\n#### Field Performance Insights\\n\\n\"\n",
    "        \n",
    "        # Get model columns\n",
    "        model_cols = [col for col in field_performance_df.columns if any(x in col for x in ['Llama', 'InternVL'])]\n",
    "        \n",
    "        if len(model_cols) >= 2:\n",
    "            # Calculate performance spread\n",
    "            field_perf_copy = field_performance_df.copy()\n",
    "            field_perf_copy['max_diff'] = field_perf_copy[model_cols].max(axis=1) - field_perf_copy[model_cols].min(axis=1)\n",
    "            \n",
    "            # Significant differences (>20% spread)\n",
    "            significant_diff = field_perf_copy[field_perf_copy['max_diff'] > 0.2].nlargest(5, 'max_diff')\n",
    "            \n",
    "            if not significant_diff.empty:\n",
    "                field_insights += \"**Fields with Significant Model Performance Differences (>20% spread):**\\n\\n\"\n",
    "                for field_name in significant_diff.index:\n",
    "                    if 'best_model' in field_perf_copy.columns and 'best_score' in field_perf_copy.columns:\n",
    "                        best_model = field_perf_copy.loc[field_name, 'best_model']\n",
    "                        best_score = field_perf_copy.loc[field_name, 'best_score']\n",
    "                        worst_score = field_perf_copy.loc[field_name, model_cols].min()\n",
    "                        advantage = best_score - worst_score\n",
    "                        field_insights += f\"- **{field_name}**: Use {best_model} ({best_score*100:.0f}% vs {worst_score*100:.0f}%, +{advantage*100:.0f}% advantage)\\n\"\n",
    "            \n",
    "            # Calculate average accuracy per field\n",
    "            field_perf_copy['avg_f1'] = field_perf_copy[model_cols].mean(axis=1)\n",
    "            \n",
    "            # Problematic fields (<50% avg accuracy)\n",
    "            problematic = field_perf_copy[field_perf_copy['avg_f1'] < 0.5]\n",
    "            \n",
    "            if not problematic.empty:\n",
    "                field_insights += \"\\n**\u26a0\ufe0f Problematic Fields Requiring Attention (<50% avg accuracy):**\\n\\n\"\n",
    "                for field_name in problematic.head(5).index:\n",
    "                    avg_acc = field_perf_copy.loc[field_name, 'avg_f1']\n",
    "                    field_insights += f\"- **{field_name}**: {avg_acc*100:.0f}% average accuracy - Consider prompt optimization or additional fine tuning\\n\"\n",
    "        \n",
    "        report += field_insights\n",
    "    \n",
    "\n",
    "    report += \"\"\"\n",
    "\n",
    "#### Production Deployment Strategy\n",
    "\n",
    "**Phase 1: Initial Deployment**\n",
    "1. Choose model based on your primary business constraint:\n",
    "   - **Financial accuracy** \u2192 Highest precision model\n",
    "   - **Data completeness** \u2192 Highest recall model\n",
    "   - **High volume** \u2192 Fastest processing model\n",
    "\n",
    "**Phase 2: Monitoring**\n",
    "2. Track in production:\n",
    "   - Hallucination rate on `NOT_FOUND` fields\n",
    "   - Manual review costs (false negatives)\n",
    "   - Error correction costs (false positives)\n",
    "\n",
    "**Phase 3: Optimization**\n",
    "3. Adjust strategy based on actual costs:\n",
    "   - If missing fields cost more \u2192 Switch to higher recall model\n",
    "   - If hallucinations cost more \u2192 Switch to higher precision model\n",
    "   - If volume is issue \u2192 Consider faster model with review pipeline\n",
    "\n",
    "**Phase 4: Advanced Optimization**\n",
    "4. Consider ensemble approaches:\n",
    "   - Use high-precision model for critical fields (amounts, dates)\n",
    "   - Use high-recall model for descriptive fields (line items)\n",
    "   - Route by document confidence scores\n",
    "\n",
    "---\n",
    "\n",
    "## Related Documentation\n",
    "\n",
    "- [FIELD_COMPARISON.md](FIELD_COMPARISON.md) - Detailed field-by-field analysis\n",
    "- [ACCURACY_PARADOX_EXPLAINED.md](ACCURACY_PARADOX_EXPLAINED.md) - Why Accuracy > F1 for extraction\n",
    "- [HALLUCINATION_ANALYSIS.md](HALLUCINATION_ANALYSIS.md) - Hallucination analysis methodology\n",
    "\n",
    "---\n",
    "\n",
    "**Report Auto-Generated**: {timestamp}\n",
    "**Source Notebook**: `model_comparison_reporter.ipynb`\n",
    "**Visualizations**: `output/visualizations/`\n",
    "**Next Update**: Re-run notebook to refresh all metrics and visualizations\n",
    "    \"\"\"\n",
    "\n",
    "    # Write to file\n",
    "    output_path = Path('MODEL_COMPARISON_REPORT.md')\n",
    "    with output_path.open('w', encoding='utf-8') as f:\n",
    "        f.write(report)\n",
    "\n",
    "    rprint(f\"[green]\u2705 MODEL_COMPARISON_REPORT.md updated ({len(report)} characters)[/green]\")\n",
    "    rprint(f\"[cyan]\ud83d\udcc4 Report generated from notebook execution at {timestamp}[/cyan]\")\n",
    "\n",
    "    return output_path\n",
    "\n",
    "# Execute report generation\n",
    "try:\n",
    "    # Pass dataframes, field_performance, and hallucination_summary as parameters\n",
    "    fp = field_performance if 'field_performance' in dir() else None\n",
    "    hs = hallucination_summary if 'hallucination_summary' in dir() else None\n",
    "    report_path = generate_model_comparison_report(\n",
    "        llama_df,\n",
    "        internvl3_8b_df,\n",
    "        internvl3_2b_df,\n",
    "        fp,\n",
    "        hs\n",
    "    )\n",
    "    rprint(f\"[bold green]\u2705 Auto-generation complete: {report_path}[/bold green]\")\n",
    "except Exception as e:\n",
    "    rprint(f\"[bold red]\u274c Report generation failed: {e}[/bold red]\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Export Notebook to HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXPORT NOTEBOOK TO HTML\n",
    "# =============================================================================\n",
    "# SAVE the notebook first (Ctrl+S) before running this cell!\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import json as json_lib\n",
    "\n",
    "NOTEBOOK_NAME = \"LMMPOC_comparison.ipynb\"\n",
    "EXPORT_DIR = Path(\"./exports\")\n",
    "EXPORT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "base_name = NOTEBOOK_NAME.replace(\".ipynb\", \"\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"EXPORTING NOTEBOOK WITH OUTPUTS\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nMake sure you SAVED the notebook before running this cell!\")\n",
    "\n",
    "print(f\"\\nReading {NOTEBOOK_NAME}...\")\n",
    "with open(NOTEBOOK_NAME, 'r', encoding='utf-8') as f:\n",
    "    notebook = json_lib.load(f)\n",
    "\n",
    "print(\"Converting to HTML...\")\n",
    "\n",
    "html_parts = [f\"\"\"<!DOCTYPE html>\n",
    "<html><head><meta charset=\"utf-8\"><title>Model Comparison: Llama vs InternVL3</title>\n",
    "<style>\n",
    "body {{ font-family: sans-serif; max-width: 1400px; margin: 0 auto; padding: 20px; }}\n",
    ".cell {{ background: white; margin: 10px 0; padding: 15px; border-radius: 8px; box-shadow: 0 1px 3px rgba(0,0,0,0.1); }}\n",
    ".input {{ background: #f8f8f8; padding: 10px; font-family: monospace; font-size: 12px; overflow-x: auto; white-space: pre-wrap; }}\n",
    ".output img {{ max-width: 100%; }}\n",
    ".output pre {{ background: #f0f0f0; padding: 10px; font-family: monospace; font-size: 11px; overflow-x: auto; }}\n",
    "h1, h2, h3 {{ color: #333; }}\n",
    "</style></head><body>\n",
    "<h1>Model Comparison: Llama vs InternVL3 vs InternVL3.5</h1>\n",
    "<p>Generated: {timestamp}</p><hr>\"\"\"]\n",
    "\n",
    "for cell in notebook['cells']:\n",
    "    source = ''.join(cell.get('source', []))\n",
    "    if cell['cell_type'] == 'markdown':\n",
    "        html_parts.append(f'<div class=\"cell\">{source}</div>')\n",
    "    elif cell['cell_type'] == 'code':\n",
    "        html_parts.append(f'<div class=\"cell\"><div class=\"input\"><pre>{source}</pre></div>')\n",
    "        for output in cell.get('outputs', []):\n",
    "            if output.get('output_type') == 'stream':\n",
    "                html_parts.append(f'<div class=\"output\"><pre>{\"\".join(output.get(\"text\", []))}</pre></div>')\n",
    "            elif 'data' in output:\n",
    "                data = output['data']\n",
    "                if 'image/png' in data:\n",
    "                    html_parts.append(f'<div class=\"output\"><img src=\"data:image/png;base64,{data[\"image/png\"]}\"></div>')\n",
    "                elif 'text/html' in data:\n",
    "                    html_parts.append(f'<div class=\"output\">{\"\".join(data[\"text/html\"])}</div>')\n",
    "                elif 'text/plain' in data:\n",
    "                    html_parts.append(f'<div class=\"output\"><pre>{\"\".join(data[\"text/plain\"])}</pre></div>')\n",
    "        html_parts.append('</div>')\n",
    "\n",
    "html_parts.append('</body></html>')\n",
    "\n",
    "html_output = EXPORT_DIR / f\"{base_name}_{timestamp}.html\"\n",
    "with open(html_output, 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(html_parts))\n",
    "\n",
    "print(f\"\\nHTML exported: {html_output}\")\n",
    "print(f\"Size: {html_output.stat().st_size / 1024:.1f} KB\")\n",
    "print(\"\\nTo create PDF: Open HTML in browser -> Print -> Save as PDF\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "du",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}