{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "project-root-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 0 - Project Root Detection\n",
    "# Allows running from subdirectories like notebooks_v2/\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "def find_project_root(marker_files=('common/__init__.py', 'CLAUDE.md', 'prompts')):\n",
    "    \"\"\"Find project root by looking for marker files, searching up from cwd.\"\"\"\n",
    "    current = Path().absolute()\n",
    "    for parent in [current] + list(current.parents):\n",
    "        for marker in marker_files:\n",
    "            if (parent / marker).exists():\n",
    "                return parent\n",
    "    return current  # Fallback to cwd if no marker found\n",
    "\n",
    "# Detect project root (works from any subdirectory)\n",
    "PROJECT_ROOT = find_project_root()\n",
    "print(f\"üìÇ Current directory: {Path().absolute()}\")\n",
    "print(f\"üè† Project root: {PROJECT_ROOT}\")\n",
    "\n",
    "# Ensure the project root is in the Python path\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "    print(f\"‚úÖ Added {PROJECT_ROOT} to sys.path\")\n",
    "\n",
    "print(\"‚úÖ Project root detection complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash\n",
    "\n",
    "# # Keep only most recent CSV files for each model pattern\n",
    "\n",
    "# cd output/csv 2>/dev/null || { echo \"output/csv directory not found, skipping cleanup\"; exit 0; }\n",
    "\n",
    "# for pattern in \"llama_batch_results_*.csv\" \"internvl3_batch_results_*.csv\" \"internvl3_2b_batch_results_*.csv\"; do\n",
    "#     files=($(ls -t $pattern 2>/dev/null))\n",
    "    \n",
    "#     if [ ${#files[@]} -gt 1 ]; then\n",
    "#         echo \"Pattern: $pattern\"\n",
    "#         echo \"  Keeping: ${files[0]}\"\n",
    "#         echo \"  Deleting $((${#files[@]}-1)) older files...\"\n",
    "        \n",
    "#         for ((i=1; i<${#files[@]}; i++)); do\n",
    "#             rm \"${files[$i]}\"\n",
    "#             echo \"    ‚úó Deleted: ${files[$i]}\"\n",
    "#         done\n",
    "#     elif [ ${#files[@]} -eq 1 ]; then\n",
    "#         echo \"Pattern: $pattern - 1 file (kept)\"\n",
    "#     else\n",
    "#         echo \"Pattern: $pattern - no files found\"\n",
    "#     fi\n",
    "# done\n",
    "\n",
    "# cd ../..\n",
    "# echo \"‚úÖ Cleanup complete\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison V2: Llama vs InternVL3 vs InternVL3.5\n",
    "\n",
    "**V2 Batch Processing Performance Analysis - Sophisticated Bank Statement Extraction**\n",
    "\n",
    "---\n",
    "\n",
    "This notebook provides a comprehensive comparison of three vision-language models using the V2 batch processing notebooks with sophisticated multi-turn bank statement extraction:\n",
    "\n",
    "| Model | Notebook | Key Features |\n",
    "|-------|----------|--------------|\n",
    "| **Llama-3.2-Vision-11B** | `llama_batch_v2.ipynb` | Multi-turn bank extraction, 8-bit quantization |\n",
    "| **InternVL3-8B** | `ivl3_8b_batch_v2.ipynb` | Multi-turn bank extraction, L40 bfloat16 |\n",
    "| **InternVL3-2B** | `ivl3_5_8b_batch_v2.ipynb` | Multi-turn bank extraction, H200 bfloat16, Cascade RL |\n",
    "\n",
    "## V2 Features:\n",
    "- **Sophisticated bank statement extraction** using UnifiedBankExtractor\n",
    "- Turn 0: Header detection (identifies actual column names)\n",
    "- Turn 1: Adaptive extraction with structure-dynamic prompts\n",
    "- Automatic strategy selection: BALANCE_DESCRIPTION, AMOUNT_DESCRIPTION, etc.\n",
    "- Optional balance-based mathematical correction\n",
    "\n",
    "## Key Business Questions Addressed:\n",
    "1. **Accuracy**: Which model extracts information more reliably?\n",
    "2. **Bank Statement Performance**: How does V2 multi-turn extraction improve bank statement accuracy?\n",
    "3. **Speed**: Which model processes documents faster?\n",
    "4. **Document Type Performance**: How do models perform on invoices, receipts, and bank statements?\n",
    "5. **Production Readiness**: Which model is recommended for deployment?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2\n",
    "\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "from rich import print as rprint\n",
    "from rich.console import Console\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "console = Console()\n",
    "\n",
    "# Set professional styling for executive presentation\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION - Edit these values to configure the comparison\n",
    "# ============================================================================\n",
    "\n",
    "# Base path for data files\n",
    "# base_data_path = '/home/jovyan/nfs_share/tod/LMM_POC'\n",
    "base_data_path = '/Users/tod/Desktop/LMM_POC'\n",
    "\n",
    "CONFIG = {\n",
    "    # Path settings\n",
    "    'output_dir': f'{base_data_path}/output/csv',\n",
    "    'ground_truth_path': f'{base_data_path}/evaluation_data/synthetic/ground_truth_synthetic.csv',\n",
    "    \n",
    "    # ============================================================================\n",
    "    # MODEL CONFIGURATIONS - Explicit file paths and display names\n",
    "    # ============================================================================\n",
    "    # Edit these to specify exact files and customize display names\n",
    "    # Set 'file' to None to skip a model\n",
    "    # ============================================================================\n",
    "    'models': {\n",
    "        'llama': {\n",
    "            'file': 'llama_batch_results_20251210_003155.csv',\n",
    "            'display_name': 'Llama-11B',\n",
    "            'color': '#3498db'  # Blue\n",
    "        },\n",
    "        'internvl3_8b': {\n",
    "            'file': 'internvl3_batch_results_20251210_005902.csv',\n",
    "            'display_name': 'IVL3-8B',\n",
    "            'color': '#e74c3c'  # Red\n",
    "        },\n",
    "        'internvl3_2b': {\n",
    "            'file': 'internvl3_2b_batch_results_20251210_013149.csv',\n",
    "            'display_name': 'IVL3-2B',\n",
    "            'color': '#2ecc71'  # Green\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    # Visualization settings\n",
    "    'figure_size': (16, 10),\n",
    "    'dpi': 300,\n",
    "    'save_format': 'png'\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# Display configuration summary\n",
    "# ============================================================================\n",
    "print(\"‚úÖ Model comparison V2 configuration loaded\")\n",
    "print(f\"üìÅ Output directory: {CONFIG['output_dir']}\")\n",
    "print(f\"üìä Ground truth: {CONFIG['ground_truth_path']}\")\n",
    "print(f\"\\nüîß Model configurations:\")\n",
    "for key, model_cfg in CONFIG['models'].items():\n",
    "    if model_cfg['file']:\n",
    "        print(f\"   ‚Ä¢ {model_cfg['display_name']}: {model_cfg['file']}\")\n",
    "    else:\n",
    "        print(f\"   ‚Ä¢ {key}: (skipped - file is None)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4\n",
    "\n",
    "def load_model_results(output_dir: str, filename: str, display_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load results from an explicit CSV file path.\n",
    "    \n",
    "    Args:\n",
    "        output_dir: Directory containing the file\n",
    "        filename: Exact filename to load\n",
    "        display_name: Display name for the model (used in 'model' column)\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with results, or empty DataFrame if file not found\n",
    "    \"\"\"\n",
    "    from pathlib import Path\n",
    "    \n",
    "    file_path = Path(output_dir) / filename\n",
    "    \n",
    "    if not file_path.exists():\n",
    "        rprint(f\"[red]‚ùå File not found: {file_path}[/red]\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    rprint(f\"[green]‚úÖ Loading {display_name} from: {filename}[/green]\")\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Add model column with display name\n",
    "        df['model'] = display_name\n",
    "        \n",
    "        # Ensure required columns exist\n",
    "        required_cols = ['overall_accuracy', 'processing_time', 'document_type']\n",
    "        missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "        \n",
    "        if missing_cols:\n",
    "            rprint(f\"[yellow]‚ö†Ô∏è Missing columns in {display_name} data: {missing_cols}[/yellow]\")\n",
    "            for col in missing_cols:\n",
    "                if col == 'overall_accuracy':\n",
    "                    df[col] = 0.0\n",
    "                elif col == 'processing_time':\n",
    "                    df[col] = 1.0\n",
    "                elif col == 'document_type':\n",
    "                    df[col] = 'unknown'\n",
    "        \n",
    "        rprint(f\"[dim]   Loaded {len(df)} records[/dim]\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        rprint(f\"[red]‚ùå Error loading {display_name}: {e}[/red]\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# ============================================================================\n",
    "# Load all configured models\n",
    "# ============================================================================\n",
    "rprint(\"[bold blue]üìä Loading Model Results[/bold blue]\\n\")\n",
    "\n",
    "model_dataframes = {}\n",
    "for model_key, model_cfg in CONFIG['models'].items():\n",
    "    if model_cfg['file']:\n",
    "        df = load_model_results(\n",
    "            CONFIG['output_dir'],\n",
    "            model_cfg['file'],\n",
    "            model_cfg['display_name']\n",
    "        )\n",
    "        if not df.empty:\n",
    "            model_dataframes[model_key] = df\n",
    "\n",
    "# Create individual named dataframes for backward compatibility\n",
    "llama_batch_df = model_dataframes.get('llama', pd.DataFrame())\n",
    "internvl_batch_df = model_dataframes.get('internvl3_8b', pd.DataFrame())\n",
    "internvl_nq_batch_df = model_dataframes.get('internvl3_2b', pd.DataFrame())\n",
    "\n",
    "# Combine all loaded models\n",
    "if model_dataframes:\n",
    "    combined_df = pd.concat(model_dataframes.values(), ignore_index=True)\n",
    "    print(f\"\\n‚úÖ Combined {len(combined_df)} total records from {len(model_dataframes)} models\")\n",
    "else:\n",
    "    combined_df = pd.DataFrame()\n",
    "    rprint(\"[red]‚ùå No model data loaded![/red]\")\n",
    "\n",
    "# Build color map from config\n",
    "MODEL_COLORS = {\n",
    "    cfg['display_name']: cfg['color'] \n",
    "    for cfg in CONFIG['models'].values() \n",
    "    if cfg['file']\n",
    "}\n",
    "\n",
    "# Backward compatibility aliases (some cells use these names)\n",
    "llama_df = llama_batch_df\n",
    "internvl3_8b_df = internvl_batch_df\n",
    "internvl3_2b_df = internvl_nq_batch_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5\n",
    "\n",
    "# Display loaded data summary (data was loaded in previous cell)\n",
    "rprint(\"[bold blue]üìä V2 Model Performance Data Summary[/bold blue]\")\n",
    "rprint(\"[cyan]V2 notebooks use sophisticated multi-turn bank statement extraction[/cyan]\\n\")\n",
    "\n",
    "if not combined_df.empty:\n",
    "    # Show what models were loaded\n",
    "    loaded_models = combined_df['model'].unique()\n",
    "    rprint(f\"[green]‚úÖ {len(loaded_models)} models loaded with {len(combined_df)} total records[/green]\")\n",
    "    \n",
    "    # Show record counts per model\n",
    "    for model in loaded_models:\n",
    "        count = len(combined_df[combined_df['model'] == model])\n",
    "        rprint(f\"   ‚Ä¢ {model}: {count} records\")\n",
    "    \n",
    "    # Show column summary\n",
    "    rprint(f\"\\n[dim]Columns: {', '.join(combined_df.columns[:10])}{'...' if len(combined_df.columns) > 10 else ''}[/dim]\")\n",
    "else:\n",
    "    rprint(\"[red]‚ùå No data loaded[/red]\")\n",
    "    rprint(\"\\n[yellow]üí° Check CONFIG['models'] in the configuration cell above[/yellow]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6\n",
    "\n",
    "def generate_executive_summary(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate executive summary statistics from combined model data.\n",
    "    \n",
    "    Args:\n",
    "        df: Combined DataFrame with all model results\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with summary statistics per model (including both mean and median)\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    summary_stats = df.groupby('model').agg({\n",
    "        'overall_accuracy': ['mean', 'median', 'std', 'min', 'max'],\n",
    "        'processing_time': ['mean', 'median', 'std', 'min', 'max'],\n",
    "        'fields_extracted': 'mean',\n",
    "        'fields_matched': 'mean'\n",
    "    }).round(2)\n",
    "    \n",
    "    # Flatten column names\n",
    "    summary_stats.columns = ['_'.join(col).strip() for col in summary_stats.columns.values]\n",
    "    \n",
    "    # Calculate throughput (documents per minute) using median time\n",
    "    summary_stats['throughput_docs_per_min'] = (60 / summary_stats['processing_time_median']).round(2)\n",
    "    \n",
    "    # Calculate efficiency score (accuracy √ó throughput) using mean accuracy\n",
    "    summary_stats['efficiency_score'] = (summary_stats['overall_accuracy_mean'] * summary_stats['throughput_docs_per_min']).round(2)\n",
    "    \n",
    "    return summary_stats.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7\n",
    "\n",
    "def create_executive_dashboard(df: pd.DataFrame, save_path: str = None):\n",
    "    \"\"\"\n",
    "    Create comprehensive 6-panel executive dashboard comparing model performance.\n",
    "    \n",
    "    Args:\n",
    "        df: Combined DataFrame with results from all models\n",
    "        save_path: Optional path to save the visualization\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        rprint(\"[red]‚ùå Cannot create dashboard - no data available[/red]\")\n",
    "        return\n",
    "    \n",
    "    # Define model order and colors for consistency\n",
    "    # Derive model names and colors from CONFIG for true configurability\n",
    "    model_order = [CONFIG['models'][key]['display_name'] for key in CONFIG['models']]\n",
    "    fixed_colors = {CONFIG['models'][key]['display_name']: CONFIG['models'][key]['color'] \n",
    "                    for key in CONFIG['models']}\n",
    "    \n",
    "    # Filter to only available models and maintain order\n",
    "    available_models = df['model'].unique()\n",
    "    models = [model for model in model_order if model in available_models]\n",
    "    model_colors = [fixed_colors[model] for model in models if model in fixed_colors]\n",
    "    \n",
    "    # Create figure with 6 subplots\n",
    "    fig = plt.figure(figsize=(20, 12))\n",
    "    gs = fig.add_gridspec(3, 2, height_ratios=[1, 1, 1], hspace=0.45, wspace=0.35)\n",
    "    \n",
    "    # 1. Overall Accuracy Comparison (Box Plot)\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    df_plot = df[df['model'].isin(models)]\n",
    "    \n",
    "    box_parts = ax1.boxplot(\n",
    "        [df_plot[df_plot['model'] == model]['overall_accuracy'].values for model in models],\n",
    "        labels=models,\n",
    "        patch_artist=True,\n",
    "        showmeans=True\n",
    "    )\n",
    "    \n",
    "    for patch, color in zip(box_parts['boxes'], model_colors):\n",
    "        patch.set_facecolor(color)\n",
    "        patch.set_alpha(0.7)\n",
    "    \n",
    "    ax1.set_ylabel('Overall Accuracy (%)', fontsize=12)\n",
    "    ax1.set_title('Overall Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "    ax1.grid(True, alpha=0.3, axis='y')\n",
    "    ax1.set_xticklabels(models, rotation=15, ha='right')\n",
    "    \n",
    "    # 2. Processing Speed Comparison (Box Plot)\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    \n",
    "    box_parts = ax2.boxplot(\n",
    "        [df_plot[df_plot['model'] == model]['processing_time'].values for model in models],\n",
    "        labels=models,\n",
    "        patch_artist=True,\n",
    "        showmeans=True\n",
    "    )\n",
    "    \n",
    "    for patch, color in zip(box_parts['boxes'], model_colors):\n",
    "        patch.set_facecolor(color)\n",
    "        patch.set_alpha(0.7)\n",
    "    \n",
    "    ax2.set_ylabel('Processing Time (seconds)', fontsize=12)\n",
    "    ax2.set_title('Processing Speed Comparison', fontsize=14, fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "    ax2.set_xticklabels(models, rotation=15, ha='right')\n",
    "    \n",
    "    # 3. Average Accuracy by Document Type (Grouped Bar Chart)\n",
    "    ax3 = fig.add_subplot(gs[1, 0])\n",
    "    \n",
    "    doc_accuracy = df_plot.groupby(['document_type', 'model'])['overall_accuracy'].mean().unstack()\n",
    "    doc_accuracy = doc_accuracy[models]  # Ensure correct order\n",
    "    \n",
    "    x = np.arange(len(doc_accuracy.index))\n",
    "    width = 0.8 / len(models)\n",
    "    \n",
    "    for idx, model in enumerate(models):\n",
    "        offset = (idx - len(models)/2 + 0.5) * width\n",
    "        ax3.bar(x + offset, doc_accuracy[model], width, \n",
    "               label=model, color=fixed_colors[model], alpha=0.8)\n",
    "    \n",
    "    ax3.set_ylabel('Average Accuracy (%)', fontsize=12)\n",
    "    ax3.set_xlabel('Document Type', fontsize=12)\n",
    "    ax3.set_title('Average Accuracy by Document Type', fontsize=14, fontweight='bold')\n",
    "    ax3.set_xticks(x)\n",
    "    ax3.set_xticklabels(doc_accuracy.index, rotation=15, ha='right')\n",
    "    ax3.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # 4. Average Processing Time by Document Type (Grouped Bar Chart)\n",
    "    ax4 = fig.add_subplot(gs[1, 1])\n",
    "    \n",
    "    doc_time = df_plot.groupby(['document_type', 'model'])['processing_time'].mean().unstack()\n",
    "    doc_time = doc_time[models]  # Ensure correct order\n",
    "    \n",
    "    for idx, model in enumerate(models):\n",
    "        offset = (idx - len(models)/2 + 0.5) * width\n",
    "        ax4.bar(x + offset, doc_time[model], width,\n",
    "               label=model, color=fixed_colors[model], alpha=0.8)\n",
    "    \n",
    "    ax4.set_ylabel('Average Processing Time (s)', fontsize=12)\n",
    "    ax4.set_xlabel('Document Type', fontsize=12)\n",
    "    ax4.set_title('Average Processing Time by Document Type', fontsize=14, fontweight='bold')\n",
    "    ax4.set_xticks(x)\n",
    "    ax4.set_xticklabels(doc_time.index, rotation=15, ha='right')\n",
    "    ax4.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # 5. Efficiency Analysis: Accuracy vs Processing Time (Scatter Plot)\n",
    "    ax5 = fig.add_subplot(gs[2, 0])\n",
    "    \n",
    "    for model in models:\n",
    "        model_data = df_plot[df_plot['model'] == model]\n",
    "        ax5.scatter(model_data['processing_time'], model_data['overall_accuracy'],\n",
    "                   label=model, color=fixed_colors[model], alpha=0.6, s=100)\n",
    "    \n",
    "    ax5.set_xlabel('Processing Time (seconds)', fontsize=12)\n",
    "    ax5.set_ylabel('Overall Accuracy (%)', fontsize=12)\n",
    "    ax5.set_title('Efficiency Analysis: Accuracy vs Processing Time', fontsize=14, fontweight='bold')\n",
    "    ax5.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add diagonal efficiency lines\n",
    "    ax5.axline((0, 0), slope=10, color='gray', linestyle='--', alpha=0.3, label='10% per second')\n",
    "    ax5.axline((0, 0), slope=5, color='gray', linestyle='--', alpha=0.3, label='5% per second')\n",
    "    \n",
    "    # 6. Performance Summary Table\n",
    "    ax6 = fig.add_subplot(gs[2, 1])\n",
    "    ax6.axis('off')\n",
    "    \n",
    "    # Generate summary statistics\n",
    "    summary_data = []\n",
    "    for model in models:\n",
    "        model_data = df_plot[df_plot['model'] == model]\n",
    "        summary_data.append([\n",
    "            model,\n",
    "            f\"{model_data['overall_accuracy'].mean():.1f}%\",\n",
    "            f\"{model_data['processing_time'].mean():.1f}s\",\n",
    "            f\"{(60 / model_data['processing_time'].mean()):.1f}\",\n",
    "            f\"{model_data['overall_accuracy'].std():.1f}%\"\n",
    "        ])\n",
    "    \n",
    "    table = ax6.table(cellText=summary_data,\n",
    "                     colLabels=['Model', 'Avg Acc', 'Avg Time', 'Docs/min', 'Std Dev'],\n",
    "                     cellLoc='center',\n",
    "                     loc='center',\n",
    "                     colWidths=[0.35, 0.15, 0.15, 0.15, 0.15])\n",
    "    \n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    table.scale(1, 2)\n",
    "    \n",
    "    # Color header row\n",
    "    for i in range(5):\n",
    "        table[(0, i)].set_facecolor('#34495E')\n",
    "        table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "    \n",
    "    # Color data rows with model colors\n",
    "    for i, model in enumerate(models):\n",
    "        if model in fixed_colors:\n",
    "            table[(i+1, 0)].set_facecolor(fixed_colors[model])\n",
    "            table[(i+1, 0)].set_text_props(color='white', weight='bold')\n",
    "            table[(i+1, 0)].set_alpha(0.7)\n",
    "    \n",
    "    ax6.set_title('Performance Summary Table', fontsize=14, fontweight='bold', pad=20)\n",
    "    \n",
    "    # Overall title\n",
    "    fig.suptitle('MODEL PERFORMANCE COMPARISON\\nBusiness Document Information Extraction',\n",
    "                fontsize=20, fontweight='bold', y=0.98)\n",
    "    \n",
    "    # Single shared legend below the title\n",
    "    handles, labels = ax3.get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc='upper center', bbox_to_anchor=(0.5, 0.93), \n",
    "               ncol=len(models), fontsize=11, framealpha=0.95)\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.90])  # Make room for title and legend\n",
    "    \n",
    "    # Save the visualization\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=CONFIG['dpi'], bbox_inches='tight')\n",
    "        rprint(f\"[green]‚úÖ Executive dashboard saved to: {save_path}[/green]\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Create the executive dashboard\n",
    "dashboard_timestamp = None\n",
    "if not combined_df.empty:\n",
    "    rprint(\"\\n[bold blue]üìä Creating Executive Dashboard[/bold blue]\")\n",
    "    \n",
    "    # Use CONFIG base path for absolute path\n",
    "    output_path = Path(CONFIG['output_dir']).parent / \"visualizations\" / \"executive_comparison.png\"\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    create_executive_dashboard(combined_df, str(output_path))\n",
    "    # Timestamp removed - always overwrite with latest version\n",
    "    \n",
    "    rprint(f\"[green]‚úÖ Executive dashboard created successfully[/green]\")\n",
    "else:\n",
    "    rprint(\"[red]‚ùå Cannot create executive dashboard - no data available[/red]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8\n",
    "\n",
    "def extract_field_level_accuracy_from_csv(output_dir: str, filename: str, model_name: str, ground_truth_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract field-level accuracy by comparing CSV batch results against ground truth.\n",
    "    \n",
    "    This function:\n",
    "    1. Loads batch results CSV (which has extracted field values)\n",
    "    2. Loads ground truth CSV\n",
    "    3. Compares field-by-field using the SAME evaluation logic as llama_batch.ipynb\n",
    "    4. Returns per-field accuracy WITHOUT re-running document-type filtering\n",
    "    \n",
    "    Args:\n",
    "        output_dir: Directory containing batch results CSV files\n",
    "        filename: Explicit CSV filename to load  \n",
    "        model_name: Name of the model\n",
    "        ground_truth_path: Path to ground truth CSV\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with columns: model, field_name, accuracy, correct_count, total_count\n",
    "    \"\"\"\n",
    "    from pathlib import Path\n",
    "    import sys\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))  # Use PROJECT_ROOT from cell 0\n",
    "    \n",
    "    from common.evaluation_metrics import load_ground_truth, calculate_field_accuracy\n",
    "    \n",
    "    file_path = Path(output_dir) / filename\n",
    "    \n",
    "    if not file_path.exists():\n",
    "        rprint(f\"[yellow]‚ö†Ô∏è Batch results not found: {filename}[/yellow]\")\n",
    "        rprint(f\"[dim]   Expected at: {file_path}[/dim]\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    latest_file = str(file_path)\n",
    "    rprint(f\"[cyan]üìÑ Evaluating {model_name} from: {filename}[/cyan]\")\n",
    "    \n",
    "    try:\n",
    "        # Load batch results CSV\n",
    "        batch_df = pd.read_csv(latest_file)\n",
    "        total_images = len(batch_df)\n",
    "        rprint(f\"[dim]  DEBUG: Loaded {total_images} rows from batch CSV[/dim]\")\n",
    "        \n",
    "        # Load ground truth\n",
    "        gt_path = Path(ground_truth_path)\n",
    "        if not gt_path.exists():\n",
    "            rprint(f\"[red]  ‚ùå Ground truth not found: {ground_truth_path}[/red]\")\n",
    "            return pd.DataFrame()\n",
    "            \n",
    "        ground_truth_map = load_ground_truth(str(gt_path), show_sample=False, verbose=False)\n",
    "        rprint(f\"[dim]  DEBUG: Ground truth has {len(ground_truth_map)} entries[/dim]\")\n",
    "        \n",
    "        # Determine which column to use for matching\n",
    "        # Ground truth uses image_file, batch CSV has both image_file and image_name\n",
    "        image_col = 'image_file' if 'image_file' in batch_df.columns else 'image_name'\n",
    "        \n",
    "        # DEBUG: Show sample image names\n",
    "        if len(batch_df) > 0:\n",
    "            sample_batch_names = batch_df[image_col].head(3).tolist()\n",
    "            rprint(f\"[dim]  DEBUG: Sample batch image names (from {image_col}): {sample_batch_names}[/dim]\")\n",
    "        \n",
    "        if len(ground_truth_map) > 0:\n",
    "            sample_gt_names = list(ground_truth_map.keys())[:3]\n",
    "            rprint(f\"[dim]  DEBUG: Sample ground truth names: {sample_gt_names}[/dim]\")\n",
    "\n",
    "        # FIX: Normalize image names by stripping extensions for matching\n",
    "        # Batch CSV has extensions (.jpeg, .png), ground truth does not\n",
    "        batch_df['image_stem'] = batch_df[image_col].apply(lambda x: Path(x).stem)\n",
    "\n",
    "        # Create ground truth mapping with stems as keys\n",
    "        ground_truth_by_stem = {}\n",
    "        for gt_name, gt_data in ground_truth_map.items():\n",
    "            stem = Path(str(gt_name)).stem  # Strip extension if present\n",
    "            ground_truth_by_stem[stem] = gt_data\n",
    "\n",
    "        rprint(f\"[dim]  DEBUG: Created stem-based mapping for {len(ground_truth_by_stem)} ground truth entries[/dim]\")\n",
    "\n",
    "\n",
    "        \n",
    "        # FILTER: Only evaluate images that have ground truth\n",
    "        # FIX: Match using image_file column (consistent with ground truth)\n",
    "        batch_df_filtered = batch_df[batch_df['image_stem'].isin(ground_truth_by_stem.keys())]\n",
    "        filtered_count = len(batch_df_filtered)\n",
    "        skipped_count = total_images - filtered_count\n",
    "        \n",
    "        rprint(f\"[dim]  DEBUG: Filtered to {filtered_count} matching images (skipped {skipped_count})[/dim]\")\n",
    "        \n",
    "        if filtered_count == 0:\n",
    "            rprint(f\"[red]  ‚ùå No images in batch match ground truth entries[/red]\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        rprint(f\"[cyan]  üìä Evaluating {filtered_count}/{total_images} images with ground truth[/cyan]\")\n",
    "        \n",
    "        # Track field accuracies - accumulate across all images\n",
    "        field_accuracies = {}\n",
    "        \n",
    "        # Get all possible field columns (exclude metadata columns)\n",
    "        metadata_cols = {'image_file', 'image_name', 'document_type', 'processing_time', \n",
    "                         'field_count', 'found_fields', 'field_coverage', 'prompt_used', \n",
    "                         'timestamp', 'overall_accuracy', 'fields_extracted', 'fields_matched', \n",
    "                         'total_fields', 'inference_only', 'model', 'image_stem',\n",
    "                         'gpu_type', 'model_version', 'precision', 'batch_size', 'max_tiles',\n",
    "                         'flash_attn', 'quantization', 'torch_dtype'}\n",
    "        \n",
    "        field_columns = [col for col in batch_df_filtered.columns if col not in metadata_cols]\n",
    "        \n",
    "        rprint(f\"[dim]  DEBUG: Found {len(field_columns)} field columns to evaluate[/dim]\")\n",
    "        \n",
    "        # Evaluate each image\n",
    "        for _, row in batch_df_filtered.iterrows():\n",
    "            image_identifier = row[image_col]\n",
    "            \n",
    "            # Get ground truth for this image (use image_identifier as key)\n",
    "            # Get ground truth using stem (without extension)\n",
    "            image_stem = Path(str(image_identifier)).stem\n",
    "            gt_data = ground_truth_by_stem.get(image_stem)\n",
    "            if not gt_data:\n",
    "                continue\n",
    "            \n",
    "            # Compare each field\n",
    "            for field_name in field_columns:\n",
    "                extracted_value = row.get(field_name, 'NOT_FOUND')\n",
    "                ground_truth_value = gt_data.get(field_name, 'NOT_FOUND')\n",
    "                \n",
    "                # Skip if both are NOT_FOUND (field not applicable)\n",
    "                if str(extracted_value).upper() == 'NOT_FOUND' and str(ground_truth_value).upper() == 'NOT_FOUND':\n",
    "                    continue\n",
    "                \n",
    "                # Calculate accuracy score using the same logic as batch notebooks\n",
    "                accuracy_score = calculate_field_accuracy(\n",
    "                    extracted_value, ground_truth_value, field_name, debug=False\n",
    "                )\n",
    "                \n",
    "                # Initialize field tracking if needed\n",
    "                if field_name not in field_accuracies:\n",
    "                    field_accuracies[field_name] = {'correct': 0.0, 'total': 0}\n",
    "                \n",
    "                field_accuracies[field_name]['total'] += 1\n",
    "                field_accuracies[field_name]['correct'] += accuracy_score\n",
    "        \n",
    "        rprint(f\"[dim]  DEBUG: Computed accuracies for {len(field_accuracies)} fields[/dim]\")\n",
    "        \n",
    "        if not field_accuracies:\n",
    "            rprint(f\"[yellow]  ‚ö†Ô∏è No field accuracies computed[/yellow]\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        field_data = []\n",
    "        for field_name, data in field_accuracies.items():\n",
    "            accuracy = data['correct'] / data['total'] if data['total'] > 0 else 0.0\n",
    "            field_data.append({\n",
    "                'model': model_name,\n",
    "                'field_name': field_name,\n",
    "                'accuracy': accuracy,\n",
    "                'correct_count': data['correct'],\n",
    "                'total_count': data['total']\n",
    "            })\n",
    "        \n",
    "        # Calculate average accuracy\n",
    "        avg_accuracy = sum(d['accuracy'] for d in field_data) / len(field_data) if field_data else 0.0\n",
    "        \n",
    "        rprint(f\"[green]  ‚úÖ Computed accuracy for {len(field_data)} fields (avg: {avg_accuracy:.1%})[/green]\")\n",
    "        return pd.DataFrame(field_data)\n",
    "        \n",
    "    except Exception as e:\n",
    "        rprint(f\"[red]‚ùå Error evaluating {model_name}: {e}[/red]\")\n",
    "        import traceback\n",
    "        rprint(f\"[dim]{traceback.format_exc()}[/dim]\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Load field-level accuracy data from CSV files (NOT re-running evaluation)\n",
    "rprint(\"\\n[bold blue]üìä Computing Field-Level Accuracy (from CSV + Ground Truth)[/bold blue]\")\n",
    "rprint(\"[dim]Comparing batch CSV results against ground truth for field-level metrics[/dim]\\n\")\n",
    "\n",
    "# Determine ground truth path - FAIL FAST if not found\n",
    "gt_path = CONFIG['ground_truth_path']\n",
    "\n",
    "rprint(f\"[cyan]Expected ground truth path: {gt_path}[/cyan]\")\n",
    "rprint(f\"[dim]  Absolute path: {Path(gt_path).absolute()}[/dim]\")\n",
    "\n",
    "if not Path(gt_path).exists():\n",
    "    rprint(f\"[bold red]‚ùå FATAL: Ground truth file not found![/bold red]\")\n",
    "    rprint(f\"[yellow]üí° Expected location: {Path(gt_path).absolute()}[/yellow]\")\n",
    "    rprint(f\"[yellow]üí° Check CONFIG['output_dir'] setting[/yellow]\")\n",
    "    rprint(f\"[yellow]üí° Current CONFIG['output_dir']: {CONFIG.get('output_dir', 'NOT SET')}[/yellow]\")\n",
    "    raise FileNotFoundError(f\"Ground truth not found: {gt_path}\")\n",
    "\n",
    "rprint(f\"[green]‚úÖ Ground truth file exists[/green]\")\n",
    "\n",
    "\n",
    "\n",
    "field_data_frames = []\n",
    "\n",
    "# Use explicit file configurations from CONFIG['models']\n",
    "for model_key, model_cfg in CONFIG['models'].items():\n",
    "    if model_cfg['file'] and model_key in model_dataframes:\n",
    "        fields_df = extract_field_level_accuracy_from_csv(\n",
    "            CONFIG['output_dir'],\n",
    "            model_cfg['file'],\n",
    "            model_cfg['display_name'],\n",
    "            gt_path\n",
    "        )\n",
    "        if not fields_df.empty:\n",
    "            field_data_frames.append(fields_df)\n",
    "\n",
    "if field_data_frames:\n",
    "    field_level_df = pd.concat(field_data_frames, ignore_index=True)\n",
    "    rprint(f\"\\n[green]‚úÖ Field-level accuracy computed: {len(field_level_df)} field measurements[/green]\")\n",
    "    rprint(f\"[cyan]üìã Unique fields: {field_level_df['field_name'].nunique()}[/cyan]\")\n",
    "    rprint(f\"[cyan]üìä Models analyzed: {field_level_df['model'].nunique()}[/cyan]\")\n",
    "else:\n",
    "    rprint(\"\\n[red]‚ùå No field-level accuracy data available[/red]\")\n",
    "    rprint(\"[yellow]üí° This requires batch results CSVs and ground truth for evaluation[/yellow]\")\n",
    "    field_level_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10\n",
    "\n",
    "def analyze_field_performance(field_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Analyze and compare field-level performance across models.\n",
    "    \n",
    "    Args:\n",
    "        field_df: DataFrame with field-level accuracy data\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with field performance comparison\n",
    "    \"\"\"\n",
    "    if field_df.empty:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Filter out metadata/internal fields that aren't business document fields\n",
    "    exclude_fields = ['quantization_used', 'model', 'timestamp', 'processing_time', 'image_name', 'image_stem', 'TRANSACTION_AMOUNTS_RECEIVED', 'ACCOUNT_BALANCE']\n",
    "    field_df = field_df[~field_df['field_name'].isin(exclude_fields)]\n",
    "    \n",
    "    if field_df.empty:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Pivot to get fields as rows, models as columns (using accuracy now)\n",
    "    field_comparison = field_df.pivot_table(\n",
    "        index='field_name',\n",
    "        columns='model',\n",
    "        values='accuracy',\n",
    "        aggfunc='mean'\n",
    "    )\n",
    "    \n",
    "    # Calculate average accuracy across all models\n",
    "    field_comparison['avg_accuracy'] = field_comparison.mean(axis=1)\n",
    "    \n",
    "    # Calculate variance to identify fields with inconsistent performance\n",
    "    field_comparison['variance'] = field_comparison.std(axis=1)\n",
    "    \n",
    "    # Identify best model for each field\n",
    "    model_cols = [col for col in field_comparison.columns if col not in ['avg_accuracy', 'variance']]\n",
    "    field_comparison['best_model'] = field_comparison[model_cols].idxmax(axis=1)\n",
    "    field_comparison['best_score'] = field_comparison[model_cols].max(axis=1)\n",
    "    \n",
    "    # Sort by average accuracy\n",
    "    field_comparison = field_comparison.sort_values('avg_accuracy', ascending=False)\n",
    "    \n",
    "    return field_comparison\n",
    "\n",
    "# Analyze field performance if data is available\n",
    "if not field_level_df.empty:\n",
    "    field_performance = analyze_field_performance(field_level_df)\n",
    "    \n",
    "    rprint(\"\\n[bold green]üìä V2 FIELD-LEVEL ACCURACY ANALYSIS[/bold green]\")\n",
    "    rprint(\"[cyan]Using sophisticated multi-turn bank statement extraction[/cyan]\")\n",
    "    \n",
    "    # Show top performing fields with color gradient\n",
    "    rprint(\"[bold blue]üìä All Fields Ranked by Accuracy:[/bold blue]\")\n",
    "    \n",
    "    # Get model columns for styling\n",
    "    model_cols = [col for col in field_performance.columns if col not in [\"avg_accuracy\", \"variance\", \"best_model\", \"best_score\"]]\n",
    "    \n",
    "    # Apply color gradient styling to all fields (already sorted by avg_accuracy descending)\n",
    "    styled_all = field_performance.style.background_gradient(\n",
    "        cmap=\"RdYlGn\",\n",
    "        subset=model_cols + [\"avg_accuracy\"],\n",
    "        vmin=0,\n",
    "        vmax=1\n",
    "    ).format(\n",
    "        {col: \"{:.1%}\" for col in field_performance.columns if col not in [\"best_model\", \"variance\", \"best_score\"]}\n",
    "    ).format(\n",
    "        {\"variance\": \"{:.3f}\"}\n",
    "    )\n",
    "    \n",
    "    display(styled_all)\n",
    "    \n",
    "    # Save all fields table as PNG\n",
    "    output_table_dir = Path(f\"{base_data_path}/output/tables\")\n",
    "    output_table_dir.mkdir(parents=True, exist_ok=True)\n",
    "    all_fields_png_path = output_table_dir / \"field_accuracy_ranked_v2.png\"\n",
    "    \n",
    "    # Render table as image using matplotlib\n",
    "    fig, ax = plt.subplots(figsize=(16, 10))  # Taller for 17 fields\n",
    "    ax.axis(\"tight\")\n",
    "    ax.axis(\"off\")\n",
    "    \n",
    "    table_data = field_performance.values\n",
    "    col_labels = list(field_performance.columns)\n",
    "    row_labels = list(field_performance.index)\n",
    "    \n",
    "    table = ax.table(cellText=table_data, colLabels=col_labels, rowLabels=row_labels,\n",
    "                     cellLoc=\"center\", loc=\"center\")\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(8)\n",
    "    table.scale(1.2, 1.8)\n",
    "    \n",
    "    plt.savefig(str(all_fields_png_path), dpi=300, bbox_inches=\"tight\", facecolor=\"white\")\n",
    "    plt.close()\n",
    "    rprint(f\"[green]‚úÖ All fields table saved: {all_fields_png_path}[/green]\")\n",
    "    \n",
    "    # Field performance summary\n",
    "    rprint(f\"[cyan]üìà Total evaluation fields: {len(field_performance)}[/cyan]\")\n",
    "    rprint(f\"[cyan]üìä Average accuracy range: {field_performance['avg_accuracy'].min():.1%} - {field_performance['avg_accuracy'].max():.1%}[/cyan]\")\n",
    "    \n",
    "    rprint(\"\\n[bold blue]üéØ Model Field Specialization:[/bold blue]\")\n",
    "    specialization = field_performance['best_model'].value_counts()\n",
    "    for model, count in specialization.items():\n",
    "        percentage = (count / len(field_performance)) * 100\n",
    "        rprint(f\"  ‚Ä¢ {model}: Best at {count} fields ({percentage:.1f}%)\")\n",
    "        \n",
    "    # Summary statistics\n",
    "    rprint(\"\\n[bold blue]üìä Overall Field Accuracy Summary:[/bold blue]\")\n",
    "    for model in field_level_df['model'].unique():\n",
    "        model_data = field_level_df[field_level_df['model'] == model]\n",
    "        avg_acc = model_data['accuracy'].mean()\n",
    "        rprint(f\"  ‚Ä¢ {model}: {avg_acc:.1%} average field accuracy\")\n",
    "else:\n",
    "    rprint(\"[red]‚ùå Cannot analyze field performance - no field-level data available[/red]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5. Comprehensive Model Comparison Dashboard\n",
    "\n",
    "**Statistical Analysis with Pairwise Tests**\n",
    "\n",
    "This dashboard provides:\n",
    "1. **Summary Statistics**: Mean F1, Median F1, Std Dev, Min/Max for all 3 models\n",
    "2. **Statistical Tests**: Pairwise t-tests with p-values and significance levels\n",
    "3. **Effect Size Analysis**: Cohen's d with interpretation (negligible/small/medium/large)\n",
    "4. **Visual Comparisons**: Box plots, bar charts, efficiency analysis\n",
    "5. **Document Type Performance**: Per-document-type accuracy comparison\n",
    "6. **Final Recommendation**: Data-driven model selection guidance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Comprehensive Model Comparison Dashboard with Statistical Tests\n",
    "# This cell creates a unified dashboard comparing all 3 models with:\n",
    "# 1. Summary statistics (Mean F1, Median, Critical thresholds)\n",
    "# 2. Pairwise statistical tests (t-test, Cohen's d)\n",
    "# 3. Visual comparison charts\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from scipy import stats\n",
    "from IPython.display import display, HTML\n",
    "from rich import print as rprint\n",
    "\n",
    "# ============================================================================\n",
    "# STATISTICAL FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def cohens_d(group1, group2):\n",
    "    \"\"\"Calculate Cohen's d effect size between two groups.\"\"\"\n",
    "    n1, n2 = len(group1), len(group2)\n",
    "    var1, var2 = group1.var(), group2.var()\n",
    "\n",
    "    # Pooled standard deviation\n",
    "    pooled_std = np.sqrt(((n1 - 1) * var1 + (n2 - 1) * var2) / (n1 + n2 - 2))\n",
    "\n",
    "    if pooled_std == 0:\n",
    "        return 0.0\n",
    "\n",
    "    return (group1.mean() - group2.mean()) / pooled_std\n",
    "\n",
    "def effect_size_interpretation(d):\n",
    "    \"\"\"Interpret Cohen's d effect size.\"\"\"\n",
    "    d_abs = abs(d)\n",
    "    if d_abs < 0.2:\n",
    "        return \"negligible\"\n",
    "    elif d_abs < 0.5:\n",
    "        return \"small\"\n",
    "    elif d_abs < 0.8:\n",
    "        return \"medium\"\n",
    "    else:\n",
    "        return \"large\"\n",
    "\n",
    "def significance_stars(p_value):\n",
    "    \"\"\"Return significance stars based on p-value.\"\"\"\n",
    "    if p_value < 0.001:\n",
    "        return \"***\"\n",
    "    elif p_value < 0.01:\n",
    "        return \"**\"\n",
    "    elif p_value < 0.05:\n",
    "        return \"*\"\n",
    "    else:\n",
    "        return \"ns\"\n",
    "\n",
    "def compute_model_f1_scores(df, model_name):\n",
    "    \"\"\"Compute per-document F1 scores from batch results DataFrame.\n",
    "    \n",
    "    NOTE: Uses overall_accuracy directly as the F1 metric since it represents\n",
    "    the mean per-field accuracy which is already F1-equivalent.\n",
    "    \n",
    "    Returns values in 0-100 PERCENTAGE scale for consistency with executive dashboard.\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return pd.Series(dtype=float)\n",
    "\n",
    "    # Use overall_accuracy directly (already in 0-100 percentage scale)\n",
    "    # This ensures consistency with executive dashboard which uses same metric\n",
    "    if 'overall_accuracy' in df.columns:\n",
    "        return df['overall_accuracy']\n",
    "    \n",
    "    # Fallback: return zeros if no accuracy data\n",
    "    return pd.Series([0.0] * len(df), index=df.index)\n",
    "\n",
    "# ============================================================================\n",
    "# SUMMARY STATISTICS\n",
    "# ============================================================================\n",
    "\n",
    "def create_summary_statistics(model_data_dict):\n",
    "    \"\"\"\n",
    "    Create summary statistics table for all models.\n",
    "\n",
    "    Args:\n",
    "        model_data_dict: Dict of {model_name: f1_scores_series}\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with summary statistics\n",
    "    \"\"\"\n",
    "    summary_data = []\n",
    "\n",
    "    for model_name, f1_scores in model_data_dict.items():\n",
    "        if len(f1_scores) == 0:\n",
    "            continue\n",
    "\n",
    "        summary_data.append({\n",
    "            'Model': model_name,\n",
    "            'Mean F1': f1_scores.mean(),\n",
    "            'Median F1': f1_scores.median(),\n",
    "            'Std Dev': f1_scores.std(),\n",
    "            'Min F1': f1_scores.min(),\n",
    "            'Max F1': f1_scores.max(),\n",
    "            '25th %ile': f1_scores.quantile(0.25),\n",
    "            '75th %ile': f1_scores.quantile(0.75),\n",
    "            'N': len(f1_scores)\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(summary_data)\n",
    "\n",
    "# ============================================================================\n",
    "# PAIRWISE STATISTICAL TESTS\n",
    "# ============================================================================\n",
    "\n",
    "def compute_pairwise_tests(model_data_dict):\n",
    "    \"\"\"\n",
    "    Compute pairwise t-tests and Cohen's d between all model pairs.\n",
    "\n",
    "    Args:\n",
    "        model_data_dict: Dict of {model_name: f1_scores_series}\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with pairwise test results\n",
    "    \"\"\"\n",
    "    model_names = list(model_data_dict.keys())\n",
    "    test_results = []\n",
    "\n",
    "    for i in range(len(model_names)):\n",
    "        for j in range(i + 1, len(model_names)):\n",
    "            model_a = model_names[i]\n",
    "            model_b = model_names[j]\n",
    "\n",
    "            scores_a = model_data_dict[model_a]\n",
    "            scores_b = model_data_dict[model_b]\n",
    "\n",
    "            if len(scores_a) == 0 or len(scores_b) == 0:\n",
    "                continue\n",
    "\n",
    "            # Independent samples t-test\n",
    "            t_stat, p_value = stats.ttest_ind(scores_a, scores_b)\n",
    "\n",
    "            # Cohen's d effect size\n",
    "            d = cohens_d(scores_a, scores_b)\n",
    "\n",
    "            # Determine winner\n",
    "            if scores_a.mean() > scores_b.mean():\n",
    "                winner = model_a\n",
    "                winner_advantage = scores_a.mean() - scores_b.mean()\n",
    "            else:\n",
    "                winner = model_b\n",
    "                winner_advantage = scores_b.mean() - scores_a.mean()\n",
    "\n",
    "            test_results.append({\n",
    "                'Comparison': f\"{model_a} vs {model_b}\",\n",
    "                'Model A': model_a,\n",
    "                'Model B': model_b,\n",
    "                'Mean A': scores_a.mean(),\n",
    "                'Mean B': scores_b.mean(),\n",
    "                't-statistic': t_stat,\n",
    "                'p-value': p_value,\n",
    "                'Significance': significance_stars(p_value),\n",
    "                \"cohens_d\": d,\n",
    "                'Effect Size': effect_size_interpretation(d),\n",
    "                'Winner': winner,\n",
    "                'Advantage': winner_advantage\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(test_results)\n",
    "\n",
    "# ============================================================================\n",
    "# COMPREHENSIVE DASHBOARD VISUALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "def create_comprehensive_dashboard(\n",
    "    model_data_dict,\n",
    "    summary_stats_df,\n",
    "    pairwise_tests_df,\n",
    "    combined_df,\n",
    "    config,\n",
    "    save_path=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Create comprehensive dashboard with summary stats, statistical tests, and visualizations.\n",
    "\n",
    "    Args:\n",
    "        model_data_dict: Dict of {model_name: f1_scores_series}\n",
    "        summary_stats_df: Summary statistics DataFrame\n",
    "        pairwise_tests_df: Pairwise test results DataFrame\n",
    "        combined_df: Combined batch results DataFrame (for accuracy/time data)\n",
    "        config: CONFIG dict with model colors\n",
    "        save_path: Optional path to save the figure\n",
    "    \"\"\"\n",
    "\n",
    "    # Get model order and colors from config\n",
    "    model_order = [config['models'][key]['display_name'] for key in config['models']]\n",
    "    model_colors = {config['models'][key]['display_name']: config['models'][key]['color']\n",
    "                    for key in config['models']}\n",
    "\n",
    "    # Filter to available models\n",
    "    available_models = list(model_data_dict.keys())\n",
    "    models = [m for m in model_order if m in available_models]\n",
    "    colors = [model_colors.get(m, '#999999') for m in models]\n",
    "\n",
    "    # Create figure with custom layout\n",
    "    fig = plt.figure(figsize=(24, 18))\n",
    "    gs = gridspec.GridSpec(4, 3, height_ratios=[0.8, 1, 1, 1], hspace=0.35, wspace=0.3)\n",
    "\n",
    "    # =========================================================================\n",
    "    # ROW 1: Summary Statistics Tables\n",
    "    # =========================================================================\n",
    "\n",
    "    # Panel 1A: Summary Statistics Table\n",
    "    ax_summary = fig.add_subplot(gs[0, 0:2])\n",
    "    ax_summary.axis('off')\n",
    "\n",
    "    # Prepare summary table data\n",
    "    table_data = []\n",
    "    for _, row in summary_stats_df.iterrows():\n",
    "        table_data.append([\n",
    "            row['Model'],\n",
    "            f\"{row['Mean F1']:.1f}%\",\n",
    "            f\"{row['Median F1']:.1f}%\",\n",
    "            f\"{row['Std Dev']:.1f}%\",\n",
    "            f\"{row['Min F1']:.1f}%\",\n",
    "            f\"{row['Max F1']:.1f}%\",\n",
    "            int(row['N'])\n",
    "        ])\n",
    "\n",
    "    table = ax_summary.table(\n",
    "        cellText=table_data,\n",
    "        colLabels=['Model', 'Mean F1', 'Median F1', 'Std Dev', 'Min', 'Max', 'N'],\n",
    "        cellLoc='center',\n",
    "        loc='center',\n",
    "        colWidths=[0.25, 0.12, 0.12, 0.12, 0.12, 0.12, 0.08]\n",
    "    )\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(11)\n",
    "    table.scale(1.2, 2.0)\n",
    "\n",
    "    # Style header row\n",
    "    for i in range(7):\n",
    "        table[(0, i)].set_facecolor('#2C3E50')\n",
    "        table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "\n",
    "    # Color model name cells\n",
    "    for row_idx, model in enumerate(summary_stats_df['Model']):\n",
    "        if model in model_colors:\n",
    "            table[(row_idx + 1, 0)].set_facecolor(model_colors[model])\n",
    "            table[(row_idx + 1, 0)].set_text_props(weight='bold', color='white')\n",
    "            table[(row_idx + 1, 0)].set_alpha(0.85)\n",
    "\n",
    "    ax_summary.set_title('A. Summary Statistics', fontsize=14, fontweight='bold', loc='left', pad=10)\n",
    "\n",
    "    # Panel 1B: Statistical Tests Table\n",
    "    ax_tests = fig.add_subplot(gs[0, 2])\n",
    "    ax_tests.axis('off')\n",
    "\n",
    "    # Prepare test results table\n",
    "    test_table_data = []\n",
    "    for _, row in pairwise_tests_df.iterrows():\n",
    "        test_table_data.append([\n",
    "            row['Comparison'],\n",
    "            f\"{row['p-value']:.4f}\",\n",
    "            row['Significance'],\n",
    "            f\"{row['cohens_d']:.2f}\",\n",
    "            row['Effect Size']\n",
    "        ])\n",
    "\n",
    "    test_table = ax_tests.table(\n",
    "        cellText=test_table_data,\n",
    "        colLabels=['Test', 'p-value', 'Sig.', \"Cohen's d\", 'Effect'],\n",
    "        cellLoc='center',\n",
    "        loc='center',\n",
    "        colWidths=[0.35, 0.15, 0.1, 0.15, 0.15]\n",
    "    )\n",
    "    test_table.auto_set_font_size(False)\n",
    "    test_table.set_fontsize(10)\n",
    "    test_table.scale(1.1, 2.0)\n",
    "\n",
    "    # Style header row\n",
    "    for i in range(5):\n",
    "        test_table[(0, i)].set_facecolor('#2C3E50')\n",
    "        test_table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "\n",
    "    # Color significance cells\n",
    "    for row_idx, (_, row) in enumerate(pairwise_tests_df.iterrows()):\n",
    "        sig = row['Significance']\n",
    "        if sig == '***':\n",
    "            test_table[(row_idx + 1, 2)].set_facecolor('#27AE60')\n",
    "            test_table[(row_idx + 1, 2)].set_text_props(weight='bold', color='white')\n",
    "        elif sig == '**':\n",
    "            test_table[(row_idx + 1, 2)].set_facecolor('#2ECC71')\n",
    "            test_table[(row_idx + 1, 2)].set_text_props(weight='bold')\n",
    "        elif sig == '*':\n",
    "            test_table[(row_idx + 1, 2)].set_facecolor('#82E0AA')\n",
    "\n",
    "        # Color effect size\n",
    "        effect = row['Effect Size']\n",
    "        if effect == 'large':\n",
    "            test_table[(row_idx + 1, 4)].set_facecolor('#E74C3C')\n",
    "            test_table[(row_idx + 1, 4)].set_text_props(weight='bold', color='white')\n",
    "        elif effect == 'medium':\n",
    "            test_table[(row_idx + 1, 4)].set_facecolor('#F39C12')\n",
    "        elif effect == 'small':\n",
    "            test_table[(row_idx + 1, 4)].set_facecolor('#F7DC6F')\n",
    "\n",
    "    ax_tests.set_title('B. Statistical Tests (Pairwise)', fontsize=14, fontweight='bold', loc='left', pad=10)\n",
    "\n",
    "    # =========================================================================\n",
    "    # ROW 2: F1 Score Distributions\n",
    "    # =========================================================================\n",
    "\n",
    "    # Panel 2A: F1 Score Box Plot\n",
    "    ax_box = fig.add_subplot(gs[1, 0])\n",
    "\n",
    "    f1_data = [model_data_dict[m].values for m in models if m in model_data_dict]\n",
    "    box_parts = ax_box.boxplot(f1_data, labels=models, patch_artist=True, showmeans=True)\n",
    "\n",
    "    for patch, color in zip(box_parts['boxes'], colors):\n",
    "        patch.set_facecolor(color)\n",
    "        patch.set_alpha(0.7)\n",
    "\n",
    "    ax_box.set_ylabel('F1 Score', fontsize=12, fontweight='bold')\n",
    "    ax_box.set_title('C. F1 Score Distribution', fontsize=14, fontweight='bold')\n",
    "    ax_box.grid(True, alpha=0.3, axis='y')\n",
    "    ax_box.set_xticklabels(models, rotation=15, ha='right')\n",
    "    # Y-axis already in percentage scale (0-100)\n",
    "\n",
    "    # Panel 2B: Mean F1 with Error Bars\n",
    "    ax_bar = fig.add_subplot(gs[1, 1])\n",
    "\n",
    "    means = [model_data_dict[m].mean() for m in models]\n",
    "    stds = [model_data_dict[m].std() for m in models]\n",
    "\n",
    "    bars = ax_bar.bar(models, means, yerr=stds, capsize=5, color=colors, alpha=0.8, edgecolor='black')\n",
    "\n",
    "    # Add value labels\n",
    "    for bar, mean, std in zip(bars, means, stds):\n",
    "        ax_bar.text(bar.get_x() + bar.get_width()/2, bar.get_height() + std + 0.01,\n",
    "                   f'{mean:.1f}%', ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "\n",
    "    ax_bar.set_ylabel('Mean F1 Score', fontsize=12, fontweight='bold')\n",
    "    ax_bar.set_title('D. Mean F1 with Std Dev', fontsize=14, fontweight='bold')\n",
    "    ax_bar.grid(True, alpha=0.3, axis='y')\n",
    "    ax_bar.set_xticklabels(models, rotation=15, ha='right')\n",
    "    ax_bar.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'{x:.0%}'))\n",
    "\n",
    "    # Panel 2C: Winner Summary\n",
    "    ax_winner = fig.add_subplot(gs[1, 2])\n",
    "    ax_winner.axis('off')\n",
    "\n",
    "    # Determine overall winner\n",
    "    best_model = summary_stats_df.loc[summary_stats_df['Mean F1'].idxmax(), 'Model']\n",
    "    best_f1 = summary_stats_df['Mean F1'].max()\n",
    "\n",
    "    # Create winner summary text\n",
    "    winner_text = f\"\"\"\n",
    "    RECOMMENDED MODEL\n",
    "\n",
    "    {best_model}\n",
    "\n",
    "    Mean F1: {best_f1:.1f}%\n",
    "\n",
    "    Statistical Evidence:\n",
    "    \"\"\"\n",
    "\n",
    "    # Add pairwise comparisons\n",
    "    for _, row in pairwise_tests_df.iterrows():\n",
    "        if row['Winner'] == best_model:\n",
    "            other = row['Model A'] if row['Model B'] == best_model else row['Model B']\n",
    "            winner_text += f\"\\n    vs {other}: +{row['Advantage']:.1f}% ({row['Significance']}, {row['Effect Size']})\"\n",
    "\n",
    "    ax_winner.text(0.5, 0.5, winner_text, transform=ax_winner.transAxes,\n",
    "                  fontsize=12, verticalalignment='center', horizontalalignment='center',\n",
    "                  bbox=dict(boxstyle='round', facecolor='#E8F6F3', edgecolor='#1ABC9C', linewidth=2),\n",
    "                  family='monospace')\n",
    "    ax_winner.set_title('E. Recommendation', fontsize=14, fontweight='bold')\n",
    "\n",
    "    # =========================================================================\n",
    "    # ROW 3: Accuracy and Speed Comparison\n",
    "    # =========================================================================\n",
    "\n",
    "    if not combined_df.empty:\n",
    "        # Panel 3A: Overall Accuracy Box Plot\n",
    "        ax_acc = fig.add_subplot(gs[2, 0])\n",
    "\n",
    "        acc_data = [combined_df[combined_df['model'] == m]['overall_accuracy'].values\n",
    "                    for m in models if m in combined_df['model'].unique()]\n",
    "\n",
    "        if acc_data:\n",
    "            box_parts = ax_acc.boxplot(acc_data, labels=models, patch_artist=True, showmeans=True)\n",
    "            for patch, color in zip(box_parts['boxes'], colors):\n",
    "                patch.set_facecolor(color)\n",
    "                patch.set_alpha(0.7)\n",
    "\n",
    "        ax_acc.set_ylabel('Overall Accuracy (%)', fontsize=12, fontweight='bold')\n",
    "        ax_acc.set_title('F. Overall Accuracy Distribution', fontsize=14, fontweight='bold')\n",
    "        ax_acc.grid(True, alpha=0.3, axis='y')\n",
    "        ax_acc.set_xticklabels(models, rotation=15, ha='right')\n",
    "\n",
    "        # Panel 3B: Processing Time Box Plot\n",
    "        ax_time = fig.add_subplot(gs[2, 1])\n",
    "\n",
    "        time_data = [combined_df[combined_df['model'] == m]['processing_time'].values\n",
    "                     for m in models if m in combined_df['model'].unique()]\n",
    "\n",
    "        if time_data:\n",
    "            box_parts = ax_time.boxplot(time_data, labels=models, patch_artist=True, showmeans=True)\n",
    "            for patch, color in zip(box_parts['boxes'], colors):\n",
    "                patch.set_facecolor(color)\n",
    "                patch.set_alpha(0.7)\n",
    "\n",
    "        ax_time.set_ylabel('Processing Time (seconds)', fontsize=12, fontweight='bold')\n",
    "        ax_time.set_title('G. Processing Speed Distribution', fontsize=14, fontweight='bold')\n",
    "        ax_time.grid(True, alpha=0.3, axis='y')\n",
    "        ax_time.set_xticklabels(models, rotation=15, ha='right')\n",
    "\n",
    "        # Panel 3C: Efficiency Scatter (Accuracy vs Speed)\n",
    "        ax_eff = fig.add_subplot(gs[2, 2])\n",
    "\n",
    "        for model, color in zip(models, colors):\n",
    "            model_data = combined_df[combined_df['model'] == model]\n",
    "            if not model_data.empty:\n",
    "                ax_eff.scatter(model_data['processing_time'], model_data['overall_accuracy'],\n",
    "                              label=model, color=color, alpha=0.6, s=80, edgecolors='black')\n",
    "\n",
    "        ax_eff.set_xlabel('Processing Time (seconds)', fontsize=12, fontweight='bold')\n",
    "        ax_eff.set_ylabel('Overall Accuracy (%)', fontsize=12, fontweight='bold')\n",
    "        ax_eff.set_title('H. Efficiency: Accuracy vs Speed', fontsize=14, fontweight='bold')\n",
    "        ax_eff.legend(loc='best', fontsize=10)\n",
    "        ax_eff.grid(True, alpha=0.3)\n",
    "\n",
    "    # =========================================================================\n",
    "    # ROW 4: Document Type Performance\n",
    "    # =========================================================================\n",
    "\n",
    "    if not combined_df.empty and 'document_type' in combined_df.columns:\n",
    "        # Panel 4A: Accuracy by Document Type\n",
    "        ax_doc = fig.add_subplot(gs[3, 0:2])\n",
    "\n",
    "        doc_accuracy = combined_df.groupby(['document_type', 'model'])['overall_accuracy'].mean().unstack()\n",
    "        doc_accuracy = doc_accuracy[[m for m in models if m in doc_accuracy.columns]]\n",
    "\n",
    "        x = np.arange(len(doc_accuracy.index))\n",
    "        width = 0.8 / len(models)\n",
    "\n",
    "        for idx, model in enumerate(models):\n",
    "            if model in doc_accuracy.columns:\n",
    "                offset = (idx - len(models)/2 + 0.5) * width\n",
    "                ax_doc.bar(x + offset, doc_accuracy[model], width,\n",
    "                          label=model, color=model_colors.get(model, '#999'), alpha=0.8)\n",
    "\n",
    "        ax_doc.set_ylabel('Average Accuracy (%)', fontsize=12, fontweight='bold')\n",
    "        ax_doc.set_xlabel('Document Type', fontsize=12, fontweight='bold')\n",
    "        ax_doc.set_title('I. Accuracy by Document Type', fontsize=14, fontweight='bold')\n",
    "        ax_doc.set_xticks(x)\n",
    "        ax_doc.set_xticklabels(doc_accuracy.index, rotation=15, ha='right')\n",
    "        ax_doc.legend(loc='upper right', fontsize=10)\n",
    "        ax_doc.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "        # Panel 4B: Best Model per Document Type\n",
    "        ax_best = fig.add_subplot(gs[3, 2])\n",
    "        ax_best.axis('off')\n",
    "\n",
    "        best_by_doc = []\n",
    "        for doc_type in doc_accuracy.index:\n",
    "            best_model = doc_accuracy.loc[doc_type].idxmax()\n",
    "            best_score = doc_accuracy.loc[doc_type].max()\n",
    "            best_by_doc.append([doc_type, best_model, f\"{best_score:.1f}%\"])\n",
    "\n",
    "        doc_table = ax_best.table(\n",
    "            cellText=best_by_doc,\n",
    "            colLabels=['Document Type', 'Best Model', 'Accuracy'],\n",
    "            cellLoc='center',\n",
    "            loc='center',\n",
    "            colWidths=[0.4, 0.35, 0.2]\n",
    "        )\n",
    "        doc_table.auto_set_font_size(False)\n",
    "        doc_table.set_fontsize(11)\n",
    "        doc_table.scale(1.1, 2.0)\n",
    "\n",
    "        for i in range(3):\n",
    "            doc_table[(0, i)].set_facecolor('#2C3E50')\n",
    "            doc_table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "\n",
    "        ax_best.set_title('J. Best Model by Document Type', fontsize=14, fontweight='bold', loc='left')\n",
    "\n",
    "    # =========================================================================\n",
    "    # Overall Title\n",
    "    # =========================================================================\n",
    "    fig.suptitle('COMPREHENSIVE MODEL COMPARISON DASHBOARD\\nStatistical Analysis of Vision-Language Models for Document Extraction',\n",
    "                fontsize=20, fontweight='bold', y=0.98)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "        rprint(f\"[green]‚úÖ Comprehensive dashboard saved to: {save_path}[/green]\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "rprint(\"[bold blue]=\" * 60 + \"[/bold blue]\")\n",
    "rprint(\"[bold blue]COMPREHENSIVE MODEL COMPARISON WITH STATISTICAL TESTS[/bold blue]\")\n",
    "rprint(\"[bold blue]=\" * 60 + \"[/bold blue]\")\n",
    "\n",
    "# Compute F1 scores for each model\n",
    "model_f1_scores = {}\n",
    "\n",
    "if not llama_df.empty:\n",
    "    model_f1_scores[CONFIG['models']['llama']['display_name']] = compute_model_f1_scores(\n",
    "        llama_df, CONFIG['models']['llama']['display_name']\n",
    "    )\n",
    "    rprint(f\"[green]‚úÖ {CONFIG['models']['llama']['display_name']}: {len(model_f1_scores[CONFIG['models']['llama']['display_name']])} F1 scores computed[/green]\")\n",
    "\n",
    "if not internvl3_8b_df.empty:\n",
    "    model_f1_scores[CONFIG['models']['internvl3_8b']['display_name']] = compute_model_f1_scores(\n",
    "        internvl3_8b_df, CONFIG['models']['internvl3_8b']['display_name']\n",
    "    )\n",
    "    rprint(f\"[green]‚úÖ {CONFIG['models']['internvl3_8b']['display_name']}: {len(model_f1_scores[CONFIG['models']['internvl3_8b']['display_name']])} F1 scores computed[/green]\")\n",
    "\n",
    "if not internvl3_2b_df.empty:\n",
    "    model_f1_scores[CONFIG['models']['internvl3_2b']['display_name']] = compute_model_f1_scores(\n",
    "        internvl3_2b_df, CONFIG['models']['internvl3_2b']['display_name']\n",
    "    )\n",
    "    rprint(f\"[green]‚úÖ {CONFIG['models']['internvl3_2b']['display_name']}: {len(model_f1_scores[CONFIG['models']['internvl3_2b']['display_name']])} F1 scores computed[/green]\")\n",
    "\n",
    "# Create summary statistics\n",
    "rprint(\"\\n[bold cyan]Computing Summary Statistics...[/bold cyan]\")\n",
    "summary_stats = create_summary_statistics(model_f1_scores)\n",
    "display(summary_stats.style.format({\n",
    "    'Mean F1': '{:.1f}%',\n",
    "    'Median F1': '{:.1f}%',\n",
    "    'Std Dev': '{:.1f}%',\n",
    "    'Min F1': '{:.1f}%',\n",
    "    'Max F1': '{:.1f}%',\n",
    "    '25th %ile': '{:.1f}%',\n",
    "    '75th %ile': '{:.1f}%'\n",
    "}).background_gradient(subset=['Mean F1', 'Median F1'], cmap='RdYlGn', vmin=0, vmax=1))\n",
    "\n",
    "# Compute pairwise statistical tests\n",
    "rprint(\"\\n[bold cyan]Computing Pairwise Statistical Tests...[/bold cyan]\")\n",
    "pairwise_tests = compute_pairwise_tests(model_f1_scores)\n",
    "\n",
    "# Display pairwise tests with styling\n",
    "styled_tests = pairwise_tests[['Comparison', 'p-value', 'Significance', 'cohens_d', 'Effect Size', 'Winner', 'Advantage']].copy()\n",
    "styled_tests['p-value'] = styled_tests['p-value'].apply(lambda x: f\"{x:.4f}\")\n",
    "styled_tests['Advantage'] = styled_tests['Advantage'].apply(lambda x: f\"{x:.1f}%\")\n",
    "\n",
    "display(styled_tests)\n",
    "\n",
    "# Statistical interpretation\n",
    "rprint(\"\\n[bold yellow]Statistical Interpretation:[/bold yellow]\")\n",
    "rprint(\"[dim]‚Ä¢ Significance: *** p<0.001, ** p<0.01, * p<0.05, ns = not significant[/dim]\")\n",
    "rprint(\"[dim]‚Ä¢ Cohen's d: |d|<0.2 negligible, 0.2-0.5 small, 0.5-0.8 medium, >0.8 large[/dim]\")\n",
    "\n",
    "for _, row in pairwise_tests.iterrows():\n",
    "    sig_text = \"SIGNIFICANT\" if row['Significance'] != 'ns' else \"NOT SIGNIFICANT\"\n",
    "    rprint(f\"\\n[cyan]{row['Comparison']}:[/cyan]\")\n",
    "    rprint(f\"  ‚Ä¢ Result: {sig_text} (p={row['p-value']:.4f})\")\n",
    "    rprint(f\"  ‚Ä¢ Effect: {row['Effect Size']} (d={row['cohens_d']:.2f})\")\n",
    "    rprint(f\"  ‚Ä¢ Winner: [bold]{row['Winner']}[/bold] (+{row['Advantage']:.1f}%)\")\n",
    "\n",
    "# Create comprehensive dashboard\n",
    "rprint(\"\\n[bold cyan]Creating Comprehensive Dashboard...[/bold cyan]\")\n",
    "\n",
    "output_path = Path(CONFIG['output_dir']).parent / \"visualizations\" / \"comprehensive_dashboard.png\"\n",
    "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "create_comprehensive_dashboard(\n",
    "    model_f1_scores,\n",
    "    summary_stats,\n",
    "    pairwise_tests,\n",
    "    combined_df,\n",
    "    CONFIG,\n",
    "    str(output_path)\n",
    ")\n",
    "\n",
    "# Final recommendation\n",
    "rprint(\"\\n[bold green]=\" * 60 + \"[/bold green]\")\n",
    "rprint(\"[bold green]FINAL RECOMMENDATION[/bold green]\")\n",
    "rprint(\"[bold green]=\" * 60 + \"[/bold green]\")\n",
    "\n",
    "best_model = summary_stats.loc[summary_stats['Mean F1'].idxmax(), 'Model']\n",
    "best_f1 = summary_stats['Mean F1'].max()\n",
    "best_median = summary_stats.loc[summary_stats['Mean F1'].idxmax(), 'Median F1']\n",
    "\n",
    "rprint(f\"\\n[bold white on green] RECOMMENDED: {best_model} [/bold white on green]\")\n",
    "rprint(f\"\\n[green]‚Ä¢ Mean F1: {best_f1:.1f}%[/green]\")\n",
    "rprint(f\"[green]‚Ä¢ Median F1: {best_median:.1f}%[/green]\")\n",
    "\n",
    "# Show advantages over other models\n",
    "for _, row in pairwise_tests.iterrows():\n",
    "    if row['Winner'] == best_model:\n",
    "        other = row['Model A'] if row['Model B'] == best_model else row['Model B']\n",
    "        rprint(f\"[green]‚Ä¢ vs {other}: +{row['Advantage']:.1f}% advantage ({row['Effect Size']} effect, p={row['p-value']:.4f})[/green]\")\n",
    "\n",
    "rprint(\"\\n[bold blue]‚úÖ Comprehensive analysis complete![/bold blue]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dashboard: Side-by-Side Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12\n",
    "\n",
    "def create_field_level_visualizations(field_df: pd.DataFrame, field_performance: pd.DataFrame, save_path: str = None):\n",
    "    \"\"\"\n",
    "    Create comprehensive field-level performance visualizations.\n",
    "    \n",
    "    Args:\n",
    "        field_df: Raw field-level accuracy data\n",
    "        field_performance: Analyzed field performance comparison\n",
    "        save_path: Optional path to save visualization\n",
    "    \"\"\"\n",
    "    if field_df.empty or field_performance.empty:\n",
    "        rprint(\"[red]‚ùå Cannot create field visualizations - no data available[/red]\")\n",
    "        return\n",
    "    \n",
    "    # Create figure with improved spacing\n",
    "    fig = plt.figure(figsize=(22, 13))\n",
    "    gs = fig.add_gridspec(2, 2, height_ratios=[1.2, 1], width_ratios=[1.5, 1], \n",
    "                          hspace=0.35, wspace=0.25, top=0.90, bottom=0.06, left=0.06, right=0.97)\n",
    "    \n",
    "    # V2: Derive model names and colors from CONFIG for true configurability\n",
    "    model_order = [CONFIG['models'][key]['display_name'] for key in CONFIG['models']]\n",
    "    fixed_colors = {CONFIG['models'][key]['display_name']: CONFIG['models'][key]['color'] \n",
    "                    for key in CONFIG['models']}\n",
    "    \n",
    "    # Get available models\n",
    "    available_models = field_df['model'].unique()\n",
    "    models = [model for model in model_order if model in available_models]\n",
    "    model_colors = {model: fixed_colors[model] for model in models if model in fixed_colors}\n",
    "    \n",
    "    # 1. Field Accuracy Comparison (Horizontal Bar Chart) - IMPROVED\n",
    "    ax1 = fig.add_subplot(gs[0, :])\n",
    "    \n",
    "    # Get all evaluation fields by average accuracy\n",
    "    all_eval_fields = field_performance\n",
    "    model_cols = [col for col in all_eval_fields.columns if col in models]\n",
    "    \n",
    "    y_pos = np.arange(len(all_eval_fields))\n",
    "    bar_height = 0.22\n",
    "    \n",
    "    # Create bars with better spacing\n",
    "    for idx, model in enumerate(model_cols):\n",
    "        offset = (idx - len(model_cols)/2 + 0.5) * bar_height\n",
    "        bars = ax1.barh(y_pos + offset, all_eval_fields[model], bar_height, \n",
    "                label=model, color=model_colors.get(model, '#999999'), alpha=0.85, edgecolor='white', linewidth=0.5)\n",
    "    \n",
    "    ax1.set_yticks(y_pos)\n",
    "    ax1.set_yticklabels(all_eval_fields.index, fontsize=11)\n",
    "    ax1.invert_yaxis()  # Match heatmap ordering (top-to-bottom)\n",
    "    ax1.set_xlabel('Field Accuracy', fontsize=13, fontweight='bold')\n",
    "    ax1.set_title('All Evaluation Fields: Accuracy by Model (Multi-Turn Bank Extraction)', fontsize=16, fontweight='bold', pad=15)\n",
    "    \n",
    "    # Improved legend placement - outside plot area to avoid occlusion\n",
    "    ax1.legend(loc='upper left', bbox_to_anchor=(1.01, 1), fontsize=11, framealpha=0.98, edgecolor='gray', shadow=True)\n",
    "    ax1.grid(True, alpha=0.3, axis='x', linestyle='--')\n",
    "    ax1.set_xlim(0, 1.05)\n",
    "    ax1.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'{x:.0%}'))\n",
    "    \n",
    "    # Add vertical reference lines\n",
    "    for x in [0.25, 0.5, 0.75, 1.0]:\n",
    "        ax1.axvline(x=x, color='gray', linestyle=':', alpha=0.3, linewidth=0.8)\n",
    "    \n",
    "    # 2. Field Accuracy Heatmap - IMPROVED\n",
    "    ax2 = fig.add_subplot(gs[1, 0])\n",
    "    \n",
    "    # Get all evaluation fields for heatmap (same as bar chart for consistency)\n",
    "    heatmap_data = field_performance[model_cols]\n",
    "    \n",
    "    # Create heatmap with better colors\n",
    "    im = ax2.imshow(heatmap_data.values, cmap='RdYlGn', aspect='auto', vmin=0, vmax=1)\n",
    "    \n",
    "    # Improved tick labels\n",
    "    ax2.set_xticks(np.arange(len(model_cols)))\n",
    "    ax2.set_yticks(np.arange(len(heatmap_data)))\n",
    "    \n",
    "    # V2: Use display names from CONFIG for consistency\n",
    "    model_labels = []\n",
    "    for model in model_cols:\n",
    "        # Use full display names for consistency across all visualizations\n",
    "        model_labels.append(model)\n",
    "    \n",
    "    ax2.set_xticklabels(model_labels, rotation=0, ha='center', fontsize=11, fontweight='bold')\n",
    "    ax2.set_yticklabels(heatmap_data.index, fontsize=10)\n",
    "    \n",
    "    # Improved colorbar\n",
    "    cbar = plt.colorbar(im, ax=ax2, fraction=0.046, pad=0.04)\n",
    "    cbar.set_label('Field Accuracy', rotation=270, labelpad=20, fontsize=11, fontweight='bold')\n",
    "    cbar.ax.tick_params(labelsize=10)\n",
    "    \n",
    "    # Add text annotations with better visibility\n",
    "    for i in range(len(heatmap_data)):\n",
    "        for j in range(len(model_cols)):\n",
    "            value = heatmap_data.iloc[i, j]\n",
    "            if not np.isnan(value):\n",
    "                text_color = 'white' if value < 0.0 else 'black'\n",
    "                ax2.text(j, i, f'{value:.0%}', ha='center', va='center', \n",
    "                        color=text_color, fontsize=9, fontweight='bold')\n",
    "    \n",
    "    ax2.set_title('Field Accuracy Heatmap: All Evaluation Fields', fontsize=15, fontweight='bold', pad=12)\n",
    "    \n",
    "    # 3. Model Specialization Pie Chart - IMPROVED\n",
    "    ax3 = fig.add_subplot(gs[1, 1])\n",
    "    \n",
    "    specialization = field_performance['best_model'].value_counts()\n",
    "    colors_list = [model_colors.get(model, '#999999') for model in specialization.index]\n",
    "    \n",
    "    # Improved pie chart with better labels\n",
    "    wedges, texts, autotexts = ax3.pie(\n",
    "        specialization.values,\n",
    "        labels=None,  # We'll add custom labels\n",
    "        autopct='%1.1f%%',\n",
    "        colors=colors_list,\n",
    "        startangle=90,\n",
    "        textprops={'fontsize': 11, 'fontweight': 'bold'},\n",
    "        explode=[0.02] * len(specialization)  # Slight separation\n",
    "    )\n",
    "    \n",
    "    # Make percentage text bold and white\n",
    "    for autotext in autotexts:\n",
    "        autotext.set_color('white')\n",
    "        autotext.set_fontweight('bold')\n",
    "    \n",
    "    # V2: Use full display names for consistency across all visualizations\n",
    "    legend_labels = []\n",
    "    for model in specialization.index:\n",
    "        count = specialization[model]\n",
    "        legend_labels.append(f'{model}: {count} fields')\n",
    "    \n",
    "    ax3.legend(legend_labels, loc='upper left', fontsize=10, framealpha=0.95, \n",
    "               bbox_to_anchor=(0.02, 0.98), edgecolor='gray')\n",
    "    \n",
    "    ax3.set_title('Model Field Specialization\\n(% of Fields Where Model Performs Best)', \n",
    "                 fontsize=14, fontweight='bold', pad=12)\n",
    "    \n",
    "    # V2: Overall title updated\n",
    "    fig.suptitle('Field-Level Accuracy Analysis\\nComparison Across Models (Multi-Turn Bank Statement Extraction)', \n",
    "                fontsize=19, fontweight='bold')\n",
    "    \n",
    "    # Save the visualization\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=CONFIG['dpi'], bbox_inches='tight', facecolor='white')\n",
    "        rprint(f\"[green]‚úÖ Field-level visualization saved to: {save_path}[/green]\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Create field-level visualizations\n",
    "if not field_level_df.empty and 'field_performance' in locals():\n",
    "    output_path = Path(\"output/visualizations/field_level_accuracy_v2.png\")\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    create_field_level_visualizations(field_level_df, field_performance, str(output_path))\n",
    "else:\n",
    "    rprint(\"[red]‚ùå Cannot create field visualizations - no field-level data available[/red]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Detailed Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14\n",
    "\n",
    "def analyze_model_strengths(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Analyze and compare model strengths and weaknesses.\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return\n",
    "    \n",
    "    rprint(\"\\n[bold green]üéØ DETAILED PERFORMANCE ANALYSIS[/bold green]\")\n",
    "    \n",
    "    # Document type winner analysis\n",
    "    doc_performance = df.groupby(['document_type', 'model'])['overall_accuracy'].mean().unstack()\n",
    "    \n",
    "    rprint(\"\\n[bold blue]üìä Document Type Performance Leaders:[/bold blue]\")\n",
    "    for doc_type in doc_performance.index:\n",
    "        best_model = doc_performance.loc[doc_type].idxmax()\n",
    "        best_score = doc_performance.loc[doc_type].max()\n",
    "        other_model = [m for m in doc_performance.columns if m != best_model][0]\n",
    "        other_score = doc_performance.loc[doc_type, other_model]\n",
    "        improvement = best_score - other_score\n",
    "        \n",
    "        rprint(f\"  ‚Ä¢ **{doc_type}**: {best_model} leads with {best_score:.1f}% (+{improvement:.1f}% advantage)\")\n",
    "    \n",
    "    # Speed analysis\n",
    "    rprint(\"\\n[bold blue]‚ö° Processing Speed Analysis:[/bold blue]\")\n",
    "    speed_comparison = df.groupby('model')['processing_time'].agg(['mean', 'min', 'max', 'std'])\n",
    "    for model in speed_comparison.index:\n",
    "        stats = speed_comparison.loc[model]\n",
    "        throughput = 60 / stats['mean']\n",
    "        rprint(f\"  ‚Ä¢ **{model}**: Avg {stats['mean']:.1f}s ({throughput:.1f} docs/min), Range {stats['min']:.1f}-{stats['max']:.1f}s\")\n",
    "    \n",
    "    # Consistency analysis\n",
    "    rprint(\"\\n[bold blue]üìà Consistency Analysis (Lower std dev = more consistent):[/bold blue]\")\n",
    "    consistency = df.groupby('model')['overall_accuracy'].std()\n",
    "    for model in consistency.index:\n",
    "        rprint(f\"  ‚Ä¢ **{model}**: ¬±{consistency[model]:.1f}% standard deviation\")\n",
    "    \n",
    "    # Efficiency score (accuracy/time ratio)\n",
    "    rprint(\"\\n[bold blue]üí° Efficiency Score (Accuracy per Second):[/bold blue]\")\n",
    "    df['efficiency_score'] = df['overall_accuracy'] / df['processing_time']\n",
    "    efficiency = df.groupby('model')['efficiency_score'].mean()\n",
    "    for model in efficiency.index:\n",
    "        rprint(f\"  ‚Ä¢ **{model}**: {efficiency[model]:.2f} accuracy points per second\")\n",
    "\n",
    "# Run detailed analysis\n",
    "if not combined_df.empty:\n",
    "    analyze_model_strengths(combined_df)\n",
    "else:\n",
    "    rprint(\"[red]‚ùå Cannot run analysis - no data available[/red]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation for Confusion Analysis\n",
    "\n",
    "Load ground truth and prepare batch data for confusion matrix analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 16\n",
    "\n",
    "# Load ground truth as DataFrame for confusion analysis\n",
    "ground_truth_path = Path(CONFIG['ground_truth_path'])\n",
    "\n",
    "if not ground_truth_path.exists():\n",
    "    rprint(f\"[red]‚ùå Ground truth file not found: {ground_truth_path}[/red]\")\n",
    "    ground_truth = pd.DataFrame()\n",
    "else:\n",
    "    # Load FULL ground truth first\n",
    "    ground_truth_full = pd.read_csv(ground_truth_path, dtype=str)\n",
    "    rprint(f\"[dim]  Loaded full ground truth: {len(ground_truth_full)} rows[/dim]\")\n",
    "\n",
    "    # Normalize column name: ground truth uses 'image_name', we need 'image_file'\n",
    "    if 'image_name' in ground_truth_full.columns and 'image_file' not in ground_truth_full.columns:\n",
    "        ground_truth_full['image_file'] = ground_truth_full['image_name']\n",
    "        rprint(f\"[dim]  Normalized: 'image_name' ‚Üí 'image_file'[/dim]\")\n",
    "    elif 'image_file' not in ground_truth_full.columns:\n",
    "        # Try other possible column names\n",
    "        possible_names = ['filename', 'file', 'image']\n",
    "        for col in possible_names:\n",
    "            if col in ground_truth_full.columns:\n",
    "                ground_truth_full['image_file'] = ground_truth_full[col]\n",
    "                rprint(f\"[dim]  Normalized: '{col}' ‚Üí 'image_file'[/dim]\")\n",
    "                break\n",
    "\n",
    "    # Add image_stem column to ground truth for matching\n",
    "    ground_truth_full['image_stem'] = ground_truth_full['image_file'].apply(lambda x: Path(x).stem)\n",
    "    ground_truth = ground_truth_full\n",
    "\n",
    "# Prepare batch dataframes with consistent naming for confusion analysis\n",
    "# Use the loaded data from cell 5\n",
    "llama_batch_df = llama_df.copy() if not llama_df.empty else pd.DataFrame()\n",
    "internvl_batch_df = internvl3_8b_df.copy() if not internvl3_8b_df.empty else pd.DataFrame()\n",
    "internvl_nq_batch_df = internvl3_2b_df.copy() if not internvl3_2b_df.empty else pd.DataFrame()\n",
    "\n",
    "# Verify data is available\n",
    "if llama_batch_df.empty:\n",
    "    rprint(\"[yellow]‚ö†Ô∏è Warning: Llama batch data not loaded. Run cell 5 first.[/yellow]\")\n",
    "else:\n",
    "    rprint(f\"[cyan]Llama batch data: {len(llama_batch_df)} rows[/cyan]\")\n",
    "    if 'document_type' in llama_batch_df.columns:\n",
    "        rprint(f\"[dim]  Predicted document types: {llama_batch_df['document_type'].value_counts().to_dict()}[/dim]\")\n",
    "\n",
    "if internvl_batch_df.empty:\n",
    "    rprint(\"[yellow]‚ö†Ô∏è Warning: InternVL3-8B batch data not loaded. Run cell 5 first.[/yellow]\")\n",
    "else:\n",
    "    rprint(f\"[cyan]InternVL3-8B batch data: {len(internvl_batch_df)} rows[/cyan]\")\n",
    "    if 'document_type' in internvl_batch_df.columns:\n",
    "        rprint(f\"[dim]  Predicted document types: {internvl_batch_df['document_type'].value_counts().to_dict()}[/dim]\")\n",
    "\n",
    "if internvl_nq_batch_df.empty:\n",
    "    rprint(\"[yellow]‚ö†Ô∏è Warning: InternVL3-2B batch data not loaded. Run cell 5 first.[/yellow]\")\n",
    "else:\n",
    "    rprint(f\"[cyan]InternVL3-2B batch data: {len(internvl_nq_batch_df)} rows[/cyan]\")\n",
    "    if 'document_type' in internvl_nq_batch_df.columns:\n",
    "        rprint(f\"[dim]  Predicted document types: {internvl_nq_batch_df['document_type'].value_counts().to_dict()}[/dim]\")\n",
    "\n",
    "if ground_truth.empty:\n",
    "    rprint(\"[yellow]‚ö†Ô∏è Warning: Ground truth not loaded. Confusion analysis will not work.[/yellow]\")\n",
    "elif 'image_file' not in ground_truth.columns:\n",
    "    rprint(\"[red]‚ùå Error: Ground truth missing 'image_file' column[/red]\")\n",
    "    rprint(f\"[yellow]   Available columns: {list(ground_truth.columns)}[/yellow]\")\n",
    "else:\n",
    "    rprint(f\"[cyan]Ground truth: {len(ground_truth)} rows[/cyan]\")\n",
    "\n",
    "rprint(\"[green]‚úÖ Data ready for confusion analysis[/green]\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Document Type Confusion Matrix\n",
    "\n",
    "**Classic confusion matrix showing document type classification accuracy.**\n",
    "\n",
    "Shows how well each model correctly identifies document types (INVOICE, RECEIPT, BANK_STATEMENT) and where misclassifications occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 18\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def abbreviate_doctype(name):\n",
    "    \"\"\"Shorten long document type names for visualization.\"\"\"\n",
    "    abbrev = {\n",
    "        'COMPULSORY THIRD PARTY PERSONAL INJURY INSURANCE GREEN SLIP CERTIFICATE': 'CTP_INSUR',\n",
    "        'E-TICKET ITINERARY, RECEIPT AND TAX INVOICE': 'E-TICKET',\n",
    "        'MOBILE APP SCREENSHOT': 'MOBILE_SS',\n",
    "        'PAYMENT ADVICE': 'PAYMENT',\n",
    "        'CRYPTO STATEMENT': 'CRYPTO',\n",
    "        'TAX INVOICE': 'TAX_INV',\n",
    "        'INVOICE': 'INVOICE',\n",
    "        'RECEIPT': 'RECEIPT',\n",
    "        'BANK_STATEMENT': 'BANK_STMT',\n",
    "        'NOT_FOUND': 'NOT_FOUND'\n",
    "    }\n",
    "    return abbrev.get(name, name[:12])  # Fallback: truncate to 12 chars\n",
    "\n",
    "def create_doctype_confusion_matrix(df_batch: pd.DataFrame, ground_truth_df: pd.DataFrame, model_name: str):\n",
    "    \"\"\"\n",
    "    Create document type confusion matrix using pandas crosstab.\n",
    "\n",
    "    Args:\n",
    "        df_batch: Batch results DataFrame with predicted document types\n",
    "        ground_truth_df: Ground truth DataFrame with true document types\n",
    "        model_name: Name of the model\n",
    "\n",
    "    Returns:\n",
    "        tuple: (confusion_matrix, classification_report_dict, col_labels, row_labels, y_true, y_pred)\n",
    "    \"\"\"\n",
    "    # Normalize image names (strip extensions) for matching\n",
    "    # Batch has extensions (.jpeg, .png), ground truth does not\n",
    "    df_batch['image_stem'] = df_batch['image_file'].apply(lambda x: Path(x).stem)\n",
    "    ground_truth_df['image_stem'] = ground_truth_df['image_file'].apply(lambda x: Path(x).stem)\n",
    "\n",
    "    # Merge to align predictions with ground truth using stems\n",
    "    merged = df_batch.merge(\n",
    "        ground_truth_df[['image_stem', 'DOCUMENT_TYPE']],\n",
    "        on='image_stem',\n",
    "        how='inner',\n",
    "        suffixes=('_pred', '_true')\n",
    "    )\n",
    "\n",
    "    # Get predicted and true document types\n",
    "    # CRITICAL: Use DOCUMENT_TYPE (extracted field) for predictions\n",
    "    #           Use DOCUMENT_TYPE from ground truth for true labels (3 types)\n",
    "\n",
    "    if 'DOCUMENT_TYPE_pred' in merged.columns:\n",
    "        y_pred = merged['DOCUMENT_TYPE_pred'].fillna('UNKNOWN').astype(str).str.strip().str.upper()\n",
    "    else:\n",
    "        y_pred = merged['DOCUMENT_TYPE'].fillna('UNKNOWN').astype(str).str.strip().str.upper()\n",
    "\n",
    "    if 'DOCUMENT_TYPE_true' in merged.columns:\n",
    "        y_true = merged['DOCUMENT_TYPE_true'].fillna('UNKNOWN').astype(str).str.strip().str.upper()\n",
    "    else:\n",
    "        y_true = merged['DOCUMENT_TYPE'].fillna('UNKNOWN').astype(str).str.strip().str.upper()\n",
    "\n",
    "    # Use pandas crosstab to create non-square confusion matrix\n",
    "    # This allows 3 rows (y_true types) √ó N columns (y_pred types)\n",
    "    cm_df = pd.crosstab(y_true, y_pred, dropna=False)\n",
    "\n",
    "    # Convert to numpy array for compatibility with seaborn heatmap\n",
    "    cm = cm_df.values\n",
    "\n",
    "    # Get labels and abbreviate them\n",
    "    labels = [abbreviate_doctype(label) for label in cm_df.columns.tolist()]\n",
    "    row_labels = [abbreviate_doctype(label) for label in cm_df.index.tolist()]\n",
    "\n",
    "    # Compute classification report (use original labels)\n",
    "    report = classification_report(y_true, y_pred, labels=cm_df.columns.tolist(), output_dict=True, zero_division=0)\n",
    "\n",
    "    return cm, report, labels, row_labels, y_true, y_pred\n",
    "\n",
    "# Create confusion matrices for all 3 models\n",
    "rprint(\"[bold cyan]Creating document type confusion matrices for 3 models...[/bold cyan]\")\n",
    "\n",
    "llama_cm, llama_report, llama_labels, llama_row_labels, llama_y_true, llama_y_pred = create_doctype_confusion_matrix(\n",
    "    llama_batch_df, ground_truth, CONFIG['models']['llama']['display_name']\n",
    ")\n",
    "internvl_q_cm, internvl_q_report, internvl_q_labels, internvl_q_row_labels, internvl_q_y_true, internvl_q_y_pred = create_doctype_confusion_matrix(\n",
    "    internvl_batch_df, ground_truth, CONFIG['models']['internvl3_8b']['display_name']\n",
    ")\n",
    "internvl_nq_cm, internvl_nq_report, internvl_nq_labels, internvl_nq_row_labels, internvl_nq_y_true, internvl_nq_y_pred = create_doctype_confusion_matrix(\n",
    "    internvl_nq_batch_df, ground_truth, CONFIG['models']['internvl3_2b']['display_name']\n",
    ")\n",
    "\n",
    "# Plot confusion matrices in 3-panel layout\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(24, 6))\n",
    "\n",
    "# Model 1 confusion matrix\n",
    "sns.heatmap(\n",
    "    llama_cm,\n",
    "    annot=True,\n",
    "    fmt='d',\n",
    "    cmap='Blues',\n",
    "    xticklabels=llama_labels,\n",
    "    yticklabels=llama_row_labels,\n",
    "    cbar_kws={'label': 'Count'},\n",
    "    ax=ax1,\n",
    "    linewidths=0.5,\n",
    "    linecolor='gray'\n",
    ")\n",
    "ax1.set_title(CONFIG['models']['llama']['display_name'], fontsize=13, fontweight='bold')\n",
    "ax1.set_xlabel('Predicted Document Type', fontsize=11, fontweight='bold')\n",
    "ax1.set_ylabel('True Document Type', fontsize=11, fontweight='bold')\n",
    "ax1.tick_params(axis='x', rotation=45, labelsize=9)\n",
    "ax1.tick_params(axis='y', rotation=0, labelsize=9)\n",
    "\n",
    "# Model 2 confusion matrix\n",
    "sns.heatmap(\n",
    "    internvl_q_cm,\n",
    "    annot=True,\n",
    "    fmt='d',\n",
    "    cmap='Blues',\n",
    "    xticklabels=internvl_q_labels,\n",
    "    yticklabels=internvl_q_row_labels,\n",
    "    cbar_kws={'label': 'Count'},\n",
    "    ax=ax2,\n",
    "    linewidths=0.5,\n",
    "    linecolor='gray'\n",
    ")\n",
    "ax2.set_title(CONFIG['models']['internvl3_8b']['display_name'], fontsize=13, fontweight='bold')\n",
    "ax2.set_xlabel('Predicted Document Type', fontsize=11, fontweight='bold')\n",
    "ax2.set_ylabel('True Document Type', fontsize=11, fontweight='bold')\n",
    "ax2.tick_params(axis='x', rotation=45, labelsize=9)\n",
    "ax2.tick_params(axis='y', rotation=0, labelsize=9)\n",
    "\n",
    "# Model 3 confusion matrix\n",
    "sns.heatmap(\n",
    "    internvl_nq_cm,\n",
    "    annot=True,\n",
    "    fmt='d',\n",
    "    cmap='Blues',\n",
    "    xticklabels=internvl_nq_labels,\n",
    "    yticklabels=internvl_nq_row_labels,\n",
    "    cbar_kws={'label': 'Count'},\n",
    "    ax=ax3,\n",
    "    linewidths=0.5,\n",
    "    linecolor='gray'\n",
    ")\n",
    "ax3.set_title(CONFIG['models']['internvl3_2b']['display_name'], fontsize=13, fontweight='bold')\n",
    "ax3.set_xlabel('Predicted Document Type', fontsize=11, fontweight='bold')\n",
    "ax3.set_ylabel('True Document Type', fontsize=11, fontweight='bold')\n",
    "ax3.tick_params(axis='x', rotation=45, labelsize=9)\n",
    "ax3.tick_params(axis='y', rotation=0, labelsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(CONFIG['output_dir'].replace('/csv', '/visualizations') + '/doctype_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "rprint(\"[green]‚úÖ 3-panel document type confusion matrices created[/green]\")\n",
    "\n",
    "# Display classification reports\n",
    "rprint(f\"\\n[bold blue]{CONFIG['models']['llama']['display_name']} - Document Type Classification Report:[/bold blue]\")\n",
    "llama_report_df = pd.DataFrame(llama_report).transpose()\n",
    "display(llama_report_df)\n",
    "\n",
    "rprint(f\"\\n[bold blue]{CONFIG['models']['internvl3_8b']['display_name']} - Document Type Classification Report:[/bold blue]\")\n",
    "internvl_q_report_df = pd.DataFrame(internvl_q_report).transpose()\n",
    "display(internvl_q_report_df)\n",
    "\n",
    "rprint(f\"\\n[bold blue]{CONFIG['models']['internvl3_2b']['display_name']} - Document Type Classification Report:[/bold blue]\")\n",
    "internvl_nq_report_df = pd.DataFrame(internvl_nq_report).transpose()\n",
    "display(internvl_nq_report_df)\n",
    "\n",
    "# Save classification reports\n",
    "llama_report_df.to_csv(CONFIG['output_dir'] + '/llama_doctype_classification_report.csv')\n",
    "internvl_q_report_df.to_csv(CONFIG['output_dir'] + '/internvl3_8b_doctype_classification_report.csv')\n",
    "internvl_nq_report_df.to_csv(CONFIG['output_dir'] + '/internvl3_2b_doctype_classification_report.csv')\n",
    "\n",
    "# Summary accuracy\n",
    "llama_accuracy = (llama_y_true == llama_y_pred).sum() / len(llama_y_true) * 100\n",
    "internvl_q_accuracy = (internvl_q_y_true == internvl_q_y_pred).sum() / len(internvl_q_y_true) * 100\n",
    "internvl_nq_accuracy = (internvl_nq_y_true == internvl_nq_y_pred).sum() / len(internvl_nq_y_true) * 100\n",
    "\n",
    "rprint(\"\\n[bold blue]Document Type Classification Accuracy:[/bold blue]\")\n",
    "rprint(f\"[cyan]{CONFIG['models']['llama']['display_name']}: {llama_accuracy:.1f}%[/cyan]\")\n",
    "rprint(f\"[cyan]{CONFIG['models']['internvl3_8b']['display_name']}: {internvl_q_accuracy:.1f}%[/cyan]\")\n",
    "rprint(f\"[cyan]{CONFIG['models']['internvl3_2b']['display_name']}: {internvl_nq_accuracy:.1f}%[/cyan]\")\n",
    "\n",
    "rprint(\"[green]‚úÖ Document type confusion analysis complete for all 3 models[/green]\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Field-Level Confusion Analysis\n",
    "\n",
    "**Confusion matrix showing field extraction status (correct/incorrect/not_found) for each field type.**\n",
    "\n",
    "This analysis reveals:\n",
    "- Which fields are most accurately extracted\n",
    "- Which fields are frequently incorrect vs not found\n",
    "- Model-specific strengths and weaknesses per field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 20\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define field columns (from llama_batch.ipynb)\n",
    "FIELD_COLUMNS = [\n",
    "    'DOCUMENT_TYPE', 'BUSINESS_ABN', 'SUPPLIER_NAME', 'BUSINESS_ADDRESS',\n",
    "    'PAYER_NAME', 'PAYER_ADDRESS', 'INVOICE_DATE', 'LINE_ITEM_DESCRIPTIONS',\n",
    "    'LINE_ITEM_QUANTITIES', 'LINE_ITEM_PRICES', 'LINE_ITEM_TOTAL_PRICES',\n",
    "    'IS_GST_INCLUDED', 'GST_AMOUNT', 'TOTAL_AMOUNT', 'STATEMENT_DATE_RANGE',\n",
    "    'TRANSACTION_DATES', 'TRANSACTION_AMOUNTS_PAID'\n",
    "]\n",
    "\n",
    "def create_field_confusion_data(df_batch: pd.DataFrame, ground_truth_df: pd.DataFrame, model_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create field-level confusion data showing correct/incorrect/not_found status.\n",
    "\n",
    "    Args:\n",
    "        df_batch: Batch results DataFrame with extracted field values\n",
    "        ground_truth_df: Ground truth DataFrame\n",
    "        model_name: Name of the model\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with columns: field, status (correct/incorrect/not_found), count, model\n",
    "    \"\"\"\n",
    "    confusion_data = []\n",
    "\n",
    "    for field in FIELD_COLUMNS:\n",
    "        if field not in df_batch.columns or field not in ground_truth_df.columns:\n",
    "            continue\n",
    "\n",
    "        correct_count = 0\n",
    "        incorrect_count = 0\n",
    "        not_found_count = 0\n",
    "\n",
    "        # Normalize image names (strip extensions) for matching\n",
    "        # Batch has extensions (.jpeg, .png), ground truth does not\n",
    "        if 'image_stem' not in df_batch.columns:\n",
    "            df_batch['image_stem'] = df_batch['image_file'].apply(lambda x: Path(x).stem)\n",
    "        if 'image_stem' not in ground_truth_df.columns:\n",
    "            ground_truth_df['image_stem'] = ground_truth_df['image_file'].apply(lambda x: Path(x).stem)\n",
    "\n",
    "        # Merge on image_stem to align predictions with ground truth\n",
    "        merged = df_batch.merge(\n",
    "            ground_truth_df[['image_stem', field]],\n",
    "            on='image_stem',\n",
    "            how='inner',\n",
    "            suffixes=('_pred', '_true')\n",
    "        )\n",
    "\n",
    "        pred_col = f'{field}_pred' if f'{field}_pred' in merged.columns else field\n",
    "        true_col = f'{field}_true' if f'{field}_true' in merged.columns else field\n",
    "\n",
    "        for _, row in merged.iterrows():\n",
    "            pred_val = str(row[pred_col]).strip().upper()\n",
    "            true_val = str(row[true_col]).strip().upper()\n",
    "\n",
    "            if pred_val == 'NOT_FOUND' or pred_val == 'NAN' or pred_val == '':\n",
    "                not_found_count += 1\n",
    "            elif pred_val == true_val:\n",
    "                correct_count += 1\n",
    "            else:\n",
    "                incorrect_count += 1\n",
    "\n",
    "        # Add rows for each status\n",
    "        confusion_data.append({\n",
    "            'field': field,\n",
    "            'status': 'correct',\n",
    "            'count': correct_count,\n",
    "            'model': model_name\n",
    "        })\n",
    "        confusion_data.append({\n",
    "            'field': field,\n",
    "            'status': 'incorrect',\n",
    "            'count': incorrect_count,\n",
    "            'model': model_name\n",
    "        })\n",
    "        confusion_data.append({\n",
    "            'field': field,\n",
    "            'status': 'not_found',\n",
    "            'count': not_found_count,\n",
    "            'model': model_name\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(confusion_data)\n",
    "\n",
    "# Create confusion data for all 3 models\n",
    "rprint(\"[bold cyan]Creating field-level confusion matrices...[/bold cyan]\")\n",
    "\n",
    "llama_confusion = create_field_confusion_data(llama_batch_df, ground_truth, CONFIG['models']['llama']['display_name'])\n",
    "internvl_confusion = create_field_confusion_data(internvl_batch_df, ground_truth, CONFIG['models']['internvl3_8b']['display_name'])\n",
    "internvl_nq_confusion = create_field_confusion_data(internvl_nq_batch_df, ground_truth, CONFIG['models']['internvl3_2b']['display_name'])\n",
    "\n",
    "# Combine all 3 models\n",
    "all_confusion = pd.concat([llama_confusion, internvl_confusion, internvl_nq_confusion], ignore_index=True)\n",
    "\n",
    "# Create pivot table for heatmap visualization\n",
    "def plot_confusion_heatmap(confusion_df: pd.DataFrame, model_name: str, ax):\n",
    "    \"\"\"Plot confusion matrix heatmap for a single model.\"\"\"\n",
    "    # Pivot to get fields x status matrix\n",
    "    pivot = confusion_df[confusion_df['model'] == model_name].pivot(\n",
    "        index='field',\n",
    "        columns='status',\n",
    "        values='count'\n",
    "    )\n",
    "\n",
    "    # Reorder columns: correct, incorrect, not_found\n",
    "    pivot = pivot[['correct', 'incorrect', 'not_found']]\n",
    "\n",
    "    # Sort by correct count (descending) for better visualization\n",
    "    pivot = pivot.sort_values('correct', ascending=False)\n",
    "\n",
    "    # Create heatmap\n",
    "    sns.heatmap(\n",
    "        pivot,\n",
    "        annot=True,\n",
    "        fmt='g',\n",
    "        cmap='RdYlGn_r',  # Red for high incorrect/not_found, green for high correct\n",
    "        cbar_kws={'label': 'Count'},\n",
    "        ax=ax,\n",
    "        linewidths=0.5,\n",
    "        linecolor='gray'\n",
    "    )\n",
    "\n",
    "    ax.set_title(f'{model_name} - Field Extraction Status', fontsize=13, fontweight='bold')\n",
    "    ax.set_xlabel('Extraction Status', fontsize=11, fontweight='bold')\n",
    "    ax.set_ylabel('Field Name', fontsize=11, fontweight='bold')\n",
    "    ax.tick_params(axis='y', rotation=0)\n",
    "\n",
    "# Create 3-panel heatmaps\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(24, 12))\n",
    "\n",
    "plot_confusion_heatmap(all_confusion, CONFIG['models']['llama']['display_name'], ax1)\n",
    "plot_confusion_heatmap(all_confusion, CONFIG['models']['internvl3_8b']['display_name'], ax2)\n",
    "plot_confusion_heatmap(all_confusion, CONFIG['models']['internvl3_2b']['display_name'], ax3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(CONFIG['output_dir'].replace('/csv', '/visualizations') + '/field_confusion_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "rprint(\"[green]‚úÖ Field-level confusion heatmaps created (3 models)[/green]\")\n",
    "\n",
    "# Create summary statistics\n",
    "rprint(\"\\n[bold blue]Field Confusion Summary:[/bold blue]\")\n",
    "for model in [CONFIG['models'][k]['display_name'] for k in CONFIG['models']]:\n",
    "    model_data = all_confusion[all_confusion['model'] == model]\n",
    "    total = model_data['count'].sum()\n",
    "    correct = model_data[model_data['status'] == 'correct']['count'].sum()\n",
    "    incorrect = model_data[model_data['status'] == 'incorrect']['count'].sum()\n",
    "    not_found = model_data[model_data['status'] == 'not_found']['count'].sum()\n",
    "\n",
    "    rprint(f\"\\n[cyan]{model}:[/cyan]\")\n",
    "    rprint(f\"  Correct: {correct}/{total} ({correct/total*100:.1f}%)\")\n",
    "    rprint(f\"  Incorrect: {incorrect}/{total} ({incorrect/total*100:.1f}%)\")\n",
    "    rprint(f\"  Not Found: {not_found}/{total} ({not_found/total*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 Per-Field Precision, Recall, and F1 Metrics\n",
    "\n",
    "**Using sklearn's classification metrics to evaluate per-field extraction performance.**\n",
    "\n",
    "Metrics explained:\n",
    "- **Precision**: Of all predicted values, what % were correct?\n",
    "- **Recall**: Of all ground truth values, what % were correctly extracted?\n",
    "- **F1 Score**: Harmonic mean of precision and recall\n",
    "- **Support**: Number of occurrences of each field in ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 22\n",
    "\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def compute_per_field_metrics(df_batch: pd.DataFrame, ground_truth_df: pd.DataFrame, model_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute precision, recall, F1 for each field using calculate_field_accuracy_f1.\n",
    "\n",
    "    Uses the SAME F1 calculation as the rest of the notebook - proper field-type-aware\n",
    "    F1 scoring with consistent TP/FP/FN counting.\n",
    "\n",
    "    Args:\n",
    "        df_batch: Batch results DataFrame with extracted field values\n",
    "        ground_truth_df: Ground truth DataFrame\n",
    "        model_name: Name of the model\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with columns: field, precision, recall, f1_score, accuracy, support, model\n",
    "    \"\"\"\n",
    "    import sys\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "    from common.evaluation_metrics import calculate_field_accuracy_f1\n",
    "\n",
    "    metrics_data = []\n",
    "\n",
    "    for field in FIELD_COLUMNS:\n",
    "        if field not in df_batch.columns or field not in ground_truth_df.columns:\n",
    "            continue\n",
    "\n",
    "        # Normalize image names (strip extensions) for matching\n",
    "        if 'image_stem' not in df_batch.columns:\n",
    "            df_batch['image_stem'] = df_batch['image_file'].apply(lambda x: Path(x).stem)\n",
    "        if 'image_stem' not in ground_truth_df.columns:\n",
    "            ground_truth_df['image_stem'] = ground_truth_df['image_file'].apply(lambda x: Path(x).stem)\n",
    "\n",
    "        # Merge on image_stem to align predictions with ground truth\n",
    "        merged = df_batch.merge(\n",
    "            ground_truth_df[['image_stem', field]],\n",
    "            on='image_stem',\n",
    "            how='inner',\n",
    "            suffixes=('_pred', '_true')\n",
    "        )\n",
    "\n",
    "        pred_col = f'{field}_pred' if f'{field}_pred' in merged.columns else field\n",
    "        true_col = f'{field}_true' if f'{field}_true' in merged.columns else field\n",
    "\n",
    "        if len(merged) == 0:\n",
    "            continue\n",
    "\n",
    "        # Collect F1 metrics from each row using calculate_field_accuracy_f1\n",
    "        f1_scores = []\n",
    "        precision_scores = []\n",
    "        recall_scores = []\n",
    "\n",
    "        for _, row in merged.iterrows():\n",
    "            pred_val = str(row[pred_col]) if pd.notna(row[pred_col]) else 'NOT_FOUND'\n",
    "            true_val = str(row[true_col]) if pd.notna(row[true_col]) else 'NOT_FOUND'\n",
    "\n",
    "            # Use calculate_field_accuracy_f1 for proper F1 metrics\n",
    "            metrics = calculate_field_accuracy_f1(pred_val, true_val, field, debug=False)\n",
    "\n",
    "            f1_scores.append(metrics['f1_score'])\n",
    "            precision_scores.append(metrics['precision'])\n",
    "            recall_scores.append(metrics['recall'])\n",
    "\n",
    "        # Average metrics across all documents for this field\n",
    "        total = len(f1_scores)\n",
    "        avg_f1 = sum(f1_scores) / total if total > 0 else 0\n",
    "        avg_precision = sum(precision_scores) / total if total > 0 else 0\n",
    "        avg_recall = sum(recall_scores) / total if total > 0 else 0\n",
    "\n",
    "        metrics_data.append({\n",
    "            'field': field,\n",
    "            'precision': avg_precision,\n",
    "            'recall': avg_recall,\n",
    "            'f1_score': avg_f1,\n",
    "            'support': total,\n",
    "            'model': model_name\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(metrics_data)\n",
    "\n",
    "# Compute metrics for all 3 models\n",
    "rprint(\"[bold cyan]Computing per-field precision/recall/F1 metrics...[/bold cyan]\")\n",
    "\n",
    "llama_metrics = compute_per_field_metrics(llama_batch_df, ground_truth, 'Llama-11B')\n",
    "internvl_metrics = compute_per_field_metrics(internvl_batch_df, ground_truth, 'InternVL3-8B')\n",
    "internvl_nq_metrics = compute_per_field_metrics(internvl_nq_batch_df, ground_truth, 'InternVL3-2B')\n",
    "\n",
    "# Combine all 3 models\n",
    "all_metrics = pd.concat([llama_metrics, internvl_metrics, internvl_nq_metrics], ignore_index=True)\n",
    "\n",
    "# Display metrics table\n",
    "rprint(\"\\n[bold blue]Per-Field Metrics Comparison:[/bold blue]\")\n",
    "display(all_metrics.sort_values(['field', 'model']))\n",
    "\n",
    "# Save to CSV\n",
    "metrics_csv_path = CONFIG['output_dir'] + '/per_field_metrics.csv'\n",
    "all_metrics.to_csv(metrics_csv_path, index=False)\n",
    "rprint(f\"[green]‚úÖ Metrics saved to: {metrics_csv_path}[/green]\")\n",
    "\n",
    "# Create visualizations - 3 charts: F1, Precision, Recall\n",
    "fig, axes = plt.subplots(1, 3, figsize=(24, 10))\n",
    "\n",
    "# Colors for 3 models\n",
    "model_colors = ['#3498db', '#e74c3c', '#2ecc71']\n",
    "\n",
    "# Sort fields by average F1 score descending (highest F1 at top of chart)\n",
    "# For barh: last item in index appears at top, so sort ascending to get highest at top\n",
    "field_order = all_metrics.groupby('field')['f1_score'].mean().sort_values(ascending=True).index.tolist()\n",
    "\n",
    "# Plot 1: F1 Score (ranked by F1 descending - highest at top)\n",
    "ax1 = axes[0]\n",
    "pivot_f1 = all_metrics.pivot(index='field', columns='model', values='f1_score').reindex(field_order)\n",
    "pivot_f1.plot(kind='barh', ax=ax1, color=model_colors)\n",
    "ax1.set_title('F1 Score by Field (Ranked)', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('F1 Score', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Field', fontsize=12, fontweight='bold')\n",
    "ax1.get_legend().remove() if ax1.get_legend() else None\n",
    "ax1.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Plot 2: Precision (same field order as F1)\n",
    "ax2 = axes[1]\n",
    "pivot_precision = all_metrics.pivot(index='field', columns='model', values='precision').reindex(field_order)\n",
    "pivot_precision.plot(kind='barh', ax=ax2, color=model_colors)\n",
    "ax2.set_title('Precision by Field', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Precision', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Field', fontsize=12, fontweight='bold')\n",
    "ax2.get_legend().remove() if ax2.get_legend() else None\n",
    "ax2.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Plot 3: Recall (same field order as F1)\n",
    "ax3 = axes[2]\n",
    "pivot_recall = all_metrics.pivot(index='field', columns='model', values='recall').reindex(field_order)\n",
    "pivot_recall.plot(kind='barh', ax=ax3, color=model_colors)\n",
    "ax3.set_title('Recall by Field', fontsize=14, fontweight='bold')\n",
    "ax3.set_xlabel('Recall', fontsize=12, fontweight='bold')\n",
    "ax3.set_ylabel('Field', fontsize=12, fontweight='bold')\n",
    "ax3.get_legend().remove() if ax3.get_legend() else None\n",
    "ax3.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add main title\n",
    "fig.suptitle('Per-Field Metrics (Ranked by F1 Score)', fontsize=18, fontweight='bold', y=1.02)\n",
    "\n",
    "# Single shared legend\n",
    "handles, labels = ax1.get_legend_handles_labels()\n",
    "fig.legend(handles, labels, title='Model', loc='upper center', bbox_to_anchor=(0.5, 0.99), ncol=3, fontsize=11, title_fontsize=12)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.savefig(CONFIG['output_dir'].replace('/csv', '/visualizations') + '/per_field_metrics.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "rprint(\"[green]‚úÖ Per-field metrics visualizations created (3 models)[/green]\")\n",
    "\n",
    "# Summary statistics - include both MEDIAN and MEAN F1\n",
    "rprint(\"\\n[bold blue]Model Performance Summary (Mean & Median):[/bold blue]\")\n",
    "summary_stats = all_metrics.groupby('model').agg({\n",
    "    'precision': ['mean', 'median'],\n",
    "    'recall': ['mean', 'median'],\n",
    "    'f1_score': ['mean', 'median']\n",
    "}).round(4)\n",
    "\n",
    "# Flatten column names for display\n",
    "summary_stats.columns = [f'{col[0]}_{col[1]}' for col in summary_stats.columns]\n",
    "display(summary_stats)\n",
    "\n",
    "# Also show a simplified view highlighting F1\n",
    "rprint(\"\\n[bold yellow]F1 Score Summary (Key Metric):[/bold yellow]\")\n",
    "f1_summary = all_metrics.groupby('model')['f1_score'].agg(['mean', 'median', 'std', 'min', 'max']).round(4)\n",
    "f1_summary.columns = ['Mean F1', 'Median F1', 'Std Dev', 'Min F1', 'Max F1']\n",
    "display(f1_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.7 Hallucination Analysis\n",
    "\n",
    "**Purpose**: Measure how often models invent values for fields that don't exist (NOT_FOUND in ground truth).\n",
    "\n",
    "**Key Metrics**:\n",
    "- **Hallucination Rate**: Percentage of NOT_FOUND fields where model extracted a value\n",
    "- **False Positive Rate (FPR)**: FP / (FP + TN) on NOT_FOUND fields\n",
    "- **Per-Field Hallucination**: Which fields are most hallucinated\n",
    "- **Per-Document Hallucination**: Distribution of hallucination rates across documents\n",
    "\n",
    "**Context**: From the accuracy paradox:\n",
    "- Low accuracy + high F1 ‚Üí High hallucination (Llama)\n",
    "- High accuracy + low F1 ‚Üí Low hallucination (InternVL3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 24\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def comprehensive_hallucination_analysis(model_df, ground_truth_df, model_name):\n",
    "    \"\"\"\n",
    "    Complete hallucination analysis for a model.\n",
    "    \n",
    "    Hallucination = Model extracts value when ground truth is NOT_FOUND\n",
    "    \"\"\"\n",
    "    \n",
    "    # Normalize image names for matching\n",
    "    if 'image_stem' not in model_df.columns:\n",
    "        model_df['image_stem'] = model_df['image_file'].apply(lambda x: Path(x).stem)\n",
    "    if 'image_stem' not in ground_truth_df.columns:\n",
    "        ground_truth_df['image_stem'] = ground_truth_df['image_file'].apply(lambda x: Path(x).stem)\n",
    "    \n",
    "    # Merge to align predictions with ground truth\n",
    "    merged = model_df.merge(\n",
    "        ground_truth_df,\n",
    "        on='image_stem',\n",
    "        how='inner',\n",
    "        suffixes=('_pred', '_true')\n",
    "    )\n",
    "    \n",
    "    total_hallucinations = 0\n",
    "    total_not_found_fields = 0\n",
    "    total_correct_not_found = 0\n",
    "    \n",
    "    field_hallucination = {}\n",
    "    doc_hallucination_scores = []\n",
    "    \n",
    "    for idx in range(len(merged)):\n",
    "        doc_hallucinations = 0\n",
    "        doc_not_found = 0\n",
    "        doc_correct_not_found = 0\n",
    "        \n",
    "        for field in FIELD_COLUMNS:\n",
    "            # SKIP excluded fields\n",
    "            if field in ['TRANSACTION_AMOUNTS_RECEIVED', 'ACCOUNT_BALANCE']:\n",
    "                continue\n",
    "            \n",
    "            # Get column names (handle suffix)\n",
    "            true_col = f'{field}_true' if f'{field}_true' in merged.columns else field\n",
    "            pred_col = f'{field}_pred' if f'{field}_pred' in merged.columns else field\n",
    "            \n",
    "            if true_col not in merged.columns or pred_col not in merged.columns:\n",
    "                continue\n",
    "            \n",
    "            gt_val = str(merged.loc[idx, true_col]).strip().upper()\n",
    "            pred_val = str(merged.loc[idx, pred_col]).strip().upper()\n",
    "            \n",
    "            if gt_val in ['NOT_FOUND', 'NAN', '']:\n",
    "                doc_not_found += 1\n",
    "                total_not_found_fields += 1\n",
    "                \n",
    "                if pred_val not in ['NOT_FOUND', 'NAN', '']:\n",
    "                    # HALLUCINATION DETECTED\n",
    "                    doc_hallucinations += 1\n",
    "                    total_hallucinations += 1\n",
    "                    \n",
    "                    # Track per-field\n",
    "                    if field not in field_hallucination:\n",
    "                        field_hallucination[field] = {'hallucinated': 0, 'not_found_total': 0}\n",
    "                    field_hallucination[field]['hallucinated'] += 1\n",
    "                    field_hallucination[field]['not_found_total'] += 1\n",
    "                else:\n",
    "                    # Correctly said NOT_FOUND\n",
    "                    doc_correct_not_found += 1\n",
    "                    total_correct_not_found += 1\n",
    "                    \n",
    "                    if field not in field_hallucination:\n",
    "                        field_hallucination[field] = {'hallucinated': 0, 'not_found_total': 0}\n",
    "                    field_hallucination[field]['not_found_total'] += 1\n",
    "        \n",
    "        # Document-level hallucination rate\n",
    "        doc_rate = doc_hallucinations / doc_not_found if doc_not_found > 0 else 0\n",
    "        doc_hallucination_scores.append({\n",
    "            'rate': doc_rate,\n",
    "            'hallucinated_count': doc_hallucinations,\n",
    "            'not_found_count': doc_not_found\n",
    "        })\n",
    "    \n",
    "    # Calculate overall rates\n",
    "    overall_rate = total_hallucinations / total_not_found_fields if total_not_found_fields > 0 else 0\n",
    "    \n",
    "    # DEBUG: Print hallucination calculations\n",
    "    rprint(f\"[yellow]DEBUG {model_name}: total_hallucinations={total_hallucinations}, total_not_found_fields={total_not_found_fields}, overall_rate={overall_rate:.2%}[/yellow]\")\n",
    "    \n",
    "    correct_not_found_rate = total_correct_not_found / total_not_found_fields if total_not_found_fields > 0 else 0\n",
    "    \n",
    "    # Per-field hallucination rates\n",
    "    field_rates = {}\n",
    "    for field, data in field_hallucination.items():\n",
    "        if data['not_found_total'] > 0:\n",
    "            field_rates[field] = {\n",
    "                'hallucination_rate': data['hallucinated'] / data['not_found_total'],\n",
    "                'hallucinated_count': data['hallucinated'],\n",
    "                'opportunities': data['not_found_total']\n",
    "            }\n",
    "    \n",
    "    return {\n",
    "        'model': model_name,\n",
    "        'overall_hallucination_rate': overall_rate,\n",
    "        'correct_not_found_rate': correct_not_found_rate,\n",
    "        'total_hallucinations': total_hallucinations,\n",
    "        'total_correct_not_found': total_correct_not_found,\n",
    "        'total_not_found_opportunities': total_not_found_fields,\n",
    "        'field_hallucination': field_rates,\n",
    "        'document_hallucination_scores': doc_hallucination_scores,\n",
    "        'mean_doc_hallucination': np.mean([d['rate'] for d in doc_hallucination_scores]),\n",
    "        'std_doc_hallucination': np.std([d['rate'] for d in doc_hallucination_scores])\n",
    "    }\n",
    "\n",
    "# Run hallucination analysis for all 3 models\n",
    "rprint(\"[bold cyan]Analyzing hallucination rates for all models...[/bold cyan]\")\n",
    "\n",
    "llama_hallucination = comprehensive_hallucination_analysis(\n",
    "    llama_df, ground_truth, 'Llama-11B'\n",
    ")\n",
    "\n",
    "internvl_hallucination = comprehensive_hallucination_analysis(\n",
    "    internvl3_8b_df, ground_truth, 'InternVL3-8B'\n",
    ")\n",
    "\n",
    "internvl_nq_hallucination = comprehensive_hallucination_analysis(\n",
    "    internvl3_2b_df, ground_truth, 'InternVL3-2B'\n",
    ")\n",
    "\n",
    "# Create summary table\n",
    "hallucination_summary = pd.DataFrame([\n",
    "    {\n",
    "        'Model': llama_hallucination['model'],\n",
    "        'Hallucination Rate': f\"{llama_hallucination['overall_hallucination_rate']:.1%}\",\n",
    "        'Correct NOT_FOUND Rate': f\"{llama_hallucination['correct_not_found_rate']:.1%}\",\n",
    "        'Total Hallucinations': llama_hallucination['total_hallucinations'],\n",
    "        'Total Opportunities': llama_hallucination['total_not_found_opportunities'],\n",
    "        'Mean Doc Hallucination': f\"{llama_hallucination['mean_doc_hallucination']:.1%}\",\n",
    "        'Std Dev': f\"{llama_hallucination['std_doc_hallucination']:.1%}\"\n",
    "    },\n",
    "    {\n",
    "        'Model': internvl_hallucination['model'],\n",
    "        'Hallucination Rate': f\"{internvl_hallucination['overall_hallucination_rate']:.1%}\",\n",
    "        'Correct NOT_FOUND Rate': f\"{internvl_hallucination['correct_not_found_rate']:.1%}\",\n",
    "        'Total Hallucinations': internvl_hallucination['total_hallucinations'],\n",
    "        'Total Opportunities': internvl_hallucination['total_not_found_opportunities'],\n",
    "        'Mean Doc Hallucination': f\"{internvl_hallucination['mean_doc_hallucination']:.1%}\",\n",
    "        'Std Dev': f\"{internvl_hallucination['std_doc_hallucination']:.1%}\"\n",
    "    },\n",
    "    {\n",
    "        'Model': internvl_nq_hallucination['model'],\n",
    "        'Hallucination Rate': f\"{internvl_nq_hallucination['overall_hallucination_rate']:.1%}\",\n",
    "        'Correct NOT_FOUND Rate': f\"{internvl_nq_hallucination['correct_not_found_rate']:.1%}\",\n",
    "        'Total Hallucinations': internvl_nq_hallucination['total_hallucinations'],\n",
    "        'Total Opportunities': internvl_nq_hallucination['total_not_found_opportunities'],\n",
    "        'Mean Doc Hallucination': f\"{internvl_nq_hallucination['mean_doc_hallucination']:.1%}\",\n",
    "        'Std Dev': f\"{internvl_nq_hallucination['std_doc_hallucination']:.1%}\"\n",
    "    }\n",
    "])\n",
    "\n",
    "rprint(\"\\n[bold blue]Hallucination Summary:[/bold blue]\")\n",
    "display(hallucination_summary)\n",
    "\n",
    "# Interpretation\n",
    "rprint(\"\\n[bold yellow]Interpretation:[/bold yellow]\")\n",
    "rprint(\"[dim]Hallucination Rate = % of NOT_FOUND fields where model invented a value[/dim]\")\n",
    "rprint(\"[dim]Correct NOT_FOUND Rate = % of NOT_FOUND fields correctly identified[/dim]\")\n",
    "rprint(\"[dim]Higher hallucination = More aggressive extraction (high recall, low accuracy)[/dim]\")\n",
    "rprint(\"[dim]Lower hallucination = More conservative extraction (low recall, high accuracy)[/dim]\")\n",
    "\n",
    "# Create visualizations\n",
    "fig = plt.figure(figsize=(24, 16))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# Plot 1: Overall Hallucination Rate Comparison\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "models = [llama_hallucination['model'], internvl_hallucination['model'], internvl_nq_hallucination['model']]\n",
    "halluc_rates = [\n",
    "    llama_hallucination['overall_hallucination_rate'],\n",
    "    internvl_hallucination['overall_hallucination_rate'],\n",
    "    internvl_nq_hallucination['overall_hallucination_rate']\n",
    "]\n",
    "colors = ['#e74c3c', '#3498db', '#2ecc71']  # Red, Blue, Green\n",
    "\n",
    "bars = ax1.bar(models, halluc_rates, color=colors, alpha=0.7)\n",
    "ax1.set_ylabel('Hallucination Rate', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Overall Hallucination Rate by Model', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylim(0, max(halluc_rates) * 1.2)\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, rate in zip(bars, halluc_rates):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{rate:.1%}',\n",
    "             ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "\n",
    "# Plot 2: Hallucination vs Correct NOT_FOUND\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "x = np.arange(len(models))\n",
    "width = 0.35\n",
    "\n",
    "halluc_counts = [\n",
    "    llama_hallucination['total_hallucinations'],\n",
    "    internvl_hallucination['total_hallucinations'],\n",
    "    internvl_nq_hallucination['total_hallucinations']\n",
    "]\n",
    "correct_counts = [\n",
    "    llama_hallucination['total_correct_not_found'],\n",
    "    internvl_hallucination['total_correct_not_found'],\n",
    "    internvl_nq_hallucination['total_correct_not_found']\n",
    "]\n",
    "\n",
    "bars1 = ax2.bar(x - width/2, halluc_counts, width, label='Hallucinated', color='#e74c3c', alpha=0.7)\n",
    "bars2 = ax2.bar(x + width/2, correct_counts, width, label='Correct NOT_FOUND', color='#2ecc71', alpha=0.7)\n",
    "\n",
    "ax2.set_ylabel('Count', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Hallucinations vs Correct NOT_FOUND', fontsize=14, fontweight='bold')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(models, rotation=15, ha='right')\n",
    "ax2.legend()\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Plot 3: Hallucination Rate vs Recall (Tradeoff)\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "\n",
    "# Calculate recall from batch results (NOT hardcoded!)\n",
    "llama_recall = (llama_df['fields_matched'] / llama_df['total_fields']).mean() if not llama_df.empty else 0\n",
    "internvl_8b_recall = (internvl3_8b_df['fields_matched'] / internvl3_8b_df['total_fields']).mean() if not internvl3_8b_df.empty else 0\n",
    "internvl_2b_recall = (internvl3_2b_df['fields_matched'] / internvl3_2b_df['total_fields']).mean() if not internvl3_2b_df.empty else 0\n",
    "\n",
    "recalls = [llama_recall, internvl_8b_recall, internvl_2b_recall]\n",
    "\n",
    "ax3.scatter(halluc_rates, recalls, s=200, c=colors, alpha=0.7, edgecolors='black', linewidth=2)\n",
    "for i, model in enumerate(models):\n",
    "    ax3.annotate(model, (halluc_rates[i], recalls[i]), \n",
    "                xytext=(10, -5), textcoords='offset points', fontsize=10, fontweight='bold')\n",
    "\n",
    "ax3.set_xlabel('Hallucination Rate', fontsize=12, fontweight='bold')\n",
    "ax3.set_ylabel('Recall', fontsize=12, fontweight='bold')\n",
    "ax3.set_title('Hallucination vs Recall Tradeoff', fontsize=14, fontweight='bold')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4, 5, 6: Per-Field Hallucination Rates (3 models)\n",
    "for idx, (halluc_data, ax_pos, model_color) in enumerate([\n",
    "    (llama_hallucination, gs[1, 0], colors[0]),\n",
    "    (internvl_hallucination, gs[1, 1], colors[1]),\n",
    "    (internvl_nq_hallucination, gs[1, 2], colors[2])\n",
    "]):\n",
    "    ax = fig.add_subplot(ax_pos)\n",
    "    \n",
    "    # Sort fields by hallucination rate\n",
    "    field_data = halluc_data['field_hallucination']\n",
    "    sorted_fields = sorted(field_data.items(), key=lambda x: x[1]['hallucination_rate'], reverse=True)\n",
    "    \n",
    "    fields = [f[0] for f in sorted_fields][:15]  # Top 15\n",
    "    rates = [f[1]['hallucination_rate'] for f in sorted_fields][:15]\n",
    "    \n",
    "    bars = ax.barh(fields, rates, color=model_color, alpha=0.7)\n",
    "    ax.set_xlabel('Hallucination Rate', fontsize=11, fontweight='bold')\n",
    "    ax.set_title(f'{halluc_data[\"model\"]} - Field Hallucination', fontsize=13, fontweight='bold')\n",
    "    ax.set_xlim(0, 1.0)\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, rate in zip(bars, rates):\n",
    "        width = bar.get_width()\n",
    "        ax.text(width, bar.get_y() + bar.get_height()/2.,\n",
    "                f'{rate:.1%}',\n",
    "                ha='left', va='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "# Plot 7, 8, 9: Document Hallucination Distribution (histograms)\n",
    "for idx, (halluc_data, ax_pos, model_color) in enumerate([\n",
    "    (llama_hallucination, gs[2, 0], colors[0]),\n",
    "    (internvl_hallucination, gs[2, 1], colors[1]),\n",
    "    (internvl_nq_hallucination, gs[2, 2], colors[2])\n",
    "]):\n",
    "    ax = fig.add_subplot(ax_pos)\n",
    "    \n",
    "    doc_rates = [d['rate'] for d in halluc_data['document_hallucination_scores']]\n",
    "    \n",
    "    ax.hist(doc_rates, bins=20, color=model_color, alpha=0.7, edgecolor='black')\n",
    "    ax.axvline(halluc_data['mean_doc_hallucination'], color='red', linestyle='--', linewidth=2, label=f'Mean: {halluc_data[\"mean_doc_hallucination\"]:.1%}')\n",
    "    ax.set_xlabel('Document Hallucination Rate', fontsize=11, fontweight='bold')\n",
    "    ax.set_ylabel('Number of Documents', fontsize=11, fontweight='bold')\n",
    "    ax.set_title(f'{halluc_data[\"model\"]} - Document Distribution', fontsize=13, fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.savefig(CONFIG['output_dir'].replace('/csv', '/visualizations') + '/hallucination_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "rprint(\"[green]‚úÖ Hallucination analysis complete (9 visualizations created)[/green]\")\n",
    "rprint(f\"[green]üíæ Saved: visualizations/hallucination_analysis.png[/green]\")\n",
    "\n",
    "# Additional insights\n",
    "rprint(\"\\n[bold magenta]Key Insights:[/bold magenta]\")\n",
    "\n",
    "# Find highest hallucination model\n",
    "max_halluc_model = max(\n",
    "    [llama_hallucination, internvl_hallucination, internvl_nq_hallucination],\n",
    "    key=lambda x: x['overall_hallucination_rate']\n",
    ")\n",
    "min_halluc_model = min(\n",
    "    [llama_hallucination, internvl_hallucination, internvl_nq_hallucination],\n",
    "    key=lambda x: x['overall_hallucination_rate']\n",
    ")\n",
    "\n",
    "rprint(f\"[red]üî¥ Highest hallucination: {max_halluc_model['model']} ({max_halluc_model['overall_hallucination_rate']:.1%})[/red]\")\n",
    "rprint(f\"[green]üü¢ Lowest hallucination: {min_halluc_model['model']} ({min_halluc_model['overall_hallucination_rate']:.1%})[/green]\")\n",
    "\n",
    "# Find most hallucinated fields across all models\n",
    "all_field_halluc = {}\n",
    "for halluc_data in [llama_hallucination, internvl_hallucination, internvl_nq_hallucination]:\n",
    "    for field, data in halluc_data['field_hallucination'].items():\n",
    "        if field not in all_field_halluc:\n",
    "            all_field_halluc[field] = []\n",
    "        all_field_halluc[field].append(data['hallucination_rate'])\n",
    "\n",
    "avg_field_halluc = {field: np.mean(rates) for field, rates in all_field_halluc.items()}\n",
    "most_hallucinated = sorted(avg_field_halluc.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "\n",
    "rprint(\"\\n[yellow]Most Hallucinated Fields (avg across models):[/yellow]\")\n",
    "for field, rate in most_hallucinated:\n",
    "    rprint(f\"  ‚Ä¢ {field}: {rate:.1%}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 25\n",
    "\n",
    "# Auto-generate MODEL_COMPARISON_REPORT.md from notebook results\n",
    "\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "def generate_model_comparison_report(llama_df, internvl3_8b_df, internvl3_2b_df, field_performance_df=None, hallucination_summary=None):\n",
    "    \"\"\"Generate comprehensive MODEL_COMPARISON_REPORT.md from notebook analysis.\n",
    "\n",
    "    Args:\n",
    "        llama_df: DataFrame with Llama batch results\n",
    "        internvl3_8b_df: DataFrame with InternVL3-8B batch results\n",
    "        internvl3_2b_df: DataFrame with InternVL3-2B batch results\n",
    "        field_performance_df: Optional DataFrame with per-field performance comparison\n",
    "        hallucination_summary: Optional DataFrame with hallucination analysis results\n",
    "    \"\"\"\n",
    "\n",
    "    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    # Calculate dataset statistics from actual data\n",
    "    all_dfs = [df for df in [llama_df, internvl3_8b_df, internvl3_2b_df] if not df.empty]\n",
    "\n",
    "    if all_dfs:\n",
    "        # Use first non-empty dataframe for dataset statistics\n",
    "        sample_df = all_dfs[0]\n",
    "        total_docs = len(sample_df)\n",
    "\n",
    "        # Count document types if column exists\n",
    "        if 'document_type' in sample_df.columns:\n",
    "            doctype_counts = sample_df['document_type'].value_counts()\n",
    "            bank_count = doctype_counts.get('bank_statement', 0)\n",
    "            invoice_count = doctype_counts.get('invoice', 0)\n",
    "            receipt_count = doctype_counts.get('receipt', 0)\n",
    "        else:\n",
    "            bank_count = invoice_count = receipt_count = 0\n",
    "\n",
    "        # Calculate total fields from field performance or use total_fields column\n",
    "        if field_performance_df is not None and not field_performance_df.empty:\n",
    "            num_fields = len(field_performance_df)\n",
    "        elif 'total_fields' in sample_df.columns:\n",
    "            num_fields = int(sample_df['total_fields'].iloc[0])\n",
    "        else:\n",
    "            num_fields = 0\n",
    "    else:\n",
    "        total_docs = bank_count = invoice_count = receipt_count = num_fields = 0\n",
    "\n",
    "    def calculate_metrics(df, model_name):\n",
    "        \"\"\"Calculate precision, recall, F1 from batch results DataFrame.\"\"\"\n",
    "        if df.empty:\n",
    "            rprint(f\"[yellow]‚ö† {model_name} dataframe is empty[/yellow]\")\n",
    "            return 0, 0, 0, 0, 0, 0\n",
    "\n",
    "        try:\n",
    "            # These columns exist in the CSV files\n",
    "            accuracy = df['overall_accuracy'].mean()\n",
    "            speed = df['processing_time'].median()\n",
    "\n",
    "            # Calculate precision, recall, F1 from field matching data\n",
    "            # Precision = fields_matched / fields_extracted (when fields_extracted > 0)\n",
    "            # Recall = fields_matched / total_fields\n",
    "            valid_rows = df[df['fields_extracted'] > 0].copy()\n",
    "\n",
    "            if len(valid_rows) > 0:\n",
    "                valid_rows['precision'] = valid_rows['fields_matched'] / valid_rows['fields_extracted']\n",
    "                valid_rows['recall'] = valid_rows['fields_matched'] / valid_rows['total_fields']\n",
    "\n",
    "                precision = valid_rows['precision'].mean()\n",
    "                recall = valid_rows['recall'].mean()\n",
    "\n",
    "                # F1 = 2 * precision * recall / (precision + recall)\n",
    "                if precision + recall > 0:\n",
    "                    f1 = 2 * precision * recall / (precision + recall)\n",
    "                else:\n",
    "                    f1 = 0\n",
    "            else:\n",
    "                precision = recall = f1 = 0\n",
    "\n",
    "            rprint(f\"[green]‚úÖ {model_name} metrics calculated successfully[/green]\")\n",
    "            rprint(f\"[cyan]  F1={f1:.4f}, P={precision:.4f}, R={recall:.4f}, Acc={accuracy:.2f}%, Speed={speed:.1f}s[/cyan]\")\n",
    "\n",
    "            # Calculate median F1 per document\n",
    "            valid_rows['f1_doc'] = 2 * (valid_rows['precision'] * valid_rows['recall']) / (valid_rows['precision'] + valid_rows['recall'] + 1e-10)\n",
    "            f1_median = valid_rows['f1_doc'].median()\n",
    "\n",
    "            return f1, f1_median, precision, recall, accuracy, speed\n",
    "\n",
    "        except Exception as e:\n",
    "            rprint(f\"[red]‚ùå Error calculating {model_name} metrics: {e}[/red]\")\n",
    "            if not df.empty:\n",
    "                rprint(f\"[cyan]Available columns: {df.columns.tolist()}[/cyan]\")\n",
    "                rprint(f\"[cyan]DataFrame shape: {df.shape}[/cyan]\")\n",
    "            return 0, 0, 0, 0, 0, 0\n",
    "\n",
    "    # Extract overall metrics for each model\n",
    "    llama_overall_f1, llama_median_f1, llama_overall_precision, llama_overall_recall, llama_overall_accuracy, llama_speed = \\\n",
    "        calculate_metrics(llama_df, 'Llama-3.2-Vision-11B')\n",
    "\n",
    "    internvl_8b_overall_f1, internvl_8b_median_f1, internvl_8b_overall_precision, internvl_8b_overall_recall, internvl_8b_overall_accuracy, internvl_8b_speed = \\\n",
    "        calculate_metrics(internvl3_8b_df, 'InternVL3-8B')\n",
    "\n",
    "    internvl_2b_overall_f1, internvl_2b_median_f1, internvl_2b_overall_precision, internvl_2b_overall_recall, internvl_2b_overall_accuracy, internvl_2b_speed = \\\n",
    "        calculate_metrics(internvl3_2b_df, 'InternVL3-2B')\n",
    "\n",
    "    # Build markdown report\n",
    "    report = f\"\"\"# Model Comparison Analysis Report\n",
    "\n",
    "**Auto-Generated from Notebook**: {timestamp}\n",
    "**Source**: `model_comparison_reporter.ipynb`\n",
    "**Dataset**: {total_docs} documents ({bank_count} bank statements, {invoice_count} invoices, {receipt_count} receipts)\n",
    "**Evaluation Fields**: {num_fields} business document fields\n",
    "\n",
    "---\n",
    "\n",
    "## Executive Summary\n",
    "\n",
    "### Overall Performance Metrics\n",
    "\n",
    "| Model | Mean F1 | Median F1 | Precision | Recall | Accuracy | Median Speed | Throughput |\n",
    "|-------|---------|-----------|-----------|--------|----------|--------------|------------|\n",
    "| **Llama-3.2-Vision-11B** | {llama_overall_f1:.4f} | {llama_median_f1:.4f} | {llama_overall_precision:.4f} | {llama_overall_recall:.4f} | {llama_overall_accuracy:.2f}% | {llama_speed:.1f}s | {60/llama_speed if llama_speed > 0 else 0:.1f} docs/min |\n",
    "| **InternVL3-8B** | {internvl_8b_overall_f1:.4f} | {internvl_8b_median_f1:.4f} | {internvl_8b_overall_precision:.4f} | {internvl_8b_overall_recall:.4f} | {internvl_8b_overall_accuracy:.2f}% | {internvl_8b_speed:.1f}s | {60/internvl_8b_speed if internvl_8b_speed > 0 else 0:.1f} docs/min |\n",
    "| **InternVL3-2B** | {internvl_2b_overall_f1:.4f} | {internvl_2b_median_f1:.4f} | {internvl_2b_overall_precision:.4f} | {internvl_2b_overall_recall:.4f} | {internvl_2b_overall_accuracy:.2f}% | {internvl_2b_speed:.1f}s | {60/internvl_2b_speed if internvl_2b_speed > 0 else 0:.1f} docs/min |\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "**Winner (F1 Score)**: {'Llama-3.2-Vision-11B' if llama_overall_f1 > max(internvl_8b_overall_f1, internvl_2b_overall_f1) else ('InternVL3-8B' if internvl_8b_overall_f1 > internvl_2b_overall_f1 else 'InternVL3-2B')}\n",
    "\n",
    "**Highest Precision**: {'Llama-3.2-Vision-11B' if llama_overall_precision > max(internvl_8b_overall_precision, internvl_2b_overall_precision) else ('InternVL3-8B' if internvl_8b_overall_precision > internvl_2b_overall_precision else 'InternVL3-2B')} ({max(llama_overall_precision, internvl_8b_overall_precision, internvl_2b_overall_precision):.4f})\n",
    "\n",
    "**Highest Recall**: {'Llama-3.2-Vision-11B' if llama_overall_recall > max(internvl_8b_overall_recall, internvl_2b_overall_recall) else ('InternVL3-8B' if internvl_8b_overall_recall > internvl_2b_overall_recall else 'InternVL3-2B')} ({max(llama_overall_recall, internvl_8b_overall_recall, internvl_2b_overall_recall):.4f})\n",
    "\n",
    "**Fastest**: {'Llama-3.2-Vision-11B' if llama_speed > 0 and llama_speed < min(filter(lambda x: x > 0, [internvl_8b_speed, internvl_2b_speed])) else ('InternVL3-8B' if internvl_8b_speed > 0 and internvl_8b_speed < internvl_2b_speed else 'InternVL3-2B')} ({min(filter(lambda x: x > 0, [llama_speed, internvl_8b_speed, internvl_2b_speed])):.1f}s)\n",
    "\n",
    "---\n",
    "\n",
    "## Visualizations\n",
    "\n",
    "All visualizations are generated in `output/visualizations/`:\n",
    "\n",
    "### 1. Executive Performance Dashboard\n",
    "![Executive Dashboard](output/visualizations/executive_comparison.png)\n",
    "\n",
    "**6-panel comprehensive view:**\n",
    "- Overall accuracy distribution (box plots)\n",
    "- Processing speed comparison\n",
    "- Accuracy by document type\n",
    "- Processing time by document type\n",
    "- Efficiency analysis (accuracy vs speed)\n",
    "- Performance summary table\n",
    "\n",
    "### 2. Document Type Classification\n",
    "![Document Type Confusion](output/visualizations/doctype_confusion_matrix.png)\n",
    "\n",
    "**3-model confusion matrices** showing classification performance for:\n",
    "- Bank Statements ({bank_count} docs, {bank_count/total_docs*100 if total_docs > 0 else 0:.1f}%)\n",
    "- Invoices ({invoice_count} docs, {invoice_count/total_docs*100 if total_docs > 0 else 0:.1f}%)\n",
    "- Receipts ({receipt_count} docs, {receipt_count/total_docs*100 if total_docs > 0 else 0:.1f}%)\n",
    "\n",
    "### 3. Field Extraction Status\n",
    "![Field Confusion Heatmap](output/visualizations/field_confusion_heatmap.png)\n",
    "\n",
    "**Breakdown of extraction status:**\n",
    "- Correct extractions (matches ground truth)\n",
    "- Incorrect extractions (wrong value)\n",
    "- Not Found (field not extracted)\n",
    "\n",
    "### 4. Per-Field Metrics\n",
    "![Per-Field Metrics](output/visualizations/per_field_metrics.png)\n",
    "\n",
    "**4-panel analysis:**\n",
    "- F1 Score by field\n",
    "- Precision by field\n",
    "- Recall by field\n",
    "- Accuracy by field\n",
    "\n",
    "\n",
    "### 5. Field-Level Accuracy Analysis\n",
    "![Field-Level Accuracy](output/visualizations/field_level_accuracy.png)\n",
    "\n",
    "**3-panel comprehensive view:**\n",
    "- Field accuracy comparison (horizontal bar chart across all models)\n",
    "- Field accuracy heatmap (color-coded performance matrix)\n",
    "- Model specialization distribution (fields where each model performs best)\n",
    "\n",
    "### 6. Hallucination Analysis\n",
    "![Hallucination Analysis](output/visualizations/hallucination_analysis.png)\n",
    "\n",
    "**9-panel breakdown:**\n",
    "- Overall hallucination rates\n",
    "- Hallucinations vs correct NOT_FOUND\n",
    "- Hallucination-recall tradeoff\n",
    "- Per-field hallucination (3 models)\n",
    "- Document-level distribution (3 models)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    # Add hallucination rates if available\n",
    "    if hallucination_summary is not None and not hallucination_summary.empty:\n",
    "        report += \"\"\"### Hallucination Rates\n",
    "\n",
    "| Model | Hallucination Rate | Correct NOT_FOUND Rate | Total Hallucinations |\n",
    "|-------|-------------------|------------------------|----------------------|\n",
    "\"\"\"\n",
    "        for _, row in hallucination_summary.iterrows():\n",
    "            model_name = row['Model']\n",
    "            hall_rate = row['Hallucination Rate']\n",
    "            not_found_rate = row['Correct NOT_FOUND Rate']\n",
    "            total_hall = row['Total Hallucinations']\n",
    "            report += f\"| **{model_name}** | {hall_rate} | {not_found_rate} | {total_hall} |\\n\"\n",
    "\n",
    "        report += \"\"\"\n",
    "**Interpretation:**\n",
    "- **Hallucination Rate**: % of NOT_FOUND fields where model invented a value\n",
    "- **Correct NOT_FOUND Rate**: % of NOT_FOUND fields correctly identified as absent\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    report += \"\"\"---\n",
    "\n",
    "## Per-Field Performance Summary\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    # Add per-field performance if available\n",
    "    if field_performance_df is not None and not field_performance_df.empty:\n",
    "        try:\n",
    "            report += \"### Field-Level Accuracy by Model\\n\\n\"\n",
    "            report += \"| Field | Llama-11B | InternVL3-8B | InternVL3-2B | Best Model | Best Score |\\n\"\n",
    "            report += \"|-------|-----------|--------------|--------------|------------|------------|\\n\"\n",
    "\n",
    "            for field in field_performance_df.index:\n",
    "                llama_acc = field_performance_df.loc[field, 'Llama-3.2-Vision-11B'] if 'Llama-3.2-Vision-11B' in field_performance_df.columns else 0\n",
    "                internvl_8b_acc = field_performance_df.loc[field, 'InternVL3-8B'] if 'InternVL3-8B' in field_performance_df.columns else 0\n",
    "                internvl_2b_acc = field_performance_df.loc[field, 'InternVL3-2B'] if 'InternVL3-2B' in field_performance_df.columns else 0\n",
    "                best_model = field_performance_df.loc[field, 'best_model'] if 'best_model' in field_performance_df.columns else 'N/A'\n",
    "                best_score = field_performance_df.loc[field, 'best_score'] if 'best_score' in field_performance_df.columns else 0\n",
    "\n",
    "                # Convert to percentages (field_performance stores as decimals 0-1)\n",
    "                report += f\"| {field} | {llama_acc*100:.1f}% | {internvl_8b_acc*100:.1f}% | {internvl_2b_acc*100:.1f}% | {best_model} | {best_score*100:.1f}% |\\n\"\n",
    "\n",
    "            report += \"\\n\"\n",
    "        except Exception as e:\n",
    "            report += f\"_(Error generating per-field metrics table: {e})_\\n\\n\"\n",
    "            rprint(f\"[yellow]‚ö† Error accessing field_performance: {e}[/yellow]\")\n",
    "    else:\n",
    "        report += \"_(Per-field performance data unavailable)_\\n\\n\"\n",
    "\n",
    "    # Add model specialization summary\n",
    "    report += \"\"\"---\n",
    "\n",
    "## Model Specialization\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    if field_performance_df is not None and not field_performance_df.empty and 'best_model' in field_performance_df.columns:\n",
    "        try:\n",
    "            llama_fields = (field_performance_df['best_model'] == 'Llama-3.2-Vision-11B').sum()\n",
    "            internvl_8b_fields = (field_performance_df['best_model'] == 'InternVL3-8B').sum()\n",
    "            internvl_2b_fields = (field_performance_df['best_model'] == 'InternVL3-2B').sum()\n",
    "            total_fields = len(field_performance_df)\n",
    "\n",
    "            report += f\"\"\"### Fields Where Each Model Performs Best\n",
    "\n",
    "| Model | Best-Performing Fields | Percentage | Count |\n",
    "|-------|----------------------|------------|-------|\n",
    "| **Llama-3.2-Vision-11B** | {llama_fields/total_fields*100:.1f}% | {llama_fields}/{total_fields} | {'PRIMARY' if llama_fields > max(internvl_8b_fields, internvl_2b_fields) else 'SECONDARY'} |\n",
    "| **InternVL3-8B** | {internvl_8b_fields/total_fields*100:.1f}% | {internvl_8b_fields}/{total_fields} | {'PRIMARY' if internvl_8b_fields > max(llama_fields, internvl_2b_fields) else 'SECONDARY'} |\n",
    "| **InternVL3-2B** | {internvl_2b_fields/total_fields*100:.1f}% | {internvl_2b_fields}/{total_fields} | {'PRIMARY' if internvl_2b_fields > max(llama_fields, internvl_8b_fields) else 'NO SPECIALIZATION' if internvl_2b_fields == 0 else 'SECONDARY'} |\n",
    "\n",
    "\"\"\"\n",
    "        except Exception as e:\n",
    "            report += f\"_(Error calculating specialization metrics: {e})_\\n\\n\"\n",
    "            rprint(f\"[yellow]‚ö† Error calculating specialization: {e}[/yellow]\")\n",
    "    else:\n",
    "        report += \"_(Model specialization data unavailable)_\\n\\n\"\n",
    "\n",
    "    # Footer\n",
    "    report += f\"\"\"---\n",
    "\n",
    "## Deployment Recommendations\n",
    "\n",
    "Based on the analysis above:\n",
    "\n",
    "### 1. Document Classification (PRIMARY)\n",
    "Use the model with highest document type classification accuracy for initial routing and categorization.\n",
    "\n",
    "### 2. Field Extraction Strategy (SECONDARY)\n",
    "Consider an ensemble approach leveraging each model's field specialization:\n",
    "- Use model-specific strengths for particular fields\n",
    "- Implement confidence-based routing\n",
    "- Fall back to best overall performer for general fields\n",
    "\n",
    "### 3. High-Volume Processing\n",
    "Balance speed vs quality based on throughput requirements:\n",
    "- **Fastest processing**: {'Llama-3.2-Vision-11B' if llama_speed > 0 and llama_speed < min(filter(lambda x: x > 0, [internvl_8b_speed, internvl_2b_speed])) else ('InternVL3-8B' if internvl_8b_speed > 0 and internvl_8b_speed < internvl_2b_speed else 'InternVL3-2B')} (~{min(filter(lambda x: x > 0, [llama_speed, internvl_8b_speed, internvl_2b_speed])):.1f}s/doc)\n",
    "- **Best accuracy**: {'Llama-3.2-Vision-11B' if llama_overall_accuracy > max(internvl_8b_overall_accuracy, internvl_2b_overall_accuracy) else ('InternVL3-8B' if internvl_8b_overall_accuracy > internvl_2b_overall_accuracy else 'InternVL3-2B')} ({max(llama_overall_accuracy, internvl_8b_overall_accuracy, internvl_2b_overall_accuracy):.2f}% overall)\n",
    "- **Best balance**: Consider throughput constraints and acceptable accuracy threshold\n",
    "\n",
    "### 4. Hallucination Sensitivity: Critical Business Decision\n",
    "\n",
    "#### Understanding Hallucination in Document Extraction\n",
    "\n",
    "**Hallucination** = Model extracts a value when ground truth is `NOT_FOUND`\n",
    "\n",
    "**Example:**\n",
    "- Ground Truth: `BUSINESS_ABN = NOT_FOUND` (field doesn't exist in document)\n",
    "- Model Output: `BUSINESS_ABN = \"12345678901\"` ‚Üê **HALLUCINATION** (invented data)\n",
    "\n",
    "#### The Tradeoff: Precision vs Recall\n",
    "\n",
    "**High Precision (Low Hallucination)**\n",
    "- Model only extracts when very confident\n",
    "- **Few false positives** (hallucinations)\n",
    "- **Many false negatives** (missed fields)\n",
    "- Conservative approach: \"Only extract what you're sure about\"\n",
    "\n",
    "**High Recall (Risk of Hallucination)**\n",
    "- Model extracts aggressively to catch all fields\n",
    "- **Few false negatives** (catches most fields)\n",
    "- **More false positives** (risk of hallucinations)\n",
    "- Aggressive approach: \"Extract everything, review later\"\n",
    "\n",
    "#### Relationship to Metrics\n",
    "\n",
    "```\n",
    "Precision = Correct Extractions / All Extractions\n",
    "  ‚Üí High precision = Low hallucination rate\n",
    "  ‚Üí Model is cautious, only extracts when confident\n",
    "\n",
    "Recall = Correct Extractions / All Fields That Should Be Extracted\n",
    "  ‚Üí High recall = Catches more fields\n",
    "  ‚Üí Risk: May hallucinate to achieve higher coverage\n",
    "\n",
    "Hallucination Rate = Hallucinations / NOT_FOUND Opportunities\n",
    "  ‚Üí Direct measure of false positive risk\n",
    "  ‚Üí Critical for production reliability\n",
    "```\n",
    "\n",
    "#### Model Selection Guide Based on Use Case\n",
    "\n",
    "**Choose HIGH PRECISION Model ({'Llama-3.2-Vision-11B' if llama_overall_precision > max(internvl_8b_overall_precision, internvl_2b_overall_precision) else ('InternVL3-8B' if internvl_8b_overall_precision > internvl_2b_overall_precision else 'InternVL3-2B')}: {max(llama_overall_precision, internvl_8b_overall_precision, internvl_2b_overall_precision):.2%}) if:**\n",
    "- ‚úÖ Processing financial/regulatory data (invoices, tax documents)\n",
    "- ‚úÖ Automated processing with no human review\n",
    "- ‚úÖ **False data is worse than missing data**\n",
    "- ‚úÖ You can afford to manually review `NOT_FOUND` fields\n",
    "- ‚úÖ Compliance and audit requirements\n",
    "- ‚úÖ Low tolerance for hallucinations\n",
    "\n",
    "**Example**: Bank reconciliation where a hallucinated amount could cause financial errors.\n",
    "\n",
    "**Choose HIGH RECALL Model ({'Llama-3.2-Vision-11B' if llama_overall_recall > max(internvl_8b_overall_recall, internvl_2b_overall_recall) else ('InternVL3-8B' if internvl_8b_overall_recall > internvl_2b_overall_recall else 'InternVL3-2B')}: {max(llama_overall_recall, internvl_8b_overall_recall, internvl_2b_overall_recall):.2%}) if:**\n",
    "- ‚úÖ Comprehensive data capture is critical\n",
    "- ‚úÖ Human review pipeline can catch errors\n",
    "- ‚úÖ **Missing data is worse than wrong data**\n",
    "- ‚úÖ Initial screening/discovery use case\n",
    "- ‚úÖ Maximizing field coverage is priority\n",
    "- ‚úÖ Can tolerate some false positives\n",
    "\n",
    "**Example**: Legal document discovery where missing a field could have serious consequences.\n",
    "\n",
    "**Choose BALANCED Model (for high-volume processing) if:**\n",
    "- ‚úÖ High-volume processing requirements\n",
    "- ‚úÖ Need reasonable precision and recall\n",
    "- ‚úÖ Speed is a critical factor\n",
    "- ‚úÖ Standard business document processing\n",
    "\n",
    "**Example**: Receipt processing for expense management with human spot-checking.\n",
    "\n",
    "#### Your Model Performance Profile\n",
    "\n",
    "Based on the analysis:\n",
    "\n",
    "| Model | Precision | Recall | F1 | Best For |\n",
    "|-------|-----------|--------|----|----|\n",
    "| **Llama-3.2-Vision-11B** | {llama_overall_precision:.2%} | {llama_overall_recall:.2%} | {llama_overall_f1:.4f} | {'üèÜ Best Precision' if llama_overall_precision > max(internvl_8b_overall_precision, internvl_2b_overall_precision) else ''}{'üèÜ Best Recall' if llama_overall_recall > max(internvl_8b_overall_recall, internvl_2b_overall_recall) else ''}{'üèÜ Best F1' if llama_overall_f1 > max(internvl_8b_overall_f1, internvl_2b_overall_f1) else ''} |\n",
    "| **InternVL3-8B** | {internvl_8b_overall_precision:.2%} | {internvl_8b_overall_recall:.2%} | {internvl_8b_overall_f1:.4f} | {'üèÜ Best Precision' if internvl_8b_overall_precision > max(llama_overall_precision, internvl_2b_overall_precision) else ''}{'üèÜ Best Recall' if internvl_8b_overall_recall > max(llama_overall_recall, internvl_2b_overall_recall) else ''}{'üèÜ Best F1' if internvl_8b_overall_f1 > max(llama_overall_f1, internvl_2b_overall_f1) else ''} |\n",
    "| **InternVL3-2B** | {internvl_2b_overall_precision:.2%} | {internvl_2b_overall_recall:.2%} | {internvl_2b_overall_f1:.4f} | {'üèÜ Best Precision' if internvl_2b_overall_precision > max(llama_overall_precision, internvl_8b_overall_precision) else ''}{'üèÜ Best Recall' if internvl_2b_overall_recall > max(llama_overall_recall, internvl_8b_overall_recall) else ''}{'üèÜ Best F1' if internvl_2b_overall_f1 > max(llama_overall_f1, internvl_8b_overall_f1) else ''} |\n",
    "\n",
    "**Key Insights:**\n",
    "- **Precision Leader**: {'Llama-3.2-Vision-11B' if llama_overall_precision > max(internvl_8b_overall_precision, internvl_2b_overall_precision) else ('InternVL3-8B' if internvl_8b_overall_precision > internvl_2b_overall_precision else 'InternVL3-2B')} ({max(llama_overall_precision, internvl_8b_overall_precision, internvl_2b_overall_precision):.2%})\n",
    "- **Recall Leader**: {'Llama-3.2-Vision-11B' if llama_overall_recall > max(internvl_8b_overall_recall, internvl_2b_overall_recall) else ('InternVL3-8B' if internvl_8b_overall_recall > internvl_2b_overall_recall else 'InternVL3-2B')} ({max(llama_overall_recall, internvl_8b_overall_recall, internvl_2b_overall_recall):.2%})\n",
    "- **F1 Leader**: {'Llama-3.2-Vision-11B' if llama_overall_f1 > max(internvl_8b_overall_f1, internvl_2b_overall_f1) else ('InternVL3-8B' if internvl_8b_overall_f1 > internvl_2b_overall_f1 else 'InternVL3-2B')} ({max(llama_overall_f1, internvl_8b_overall_f1, internvl_2b_overall_f1):.4f})\n",
    "- **Speed vs Accuracy Tradeoff**: Consider throughput requirements against quality needs\n",
    "\n",
    "#### Efficiency Analysis\n",
    "\n",
    "**Performance Efficiency Score** = Accuracy √ó Throughput (docs/min)\n",
    "\n",
    "| Model | Avg Accuracy | Avg Speed | Throughput | Efficiency Score |\n",
    "|-------|--------------|-----------|------------|------------------|\n",
    "| **Llama-3.2-Vision-11B** | {llama_overall_accuracy:.2f}% | {llama_speed:.1f}s | {60/llama_speed if llama_speed > 0 else 0:.1f} docs/min | {(llama_overall_accuracy * (60/llama_speed if llama_speed > 0 else 0)):.1f} |\n",
    "| **InternVL3-8B** | {internvl_8b_overall_accuracy:.2f}% | {internvl_8b_speed:.1f}s | {60/internvl_8b_speed if internvl_8b_speed > 0 else 0:.1f} docs/min | {(internvl_8b_overall_accuracy * (60/internvl_8b_speed if internvl_8b_speed > 0 else 0)):.1f} |\n",
    "| **InternVL3-2B** | {internvl_2b_overall_accuracy:.2f}% | {internvl_2b_speed:.1f}s | {60/internvl_2b_speed if internvl_2b_speed > 0 else 0:.1f} docs/min | {(internvl_2b_overall_accuracy * (60/internvl_2b_speed if internvl_2b_speed > 0 else 0)):.1f} |\n",
    "\n",
    "**Highest Efficiency**: {'Llama-3.2-Vision-11B' if (llama_overall_accuracy * (60/llama_speed if llama_speed > 0 else 0)) > max((internvl_8b_overall_accuracy * (60/internvl_8b_speed if internvl_8b_speed > 0 else 0)), (internvl_2b_overall_accuracy * (60/internvl_2b_speed if internvl_2b_speed > 0 else 0))) else ('InternVL3-8B' if (internvl_8b_overall_accuracy * (60/internvl_8b_speed if internvl_8b_speed > 0 else 0)) > (internvl_2b_overall_accuracy * (60/internvl_2b_speed if internvl_2b_speed > 0 else 0)) else 'InternVL3-2B')}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    # Calculate document-type specific recommendations\n",
    "    doc_type_section = \"\"\n",
    "    all_dfs_list = [(llama_df, 'Llama-3.2-Vision-11B'), \n",
    "                    (internvl3_8b_df, 'InternVL3-8B'),\n",
    "                    (internvl3_2b_df, 'InternVL3-2B')]\n",
    "    \n",
    "    # Combine all dataframes with model labels\n",
    "    combined_for_doctype = []\n",
    "    for df, model_name in all_dfs_list:\n",
    "        if not df.empty and 'document_type' in df.columns:\n",
    "            temp_df = df.copy()\n",
    "            temp_df['model'] = model_name\n",
    "            combined_for_doctype.append(temp_df)\n",
    "    \n",
    "    if combined_for_doctype:\n",
    "        import pandas as pd\n",
    "        all_data = pd.concat(combined_for_doctype, ignore_index=True)\n",
    "        doc_performance = all_data.groupby(['document_type', 'model'])['overall_accuracy'].mean().unstack()\n",
    "        \n",
    "        doc_type_section += \"\\n#### Document-Type Specific Recommendations\\n\\n\"\n",
    "        doc_type_section += \"**Best Model by Document Type:**\\n\\n\"\n",
    "        for doc_type in doc_performance.index:\n",
    "            best_model = doc_performance.loc[doc_type].idxmax()\n",
    "            best_score = doc_performance.loc[doc_type].max()\n",
    "            doc_type_section += f\"- **{doc_type.replace('_', ' ').title()}**: {best_model} ({best_score:.2f}% accuracy)\\n\"\n",
    "    \n",
    "    report += doc_type_section\n",
    "    \n",
    "    # Field Performance Insights\n",
    "    if field_performance_df is not None and not field_performance_df.empty:\n",
    "        field_insights = \"\\n#### Field Performance Insights\\n\\n\"\n",
    "        \n",
    "        # Get model columns\n",
    "        model_cols = [col for col in field_performance_df.columns if any(x in col for x in ['Llama', 'InternVL'])]\n",
    "        \n",
    "        if len(model_cols) >= 2:\n",
    "            # Calculate performance spread\n",
    "            field_perf_copy = field_performance_df.copy()\n",
    "            field_perf_copy['max_diff'] = field_perf_copy[model_cols].max(axis=1) - field_perf_copy[model_cols].min(axis=1)\n",
    "            \n",
    "            # Significant differences (>20% spread)\n",
    "            significant_diff = field_perf_copy[field_perf_copy['max_diff'] > 0.2].nlargest(5, 'max_diff')\n",
    "            \n",
    "            if not significant_diff.empty:\n",
    "                field_insights += \"**Fields with Significant Model Performance Differences (>20% spread):**\\n\\n\"\n",
    "                for field_name in significant_diff.index:\n",
    "                    if 'best_model' in field_perf_copy.columns and 'best_score' in field_perf_copy.columns:\n",
    "                        best_model = field_perf_copy.loc[field_name, 'best_model']\n",
    "                        best_score = field_perf_copy.loc[field_name, 'best_score']\n",
    "                        worst_score = field_perf_copy.loc[field_name, model_cols].min()\n",
    "                        advantage = best_score - worst_score\n",
    "                        field_insights += f\"- **{field_name}**: Use {best_model} ({best_score*100:.0f}% vs {worst_score*100:.0f}%, +{advantage*100:.0f}% advantage)\\n\"\n",
    "            \n",
    "            # Calculate average accuracy per field\n",
    "            field_perf_copy['avg_accuracy'] = field_perf_copy[model_cols].mean(axis=1)\n",
    "            \n",
    "            # Problematic fields (<50% avg accuracy)\n",
    "            problematic = field_perf_copy[field_perf_copy['avg_accuracy'] < 0.5]\n",
    "            \n",
    "            if not problematic.empty:\n",
    "                field_insights += \"\\n**‚ö†Ô∏è Problematic Fields Requiring Attention (<50% avg accuracy):**\\n\\n\"\n",
    "                for field_name in problematic.head(5).index:\n",
    "                    avg_acc = field_perf_copy.loc[field_name, 'avg_accuracy']\n",
    "                    field_insights += f\"- **{field_name}**: {avg_acc*100:.0f}% average accuracy - Consider prompt optimization or additional fine tuning\\n\"\n",
    "        \n",
    "        report += field_insights\n",
    "    \n",
    "\n",
    "    report += \"\"\"\n",
    "\n",
    "#### Production Deployment Strategy\n",
    "\n",
    "**Phase 1: Initial Deployment**\n",
    "1. Choose model based on your primary business constraint:\n",
    "   - **Financial accuracy** ‚Üí Highest precision model\n",
    "   - **Data completeness** ‚Üí Highest recall model\n",
    "   - **High volume** ‚Üí Fastest processing model\n",
    "\n",
    "**Phase 2: Monitoring**\n",
    "2. Track in production:\n",
    "   - Hallucination rate on `NOT_FOUND` fields\n",
    "   - Manual review costs (false negatives)\n",
    "   - Error correction costs (false positives)\n",
    "\n",
    "**Phase 3: Optimization**\n",
    "3. Adjust strategy based on actual costs:\n",
    "   - If missing fields cost more ‚Üí Switch to higher recall model\n",
    "   - If hallucinations cost more ‚Üí Switch to higher precision model\n",
    "   - If volume is issue ‚Üí Consider faster model with review pipeline\n",
    "\n",
    "**Phase 4: Advanced Optimization**\n",
    "4. Consider ensemble approaches:\n",
    "   - Use high-precision model for critical fields (amounts, dates)\n",
    "   - Use high-recall model for descriptive fields (line items)\n",
    "   - Route by document confidence scores\n",
    "\n",
    "---\n",
    "\n",
    "## Related Documentation\n",
    "\n",
    "- [FIELD_COMPARISON.md](FIELD_COMPARISON.md) - Detailed field-by-field analysis\n",
    "- [ACCURACY_PARADOX_EXPLAINED.md](ACCURACY_PARADOX_EXPLAINED.md) - Why Accuracy > F1 for extraction\n",
    "- [HALLUCINATION_ANALYSIS.md](HALLUCINATION_ANALYSIS.md) - Hallucination analysis methodology\n",
    "\n",
    "---\n",
    "\n",
    "**Report Auto-Generated**: {timestamp}\n",
    "**Source Notebook**: `model_comparison_reporter.ipynb`\n",
    "**Visualizations**: `output/visualizations/`\n",
    "**Next Update**: Re-run notebook to refresh all metrics and visualizations\n",
    "    \"\"\"\n",
    "\n",
    "    # Write to file\n",
    "    output_path = Path('MODEL_COMPARISON_REPORT.md')\n",
    "    with output_path.open('w', encoding='utf-8') as f:\n",
    "        f.write(report)\n",
    "\n",
    "    rprint(f\"[green]‚úÖ MODEL_COMPARISON_REPORT.md updated ({len(report)} characters)[/green]\")\n",
    "    rprint(f\"[cyan]üìÑ Report generated from notebook execution at {timestamp}[/cyan]\")\n",
    "\n",
    "    return output_path\n",
    "\n",
    "# Execute report generation\n",
    "try:\n",
    "    # Pass dataframes, field_performance, and hallucination_summary as parameters\n",
    "    fp = field_performance if 'field_performance' in dir() else None\n",
    "    hs = hallucination_summary if 'hallucination_summary' in dir() else None\n",
    "    report_path = generate_model_comparison_report(\n",
    "        llama_df,\n",
    "        internvl3_8b_df,\n",
    "        internvl3_2b_df,\n",
    "        fp,\n",
    "        hs\n",
    "    )\n",
    "    rprint(f\"[bold green]‚úÖ Auto-generation complete: {report_path}[/bold green]\")\n",
    "except Exception as e:\n",
    "    rprint(f\"[bold red]‚ùå Report generation failed: {e}[/bold red]\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "du",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
