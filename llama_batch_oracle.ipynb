{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 2: Oracle Extraction - Llama Batch Processing\n",
    "\n",
    "**Oracle Strategy**: Use ground truth document type to select correct extraction schema.\n",
    "\n",
    "**Key differences from baseline:**\n",
    "- No classification step - ground truth provides document type\n",
    "- Always uses correct field set (perfect classification)\n",
    "- 5 fields for bank statements, 14 fields for invoice/receipt\n",
    "\n",
    "**Purpose**: Quantify classification penalty by eliminating classification errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#Cell 1\n# Imports and setup\n%load_ext autoreload\n%autoreload 2\n\nimport os\nos.environ['EVALUATION_METHOD'] = 'order_aware_f1'\n\nimport warnings\nimport yaml\nfrom datetime import datetime\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom PIL import Image as PILImage\nfrom rich import print as rprint\nfrom rich.console import Console\n\nfrom common.evaluation_metrics import (\n    load_ground_truth,\n    calculate_field_accuracy_with_method\n)\nfrom common.extraction_parser import (\n    discover_images,\n    parse_extraction_response\n)\nfrom common.gpu_optimization import emergency_cleanup\nfrom common.llama_model_loader_robust import load_llama_model_robust\n\nconsole = Console()\nwarnings.filterwarnings('ignore')\n\nrprint(\"[green]‚úÖ Imports loaded successfully[/green]\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 2\n",
    "# Pre-emptive memory cleanup\n",
    "rprint(\"[bold red]üßπ PRE-EMPTIVE V100 MEMORY CLEANUP[/bold red]\")\n",
    "rprint(\"[yellow]Clearing any existing model caches before loading...[/yellow]\")\n",
    "\n",
    "emergency_cleanup(verbose=True)\n",
    "\n",
    "rprint(\"[green]‚úÖ Memory cleanup complete - ready for model loading[/green]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#Cell 3\n# Environment-specific base paths\nENVIRONMENT_BASES = {\n    'sandbox': '/home/jovyan/nfs_share/tod',\n    'efs': '/efs/shared/PoC_data'\n}\nbase_data_path = ENVIRONMENT_BASES['efs']\n\nCONFIG = {\n    # Model settings\n    # 'MODEL_PATH': \"/home/jovyan/nfs_share/models/Llama-3.2-11B-Vision-Instruct\",\n    'MODEL_PATH': \"/efs/shared/PTM/Llama-3.2-11B-Vision-Instruct\",\n    \n    # Batch settings\n    # 'DATA_DIR': f'{base_data_path}/evaluation_data',\n    # 'GROUND_TRUTH': f'{base_data_path}/evaluation_data/ground_truth.csv',\n    'DATA_DIR': \"/home/jovyan/_LMM_POC/evaluation_data/bank\",\n    'GROUND_TRUTH': \"/home/jovyan/_LMM_POC/evaluation_data/bank/bank_gt.csv\",\n    \n    # 'DATA_DIR': \"/home/jovyan/shared_PoC_data/annotation_images_edited_short_filename\",\n    # 'GROUND_TRUTH': \"/home/jovyan/shared_PoC_data/evaluation_data/ground_truth_2025_11_03/ground_truth_2025_11_03_with_tods_bank_images.csv\",\n    \n    'OUTPUT_BASE': f'{base_data_path}/LMM_POC/output',\n    'MAX_IMAGES': None,\n    \n    # Verbosity\n    'VERBOSE': True,\n    'SHOW_PROMPTS': True,\n    \n    # Model settings\n    'USE_QUANTIZATION': False,\n    'DEVICE_MAP': 'auto',\n    'MAX_NEW_TOKENS': 2000,\n    'TORCH_DTYPE': 'bfloat16',\n    'LOW_CPU_MEM_USAGE': True,\n    \n    # Preprocessing\n    'ENABLE_PREPROCESSING': True,\n    'PREPROCESSING_MODE': 'adaptive',\n    'SAVE_PREPROCESSED': False,\n    'PREPROCESSED_DIR': None,\n}\n\nrprint(\"[green]‚úÖ Configuration set[/green]\")\nrprint(f\"[cyan]üìÇ Data: {CONFIG['DATA_DIR']}[/cyan]\")\nrprint(f\"[cyan]üìä Ground truth: {CONFIG['GROUND_TRUTH']}[/cyan]\")\nrprint(f\"[cyan]ü§ñ Model: {CONFIG['MODEL_PATH']}[/cyan]\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 4\n",
    "# Setup output directories\n",
    "OUTPUT_BASE = Path(CONFIG['OUTPUT_BASE'])\n",
    "if not OUTPUT_BASE.is_absolute():\n",
    "    OUTPUT_BASE = Path.cwd() / OUTPUT_BASE\n",
    "\n",
    "BATCH_TIMESTAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "OUTPUT_DIRS = {\n",
    "    'base': OUTPUT_BASE,\n",
    "    'batch': OUTPUT_BASE / 'batch_results',\n",
    "    'csv': OUTPUT_BASE / 'csv',\n",
    "    'visualizations': OUTPUT_BASE / 'visualizations',\n",
    "    'reports': OUTPUT_BASE / 'reports'\n",
    "}\n",
    "\n",
    "for dir_path in OUTPUT_DIRS.values():\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "rprint(\"[green]‚úÖ Output directories created[/green]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 5\n",
    "# Load model\n",
    "rprint(\"[bold green]Loading Llama model...[/bold green]\")\n",
    "\n",
    "model, processor = load_llama_model_robust(\n",
    "    model_path=CONFIG['MODEL_PATH'],\n",
    "    use_quantization=CONFIG['USE_QUANTIZATION'],\n",
    "    device_map=CONFIG['DEVICE_MAP'],\n",
    "    max_new_tokens=CONFIG['MAX_NEW_TOKENS'],\n",
    "    torch_dtype=CONFIG['TORCH_DTYPE'],\n",
    "    low_cpu_mem_usage=CONFIG['LOW_CPU_MEM_USAGE'],\n",
    "    verbose=CONFIG['VERBOSE']\n",
    ")\n",
    "\n",
    "rprint(\"[bold green]‚úÖ Model loaded successfully[/bold green]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 6\n",
    "# Load oracle extraction prompts from YAML files\n",
    "# We need separate prompts for invoice/receipt (14 fields) and bank statement (5 fields)\n",
    "\n",
    "PROMPT_FILES = {\n",
    "    'invoice': 'prompts/generated/llama_invoice_prompt.yaml',\n",
    "    'receipt': 'prompts/generated/llama_receipt_prompt.yaml',\n",
    "    'bank_statement': 'prompts/generated/llama_bank_statement_prompt.yaml'\n",
    "}\n",
    "\n",
    "ORACLE_PROMPTS = {}\n",
    "\n",
    "for doc_type, prompt_file in PROMPT_FILES.items():\n",
    "    prompt_path = Path(prompt_file)\n",
    "    \n",
    "    if not prompt_path.exists():\n",
    "        rprint(f\"[red]‚ùå Prompt file not found: {prompt_file}[/red]\")\n",
    "        continue\n",
    "    \n",
    "    with open(prompt_path, 'r') as f:\n",
    "        prompt_data = yaml.safe_load(f)\n",
    "    \n",
    "    # Extract the prompt from the YAML structure\n",
    "    # Structure: prompts -> {doc_type} -> prompt\n",
    "    if 'prompts' in prompt_data and doc_type in prompt_data['prompts']:\n",
    "        ORACLE_PROMPTS[doc_type] = prompt_data['prompts'][doc_type]['prompt']\n",
    "        rprint(f\"[green]‚úÖ Loaded {doc_type} prompt from {prompt_file}[/green]\")\n",
    "    else:\n",
    "        rprint(f\"[yellow]‚ö†Ô∏è  Could not find prompt in expected structure for {doc_type}[/yellow]\")\n",
    "\n",
    "rprint(f\"[cyan]üìã Loaded {len(ORACLE_PROMPTS)} oracle prompts[/cyan]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 6.5\n",
    "# Display oracle prompts\n",
    "if CONFIG.get('SHOW_PROMPTS', True) and ORACLE_PROMPTS:\n",
    "    console.rule(\"[bold cyan]Oracle Extraction Prompts[/bold cyan]\")\n",
    "    \n",
    "    for doc_type, prompt in ORACLE_PROMPTS.items():\n",
    "        console.rule(f\"[cyan]{doc_type.upper()}[/cyan]\")\n",
    "        # Show first 500 chars of each prompt\n",
    "        preview = prompt[:500] + \"...\" if len(prompt) > 500 else prompt\n",
    "        print(preview)\n",
    "    \n",
    "    console.rule(\"[bold cyan]End of Prompts[/bold cyan]\")\n",
    "else:\n",
    "    rprint(\"[dim]Prompt display disabled[/dim]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 7\n",
    "# Discover images and load ground truth\n",
    "data_dir = Path(CONFIG['DATA_DIR'])\n",
    "if not data_dir.is_absolute():\n",
    "    data_dir = Path.cwd() / data_dir\n",
    "\n",
    "all_images = discover_images(str(data_dir))\n",
    "\n",
    "# Apply preprocessing if enabled\n",
    "if CONFIG['ENABLE_PREPROCESSING']:\n",
    "    import tempfile\n",
    "    from common.image_preprocessing import (\n",
    "        enhance_statement_quality,\n",
    "        enhance_for_llama,\n",
    "        preprocess_statement_for_llama,\n",
    "        adaptive_enhance,\n",
    "        preprocess_recommended\n",
    "    )\n",
    "    \n",
    "    preprocess_functions = {\n",
    "        'light': enhance_statement_quality,\n",
    "        'moderate': enhance_for_llama,\n",
    "        'aggressive': preprocess_statement_for_llama,\n",
    "        'adaptive': adaptive_enhance,\n",
    "        'recommended': preprocess_recommended\n",
    "    }\n",
    "    \n",
    "    preprocess_fn = preprocess_functions[CONFIG['PREPROCESSING_MODE']]\n",
    "    preprocessed_images = []\n",
    "    \n",
    "    rprint(f\"[cyan]üîß Preprocessing {len(all_images)} images (mode: {CONFIG['PREPROCESSING_MODE']})[/cyan]\")\n",
    "    \n",
    "    if CONFIG['SAVE_PREPROCESSED']:\n",
    "        preprocessed_dir = Path(CONFIG['PREPROCESSED_DIR'] or 'preprocessed_images')\n",
    "        preprocessed_dir.mkdir(parents=True, exist_ok=True)\n",
    "    else:\n",
    "        preprocessed_dir = Path(tempfile.mkdtemp(prefix='preprocessed_'))\n",
    "    \n",
    "    for img_path in all_images:\n",
    "        original_filename = Path(img_path).name\n",
    "        try:\n",
    "            preprocessed_img = preprocess_fn(img_path)\n",
    "            preprocessed_path = preprocessed_dir / original_filename\n",
    "            preprocessed_img.save(preprocessed_path)\n",
    "            preprocessed_images.append(str(preprocessed_path))\n",
    "        except Exception as e:\n",
    "            rprint(f\"[yellow]‚ö†Ô∏è  Preprocessing failed for {original_filename}: {e}[/yellow]\")\n",
    "            preprocessed_images.append(img_path)\n",
    "    \n",
    "    all_images = preprocessed_images\n",
    "    rprint(f\"[green]‚úÖ Preprocessing complete[/green]\")\n",
    "\n",
    "# Load ground truth\n",
    "ground_truth_path = Path(CONFIG['GROUND_TRUTH'])\n",
    "if not ground_truth_path.is_absolute():\n",
    "    ground_truth_path = Path.cwd() / ground_truth_path\n",
    "\n",
    "ground_truth = load_ground_truth(str(ground_truth_path), verbose=CONFIG['VERBOSE'])\n",
    "\n",
    "# Apply max images limit\n",
    "if CONFIG['MAX_IMAGES']:\n",
    "    all_images = all_images[:CONFIG['MAX_IMAGES']]\n",
    "\n",
    "rprint(f\"[bold green]Ready to process {len(all_images)} images[/bold green]\")\n",
    "rprint(f\"[cyan]Ground truth loaded for {len(ground_truth)} images[/cyan]\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "#Cell 7.75\n# Ground Truth Debug - validate GT matching before processing\nif ground_truth:\n    rprint(\"\\n[bold yellow]üîç Ground Truth Debug Info[/bold yellow]\")\n    rprint(f\"[cyan]Total ground truth entries: {len(ground_truth)}[/cyan]\")\n    rprint(f\"[cyan]Total images to process: {len(all_images)}[/cyan]\")\n    \n    # Show first 3 ground truth keys\n    gt_keys = list(ground_truth.keys())[:3]\n    rprint(f\"[cyan]Sample GT keys: {gt_keys}[/cyan]\")\n    \n    # Show first 3 image filenames (with and without extensions)\n    img_names_full = [Path(img).name for img in all_images[:3]]\n    img_names_no_ext = [Path(img).stem for img in all_images[:3]]\n    rprint(f\"[cyan]Sample image names (full): {img_names_full}[/cyan]\")\n    rprint(f\"[cyan]Sample image names (no ext): {img_names_no_ext}[/cyan]\")\n    \n    # Check for mismatches using filename WITHOUT extension (Path.stem)\n    missing_gt = []\n    for img in all_images:\n        img_name_no_ext = Path(img).stem  # Strip extension for GT lookup\n        if img_name_no_ext not in ground_truth:\n            missing_gt.append(Path(img).name)  # Show full name in error\n    \n    if missing_gt:\n        rprint(f\"[red]‚ö†Ô∏è  WARNING: {len(missing_gt)} images missing from ground truth![/red]\")\n        rprint(f\"[red]First 5 missing: {missing_gt[:5]}[/red]\")\n    else:\n        rprint(f\"[green]‚úÖ All {len(all_images)} images have ground truth entries (using stem lookup)[/green]\")\n    \n    console.rule()\nelse:\n    rprint(\"[yellow]‚ö†Ô∏è  No ground truth loaded (inference-only mode)[/yellow]\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "#Cell 8\n# Load document-specific field mappings for evaluation\nfield_defs_path = Path('config/field_definitions.yaml')\nwith open(field_defs_path, 'r') as f:\n    field_defs = yaml.safe_load(f)\n\nDOC_TYPE_FIELDS = {\n    'invoice': field_defs['document_fields']['invoice']['fields'],\n    'receipt': field_defs['document_fields']['receipt']['fields'],\n    'bank_statement': field_defs['document_fields']['bank_statement']['fields'],\n    'statement': field_defs['document_fields']['bank_statement']['fields'],  # Alias\n}\n\n# Oracle batch processing - use ground truth doc type to select prompt\nconsole.rule(\"[bold cyan]Oracle Batch Processing[/bold cyan]\")\n\nbatch_results = []\nprocessing_times = []\nskipped_count = 0\n\nfor idx, image_path in enumerate(all_images, 1):\n    # CRITICAL FIX: Use stem (no extension) to match ground truth keys\n    # Ground truth image_name column has no file extensions\n    image_name = Path(image_path).stem\n    image_display_name = Path(image_path).name  # For display only\n    \n    rprint(f\"\\n[bold cyan]Processing {idx}/{len(all_images)}: {image_display_name}[/bold cyan]\")\n    \n    # Check if ground truth exists (using name without extension)\n    if image_name not in ground_truth:\n        rprint(f\"[yellow]‚ö†Ô∏è  No ground truth for {image_name} - skipping[/yellow]\")\n        skipped_count += 1\n        continue\n    \n    gt_data = ground_truth[image_name]\n    gt_doc_type = gt_data.get('DOCUMENT_TYPE', '').lower()\n    \n    # Map ground truth doc type to prompt key\n    if 'statement' in gt_doc_type or 'bank' in gt_doc_type:\n        prompt_key = 'bank_statement'\n    elif 'invoice' in gt_doc_type:\n        prompt_key = 'invoice'\n    elif 'receipt' in gt_doc_type:\n        prompt_key = 'receipt'\n    else:\n        rprint(f\"[red]‚ùå Unknown doc type: {gt_doc_type} - skipping[/red]\")\n        skipped_count += 1\n        continue\n    \n    # Get oracle prompt\n    if prompt_key not in ORACLE_PROMPTS:\n        rprint(f\"[red]‚ùå No prompt for {prompt_key} - skipping[/red]\")\n        skipped_count += 1\n        continue\n    \n    oracle_prompt = ORACLE_PROMPTS[prompt_key]\n    \n    rprint(f\"[cyan]üìã Using oracle prompt: {prompt_key} (from ground truth: {gt_doc_type})[/cyan]\")\n    \n    # Load and process image\n    try:\n        image = PILImage.open(image_path)\n        \n        # Create messages in Llama chat format\n        messages = [\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\"type\": \"image\"},\n                    {\"type\": \"text\", \"text\": oracle_prompt}\n                ]\n            }\n        ]\n        \n        # Apply chat template\n        input_text = processor.apply_chat_template(messages, add_generation_prompt=True)\n        \n        # Process inputs\n        inputs = processor(\n            image,\n            input_text,\n            return_tensors=\"pt\"\n        ).to(model.device)\n        \n        # Generate response\n        start_time = datetime.now()\n        \n        with torch.inference_mode():\n            output = model.generate(\n                **inputs,\n                max_new_tokens=CONFIG['MAX_NEW_TOKENS'],\n                do_sample=False\n            )\n        \n        processing_time = (datetime.now() - start_time).total_seconds()\n        processing_times.append(processing_time)\n        \n        # Decode response\n        response = processor.decode(output[0], skip_special_tokens=True)\n        \n        # Extract only the assistant's response (after \"assistant\\n\\n\")\n        if \"assistant\\n\\n\" in response:\n            response = response.split(\"assistant\\n\\n\", 1)[1]\n        \n        # Parse extraction response (field-by-field format, NOT JSON)\n        extracted_data = parse_extraction_response(response)\n        \n        # Manual evaluation - get relevant fields for this doc type\n        doc_type_normalized = gt_doc_type.replace(' ', '_')\n        relevant_fields = DOC_TYPE_FIELDS.get(doc_type_normalized, DOC_TYPE_FIELDS.get(prompt_key, []))\n        \n        # Evaluate each field\n        field_scores = {}\n        total_f1 = 0.0\n        fields_evaluated = 0\n        fields_matched = 0\n        \n        for field in relevant_fields:\n            extracted_value = extracted_data.get(field, \"NOT_FOUND\")\n            gt_value = gt_data.get(field, \"NOT_FOUND\")\n            \n            # Skip if both are NOT_FOUND\n            if extracted_value == \"NOT_FOUND\" and gt_value == \"NOT_FOUND\":\n                continue\n            \n            fields_evaluated += 1\n            \n            try:\n                metrics = calculate_field_accuracy_with_method(\n                    extracted_value, gt_value, field, \n                    method=os.environ.get('EVALUATION_METHOD', 'order_aware_f1')\n                )\n            except Exception as e:\n                rprint(f\"[yellow]‚ö†Ô∏è  Error evaluating {field}: {e}[/yellow]\")\n                metrics = {'f1_score': 0.0, 'precision': 0.0, 'recall': 0.0}\n            \n            field_scores[field] = metrics\n            total_f1 += metrics.get('f1_score', 0.0)\n            \n            if metrics.get('f1_score', 0.0) > 0.9:\n                fields_matched += 1\n        \n        # Calculate overall accuracy\n        overall_accuracy = (total_f1 / fields_evaluated) if fields_evaluated > 0 else 0.0\n        \n        # Store evaluation results\n        evaluation = {\n            'overall_accuracy': overall_accuracy,\n            'fields_evaluated': fields_evaluated,\n            'fields_matched': fields_matched,\n            'total_fields': len(relevant_fields),\n            'field_scores': field_scores\n        }\n        \n        # Store results (use stem for image_name to match GT)\n        result = {\n            'image_name': image_name,  # Without extension - matches GT\n            'image_path': image_path,\n            'oracle_doc_type': gt_doc_type,\n            'prompt_used': prompt_key,\n            'extracted_data': extracted_data,\n            'evaluation': evaluation,\n            'processing_time': processing_time,\n            'raw_response': response\n        }\n        \n        batch_results.append(result)\n        \n        # Show summary\n        accuracy = evaluation.get('overall_accuracy', 0) * 100\n        fields_matched = evaluation.get('fields_matched', 0)\n        total_fields = evaluation.get('total_fields', 0)\n        \n        rprint(f\"[green]‚úÖ Accuracy: {accuracy:.1f}% ({fields_matched}/{total_fields} fields)[/green]\")\n        rprint(f\"[cyan]‚è±Ô∏è  Time: {processing_time:.2f}s[/cyan]\")\n        \n    except Exception as e:\n        rprint(f\"[red]‚ùå Error processing {image_display_name}: {e}[/red]\")\n        import traceback\n        rprint(f\"[red]{traceback.format_exc()}[/red]\")\n        continue\n\nconsole.rule(\"[bold green]Oracle Processing Complete[/bold green]\")\nrprint(f\"[green]‚úÖ Processed: {len(batch_results)} images[/green]\")\nrprint(f\"[yellow]‚ö†Ô∏è  Skipped: {skipped_count} images[/yellow]\")\n\nif processing_times:\n    avg_time = np.mean(processing_times)\n    avg_accuracy = np.mean([r['evaluation']['overall_accuracy'] * 100 for r in batch_results])\n    rprint(f\"[cyan]Average time: {avg_time:.2f}s[/cyan]\")\n    rprint(f\"[cyan]Average accuracy: {avg_accuracy:.1f}%[/cyan]\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "#Cell 8.5\n# Load field columns for CSV export\nfield_defs_path = Path('config/field_definitions.yaml')\n\nwith open(field_defs_path, 'r') as f:\n    field_defs = yaml.safe_load(f)\n\n# Get universal fields from YAML (19 total)\nall_universal_fields = field_defs['document_fields']['universal']['fields']\n\n# Remove fields no longer extracted (TRANSACTION_AMOUNTS_RECEIVED, ACCOUNT_BALANCE)\n# Final 17 fields for CSV columns\nEXCLUDED_FIELDS = ['TRANSACTION_AMOUNTS_RECEIVED', 'ACCOUNT_BALANCE']\nFIELD_COLUMNS = [f for f in all_universal_fields if f not in EXCLUDED_FIELDS]\n\nrprint(f\"[green]‚úÖ Loaded {len(FIELD_COLUMNS)} field columns for CSV export[/green]\")\nrprint(f\"[cyan]Excluded: {', '.join(EXCLUDED_FIELDS)}[/cyan]\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#Cell 9\n# Create oracle CSV results file\n# Match structure expected by model_comparison.ipynb\n# FIELD_COLUMNS loaded from YAML in previous cell\n\noracle_csv_data = []\n\nfor i, result in enumerate(batch_results):\n    image_name = result['image_name']\n    oracle_doc_type = result['oracle_doc_type']\n    prompt_used = f\"llama_oracle_{result['prompt_used']}\"\n    processing_time = result['processing_time']\n    \n    extracted_data = result['extracted_data']\n    evaluation = result['evaluation']\n    \n    # Count fields\n    found_fields = sum(1 for field in FIELD_COLUMNS if extracted_data.get(field, 'NOT_FOUND') != 'NOT_FOUND')\n    field_coverage = (found_fields / len(FIELD_COLUMNS) * 100) if FIELD_COLUMNS else 0\n    \n    # Create row\n    row_data = {\n        'image_file': image_name,\n        'image_name': image_name,\n        'document_type': oracle_doc_type,\n        'oracle_doc_type': oracle_doc_type,\n        'processing_time': processing_time,\n        'field_count': evaluation.get('total_fields', 0),\n        'found_fields': evaluation.get('fields_extracted', 0),\n        'field_coverage': field_coverage,\n        'prompt_used': prompt_used,\n        'timestamp': datetime.now().isoformat(),\n        'overall_accuracy': evaluation.get('overall_accuracy', 0) * 100,\n        'fields_extracted': evaluation.get('fields_extracted', 0),\n        'fields_matched': evaluation.get('fields_matched', 0),\n        'total_fields': evaluation.get('total_fields', 0)\n    }\n    \n    # Add all field values\n    for field in FIELD_COLUMNS:\n        row_data[field] = extracted_data.get(field, 'NOT_FOUND')\n    \n    oracle_csv_data.append(row_data)\n\n# Create DataFrame and save\noracle_df = pd.DataFrame(oracle_csv_data)\noracle_csv_path = OUTPUT_DIRS['csv'] / f\"llama_oracle_batch_results_{BATCH_TIMESTAMP}.csv\"\noracle_df.to_csv(oracle_csv_path, index=False)\n\nrprint(\"[bold green]‚úÖ Oracle CSV exported:[/bold green]\")\nrprint(f\"[cyan]üìÑ File: {oracle_csv_path}[/cyan]\")\nrprint(f\"[cyan]üìä Structure: {len(oracle_df)} rows √ó {len(oracle_df.columns)} columns[/cyan]\")\nrprint(\"[cyan]üîó Compatible with model_comparison.ipynb pattern: *llama*oracle*batch*results*.csv[/cyan]\")\n\n# Display sample\nrprint(\"\\n[bold blue]üìã Sample exported data:[/bold blue]\")\nsample_cols = ['image_file', 'oracle_doc_type', 'overall_accuracy', 'processing_time', 'fields_matched', 'total_fields']\nif len(oracle_df) > 0:\n    display(oracle_df[sample_cols].head(3))\nelse:\n    rprint(\"[yellow]‚ö†Ô∏è No data to display[/yellow]\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 10\n",
    "# Summary statistics by document type\n",
    "console.rule(\"[bold cyan]Oracle Results by Document Type[/bold cyan]\")\n",
    "\n",
    "# Group by oracle doc type\n",
    "doc_type_stats = {}\n",
    "\n",
    "for result in batch_results:\n",
    "    doc_type = result['oracle_doc_type']\n",
    "    \n",
    "    if doc_type not in doc_type_stats:\n",
    "        doc_type_stats[doc_type] = {\n",
    "            'count': 0,\n",
    "            'accuracies': [],\n",
    "            'times': [],\n",
    "            'fields_matched': [],\n",
    "            'total_fields': []\n",
    "        }\n",
    "    \n",
    "    stats = doc_type_stats[doc_type]\n",
    "    evaluation = result['evaluation']\n",
    "    \n",
    "    stats['count'] += 1\n",
    "    stats['accuracies'].append(evaluation.get('overall_accuracy', 0) * 100)\n",
    "    stats['times'].append(result['processing_time'])\n",
    "    stats['fields_matched'].append(evaluation.get('fields_matched', 0))\n",
    "    stats['total_fields'].append(evaluation.get('total_fields', 0))\n",
    "\n",
    "# Display statistics\n",
    "for doc_type, stats in doc_type_stats.items():\n",
    "    console.rule(f\"[cyan]{doc_type.upper()}[/cyan]\")\n",
    "    \n",
    "    rprint(f\"[cyan]Count: {stats['count']}[/cyan]\")\n",
    "    rprint(f\"[cyan]Average accuracy: {np.mean(stats['accuracies']):.1f}%[/cyan]\")\n",
    "    rprint(f\"[cyan]Average time: {np.mean(stats['times']):.2f}s[/cyan]\")\n",
    "    rprint(f\"[cyan]Fields matched: {np.mean(stats['fields_matched']):.1f}/{np.mean(stats['total_fields']):.1f}[/cyan]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 11\n",
    "# Comparison: Oracle vs Expected Performance\n",
    "console.rule(\"[bold cyan]Oracle Performance Analysis[/bold cyan]\")\n",
    "\n",
    "rprint(\"[bold]Key Insights:[/bold]\")\n",
    "rprint(\"\\n[cyan]1. Perfect Classification:[/cyan]\")\n",
    "rprint(\"   Oracle method uses ground truth doc type - no classification errors\")\n",
    "rprint(\"   Always extracts correct field set for each document type\")\n",
    "\n",
    "rprint(\"\\n[cyan]2. Performance Metrics:[/cyan]\")\n",
    "if batch_results:\n",
    "    avg_oracle_accuracy = np.mean([r['evaluation']['overall_accuracy'] * 100 for r in batch_results])\n",
    "    rprint(f\"   Oracle accuracy: {avg_oracle_accuracy:.1f}%\")\n",
    "    rprint(\"   This represents extraction accuracy WITHOUT classification penalty\")\n",
    "\n",
    "rprint(\"\\n[cyan]3. Comparison Analysis:[/cyan]\")\n",
    "rprint(\"   Compare this CSV with baseline (llama_batch_results_*.csv) to quantify:\")\n",
    "rprint(\"   ‚Ä¢ Classification penalty = Baseline accuracy - Oracle accuracy\")\n",
    "rprint(\"   ‚Ä¢ Extraction ceiling = Maximum achievable with perfect classification\")\n",
    "\n",
    "rprint(f\"\\n[bold green]‚úÖ Oracle results saved to: {oracle_csv_path}[/bold green]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}