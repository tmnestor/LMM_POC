{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# InternVL3.5-8B: 2-Turn Balance-Description Bank Statement Extraction\n",
    "\n",
    "**Protocol**: Two independent single-turn prompts using Balance-Description approach\n",
    "\n",
    "**Key Insight**: Anchoring extraction to the Balance column works for BOTH date-per-row AND date-grouped formats!\n",
    "\n",
    "**Model**: InternVL3.5-8B (non-quantized, bfloat16) optimized for H200 GPU\n",
    "\n",
    "---\n",
    "\n",
    "## Complete Workflow\n",
    "\n",
    "```\n",
    "Turn 0: Image + Prompt ‚Üí Headers (fresh context)\n",
    "        ‚Üì (Python pattern matching)\n",
    "Turn 1: Image + Balance-Description Prompt ‚Üí Hierarchical balance list (fresh context)\n",
    "        ‚Üì (Python parsing + filtering)\n",
    "Schema Fields: TRANSACTION_DATES, LINE_ITEM_DESCRIPTIONS, TRANSACTION_AMOUNTS_PAID\n",
    "```\n",
    "\n",
    "### Why Balance-Description Works:\n",
    "- **Balance anchoring**: Every transaction has a balance - use it as the anchor point\n",
    "- **Format agnostic**: Works for both date-per-row and date-grouped formats\n",
    "- **No date format classification needed**: Eliminates Turn 0.5 entirely\n",
    "- **Hierarchical output**: Balance ‚Üí Date, Description, Debit/Credit amounts\n",
    "\n",
    "### Pipeline Stages:\n",
    "1. **Turn 0 (LLM)**: Identify column headers from image\n",
    "2. **Pattern Matching (Python)**: Map headers to concepts (Date, Description, Debit, Credit, Balance)\n",
    "3. **Turn 1 (LLM)**: Extract using balance-description prompt (format-agnostic)\n",
    "4. **Python Parsing**: Parse hierarchical output ‚Üí Filter for debits ‚Üí Extract schema fields\n",
    "\n",
    "### Critical Features:\n",
    "- ‚ùå **No Turn 0.5** - No date format classification needed!\n",
    "- ‚úÖ **Balance anchoring** - Works for all statement formats\n",
    "- ‚úÖ **Python filtering** - Reliable debit/credit separation\n",
    "- ‚úÖ **Hierarchical output** - Clear structure for parsing\n",
    "\n",
    "### Model: InternVL3.5-8B\n",
    "- **Non-quantized** for H200 GPU with 80GB HBM3\n",
    "- **bfloat16** precision for optimal performance\n",
    "- **Flash Attention** enabled for efficiency\n",
    "- Higher capacity than 2B variant\n",
    "- Simple API with `model.chat()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and Configuration\n",
    "\n",
    "from pathlib import Path\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "\n",
    "# IPython display for rendering markdown\n",
    "from IPython.display import display, Markdown, HTML\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION - All settings explicit in notebook\n",
    "# ============================================================================\n",
    "CONFIG = {\n",
    "    # Model settings - InternVL3.5-8B (H200 optimized)\n",
    "    'MODEL_PATH': '/home/jovyan/nfs_share/models/InternVL3_5-8B',\n",
    "    \n",
    "    # H200 TILE CONFIGURATION\n",
    "    'MAX_TILES': 36,  # H200 optimized - InternVL3.5 training max for dense OCR\n",
    "    \n",
    "    # Generation settings\n",
    "    'MAX_NEW_TOKENS': 2000,\n",
    "    \n",
    "    # H200 precision settings\n",
    "    'TORCH_DTYPE': 'bfloat16',\n",
    "    'USE_FLASH_ATTN': True,\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Imports loaded\")\n",
    "print(f\"üî≤ Max Tiles: {CONFIG['MAX_TILES']} (H200 optimized)\")\n",
    "print(f\"ü§ñ Model: {Path(CONFIG['MODEL_PATH']).name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "# Set Random Seed for Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Set random seed\n",
    "\n",
    "from common.reproducibility import set_seed\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "# Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Load InternVL3.5-8B model (non-quantized for H200)\n",
    "\n",
    "print(\"üîß Loading InternVL3.5-8B (non-quantized, bfloat16) for H200 GPU...\")\n",
    "\n",
    "# Load model with bfloat16 and flash attention for H200\n",
    "model = AutoModel.from_pretrained(\n",
    "    CONFIG['MODEL_PATH'],\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    low_cpu_mem_usage=True,\n",
    "    use_flash_attn=CONFIG['USE_FLASH_ATTN'],\n",
    ").eval()\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    CONFIG['MODEL_PATH'],\n",
    "    trust_remote_code=True,\n",
    "    use_fast=False\n",
    ")\n",
    "\n",
    "# Set generation config on model\n",
    "model.config.max_new_tokens = CONFIG['MAX_NEW_TOKENS']\n",
    "\n",
    "# Fix pad_token_id to suppress warnings\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "print(f\"‚úÖ InternVL3.5-8B model loaded successfully!\")\n",
    "print(f\"‚úÖ Model distributed across devices: {model.hf_device_map}\")\n",
    "print(f\"üìä Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"üìä Precision: bfloat16 with flash attention\")\n",
    "print(f\"üî≤ Max Tiles: {CONFIG['MAX_TILES']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "# Load the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Load bank statement image\n",
    "# Update this path to your test image\n",
    "# imageName = \"/home/jovyan/nfs_share/tod/LMM_POC/evaluation_data/synthetic_bank_images/cba_amount_balance.png\"\n",
    "# imageName = \"/home/jovyan/nfs_share/tod/LMM_POC/evaluation_data/synthetic_bank_images/cba_date_grouped_cont.png\"\n",
    "# imageName = \"/home/jovyan/nfs_share/tod/LMM_POC/evaluation_data/synthetic_bank_images/cba_debit_credit.png\"\n",
    "# imageName = \"/home/jovyan/nfs_share/tod/LMM_POC/evaluation_data/synthetic_bank_images/cba_highligted.png\"\n",
    "# imageName = \"/home/jovyan/nfs_share/tod/LMM_POC/evaluation_data/synthetic_bank_images/low_contrast_fixed.png\"\n",
    "# imageName = \"/home/jovyan/nfs_share/tod/LMM_POC/evaluation_data/synthetic_bank_images/nab_classic_highligted.png\"\n",
    "# imageName = \"/home/jovyan/nfs_share/tod/LMM_POC/evaluation_data/synthetic_bank_images/westpac_debit_credit.png\"\n",
    "# imageName = \"/home/jovyan/nfs_share/tod/LMM_POC/evaluation_data/synthetic_bank_images/transaction_summary.png\"\n",
    "imageName = \"/home/jovyan/nfs_share/tod/LMM_POC/evaluation_data/cba_date_grouped.png\"\n",
    "\n",
    "\n",
    "\n",
    "print(\"üìÅ Loading image...\")\n",
    "image = Image.open(imageName)\n",
    "\n",
    "# CRITICAL: Store as list for multi-turn compatibility\n",
    "images = [image]\n",
    "\n",
    "print(f\"‚úÖ Image loaded: {image.size}\")\n",
    "print(f\"‚úÖ Images list created with {len(images)} image(s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the loaded image for visual verification\n",
    "print(\"üñºÔ∏è  Bank statement image:\")\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Image preprocessing for InternVL3.5 (Official implementation)\n",
    "\n",
    "# Official InternVL3 image preprocessing (from docs)\n",
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD = (0.229, 0.224, 0.229)\n",
    "\n",
    "def build_transform(input_size):\n",
    "    \"\"\"Build image transformation pipeline with ImageNet normalization.\"\"\"\n",
    "    transform = T.Compose([\n",
    "        T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
    "        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "    ])\n",
    "    return transform\n",
    "\n",
    "def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):\n",
    "    \"\"\"Find the closest aspect ratio from target ratios based on image dimensions.\"\"\"\n",
    "    best_ratio_diff = float('inf')\n",
    "    best_ratio = (1, 1)\n",
    "    area = width * height\n",
    "    for ratio in target_ratios:\n",
    "        target_aspect_ratio = ratio[0] / ratio[1]\n",
    "        ratio_diff = abs(aspect_ratio - target_aspect_ratio)\n",
    "        if ratio_diff < best_ratio_diff:\n",
    "            best_ratio_diff = ratio_diff\n",
    "            best_ratio = ratio\n",
    "        elif ratio_diff == best_ratio_diff:\n",
    "            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\n",
    "                best_ratio = ratio\n",
    "    return best_ratio\n",
    "\n",
    "def dynamic_preprocess(image, min_num=1, max_num=None, image_size=448, use_thumbnail=False):\n",
    "    \"\"\"\n",
    "    Dynamically preprocess image by splitting into tiles based on aspect ratio.\n",
    "    \n",
    "    Args:\n",
    "        image: PIL Image\n",
    "        min_num: Minimum number of tiles\n",
    "        max_num: Maximum number of tiles (from CONFIG['MAX_TILES'])\n",
    "        image_size: Size of each tile (448 for InternVL3)\n",
    "        use_thumbnail: Whether to include thumbnail image\n",
    "    \n",
    "    Returns:\n",
    "        List of PIL Image tiles\n",
    "    \"\"\"\n",
    "    # Use CONFIG if max_num not specified\n",
    "    if max_num is None:\n",
    "        max_num = CONFIG['MAX_TILES']\n",
    "    \n",
    "    orig_width, orig_height = image.size\n",
    "    aspect_ratio = orig_width / orig_height\n",
    "\n",
    "    # Generate target aspect ratios\n",
    "    target_ratios = set(\n",
    "        (i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if\n",
    "        i * j <= max_num and i * j >= min_num)\n",
    "    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n",
    "\n",
    "    # Find best aspect ratio\n",
    "    target_aspect_ratio = find_closest_aspect_ratio(\n",
    "        aspect_ratio, target_ratios, orig_width, orig_height, image_size)\n",
    "\n",
    "    # Calculate target dimensions\n",
    "    target_width = image_size * target_aspect_ratio[0]\n",
    "    target_height = image_size * target_aspect_ratio[1]\n",
    "    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]\n",
    "\n",
    "    # Resize and split into tiles\n",
    "    resized_img = image.resize((target_width, target_height))\n",
    "    processed_images = []\n",
    "    for i in range(blocks):\n",
    "        box = (\n",
    "            (i % (target_width // image_size)) * image_size,\n",
    "            (i // (target_width // image_size)) * image_size,\n",
    "            ((i % (target_width // image_size)) + 1) * image_size,\n",
    "            ((i // (target_width // image_size)) + 1) * image_size\n",
    "        )\n",
    "        split_img = resized_img.crop(box)\n",
    "        processed_images.append(split_img)\n",
    "    \n",
    "    assert len(processed_images) == blocks\n",
    "    \n",
    "    # Add thumbnail if requested\n",
    "    if use_thumbnail and len(processed_images) != 1:\n",
    "        thumbnail_img = image.resize((image_size, image_size))\n",
    "        processed_images.append(thumbnail_img)\n",
    "    \n",
    "    return processed_images\n",
    "\n",
    "def load_image(image_file, input_size=448, max_num=None):\n",
    "    \"\"\"\n",
    "    Load and preprocess image for InternVL3.5.\n",
    "    \n",
    "    Args:\n",
    "        image_file: Path to image or PIL Image object\n",
    "        input_size: Size of each tile (448 for InternVL3)\n",
    "        max_num: Max number of tiles (uses CONFIG['MAX_TILES'] if None)\n",
    "    \n",
    "    Returns:\n",
    "        pixel_values: Preprocessed tensor ready for model.chat()\n",
    "    \"\"\"\n",
    "    # Use CONFIG if max_num not specified\n",
    "    if max_num is None:\n",
    "        max_num = CONFIG['MAX_TILES']\n",
    "    \n",
    "    # Handle both path string and PIL Image\n",
    "    if isinstance(image_file, str):\n",
    "        image = Image.open(image_file).convert('RGB')\n",
    "    else:\n",
    "        image = image_file\n",
    "    \n",
    "    # Build transform and preprocess\n",
    "    transform = build_transform(input_size=input_size)\n",
    "    images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)\n",
    "    pixel_values = [transform(img) for img in images]\n",
    "    pixel_values = torch.stack(pixel_values)\n",
    "    \n",
    "    return pixel_values\n",
    "\n",
    "print(\"‚úÖ InternVL3.5 image preprocessing functions defined\")\n",
    "print(f\"üî≤ Using max_num={CONFIG['MAX_TILES']} tiles (from CONFIG)\")\n",
    "print(f\"üí° Image preprocessing: ImageNet normalization + dynamic tiling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "# Bank Statement Extraction Protocol (2-Turn Balance-Description)\n",
    "- Turn 0: Identify actual table headers\n",
    "- Turn 1: Extract using balance-description prompt (format-agnostic)\n",
    "- Python: Parse hierarchical output, filter, and extract schema fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Turn 0 - Identify table headers (prompt)\n",
    "# TURN 0: Identify Table Headers\n",
    "# First, identify the actual column headers used in this specific bank statement\n",
    "\n",
    "prompt = \"\"\"\n",
    "Look at the transaction table in this bank statement image.\n",
    "\n",
    "IMPORTANT STRUCTURAL NOTE:\n",
    "Some bank statements show dates as section headings with multiple transactions underneath.\n",
    "If you see this structure, remember that each transaction needs its explicit date in the final output.\n",
    "\n",
    "What are the exact column header names used in the transaction table?\n",
    "\n",
    "List each column header exactly as it appears, in order from left to right.\n",
    "Do not interpret or rename them - use the EXACT text from the image.\n",
    "\"\"\"\n",
    "\n",
    "print(\"üí¨ TURN 0: Identifying actual table headers\")\n",
    "print(\"ü§ñ Generating response with InternVL3.5-8B...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Turn 0 - Execute and parse headers (InternVL3.5-8B version)\n",
    "\n",
    "print(\"üí¨ TURN 0: Identifying actual table headers\")\n",
    "print(\"ü§ñ Generating response with InternVL3.5-8B (bfloat16)...\")\n",
    "\n",
    "# Load and preprocess image for InternVL3.5 (uses CONFIG['MAX_TILES'])\n",
    "pixel_values = load_image(imageName, input_size=448)\n",
    "\n",
    "# Move to correct device and dtype for InternVL3.5-8B\n",
    "# CRITICAL: Use bfloat16 for non-quantized H200 models\n",
    "vision_device = 'cuda:0'  # Vision model is on GPU 0\n",
    "model_dtype = torch.bfloat16\n",
    "pixel_values = pixel_values.to(dtype=model_dtype, device=vision_device)\n",
    "\n",
    "# Generate response using chat() method\n",
    "cleanedOutput = model.chat(\n",
    "    tokenizer=tokenizer,\n",
    "    pixel_values=pixel_values,\n",
    "    question=prompt,\n",
    "    generation_config={\n",
    "        \"max_new_tokens\": CONFIG['MAX_NEW_TOKENS'],\n",
    "        \"do_sample\": False  # Greedy decoding for consistency\n",
    "    }\n",
    ")\n",
    "\n",
    "# Clean InternVL3 artifacts:\n",
    "# 1. Remove image markdown placeholder (![...])\n",
    "# 2. Remove markdown code fences (```markdown and ```)\n",
    "lines = cleanedOutput.split(\"\\n\")\n",
    "cleaned_lines = []\n",
    "for line in lines:\n",
    "    stripped = line.strip()\n",
    "    # Skip image markdown, code fences, and empty fence markers\n",
    "    if stripped.startswith(\"![\"):\n",
    "        continue\n",
    "    if stripped in [\"```markdown\", \"```\", \"```md\"]:\n",
    "        continue\n",
    "    cleaned_lines.append(line)\n",
    "\n",
    "cleanedOutput = \"\\n\".join(cleaned_lines)\n",
    "\n",
    "print(\"‚úÖ Response generated successfully!\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TURN 0 - IDENTIFIED TABLE HEADERS:\")\n",
    "print(\"=\" * 60)\n",
    "print(cleanedOutput)\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# CRITICAL: Parse the identified headers for use in subsequent turns\n",
    "# Extract column names from the response\n",
    "header_lines = [line.strip() for line in cleanedOutput.split('\\n') if line.strip()]\n",
    "identified_headers = []\n",
    "\n",
    "# Look for numbered list or bullet points\n",
    "for line in header_lines:\n",
    "    # Remove common list markers\n",
    "    cleaned = line.lstrip('0123456789.-‚Ä¢* ').strip()\n",
    "    \n",
    "    # Strip markdown bold formatting\n",
    "    cleaned = cleaned.replace('**', '').replace('__', '')\n",
    "    \n",
    "    # Skip section headers (lines ending with colon)\n",
    "    if cleaned.endswith(':'):\n",
    "        continue\n",
    "    \n",
    "    # Skip long sentences (likely explanatory text, not headers)\n",
    "    if len(cleaned) > 40:\n",
    "        continue\n",
    "        \n",
    "    if cleaned and len(cleaned) > 2:  # Ignore very short strings\n",
    "        identified_headers.append(cleaned)\n",
    "\n",
    "print(f\"\\nüìã Parsed {len(identified_headers)} column headers:\")\n",
    "for i, header in enumerate(identified_headers, 1):\n",
    "    print(f\"  {i}. '{header}'\")\n",
    "\n",
    "# Store headers for use in subsequent turns\n",
    "table_headers = identified_headers\n",
    "\n",
    "# Save the table headers\n",
    "output_path = Path(\"ivl3_5_8b_2turn_table_headers.txt\")\n",
    "with output_path.open(\"w\", encoding=\"utf-8\") as text_file:\n",
    "    text_file.write(cleanedOutput)\n",
    "\n",
    "print(f\"\\n‚úÖ Table headers saved to: {output_path}\")\n",
    "print(\"üí° These LITERAL header names will be used in Turn 1 balance-description prompt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Pattern Matching: Map Generic Concepts to Actual Headers\n",
    "\n",
    "Different bank statements use different column names. Use pattern matching to identify:\n",
    "- Which header represents **Date**\n",
    "- Which header represents **Description/Details**  \n",
    "- Which header represents **Debit/Withdrawal**\n",
    "- Which header represents **Credit/Deposit**\n",
    "- Which header represents **Balance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Pattern Matching - Map headers to generic columns\n",
    "# Pattern Matching: Map extracted headers to generic concepts\n",
    "# This handles variety in bank statement column naming conventions\n",
    "\n",
    "# Pattern keywords for each concept (in priority order)\n",
    "DATE_PATTERNS = ['date', 'day', 'transaction date', 'trans date']\n",
    "DESCRIPTION_PATTERNS = [\n",
    "    'description', 'details', 'transaction details', 'trans details',\n",
    "    'particulars', 'narrative', 'transaction', 'trans'\n",
    "]\n",
    "DEBIT_PATTERNS = ['debit', 'withdrawal', 'withdrawals', 'paid', 'paid out', 'spent', 'dr']\n",
    "CREDIT_PATTERNS = ['credit', 'deposit', 'deposits', 'received', 'cr']\n",
    "BALANCE_PATTERNS = ['balance', 'bal', 'running balance']\n",
    "\n",
    "# NEW: Pattern for single-column transaction formats (e.g., \"Amount\" instead of separate Debit/Credit)\n",
    "AMOUNT_PATTERNS = ['amount', 'amt', 'value', 'total']\n",
    "\n",
    "def match_header(headers, patterns, fallback=None):\n",
    "    \"\"\"Match a header using pattern keywords.\n",
    "    \n",
    "    Matching strategy:\n",
    "    1. Exact match (case-insensitive)\n",
    "    2. Substring match (only for patterns with length > 2 to avoid false positives)\n",
    "    \"\"\"\n",
    "    headers_lower = [h.lower() for h in headers]\n",
    "    \n",
    "    # Try exact match first\n",
    "    for pattern in patterns:\n",
    "        for i, header_lower in enumerate(headers_lower):\n",
    "            if pattern == header_lower:\n",
    "                return headers[i]\n",
    "    \n",
    "    # Try substring match (only for patterns longer than 2 chars)\n",
    "    for pattern in patterns:\n",
    "        if len(pattern) > 2:  # Avoid false positives like 'cr' matching 'description'\n",
    "            for i, header_lower in enumerate(headers_lower):\n",
    "                if pattern in header_lower:\n",
    "                    return headers[i]\n",
    "    \n",
    "    return fallback\n",
    "\n",
    "# Perform pattern matching on extracted headers\n",
    "date_col = match_header(table_headers, DATE_PATTERNS, fallback=table_headers[0] if table_headers else 'Date')\n",
    "desc_col = match_header(table_headers, DESCRIPTION_PATTERNS, fallback=table_headers[1] if len(table_headers) > 1 else 'Description')\n",
    "\n",
    "# NEW: First try to match a generic \"Amount\" column (for 4-column formats)\n",
    "amount_col = match_header(table_headers, AMOUNT_PATTERNS, fallback=None)\n",
    "\n",
    "# Use amount_col as fallback if no separate debit/credit columns exist\n",
    "# This handles formats like: Date | Description | Amount | Balance\n",
    "debit_col = match_header(table_headers, DEBIT_PATTERNS, fallback=amount_col if amount_col else 'Debit')\n",
    "credit_col = match_header(table_headers, CREDIT_PATTERNS, fallback=amount_col if amount_col else 'Credit')\n",
    "balance_col = match_header(table_headers, BALANCE_PATTERNS, fallback='Balance')\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PATTERN MATCHING RESULTS:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"üìã Extracted Headers: {table_headers}\")\n",
    "print(f\"\\nüîç Mapped Columns:\")\n",
    "print(f\"  Date        ‚Üí '{date_col}'\")\n",
    "print(f\"  Description ‚Üí '{desc_col}'\")\n",
    "print(f\"  Debit       ‚Üí '{debit_col}'\")\n",
    "print(f\"  Credit      ‚Üí '{credit_col}'\")\n",
    "print(f\"  Balance     ‚Üí '{balance_col}'\")\n",
    "if amount_col:\n",
    "    print(f\"\\nüí° Single-column format detected: '{amount_col}' used for both debit and credit\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n‚úÖ These literal column names will be used in Turn 1 balance-description prompt\")\n",
    "print(\"üí° Adjust patterns above if matching fails for your bank statement format\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## Turn 1: Balance-Description Extraction Prompt\n",
    "\n",
    "**Key Insight**: Instead of extracting a markdown table, we ask the model to list balances with their associated transaction details. This approach:\n",
    "\n",
    "1. **Anchors to Balance column** - Every transaction has a balance\n",
    "2. **Works for all formats** - Date-per-row AND date-grouped formats\n",
    "3. **Hierarchical output** - Easy to parse programmatically\n",
    "4. **No date format classification** - Eliminates Turn 0.5 entirely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 16: Generate Balance-Description Extraction Prompt\n",
    "\n",
    "# Build the balance-description extraction prompt using LITERAL column names from Turn 0\n",
    "extraction_prompt = f\"\"\"List all the balances in the {balance_col} column, including:\n",
    "- Date from the Date Header of the balance\n",
    "- {desc_col}\n",
    "- {debit_col} Amount or \"NOT_FOUND\"\n",
    "- {credit_col} Amount or \"NOT_FOUND\"\n",
    "\n",
    "Format each balance entry like this:\n",
    "1. **[Date]**\n",
    "   - {desc_col}: [description text]\n",
    "   - {debit_col}: [amount or NOT_FOUND]\n",
    "   - {credit_col}: [amount or NOT_FOUND]\n",
    "   - {balance_col}: [balance amount]\n",
    "\n",
    "CRITICAL RULES:\n",
    "1. List EVERY balance entry in order from top to bottom\n",
    "2. For date-grouped statements: Use the date heading for all transactions under it\n",
    "3. Include the FULL description text, not abbreviated\n",
    "4. If amount is in {debit_col} column, put it there and use NOT_FOUND for {credit_col}\n",
    "5. If amount is in {credit_col} column, put it there and use NOT_FOUND for {debit_col}\n",
    "6. Do NOT skip any transactions\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Turn 1 Balance-Description Extraction Prompt:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n{extraction_prompt}\")\n",
    "print(\"=\" * 60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 17: Turn 1 - Execute Balance-Description Extraction (InternVL3.5-8B, INDEPENDENT fresh context)\n",
    "\n",
    "print(\"ü§ñ Generating response with InternVL3.5-8B (bfloat16)...\")\n",
    "\n",
    "# CRITICAL: Reload image for fresh context (independent turn, not continuing conversation)\n",
    "# Uses CONFIG['MAX_TILES'] automatically\n",
    "pixel_values = load_image(imageName, input_size=448)\n",
    "\n",
    "# Move to correct device and dtype\n",
    "# CRITICAL: Use bfloat16 for non-quantized H200 models\n",
    "pixel_values = pixel_values.to(dtype=torch.bfloat16, device='cuda:0')\n",
    "\n",
    "# Generate response using chat() method with balance-description prompt\n",
    "cleanedOutput2 = model.chat(\n",
    "    tokenizer=tokenizer,\n",
    "    pixel_values=pixel_values,\n",
    "    question=extraction_prompt,\n",
    "    generation_config={\n",
    "        \"max_new_tokens\": CONFIG['MAX_NEW_TOKENS'],\n",
    "        \"do_sample\": False\n",
    "    }\n",
    ")\n",
    "\n",
    "# Clean InternVL3 artifacts:\n",
    "# 1. Remove image markdown placeholder (![...])\n",
    "# 2. Remove markdown code fences (```markdown and ```)\n",
    "lines = cleanedOutput2.split(\"\\n\")\n",
    "cleaned_lines = []\n",
    "for line in lines:\n",
    "    stripped = line.strip()\n",
    "    # Skip image markdown, code fences, and empty fence markers\n",
    "    if stripped.startswith(\"![\"):\n",
    "        continue\n",
    "    if stripped in [\"```markdown\", \"```\", \"```md\"]:\n",
    "        continue\n",
    "    cleaned_lines.append(line)\n",
    "\n",
    "cleanedOutput2 = \"\\n\".join(cleaned_lines)\n",
    "\n",
    "print(\"\\n‚úÖ Turn 1 balance-description extraction complete!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TURN 1 - BALANCE-DESCRIPTION OUTPUT:\")\n",
    "print(\"=\" * 60)\n",
    "print(cleanedOutput2)\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Save the balance-description output\n",
    "output_path = Path(\"ivl3_5_8b_2turn_balance_description.txt\")\n",
    "with output_path.open(\"w\", encoding=\"utf-8\") as text_file:\n",
    "    text_file.write(cleanedOutput2)\n",
    "\n",
    "print(f\"\\n‚úÖ Balance-description output saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## Python Parsing: Parse Balance-Description Output\n",
    "\n",
    "Parse the hierarchical balance-description output into structured rows, filter for debit transactions, and extract schema fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 19: Parse Balance-Description Output and Extract Schema Fields\n",
    "\n",
    "def parse_balance_description_response(response_text, date_col, desc_col, debit_col, credit_col, balance_col):\n",
    "    \"\"\"Parse hierarchical balance-description response into list of transaction dictionaries.\n",
    "    \n",
    "    Expected format:\n",
    "    1. **[Date]**\n",
    "       - Description: [text]\n",
    "       - Debit: [amount or NOT_FOUND]\n",
    "       - Credit: [amount or NOT_FOUND]\n",
    "       - Balance: [amount]\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    current_date = None\n",
    "    current_transaction = {}\n",
    "    \n",
    "    lines = response_text.strip().split(\"\\n\")\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        \n",
    "        # Check for date header patterns:\n",
    "        # Pattern 1: \"1. **15 Mar 2024**\" or \"1. **Mon 15 Mar 2024**\"\n",
    "        date_match = re.match(r\"^\\d+\\.\\s*\\*?\\*?([A-Za-z]{3}\\s+\\d{1,2}\\s+[A-Za-z]{3}\\s+\\d{4})\\*?\\*?\", line)\n",
    "        if not date_match:\n",
    "            # Pattern 2: \"1. **15/03/2024**\" or similar\n",
    "            date_match = re.match(r\"^\\d+\\.\\s*\\*?\\*?(\\d{1,2}[/\\-]\\d{1,2}[/\\-]\\d{2,4})\\*?\\*?\", line)\n",
    "        if not date_match:\n",
    "            # Pattern 3: \"1. **March 15, 2024**\"\n",
    "            date_match = re.match(r\"^\\d+\\.\\s*\\*?\\*?([A-Za-z]+\\s+\\d{1,2},?\\s+\\d{4})\\*?\\*?\", line)\n",
    "        if not date_match:\n",
    "            # Pattern 4: \"**15 Mar 2024**\" (without number prefix)\n",
    "            date_match = re.match(r\"^\\*\\*([A-Za-z]{3}\\s+\\d{1,2}\\s+[A-Za-z]{3}\\s+\\d{4})\\*\\*\", line)\n",
    "        if not date_match:\n",
    "            # Pattern 5: Just the date with number prefix \"1. 15 Mar 2024\"\n",
    "            date_match = re.match(r\"^\\d+\\.\\s+([A-Za-z]{3}\\s+\\d{1,2}\\s+[A-Za-z]{3}\\s+\\d{4})\", line)\n",
    "        \n",
    "        if date_match:\n",
    "            # Save previous transaction if exists\n",
    "            if current_transaction and current_date:\n",
    "                current_transaction[date_col] = current_date\n",
    "                rows.append(current_transaction)\n",
    "            \n",
    "            # Start new transaction with this date\n",
    "            current_date = date_match.group(1).strip('*').strip()\n",
    "            current_transaction = {}\n",
    "            continue\n",
    "        \n",
    "        # Check for field lines: \"- Description: ...\" or \"- Transaction: ...\"\n",
    "        field_match = re.match(r\"^\\s*-\\s*([\\w\\s]+):\\s*(.+)$\", line)\n",
    "        if field_match:\n",
    "            field_name = field_match.group(1).strip()\n",
    "            field_value = field_match.group(2).strip()\n",
    "            \n",
    "            # Map field names to column names (case-insensitive)\n",
    "            field_lower = field_name.lower()\n",
    "            \n",
    "            if field_lower in ['description', 'transaction', 'details', 'particulars', desc_col.lower()]:\n",
    "                current_transaction[desc_col] = field_value\n",
    "            elif field_lower in ['debit', 'withdrawal', 'dr', debit_col.lower()]:\n",
    "                # Handle NOT_FOUND\n",
    "                if field_value.upper() == 'NOT_FOUND' or field_value == '-':\n",
    "                    current_transaction[debit_col] = ''\n",
    "                else:\n",
    "                    current_transaction[debit_col] = field_value\n",
    "            elif field_lower in ['credit', 'deposit', 'cr', credit_col.lower()]:\n",
    "                # Handle NOT_FOUND\n",
    "                if field_value.upper() == 'NOT_FOUND' or field_value == '-':\n",
    "                    current_transaction[credit_col] = ''\n",
    "                else:\n",
    "                    current_transaction[credit_col] = field_value\n",
    "            elif field_lower in ['balance', 'bal', balance_col.lower()]:\n",
    "                current_transaction[balance_col] = field_value\n",
    "    \n",
    "    # Don't forget the last transaction\n",
    "    if current_transaction and current_date:\n",
    "        current_transaction[date_col] = current_date\n",
    "        rows.append(current_transaction)\n",
    "    \n",
    "    return rows\n",
    "\n",
    "\n",
    "def parse_markdown_table(markdown_text):\n",
    "    \"\"\"Fallback: Parse markdown table into list of dictionaries.\n",
    "    \n",
    "    CRITICAL: Must preserve empty columns for correct Debit/Credit alignment!\n",
    "    \"\"\"\n",
    "    lines = [line.strip() for line in markdown_text.strip().split('\\n') if line.strip()]\n",
    "    \n",
    "    # Find header row (first line with pipes)\n",
    "    header_idx = None\n",
    "    for i, line in enumerate(lines):\n",
    "        if '|' in line:\n",
    "            # Skip separator rows (contain only pipes, hyphens, and spaces)\n",
    "            cleaned = line.replace('|', '').replace('-', '').replace(' ', '')\n",
    "            if cleaned:  # Has actual content, not just separators\n",
    "                header_idx = i\n",
    "                break\n",
    "    \n",
    "    if header_idx is None:\n",
    "        return []\n",
    "    \n",
    "    # Parse headers - KEEP empty values to preserve column positions\n",
    "    header_line = lines[header_idx]\n",
    "    header_parts = [h.strip() for h in header_line.split('|')]\n",
    "    # Remove leading/trailing empty strings from pipe delimiters\n",
    "    if header_parts and header_parts[0] == '':\n",
    "        header_parts = header_parts[1:]\n",
    "    if header_parts and header_parts[-1] == '':\n",
    "        header_parts = header_parts[:-1]\n",
    "    # Filter out any remaining empty headers\n",
    "    headers = [h for h in header_parts if h]\n",
    "    \n",
    "    # Parse data rows (skip header and separator)\n",
    "    rows = []\n",
    "    for idx, line in enumerate(lines[header_idx + 1:], start=header_idx+1):\n",
    "        if '|' not in line:\n",
    "            continue\n",
    "            \n",
    "        # Skip separator rows\n",
    "        cleaned = line.replace(\"|\", \"\").replace(\"-\", \"\").replace(\" \", \"\").replace(\":\", \"\")\n",
    "        if not cleaned:\n",
    "            continue\n",
    "        \n",
    "        # Parse values - KEEP empty values to preserve column positions!\n",
    "        value_parts = [v.strip() for v in line.split('|')]\n",
    "        # Remove leading/trailing empty strings from pipe delimiters\n",
    "        if value_parts and value_parts[0] == '':\n",
    "            value_parts = value_parts[1:]\n",
    "        if value_parts and value_parts[-1] == '':\n",
    "            value_parts = value_parts[:-1]\n",
    "        \n",
    "        # Match to headers length\n",
    "        if len(value_parts) == len(headers):\n",
    "            rows.append(dict(zip(headers, value_parts)))\n",
    "    \n",
    "    return rows\n",
    "\n",
    "\n",
    "def filter_debit_transactions(rows, debit_col):\n",
    "    \"\"\"Filter rows to only those with debit (purchase) amounts > 0.\n",
    "    \n",
    "    CRITICAL: For tax purposes, we only want transactions where taxpayer PAID money (debits).\n",
    "    \"\"\"\n",
    "    debit_rows = []\n",
    "    for row in rows:\n",
    "        debit_value = row.get(debit_col, '').strip()\n",
    "        # Include row if debit column has a numeric value (not empty, not NOT_FOUND)\n",
    "        if debit_value and debit_value.upper() != 'NOT_FOUND':\n",
    "            # Try to parse as number to confirm it's a valid amount\n",
    "            try:\n",
    "                amount = float(debit_value.replace(',', '').replace('$', '').strip())\n",
    "                if amount > 0:\n",
    "                    debit_rows.append(row)\n",
    "            except ValueError:\n",
    "                # If can't parse as number, skip this row\n",
    "                pass\n",
    "    \n",
    "    return debit_rows\n",
    "\n",
    "\n",
    "def extract_schema_fields(rows, date_col, desc_col, debit_col):\n",
    "    \"\"\"Extract fields in universal.yaml schema format.\"\"\"\n",
    "    if not rows:\n",
    "        return {\n",
    "            'TRANSACTION_DATES': 'NOT_FOUND',\n",
    "            'LINE_ITEM_DESCRIPTIONS': 'NOT_FOUND',\n",
    "            'TRANSACTION_AMOUNTS_PAID': 'NOT_FOUND',\n",
    "            'STATEMENT_DATE_RANGE': 'NOT_FOUND'\n",
    "        }\n",
    "    \n",
    "    # Extract lists\n",
    "    dates = []\n",
    "    descriptions = []\n",
    "    amounts = []\n",
    "    \n",
    "    for row in rows:\n",
    "        date = row.get(date_col, '').strip()\n",
    "        desc = row.get(desc_col, '').strip()\n",
    "        amount = row.get(debit_col, '').strip()\n",
    "        \n",
    "        if date:\n",
    "            dates.append(date)\n",
    "        if desc:\n",
    "            descriptions.append(desc)\n",
    "        if amount:\n",
    "            amounts.append(amount)\n",
    "    \n",
    "    # Calculate statement date range - use literal date format from image\n",
    "    # No parsing, no year assumption - just \"earliest date - latest date\"\n",
    "    date_range = 'NOT_FOUND'\n",
    "    if dates:\n",
    "        # Use first and last date as-is (same format as in the image)\n",
    "        date_range = f\"{dates[0]} - {dates[-1]}\"\n",
    "    \n",
    "    return {\n",
    "        'TRANSACTION_DATES': ' | '.join(dates) if dates else 'NOT_FOUND',\n",
    "        'LINE_ITEM_DESCRIPTIONS': ' | '.join(descriptions) if descriptions else 'NOT_FOUND',\n",
    "        'TRANSACTION_AMOUNTS_PAID': ' | '.join(amounts) if amounts else 'NOT_FOUND',\n",
    "        'STATEMENT_DATE_RANGE': date_range\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PARSING TURN 1 BALANCE-DESCRIPTION OUTPUT:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Try balance-description parsing first\n",
    "all_rows = parse_balance_description_response(cleanedOutput2, date_col, desc_col, debit_col, credit_col, balance_col)\n",
    "\n",
    "# Fallback to markdown table parsing if balance-description parsing fails\n",
    "if not all_rows:\n",
    "    print(\"‚ö†Ô∏è  Balance-description parsing returned 0 rows, trying markdown table fallback...\")\n",
    "    all_rows = parse_markdown_table(cleanedOutput2)\n",
    "\n",
    "print(f\"\\nüìä Parsed {len(all_rows)} total transactions\")\n",
    "\n",
    "if all_rows:\n",
    "    # Show sample parsed row\n",
    "    print(f\"\\nüîç Sample parsed row:\")\n",
    "    for key, value in all_rows[0].items():\n",
    "        print(f\"  {key}: '{value}'\")\n",
    "\n",
    "# Filter to only debit (purchase) transactions - Python filtering, not LLM!\n",
    "debit_rows = filter_debit_transactions(all_rows, debit_col)\n",
    "\n",
    "print(f\"\\nüí∞ Filtered to {len(debit_rows)} debit transactions (taxpayer purchases)\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DEBIT TRANSACTIONS (WHAT TAXPAYER PAID):\")\n",
    "print(\"=\" * 60)\n",
    "for i, row in enumerate(debit_rows, 1):\n",
    "    print(f\"\\nTransaction {i}:\")\n",
    "    print(f\"  {date_col}: {row.get(date_col, '')}\")\n",
    "    print(f\"  {desc_col}: {row.get(desc_col, '')}\")\n",
    "    print(f\"  {debit_col}: {row.get(debit_col, '')}\")\n",
    "\n",
    "# Extract schema fields using the LITERAL column names from pattern matching\n",
    "schema_fields = extract_schema_fields(debit_rows, date_col, desc_col, debit_col)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EXTRACTED SCHEMA FIELDS (TAX-RELEVANT DATA):\")\n",
    "print(\"=\" * 60)\n",
    "for field, value in schema_fields.items():\n",
    "    print(f\"{field}: {value}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Save to file\n",
    "output_path = Path(\"ivl3_5_8b_2turn_extracted_fields.txt\")\n",
    "with output_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    for field, value in schema_fields.items():\n",
    "        f.write(f\"{field}: {value}\\n\")\n",
    "\n",
    "print(f\"\\n‚úÖ Schema fields saved to: {output_path}\")\n",
    "print(f\"üí° Fields extracted from columns: '{date_col}' | '{desc_col}' | '{debit_col}'\")\n",
    "print(f\"üéØ Success: 2-Turn Balance-Description approach with Python parsing + filtering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## Summary: 2-Turn Balance-Description Approach\n",
    "\n",
    "### What We Achieved:\n",
    "1. **Turn 0**: Identified column headers from the bank statement image\n",
    "2. **Turn 1**: Extracted transactions using balance-description prompt (format-agnostic)\n",
    "3. **Python**: Parsed hierarchical output, filtered debits, extracted schema fields\n",
    "\n",
    "### Why This Works:\n",
    "- **Balance anchoring**: Every transaction has a balance - use it as the anchor point\n",
    "- **No date format classification**: Works for BOTH date-per-row AND date-grouped formats\n",
    "- **Hierarchical output**: Clear structure for reliable parsing\n",
    "- **Python filtering**: Accurate debit/credit separation for tax purposes\n",
    "\n",
    "### Comparison to 3-Turn Table Approach:\n",
    "| Aspect | 3-Turn Table | 2-Turn Balance-Description |\n",
    "|--------|--------------|---------------------------|\n",
    "| Turns needed | 3 (headers, date format, table) | 2 (headers, extraction) |\n",
    "| Date format handling | Turn 0.5 classification | Built-in to prompt |\n",
    "| Output format | Markdown table | Hierarchical list |\n",
    "| Complexity | Higher | Lower |\n",
    "| Format flexibility | Needs format-specific prompts | Works for all formats |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
