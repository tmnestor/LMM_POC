{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c12cb8ec",
   "metadata": {},
   "source": [
    "# InternVL3-2B Pipeline-Oriented Batch Processing\n",
    "\n",
    "**Transparent pandas-based pipeline for processing images**\n",
    "\n",
    "Clear separation of extraction ‚Üí parsing ‚Üí cleaning ‚Üí evaluation stages:\n",
    "1. **Stage 0**: Classify document type (INVOICE/RECEIPT/BANK_STATEMENT)\n",
    "2. **Stage 1**: Classify structure (if BANK_STATEMENT: FLAT/GROUPED)\n",
    "3. **Stage 2**: Extract field values using document-type-aware prompts\n",
    "4. **Stage 3**: Parse responses (text ‚Üí structured fields) using `hybrid_parse_response`\n",
    "5. **Stage 4**: Clean and normalize field values using `ExtractionCleaner`\n",
    "6. **Stage 5**: Evaluate against ground truth (optional)\n",
    "\n",
    "**Key Features:**\n",
    "- Inspectable at every stage\n",
    "- Checkpointing support\n",
    "- Scalable to 10,000+ images\n",
    "- Compatible with model_comparison.ipynb\n",
    "- Uses official InternVL3 multi-turn pattern (`return_history=True`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d0d5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 1\n",
    "import gc\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import yaml\n",
    "from PIL import Image\n",
    "from rich import print as rprint\n",
    "from rich.console import Console\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# Pipeline library imports - NEW lightweight library (replacing common/)\n",
    "from pipeline_lib import (\n",
    "    ExtractionCleaner,\n",
    "    calculate_field_accuracy,\n",
    "    hybrid_parse_response,\n",
    "    load_ground_truth,\n",
    "    show_pipeline_memory,\n",
    "    stage_3_parsing,\n",
    "    stage_4_cleaning,\n",
    "    stage_5_evaluation,\n",
    ")\n",
    "\n",
    "# Initialize console for rich output\n",
    "console = Console()\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "rprint(\"[green]‚úÖ Imports loaded (using pipeline_lib)[/green]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b69b25",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d126c74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 2\n",
    "# Environment-specific base paths\n",
    "ENVIRONMENT_BASES = {\n",
    "    'sandbox': '/home/jovyan/nfs_share/tod',\n",
    "    'efs': '/efs/shared/PoC_data'\n",
    "}\n",
    "base_data_path = ENVIRONMENT_BASES['sandbox']\n",
    "\n",
    "CONFIG = {\n",
    "    # Model settings\n",
    "    'MODEL_PATH': '/home/jovyan/nfs_share/models/InternVL3-2B',\n",
    "\n",
    "    # Data paths - Using base path for consistency\n",
    "    'DATA_DIR': f'{base_data_path}/LMM_POC/evaluation_data',\n",
    "    'GROUND_TRUTH': f'{base_data_path}/LMM_POC/evaluation_data/ground_truth.csv',\n",
    "\n",
    "    # Prompt files - Using base path for consistency\n",
    "    'PROMPT_FILE_DOCTYPE': f'{base_data_path}/LMM_POC/prompts/document_type_detection.yaml',\n",
    "    'PROMPT_FILE_INTERNVL3': f'{base_data_path}/LMM_POC/prompts/internvl3_prompts.yaml',\n",
    "\n",
    "    # Output directory - Using base path for consistency\n",
    "    'OUTPUT_DIR': f'{base_data_path}/LMM_POC/output',\n",
    "\n",
    "    # Token limits\n",
    "    'MAX_NEW_TOKENS_DOCTYPE': 50,\n",
    "    'MAX_NEW_TOKENS_STRUCTURE': 50,\n",
    "    'MAX_NEW_TOKENS_EXTRACT': 2000,\n",
    "    \n",
    "    # Pipeline control\n",
    "    'INFERENCE_ONLY': False,  # Set to True to skip ground truth evaluation\n",
    "    \n",
    "    # Verbosity control\n",
    "    'VERBOSE': True,  # Show stage-by-stage progress\n",
    "    'SHOW_PROMPTS': False,  # Show actual prompts being used\n",
    "}\n",
    "\n",
    "# Make GROUND_TRUTH conditional based on INFERENCE_ONLY mode\n",
    "if CONFIG['INFERENCE_ONLY']:\n",
    "    CONFIG['GROUND_TRUTH'] = None\n",
    "\n",
    "# Define expected fields (matching ground truth)\n",
    "FIELD_COLUMNS = [\n",
    "    'DOCUMENT_TYPE', 'BUSINESS_ABN', 'SUPPLIER_NAME', 'BUSINESS_ADDRESS',\n",
    "    'PAYER_NAME', 'PAYER_ADDRESS', 'INVOICE_DATE', 'LINE_ITEM_DESCRIPTIONS',\n",
    "    'LINE_ITEM_QUANTITIES', 'LINE_ITEM_PRICES', 'LINE_ITEM_TOTAL_PRICES',\n",
    "    'IS_GST_INCLUDED', 'GST_AMOUNT', 'TOTAL_AMOUNT', 'STATEMENT_DATE_RANGE',\n",
    "    'TRANSACTION_DATES', 'TRANSACTION_AMOUNTS_PAID'\n",
    "]\n",
    "\n",
    "# Create output directory\n",
    "output_dir = Path(CONFIG['OUTPUT_DIR'])\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Create checkpoint directory\n",
    "checkpoint_dir = output_dir / 'checkpoints'\n",
    "checkpoint_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Timestamp for output files\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Initialize extraction cleaner\n",
    "cleaner = ExtractionCleaner(debug=CONFIG['VERBOSE'])\n",
    "\n",
    "rprint(\"[green]‚úÖ Configuration loaded[/green]\")\n",
    "rprint(f\"[cyan]  Environment: {[k for k, v in ENVIRONMENT_BASES.items() if v == base_data_path][0]}[/cyan]\")\n",
    "rprint(f\"[cyan]  Base path: {base_data_path}[/cyan]\")\n",
    "rprint(f\"[cyan]  Output directory: {output_dir}[/cyan]\")\n",
    "rprint(f\"[cyan]  Checkpoint directory: {checkpoint_dir}[/cyan]\")\n",
    "rprint(f\"[cyan]  Timestamp: {TIMESTAMP}[/cyan]\")\n",
    "rprint(f\"[cyan]  Mode: {'Inference-only' if CONFIG['INFERENCE_ONLY'] else 'Evaluation mode'}[/cyan]\")\n",
    "rprint(f\"[cyan]  Expected fields: {len(FIELD_COLUMNS)}[/cyan]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd950364",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2799393",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 3\n",
    "# Load InternVL3 model with diagnostics\n",
    "from pipeline_lib.model_diagnostics import show_model_diagnostics\n",
    "\n",
    "rprint(\"[bold green]üîß Loading InternVL3 model...[/bold green]\")\n",
    "\n",
    "model = AutoModel.from_pretrained(\n",
    "    CONFIG['MODEL_PATH'],\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    use_flash_attn=False,  # V100 compatible\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\"\n",
    ").eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    CONFIG['MODEL_PATH'],\n",
    "    trust_remote_code=True,\n",
    "    use_fast=False\n",
    ")\n",
    "\n",
    "# Fix pad_token_id\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Show diagnostics (only if VERBOSE is True)\n",
    "show_model_diagnostics(\n",
    "    model, \n",
    "    tokenizer,  # InternVL3 uses tokenizer instead of processor\n",
    "    CONFIG['MODEL_PATH'], \n",
    "    CONFIG['MAX_NEW_TOKENS_EXTRACT'],\n",
    "    verbose=CONFIG['VERBOSE']\n",
    ")\n",
    "\n",
    "rprint(\"[green]‚úÖ Model and tokenizer loaded[/green]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382eb2b8",
   "metadata": {},
   "source": [
    "## Image Preprocessing\n",
    "\n",
    "InternVL3-specific image loading with dynamic tiling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5be3978",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 4\n",
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "def build_transform(input_size):\n",
    "    \"\"\"Build InternVL3 image transform.\"\"\"\n",
    "    return T.Compose([\n",
    "        T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
    "        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "    ])\n",
    "\n",
    "def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):\n",
    "    \"\"\"Find closest aspect ratio for tiling.\"\"\"\n",
    "    best_ratio_diff = float('inf')\n",
    "    best_ratio = (1, 1)\n",
    "    area = width * height\n",
    "    for ratio in target_ratios:\n",
    "        target_aspect_ratio = ratio[0] / ratio[1]\n",
    "        ratio_diff = abs(aspect_ratio - target_aspect_ratio)\n",
    "        if ratio_diff < best_ratio_diff:\n",
    "            best_ratio_diff = ratio_diff\n",
    "            best_ratio = ratio\n",
    "        elif ratio_diff == best_ratio_diff:\n",
    "            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\n",
    "                best_ratio = ratio\n",
    "    return best_ratio\n",
    "\n",
    "def dynamic_preprocess(image, min_num=1, max_num=12, image_size=448, use_thumbnail=False):\n",
    "    \"\"\"InternVL3 dynamic preprocessing into tiles.\"\"\"\n",
    "    orig_width, orig_height = image.size\n",
    "    aspect_ratio = orig_width / orig_height\n",
    "\n",
    "    # Calculate target ratios\n",
    "    target_ratios = set(\n",
    "        (i, j) for n in range(min_num, max_num + 1)\n",
    "        for i in range(1, n + 1) for j in range(1, n + 1)\n",
    "        if i * j <= max_num and i * j >= min_num\n",
    "    )\n",
    "    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n",
    "\n",
    "    # Find best fit\n",
    "    target_aspect_ratio = find_closest_aspect_ratio(\n",
    "        aspect_ratio, target_ratios, orig_width, orig_height, image_size\n",
    "    )\n",
    "\n",
    "    # Calculate dimensions\n",
    "    target_width = image_size * target_aspect_ratio[0]\n",
    "    target_height = image_size * target_aspect_ratio[1]\n",
    "    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]\n",
    "\n",
    "    # Resize image\n",
    "    resized_img = image.resize((target_width, target_height))\n",
    "    processed_images = []\n",
    "\n",
    "    for i in range(blocks):\n",
    "        box = (\n",
    "            (i % (target_width // image_size)) * image_size,\n",
    "            (i // (target_width // image_size)) * image_size,\n",
    "            ((i % (target_width // image_size)) + 1) * image_size,\n",
    "            ((i // (target_width // image_size)) + 1) * image_size\n",
    "        )\n",
    "        split_img = resized_img.crop(box)\n",
    "        processed_images.append(split_img)\n",
    "\n",
    "    if use_thumbnail and len(processed_images) != 1:\n",
    "        thumbnail_img = image.resize((image_size, image_size))\n",
    "        processed_images.append(thumbnail_img)\n",
    "\n",
    "    return processed_images\n",
    "\n",
    "def load_image(image_path, input_size=448, max_num=24, debug=None):\n",
    "    \"\"\"Complete InternVL3 image loading pipeline - matches working processor.\"\"\"\n",
    "    # Use CONFIG VERBOSE if debug not specified\n",
    "    if debug is None:\n",
    "        debug = CONFIG.get('VERBOSE', False)\n",
    "    \n",
    "    if debug:\n",
    "        rprint(f\"[blue]üîç LOAD_IMAGE: max_num={max_num}, input_size={input_size}[/blue]\")\n",
    "    \n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    images = dynamic_preprocess(image, min_num=1, max_num=max_num,\n",
    "                                image_size=input_size, use_thumbnail=True)\n",
    "    transform = build_transform(input_size=input_size)\n",
    "    pixel_values = [transform(img) for img in images]\n",
    "    pixel_values = torch.stack(pixel_values)\n",
    "\n",
    "    # CRITICAL: Robust dtype detection (copied from working processor)\n",
    "    try:\n",
    "        # Try vision model's embedding layer dtype (most reliable)\n",
    "        if hasattr(model, 'vision_model') and hasattr(model.vision_model, 'embeddings'):\n",
    "            vision_dtype = next(model.vision_model.embeddings.parameters()).dtype\n",
    "            pixel_values = pixel_values.to(dtype=vision_dtype)\n",
    "            if debug:\n",
    "                rprint(f\"[blue]üîß TENSOR_DTYPE: Using vision model dtype {vision_dtype}[/blue]\")\n",
    "        elif hasattr(model, 'dtype'):\n",
    "            pixel_values = pixel_values.to(dtype=model.dtype)\n",
    "            if debug:\n",
    "                rprint(f\"[blue]üîß TENSOR_DTYPE: Using model.dtype {model.dtype}[/blue]\")\n",
    "        else:\n",
    "            # Fall back to parameter dtype\n",
    "            model_dtype = next(model.parameters()).dtype\n",
    "            pixel_values = pixel_values.to(dtype=model_dtype)\n",
    "            if debug:\n",
    "                rprint(f\"[blue]üîß TENSOR_DTYPE: Using parameter dtype {model_dtype}[/blue]\")\n",
    "    except Exception:\n",
    "        # Safe fallback: use bfloat16 for 2B model\n",
    "        pixel_values = pixel_values.to(dtype=torch.bfloat16)\n",
    "        if debug:\n",
    "            rprint(\"[blue]üîß TENSOR_DTYPE: Using bfloat16 (fallback)[/blue]\")\n",
    "    \n",
    "    if debug:\n",
    "        rprint(f\"[blue]üìê TENSOR_SHAPE: {pixel_values.shape} (batch_size={pixel_values.shape[0]} tiles)[/blue]\")\n",
    "        rprint(f\"[blue]üìä TENSOR_DTYPE: {pixel_values.dtype}[/blue]\")\n",
    "\n",
    "    return pixel_values\n",
    "\n",
    "rprint(\"[green]‚úÖ Image preprocessing functions defined[/green]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8feca160",
   "metadata": {},
   "source": [
    "## Load All Prompts\n",
    "\n",
    "Loading prompts for:\n",
    "- Document type detection\n",
    "- Invoice extraction\n",
    "- Receipt extraction\n",
    "- Bank statement extraction (flat and grouped variants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7750b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 5\n",
    "# Load all prompts from YAML files\n",
    "\n",
    "# Document type detection prompt\n",
    "with open(CONFIG['PROMPT_FILE_DOCTYPE'], 'r') as f:\n",
    "    doctype_data = yaml.safe_load(f)\n",
    "    DOCTYPE_PROMPT = doctype_data['prompts']['detection']['prompt']\n",
    "\n",
    "# Load all InternVL3 prompts from single file\n",
    "with open(CONFIG['PROMPT_FILE_INTERNVL3'], 'r') as f:\n",
    "    internvl3_data = yaml.safe_load(f)\n",
    "    INVOICE_PROMPT = internvl3_data['prompts']['invoice']['prompt']\n",
    "    RECEIPT_PROMPT = internvl3_data['prompts']['receipt']['prompt']\n",
    "    BANK_PROMPTS = {\n",
    "        'flat': internvl3_data['prompts']['bank_statement_flat']['prompt'],\n",
    "        'date_grouped': internvl3_data['prompts']['bank_statement_date_grouped']['prompt']\n",
    "    }\n",
    "\n",
    "# Bank statement structure classification prompt\n",
    "STRUCTURE_CLASSIFICATION_PROMPT = \"\"\"Look at how dates are displayed in this bank statement's transaction list.\n",
    "\n",
    "Answer with ONLY one word:\n",
    "- FLAT (if dates appear as the FIRST COLUMN in a table row, like: \"05/05/2025 | Purchase | $22.50\")\n",
    "- GROUPED (if dates appear as SECTION HEADERS above transactions, like: \"Thu 05 Sep 2025\" followed by indented transaction details below)\n",
    "\n",
    "The key difference: FLAT has dates IN the table columns, GROUPED has dates AS headers ABOVE the rows.\n",
    "\n",
    "Answer (one word only):\"\"\"\n",
    "\n",
    "rprint(\"[green]‚úÖ All prompts loaded[/green]\")\n",
    "rprint(f\"[cyan]  Document type detection: {len(DOCTYPE_PROMPT)} chars[/cyan]\")\n",
    "rprint(f\"[cyan]  Invoice extraction: {len(INVOICE_PROMPT)} chars[/cyan]\")\n",
    "rprint(f\"[cyan]  Receipt extraction: {len(RECEIPT_PROMPT)} chars[/cyan]\")\n",
    "rprint(f\"[cyan]  Bank flat extraction: {len(BANK_PROMPTS['flat'])} chars[/cyan]\")\n",
    "rprint(f\"[cyan]  Bank grouped extraction: {len(BANK_PROMPTS['date_grouped'])} chars[/cyan]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6a78eb",
   "metadata": {},
   "source": [
    "## Multi-Turn Chat Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1821f915",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 6\n",
    "def chat_with_internvl(model, tokenizer, prompt, pixel_values, history=None,\n",
    "                       max_new_tokens=2000, do_sample=False, debug=None):\n",
    "    \"\"\"\n",
    "    Multi-turn chat with InternVL3 using conversation history.\n",
    "\n",
    "    OFFICIAL PATTERN: Uses SAME pixel_values with return_history=True.\n",
    "    Based on: https://internvl.readthedocs.io/en/latest/internvl3.0/quick_start.html\n",
    "\n",
    "    Args:\n",
    "        model: InternVL3 model\n",
    "        tokenizer: InternVL3 tokenizer\n",
    "        prompt: Text prompt for this turn\n",
    "        pixel_values: Preprocessed image tensor (REUSED across turns)\n",
    "        history: Conversation history from previous turn or None\n",
    "        max_new_tokens: Maximum tokens to generate\n",
    "        do_sample: Whether to use sampling\n",
    "        debug: Show debug output (uses CONFIG['VERBOSE'] if None)\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (response, updated_history)\n",
    "    \"\"\"\n",
    "    # Use CONFIG VERBOSE if debug not specified\n",
    "    if debug is None:\n",
    "        debug = CONFIG.get('VERBOSE', False)\n",
    "    \n",
    "    if debug:\n",
    "        rprint(f\"[magenta]üí≠ Generating with max_new_tokens={max_new_tokens}[/magenta]\")\n",
    "        if CONFIG.get('SHOW_PROMPTS', False):\n",
    "            rprint(f\"[yellow]üìù Prompt ({len(prompt)} chars):[/yellow]\")\n",
    "            rprint(\"[dim]\" + \"=\"*80 + \"[/dim]\")\n",
    "            # Show first 500 chars of prompt\n",
    "            preview = prompt[:500] + (\"...\" if len(prompt) > 500 else \"\")\n",
    "            rprint(f\"[dim]{preview}[/dim]\")\n",
    "            rprint(\"[dim]\" + \"=\"*80 + \"[/dim]\")\n",
    "    \n",
    "    # Build generation config\n",
    "    generation_config = {\n",
    "        \"max_new_tokens\": max_new_tokens,\n",
    "        \"temperature\": None if not do_sample else 0.6,\n",
    "        \"do_sample\": do_sample,\n",
    "        \"top_p\": 0.9 if do_sample else None,\n",
    "        \"pad_token_id\": tokenizer.eos_token_id,\n",
    "    }\n",
    "\n",
    "    # OFFICIAL PATTERN: Use model.chat() with return_history=True\n",
    "    response, history = model.chat(\n",
    "        tokenizer,\n",
    "        pixel_values,\n",
    "        prompt,\n",
    "        generation_config=generation_config,\n",
    "        history=history,  # None for first turn, then pass returned history\n",
    "        return_history=True  # CRITICAL: Must be True for multi-turn\n",
    "    )\n",
    "    \n",
    "    if debug:\n",
    "        rprint(f\"[magenta]üìÑ Model response ({len(response)} chars):[/magenta]\")\n",
    "        if CONFIG.get('SHOW_PROMPTS', False):\n",
    "            rprint(\"[dim]\" + \"=\"*80 + \"[/dim]\")\n",
    "            # Show first 500 chars of response\n",
    "            preview = response[:500] + (\"...\" if len(response) > 500 else \"\")\n",
    "            rprint(f\"[dim]{preview}[/dim]\")\n",
    "            rprint(\"[dim]\" + \"=\"*80 + \"[/dim]\")\n",
    "\n",
    "    return response, history\n",
    "\n",
    "rprint(\"[green]‚úÖ Multi-turn chat function defined[/green]\")\n",
    "rprint(\"[cyan]üí° Using official InternVL3 multi-turn pattern (return_history=True)[/cyan]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93ff557",
   "metadata": {},
   "source": [
    "## Parser Functions\n",
    "\n",
    "Functions to parse VLM responses:\n",
    "- Document type classification\n",
    "- Bank statement structure classification\n",
    "- Field extraction parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f0e48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 7\n",
    "def parse_document_type(response):\n",
    "    \"\"\"Parse document type from VLM response.\"\"\"\n",
    "    response = response.strip().upper()\n",
    "    if \"INVOICE\" in response:\n",
    "        return \"INVOICE\"\n",
    "    elif \"RECEIPT\" in response:\n",
    "        return \"RECEIPT\"\n",
    "    elif \"BANK\" in response or \"STATEMENT\" in response:\n",
    "        return \"BANK_STATEMENT\"\n",
    "    else:\n",
    "        return \"INVOICE\"  # Default fallback\n",
    "\n",
    "def parse_structure_type(response):\n",
    "    \"\"\"Parse bank statement structure type from VLM response.\"\"\"\n",
    "    response = response.strip().upper()\n",
    "    if \"FLAT\" in response:\n",
    "        return \"flat\"\n",
    "    elif \"GROUPED\" in response or \"DATE\" in response:\n",
    "        return \"date_grouped\"\n",
    "    else:\n",
    "        return \"flat\"  # Default fallback\n",
    "\n",
    "def parse_extraction(extraction_text):\n",
    "    \"\"\"Parse extraction text into field dictionary.\"\"\"\n",
    "    extracted_fields = {}\n",
    "\n",
    "    for line in extraction_text.split('\\n'):\n",
    "        line = line.strip()\n",
    "        if ':' in line and not line.startswith('#'):\n",
    "            parts = line.split(':', 1)\n",
    "            if len(parts) == 2:\n",
    "                field_name = parts[0].strip()\n",
    "                field_value = parts[1].strip()\n",
    "                extracted_fields[field_name] = field_value if field_value else 'NOT_FOUND'\n",
    "\n",
    "    return extracted_fields\n",
    "\n",
    "rprint(\"[green]‚úÖ Parser functions defined[/green]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19f5f24",
   "metadata": {},
   "source": [
    "## Image Discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5e08c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 8\n",
    "# Discover all images (no filtering by document type)\n",
    "data_dir = Path(CONFIG['DATA_DIR'])\n",
    "image_files = sorted(data_dir.glob(\"*.png\"))\n",
    "\n",
    "rprint(f\"[green]‚úÖ Found {len(image_files)} images to process[/green]\")\n",
    "\n",
    "rprint(\"[bold blue]Images to process:[/bold blue]\")\n",
    "for img in image_files:\n",
    "    rprint(f\"[cyan]  - {img.name}[/cyan]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q2rncf5lxkk",
   "metadata": {},
   "source": [
    "## Pipeline Stage 0-2 Functions\n",
    "\n",
    "Define pipeline stages as functions that can be applied to DataFrame rows.\n",
    "\n",
    "**InternVL3-specific pattern**: Load pixel_values ONCE in Stage 0, then REUSE across all stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0qe3ebq3h4mb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 9\n",
    "def stage_0_doctype_detection(row):\n",
    "    \"\"\"\n",
    "    Stage 0: Document type detection.\n",
    "    \n",
    "    InternVL3-specific: Loads pixel_values ONCE and stores for reuse.\n",
    "    \n",
    "    Args:\n",
    "        row: DataFrame row with 'image_path' column\n",
    "    \n",
    "    Returns:\n",
    "        dict: {'raw_response': str, 'processing_time': float, \n",
    "               'pixel_values': tensor, 'history': list}\n",
    "    \"\"\"\n",
    "    image_path = row['image_path']\n",
    "    \n",
    "    # CRITICAL: Load pixel_values ONCE for InternVL3\n",
    "    pixel_values = load_image(str(image_path), debug=False)\n",
    "    history = None\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    doctype_answer, history = chat_with_internvl(\n",
    "        model, tokenizer, DOCTYPE_PROMPT, pixel_values, history,\n",
    "        max_new_tokens=CONFIG['MAX_NEW_TOKENS_DOCTYPE'],\n",
    "        debug=False\n",
    "    )\n",
    "    \n",
    "    processing_time = time.time() - start_time\n",
    "    \n",
    "    # GPU cleanup\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return {\n",
    "        'raw_response': doctype_answer,\n",
    "        'processing_time': processing_time,\n",
    "        'pixel_values': pixel_values,  # STORE for reuse\n",
    "        'history': history  # STORE for multi-turn\n",
    "    }\n",
    "\n",
    "\n",
    "def stage_1_structure_classification(row):\n",
    "    \"\"\"\n",
    "    Stage 1: Structure classification for bank statements.\n",
    "    \n",
    "    InternVL3-specific: REUSES pixel_values and history from Stage 0.\n",
    "    \n",
    "    Args:\n",
    "        row: DataFrame row with 'document_type', 'pixel_values_after_stage0', \n",
    "             'history_after_stage0'\n",
    "    \n",
    "    Returns:\n",
    "        dict or None: {'raw_response': str, 'processing_time': float, 'history': list}\n",
    "                     or None if not a bank statement\n",
    "    \"\"\"\n",
    "    if row['document_type'] != 'BANK_STATEMENT':\n",
    "        return None\n",
    "    \n",
    "    # CRITICAL: REUSE pixel_values and history from Stage 0\n",
    "    pixel_values = row['pixel_values_after_stage0']\n",
    "    history = row['history_after_stage0']\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    structure_answer, history = chat_with_internvl(\n",
    "        model, tokenizer, STRUCTURE_CLASSIFICATION_PROMPT, pixel_values, history,\n",
    "        max_new_tokens=CONFIG['MAX_NEW_TOKENS_STRUCTURE'],\n",
    "        debug=False\n",
    "    )\n",
    "    \n",
    "    processing_time = time.time() - start_time\n",
    "    \n",
    "    # GPU cleanup\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return {\n",
    "        'raw_response': structure_answer,\n",
    "        'processing_time': processing_time,\n",
    "        'history': history  # STORE updated history\n",
    "    }\n",
    "\n",
    "\n",
    "def stage_2_extraction(row):\n",
    "    \"\"\"\n",
    "    Stage 2: Field extraction with document-type-aware prompts.\n",
    "    \n",
    "    InternVL3-specific: REUSES pixel_values and history from previous stages.\n",
    "    \n",
    "    Args:\n",
    "        row: DataFrame row with all previous stage data\n",
    "    \n",
    "    Returns:\n",
    "        dict: {'raw_response': str, 'processing_time': float, 'prompt_used': str}\n",
    "    \"\"\"\n",
    "    document_type = row['document_type']\n",
    "    structure_type = row['structure_type']\n",
    "    \n",
    "    # CRITICAL: REUSE pixel_values\n",
    "    pixel_values = row['pixel_values_after_stage0']\n",
    "    \n",
    "    # Determine which prompt to use and which history to use\n",
    "    if document_type == 'BANK_STATEMENT':\n",
    "        extraction_prompt = BANK_PROMPTS[structure_type]\n",
    "        prompt_key = f\"internvl3_bank_statement_{structure_type}\"\n",
    "        history = row['history_after_stage1']  # Use history from Stage 1\n",
    "    elif document_type == 'INVOICE':\n",
    "        extraction_prompt = INVOICE_PROMPT\n",
    "        prompt_key = \"internvl3_invoice\"\n",
    "        history = row['history_after_stage0']  # Use history from Stage 0\n",
    "    elif document_type == 'RECEIPT':\n",
    "        extraction_prompt = RECEIPT_PROMPT\n",
    "        prompt_key = \"internvl3_receipt\"\n",
    "        history = row['history_after_stage0']  # Use history from Stage 0\n",
    "    else:\n",
    "        # Fallback\n",
    "        extraction_prompt = INVOICE_PROMPT\n",
    "        prompt_key = \"internvl3_invoice_fallback\"\n",
    "        history = row['history_after_stage0']  # Use history from Stage 0\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    extraction_result, history = chat_with_internvl(\n",
    "        model, tokenizer, extraction_prompt, pixel_values, history,\n",
    "        max_new_tokens=CONFIG['MAX_NEW_TOKENS_EXTRACT'],\n",
    "        debug=False\n",
    "    )\n",
    "    \n",
    "    processing_time = time.time() - start_time\n",
    "    \n",
    "    # GPU cleanup\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return {\n",
    "        'raw_response': extraction_result,\n",
    "        'processing_time': processing_time,\n",
    "        'prompt_used': prompt_key\n",
    "    }\n",
    "\n",
    "\n",
    "rprint(\"[green]‚úÖ Pipeline stage functions 0-2 defined (InternVL3-specific)[/green]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5354152",
   "metadata": {},
   "source": [
    "## Pipeline Stage 3-5 Functions\n",
    "- now imported from pipeline_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dlj24df2l16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 10\n",
    "# Pipeline Stage Functions\n",
    "#\n",
    "# Stage 3-5 functions are now imported from pipeline_lib.stages\n",
    "#\n",
    "# Usage:\n",
    "#   df[\"parsed_fields\"] = df[\"extraction_response\"].apply(\n",
    "#       lambda x: stage_3_parsing(x, expected_fields=FIELD_COLUMNS)\n",
    "#   )\n",
    "#\n",
    "#   df[\"cleaned_fields\"] = df[\"parsed_fields\"].apply(\n",
    "#       lambda x: stage_4_cleaning(x, cleaner=cleaner)\n",
    "#   )\n",
    "#\n",
    "#   df[\"evaluation\"] = df.apply(\n",
    "#       lambda row: stage_5_evaluation(row, ground_truth, expected_fields=FIELD_COLUMNS),\n",
    "#       axis=1\n",
    "#   )\n",
    "\n",
    "rprint(\"[green]‚úÖ Pipeline stage functions imported from pipeline_lib[/green]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26301200",
   "metadata": {},
   "source": [
    "## Full Batch Pipeline\n",
    "- Process images in configurable batches\n",
    "- Run all stages sequentially on DataFrame\n",
    "- Save checkpoints after each batch"
   ]
  },
  {
   "cell_type": "code",
   "id": "dnp62t109nm",
   "source": "#Cell 11\n# ============================================================================\n# BATCH PROCESSING CONFIGURATION\n# ============================================================================\nBATCH_SIZE = 1000  # Process images in batches (configurable)\n\n# ============================================================================\n# INITIALIZE PROCESSING\n# ============================================================================\nfrom tqdm.auto import tqdm\n\nall_image_files = image_files  # All images to process\ntotal_images = len(all_image_files)\nnum_batches = (total_images + BATCH_SIZE - 1) // BATCH_SIZE\n\nrprint(f\"\\n[bold green]üöÄ Starting batch pipeline processing...[/bold green]\")\nrprint(f\"[cyan]  Total images: {total_images}[/cyan]\")\nrprint(f\"[cyan]  Batch size: {BATCH_SIZE}[/cyan]\")\nrprint(f\"[cyan]  Number of batches: {num_batches}[/cyan]\\n\")\n\n# Store all batch results\nall_results = []\n\n# ============================================================================\n# PROCESS EACH BATCH\n# ============================================================================\nfor batch_num in range(num_batches):\n    batch_start = batch_num * BATCH_SIZE\n    batch_end = min(batch_start + BATCH_SIZE, total_images)\n    batch_files = all_image_files[batch_start:batch_end]\n    batch_size = len(batch_files)\n    \n    console.rule(f\"[bold magenta]Batch {batch_num + 1}/{num_batches} ({batch_size} images)[/bold magenta]\")\n    \n    # Initialize DataFrame for this batch\n    df = pd.DataFrame({'image_path': [str(p) for p in batch_files]})\n    df['image_name'] = df['image_path'].apply(lambda x: Path(x).name)\n    \n    # ------------------------------------------------------------------------\n    # STAGE 0: Document Type Detection (GPU - use tqdm for OOM control)\n    # ------------------------------------------------------------------------\n    console.rule(\"[bold cyan]Stage 0: Document Type Detection[/bold cyan]\")\n    \n    doctype_results = []\n    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Stage 0: Doc Type\"):\n        doctype_results.append(stage_0_doctype_detection(row))\n    df['doctype_raw'] = doctype_results\n    \n    # Extract components (InternVL3-specific: includes pixel_values and history)\n    df['doctype_response'] = df['doctype_raw'].apply(lambda x: x['raw_response'])\n    df['doctype_time'] = df['doctype_raw'].apply(lambda x: x['processing_time'])\n    df['pixel_values_after_stage0'] = df['doctype_raw'].apply(lambda x: x['pixel_values'])\n    df['history_after_stage0'] = df['doctype_raw'].apply(lambda x: x['history'])\n    \n    # Parse document type\n    df['document_type'] = df['doctype_response'].apply(parse_document_type)\n    \n    # Summary\n    doctype_counts = df['document_type'].value_counts().to_dict()\n    rprint(f\"[green]‚úÖ Stage 0 complete: {len(df)} documents classified[/green]\")\n    rprint(f\"[cyan]   Invoices: {doctype_counts.get('INVOICE', 0)}[/cyan]\")\n    rprint(f\"[cyan]   Receipts: {doctype_counts.get('RECEIPT', 0)}[/cyan]\")\n    rprint(f\"[cyan]   Bank Statements: {doctype_counts.get('BANK_STATEMENT', 0)}[/cyan]\")\n    show_pipeline_memory(df, \"Stage 0\")\n    \n    # ------------------------------------------------------------------------\n    # STAGE 1: Structure Classification (GPU - use tqdm for OOM control)\n    # ------------------------------------------------------------------------\n    console.rule(\"[bold cyan]Stage 1: Structure Classification[/bold cyan]\")\n    \n    structure_results = []\n    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Stage 1: Structure\"):\n        structure_results.append(stage_1_structure_classification(row))\n    df['structure_raw'] = structure_results\n    \n    # Extract components (handle None for non-bank-statements)\n    df['structure_response'] = df['structure_raw'].apply(\n        lambda x: x['raw_response'] if x else 'N/A'\n    )\n    df['structure_time'] = df['structure_raw'].apply(\n        lambda x: x['processing_time'] if x else 0\n    )\n    df['history_after_stage1'] = df['structure_raw'].apply(\n        lambda x: x['history'] if x else None\n    )\n    \n    # Parse structure type\n    df['structure_type'] = df['structure_response'].apply(parse_structure_type)\n    \n    # Summary\n    bank_count = (df['document_type'] == 'BANK_STATEMENT').sum()\n    structure_counts = df[df['document_type'] == 'BANK_STATEMENT']['structure_type'].value_counts().to_dict()\n    rprint(f\"[green]‚úÖ Stage 1 complete: {bank_count} bank statements classified[/green]\")\n    if bank_count > 0:\n        rprint(f\"[cyan]   Flat: {structure_counts.get('flat', 0)}[/cyan]\")\n        rprint(f\"[cyan]   Date-grouped: {structure_counts.get('date_grouped', 0)}[/cyan]\")\n    show_pipeline_memory(df, \"Stage 1\")\n    \n    # ------------------------------------------------------------------------\n    # STAGE 2: Extraction (GPU - CRITICAL: Manual loop for OOM control)\n    # ------------------------------------------------------------------------\n    console.rule(\"[bold cyan]Stage 2: Document-Type-Aware Extraction[/bold cyan]\")\n    \n    extraction_results = []\n    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Stage 2: Extraction\"):\n        extraction_results.append(stage_2_extraction(row))\n        \n        # CRITICAL: Periodic garbage collection for GPU OOM control\n        if (idx + 1) % 3 == 0:\n            gc.collect()\n    \n    df['extraction_raw'] = extraction_results\n    \n    # Extract components\n    df['extraction_response'] = df['extraction_raw'].apply(lambda x: x['raw_response'])\n    df['extraction_time'] = df['extraction_raw'].apply(lambda x: x['processing_time'])\n    df['prompt_used'] = df['extraction_raw'].apply(lambda x: x['prompt_used'])\n    \n    # Calculate total processing time\n    df['total_time'] = df['doctype_time'] + df['structure_time'] + df['extraction_time']\n    \n    rprint(f\"[green]‚úÖ Stage 2 complete: {len(df)} extractions done[/green]\")\n    rprint(f\"[cyan]   Average extraction time: {df['extraction_time'].mean():.2f}s[/cyan]\")\n    rprint(f\"[cyan]   Total time (avg): {df['total_time'].mean():.2f}s[/cyan]\")\n    show_pipeline_memory(df, \"Stage 2\")\n    \n    # ------------------------------------------------------------------------\n    # STAGE 3: Parsing (CPU - safe to use progress_apply)\n    # ------------------------------------------------------------------------\n    console.rule(\"[bold cyan]Stage 3: Parsing (Text ‚Üí Fields)[/bold cyan]\")\n    \n    tqdm.pandas(desc=\"Stage 3: Parsing\")\n    df['parsed_fields'] = df['extraction_response'].progress_apply(\n        lambda x: stage_3_parsing(x, expected_fields=FIELD_COLUMNS)\n    )\n    \n    # Count fields found\n    df['fields_found'] = df['parsed_fields'].apply(\n        lambda x: sum(1 for v in x.values() if v != 'NOT_FOUND')\n    )\n    \n    rprint(f\"[green]‚úÖ Stage 3 complete: {len(df)} responses parsed[/green]\")\n    rprint(f\"[cyan]   Average fields found: {df['fields_found'].mean():.1f}/{len(FIELD_COLUMNS)}[/cyan]\")\n    show_pipeline_memory(df, \"Stage 3\")\n    \n    # ------------------------------------------------------------------------\n    # STAGE 4: Cleaning (CPU - safe to use progress_apply)\n    # ------------------------------------------------------------------------\n    console.rule(\"[bold cyan]Stage 4: Cleaning & Normalization[/bold cyan]\")\n    \n    tqdm.pandas(desc=\"Stage 4: Cleaning\")\n    df['cleaned_fields'] = df['parsed_fields'].progress_apply(\n        lambda x: stage_4_cleaning(x, cleaner=cleaner)\n    )\n    \n    # Count fields after cleaning\n    df['fields_cleaned'] = df['cleaned_fields'].apply(\n        lambda x: sum(1 for v in x.values() if v != 'NOT_FOUND')\n    )\n    \n    rprint(f\"[green]‚úÖ Stage 4 complete: {len(df)} field sets cleaned[/green]\")\n    rprint(f\"[cyan]   Average fields cleaned: {df['fields_cleaned'].mean():.1f}/{len(FIELD_COLUMNS)}[/cyan]\")\n    show_pipeline_memory(df, \"Stage 4\")\n    \n    # ------------------------------------------------------------------------\n    # STAGE 5: Evaluation (CPU - safe to use progress_apply)\n    # ------------------------------------------------------------------------\n    console.rule(\"[bold cyan]Stage 5: Evaluation (Optional)[/bold cyan]\")\n    \n    # Load ground truth if available\n    if not CONFIG['INFERENCE_ONLY'] and CONFIG.get('GROUND_TRUTH'):\n        if batch_num == 0:  # Load ground truth only once\n            rprint(\"[cyan]Loading ground truth for evaluation...[/cyan]\")\n            ground_truth = load_ground_truth(CONFIG['GROUND_TRUTH'], verbose=False)\n            rprint(f\"[green]‚úÖ Ground truth loaded for {len(ground_truth)} images[/green]\")\n        \n        # Apply evaluation with progress bar\n        tqdm.pandas(desc=\"Stage 5: Evaluation\")\n        df['evaluation'] = df.progress_apply(\n            lambda row: stage_5_evaluation(row, ground_truth, expected_fields=FIELD_COLUMNS),\n            axis=1\n        )\n        \n        # Extract accuracy metrics\n        df['overall_accuracy'] = df['evaluation'].apply(\n            lambda x: x.get('overall_accuracy', 0) * 100 if x and 'error' not in x else None\n        )\n        df['fields_matched'] = df['evaluation'].apply(\n            lambda x: x.get('fields_matched', 0) if x and 'error' not in x else None\n        )\n        df['fields_extracted'] = df['evaluation'].apply(\n            lambda x: x.get('fields_extracted', 0) if x and 'error' not in x else None\n        )\n        \n        rprint(f\"[green]‚úÖ Stage 5 complete: {len(df)} extractions evaluated[/green]\")\n        rprint(f\"[cyan]   Average accuracy: {df['overall_accuracy'].mean():.2f}%[/cyan]\")\n        rprint(f\"[cyan]   Median accuracy: {df['overall_accuracy'].median():.2f}%[/cyan]\")\n    else:\n        df['evaluation'] = None\n        df['overall_accuracy'] = None\n        df['fields_matched'] = None\n        df['fields_extracted'] = None\n        if batch_num == 0:\n            rprint(\"[yellow]‚ö†Ô∏è  Inference-only mode - skipping evaluation[/yellow]\")\n    \n    show_pipeline_memory(df, \"Stage 5\")\n    \n    # ------------------------------------------------------------------------\n    # CLEANUP PIXEL_VALUES BEFORE CHECKPOINTING (memory optimization)\n    # ------------------------------------------------------------------------\n    # Remove pixel_values and history columns before saving (large tensors)\n    df_checkpoint = df.drop(columns=['pixel_values_after_stage0', 'history_after_stage0', \n                                     'history_after_stage1'], errors='ignore')\n    \n    # ------------------------------------------------------------------------\n    # SAVE BATCH CHECKPOINT\n    # ------------------------------------------------------------------------\n    batch_checkpoint = checkpoint_dir / f'batch_{batch_num + 1:04d}_{TIMESTAMP}.pkl'\n    df_checkpoint.to_pickle(batch_checkpoint)\n    rprint(f\"[green]‚úÖ Batch {batch_num + 1} checkpoint saved: {batch_checkpoint.name}[/green]\")\n    \n    # Store batch result (also without tensors)\n    all_results.append(df_checkpoint)\n    \n    # ------------------------------------------------------------------------\n    # MEMORY CLEANUP BETWEEN BATCHES\n    # ------------------------------------------------------------------------\n    if batch_num < num_batches - 1:  # Not the last batch\n        rprint(\"[yellow]üßπ Cleaning up memory before next batch...[/yellow]\")\n        del df\n        del df_checkpoint\n        gc.collect()\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n        rprint(\"[green]‚úÖ Memory cleanup complete[/green]\\n\")\n    else:\n        # Last batch - clean up pixel_values from final df too\n        df = df_checkpoint\n\n# ============================================================================\n# COMBINE ALL BATCHES\n# ============================================================================\nconsole.rule(\"[bold green]Combining All Batches[/bold green]\")\n\ndf = pd.concat(all_results, ignore_index=True)\n\nrprint(f\"[bold green]‚úÖ All {len(df)} images processed through pipeline[/bold green]\")\nrprint(f\"[cyan]   Total processing time: {df['total_time'].sum():.2f}s[/cyan]\")\nrprint(f\"[cyan]   Average per image: {df['total_time'].mean():.2f}s[/cyan]\")\n\n# Final checkpoint with combined data\nfinal_checkpoint = checkpoint_dir / f'final_combined_{TIMESTAMP}.pkl'\ndf.to_pickle(final_checkpoint)\nrprint(f\"[green]‚úÖ Final combined checkpoint: {final_checkpoint.name}[/green]\")\n\nconsole.rule(\"[bold green]Pipeline Complete[/bold green]\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "uisjubt6pg",
   "source": "## Export Results",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "ktws1yn2us",
   "source": "#Cell 12\n# ============================================================================\n# EXPORT RESULTS WITH EXPANDED FIELDS\n# ============================================================================\nconsole.rule(\"[bold blue]Exporting Results[/bold blue]\")\n\n# Create export data with individual field columns\nexport_data = []\n\nfor idx, row in df.iterrows():\n    record = {\n        'image_file': row['image_name'],\n        'document_type': row['document_type'],\n        'structure_type': row['structure_type'],\n        'prompt_used': row['prompt_used'],\n        'total_time': row['total_time'],\n        'fields_found': row['fields_found'],\n        'fields_cleaned': row['fields_cleaned'],\n    }\n    \n    # Add all cleaned field values as individual columns\n    cleaned = row['cleaned_fields']\n    for field in FIELD_COLUMNS:\n        record[field] = cleaned.get(field, 'NOT_FOUND')\n    \n    # Add evaluation metrics if available\n    if row['evaluation'] is not None and row['overall_accuracy'] is not None:\n        record['overall_accuracy'] = row['overall_accuracy']\n        record['fields_matched'] = row['fields_matched']\n        record['fields_extracted'] = row['fields_extracted']\n    \n    export_data.append(record)\n\nexport_df = pd.DataFrame(export_data)\n\n# Save CSV (compatible with model_comparison.ipynb)\ncsv_output = output_dir / f\"internvl3_pipeline_results_{TIMESTAMP}.csv\"\nexport_df.to_csv(csv_output, index=False)\n\nrprint(f\"[green]‚úÖ CSV exported: {csv_output}[/green]\")\nrprint(f\"[cyan]   Rows: {len(export_df)}[/cyan]\")\nrprint(f\"[cyan]   Columns: {len(export_df.columns)}[/cyan]\")\nrprint(f\"[cyan]   Pattern: *internvl3*pipeline*results*.csv (compatible with model_comparison.ipynb)[/cyan]\")\n\n# Save detailed JSON with full pipeline data\njson_data = []\n\nfor idx, row in df.iterrows():\n    record = {\n        'image_file': row['image_name'],\n        'pipeline_stages': {\n            'stage_0_doctype': {\n                'raw_response': row['doctype_response'],\n                'document_type': row['document_type'],\n                'processing_time': row['doctype_time']\n            },\n            'stage_1_structure': {\n                'raw_response': row['structure_response'],\n                'structure_type': row['structure_type'],\n                'processing_time': row['structure_time']\n            },\n            'stage_2_extraction': {\n                'raw_response': row['extraction_response'],\n                'prompt_used': row['prompt_used'],\n                'processing_time': row['extraction_time']\n            },\n            'stage_3_parsing': {\n                'parsed_fields': row['parsed_fields']\n            },\n            'stage_4_cleaning': {\n                'cleaned_fields': row['cleaned_fields']\n            }\n        },\n        'total_processing_time': row['total_time']\n    }\n    \n    if row['evaluation'] is not None:\n        record['stage_5_evaluation'] = row['evaluation']\n    \n    json_data.append(record)\n\njson_output = output_dir / f\"internvl3_pipeline_full_{TIMESTAMP}.json\"\nwith open(json_output, 'w') as f:\n    json.dump(json_data, f, indent=2)\n\nrprint(f\"[green]‚úÖ JSON exported: {json_output}[/green]\")\nrprint(f\"[cyan]   Full pipeline data saved for all {len(json_data)} images[/cyan]\")\n\n# Display sample of exported data\nrprint(\"\\n[bold blue]üìã Sample Exported Data:[/bold blue]\")\nif not CONFIG['INFERENCE_ONLY']:\n    sample_cols = ['image_file', 'document_type', 'overall_accuracy', 'total_time', 'fields_cleaned']\nelse:\n    sample_cols = ['image_file', 'document_type', 'total_time', 'fields_cleaned']\n\nrprint(export_df[sample_cols].head(3).to_string(index=False))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "lzkf9nuiy9",
   "source": "## Pipeline Debugging Utilities\n\nFunctions to inspect and debug the pipeline at any stage",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "hhq7739rcx7",
   "source": "#Cell 13\ndef inspect_pipeline(df, image_name, stages=['all']):\n    \"\"\"\n    Inspect all pipeline stages for a specific image.\n    \n    Args:\n        df: DataFrame with pipeline results\n        image_name: Name of image to inspect (e.g., 'image_003.png')\n        stages: List of stages to show or 'all'\n    \"\"\"\n    row = df[df['image_name'] == image_name].iloc[0]\n    \n    print(f\"\\n{'='*80}\")\n    print(f\"üîç Pipeline Inspection: {image_name}\")\n    print(f\"{'='*80}\")\n    \n    if 'all' in stages or 'doctype' in stages:\n        print(f\"\\n[STAGE 0: Document Type Detection]\")\n        print(f\"Response: {row['doctype_response']}\")\n        print(f\"Parsed: {row['document_type']}\")\n        print(f\"Time: {row['doctype_time']:.2f}s\")\n    \n    if 'all' in stages or 'structure' in stages:\n        print(f\"\\n[STAGE 1: Structure Classification]\")\n        print(f\"Response: {row['structure_response']}\")\n        print(f\"Parsed: {row['structure_type']}\")\n        print(f\"Time: {row['structure_time']:.2f}s\")\n    \n    if 'all' in stages or 'extraction' in stages:\n        print(f\"\\n[STAGE 2: Extraction]\")\n        print(f\"Prompt Used: {row['prompt_used']}\")\n        response = row['extraction_response']\n        print(f\"Response ({len(response)} chars):\")\n        print(response[:500] + \"...\" if len(response) > 500 else response)\n        print(f\"Time: {row['extraction_time']:.2f}s\")\n    \n    if 'all' in stages or 'parsing' in stages:\n        print(f\"\\n[STAGE 3: Parsing]\")\n        parsed = row['parsed_fields']\n        found_fields = [k for k, v in parsed.items() if v != 'NOT_FOUND']\n        print(f\"Found {len(found_fields)}/{len(parsed)} fields:\")\n        for field in found_fields[:5]:\n            value = parsed[field]\n            print(f\"  {field}: {value[:50]}...\" if len(value) > 50 else f\"  {field}: {value}\")\n    \n    if 'all' in stages or 'cleaning' in stages:\n        print(f\"\\n[STAGE 4: Cleaning]\")\n        cleaned = row['cleaned_fields']\n        found_fields = [k for k, v in cleaned.items() if v != 'NOT_FOUND']\n        print(f\"Cleaned {len(found_fields)}/{len(cleaned)} fields:\")\n        \n        # Show before/after for changed fields\n        changes_shown = 0\n        for field in found_fields:\n            parsed_val = row['parsed_fields'][field]\n            cleaned_val = cleaned[field]\n            if parsed_val != cleaned_val:\n                print(f\"  {field}:\")\n                print(f\"    Before: {parsed_val[:50]}...\" if len(parsed_val) > 50 else f\"    Before: {parsed_val}\")\n                print(f\"    After:  {cleaned_val[:50]}...\" if len(cleaned_val) > 50 else f\"    After:  {cleaned_val}\")\n                changes_shown += 1\n                if changes_shown >= 5:\n                    break\n    \n    if 'all' in stages or 'evaluation' in stages:\n        if row['evaluation'] is not None:\n            print(f\"\\n[STAGE 5: Evaluation]\")\n            print(f\"Overall Accuracy: {row['overall_accuracy']:.2f}%\")\n            print(f\"Fields Matched: {row['fields_matched']}/{row['fields_extracted']}\")\n    \n    print(f\"\\n{'='*80}\\n\")\n\n\ndef compare_parsing_cleaning(df, image_name):\n    \"\"\"Show side-by-side comparison of parsed vs cleaned fields.\"\"\"\n    row = df[df['image_name'] == image_name].iloc[0]\n    \n    parsed = row['parsed_fields']\n    cleaned = row['cleaned_fields']\n    \n    print(f\"\\nüìä Parsing vs Cleaning Comparison: {image_name}\")\n    print(f\"{'Field':<30} {'Parsed':<40} {'Cleaned':<40}\")\n    print(\"=\"*110)\n    \n    for field in FIELD_COLUMNS:\n        parsed_val = parsed.get(field, 'NOT_FOUND')\n        cleaned_val = cleaned.get(field, 'NOT_FOUND')\n        \n        if parsed_val != cleaned_val:\n            p_display = parsed_val[:37] + \"...\" if len(parsed_val) > 40 else parsed_val\n            c_display = cleaned_val[:37] + \"...\" if len(cleaned_val) > 40 else cleaned_val\n            \n            print(f\"{field:<30} {p_display:<40} {c_display:<40}\")\n\n\ndef field_coverage_report(df):\n    \"\"\"Generate field coverage statistics across all images.\"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(\"üìà FIELD COVERAGE REPORT\")\n    print(\"=\"*80)\n    \n    coverage_data = []\n    \n    for field in FIELD_COLUMNS:\n        parsed_count = sum(\n            1 for idx, row in df.iterrows()\n            if row['parsed_fields'].get(field, 'NOT_FOUND') != 'NOT_FOUND'\n        )\n        cleaned_count = sum(\n            1 for idx, row in df.iterrows()\n            if row['cleaned_fields'].get(field, 'NOT_FOUND') != 'NOT_FOUND'\n        )\n        \n        parsed_pct = (parsed_count / len(df)) * 100\n        cleaned_pct = (cleaned_count / len(df)) * 100\n        \n        coverage_data.append({\n            'Field': field,\n            'Parsed': f\"{parsed_count}/{len(df)} ({parsed_pct:.1f}%)\",\n            'Cleaned': f\"{cleaned_count}/{len(df)} ({cleaned_pct:.1f}%)\",\n            'Change': cleaned_count - parsed_count\n        })\n    \n    coverage_df = pd.DataFrame(coverage_data)\n    print(coverage_df.to_string(index=False))\n    print(\"=\"*80 + \"\\n\")\n\n\nrprint(\"[green]‚úÖ Debugging utilities defined[/green]\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "bgtb7uhph5",
   "source": "## Pipeline Summary and Next Steps\n\nFinal summary of pipeline execution and tips for working with large batches",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "8ubzq9topud",
   "source": "#Cell 14\n# ============================================================================\n# PIPELINE SUMMARY\n# ============================================================================\nconsole.rule(\"[bold green]Pipeline Execution Summary[/bold green]\")\n\nprint(\"\\nüìä INTERNVL3 PIPELINE PROCESSING SUMMARY\")\nprint(\"=\"*80)\nprint(f\"Total images processed: {len(df)}\")\nprint(f\"Mode: {'Inference-only' if CONFIG['INFERENCE_ONLY'] else 'Evaluation mode'}\")\nprint()\n\n# Document type distribution\nprint(\"Document Type Distribution:\")\nfor doc_type, count in df['document_type'].value_counts().items():\n    print(f\"  {doc_type}: {count}\")\nprint()\n\n# Bank statement structure distribution\nbank_count = (df['document_type'] == 'BANK_STATEMENT').sum()\nif bank_count > 0:\n    print(\"Bank Statement Structure Distribution:\")\n    for struct_type, count in df[df['document_type'] == 'BANK_STATEMENT']['structure_type'].value_counts().items():\n        print(f\"  {struct_type}: {count}\")\n    print()\n\n# Processing time statistics\nprint(\"Processing Time Statistics:\")\nprint(f\"  Total time: {df['total_time'].sum():.2f}s\")\nprint(f\"  Average per image: {df['total_time'].mean():.2f}s\")\nprint(f\"  Min: {df['total_time'].min():.2f}s\")\nprint(f\"  Max: {df['total_time'].max():.2f}s\")\nprint()\n\n# Field extraction statistics\nprint(\"Field Extraction Statistics:\")\nprint(f\"  Average fields parsed: {df['fields_found'].mean():.1f}/{len(FIELD_COLUMNS)}\")\nprint(f\"  Average fields cleaned: {df['fields_cleaned'].mean():.1f}/{len(FIELD_COLUMNS)}\")\nprint()\n\n# Accuracy statistics (if available)\nif not CONFIG['INFERENCE_ONLY']:\n    print(\"Accuracy Statistics:\")\n    print(f\"  Average accuracy: {df['overall_accuracy'].mean():.2f}%\")\n    print(f\"  Median accuracy: {df['overall_accuracy'].median():.2f}%\")\n    print(f\"  Min accuracy: {df['overall_accuracy'].min():.2f}%\")\n    print(f\"  Max accuracy: {df['overall_accuracy'].max():.2f}%\")\n    print()\n\nprint(\"=\"*80)\n\n# ============================================================================\n# TIPS FOR LARGE BATCHES (10,000+ images)\n# ============================================================================\nprint(\"\\nüí° TIPS FOR PROCESSING LARGE BATCHES (10,000+ images)\")\nprint(\"=\"*80)\nprint(\"\"\"\n1. CHECKPOINTING:\n   - Checkpoints are automatically saved after each batch\n   - Resume from checkpoint:\n     df = pd.read_pickle('checkpoints/batch_0001_TIMESTAMP.pkl')\n\n2. BATCH PROCESSING:\n   - Process in batches of 1000 images to manage memory\n   - Adjust BATCH_SIZE configuration as needed\n   \n3. PARALLEL PROCESSING:\n   - Install pandarallel: pip install pandarallel\n   - Use parallel_apply for stages 3 and 4 (CPU-bound)\n   - Stages 0-2 (GPU-bound) must remain sequential\n\n4. MEMORY MANAGEMENT:\n   - Periodic garbage collection already enabled (every 3 images)\n   - GPU cache clearing after each image\n   - pixel_values removed before checkpointing to save space\n   - Monitor with: nvidia-smi\n\n5. INTERNVL3-SPECIFIC NOTES:\n   - pixel_values loaded ONCE in Stage 0, then REUSED across stages\n   - history maintained across multi-turn conversations\n   - Dynamic image tiling adjusts to image aspect ratio\n\n6. INSPECTION AND DEBUGGING:\n   - Use inspect_pipeline(df, 'image_name') to debug specific images\n   - Use compare_parsing_cleaning(df, 'image_name') to see cleaning effects\n   - Use field_coverage_report(df) for overall statistics\n\n7. OUTPUT FILES:\n   - CSV: internvl3_pipeline_results_TIMESTAMP.csv\n   - JSON: internvl3_pipeline_full_TIMESTAMP.json\n   - Checkpoints: checkpoints/batch_XXXX_TIMESTAMP.pkl\n\"\"\")\nprint(\"=\"*80)\n\n# ============================================================================\n# DATAFRAME COLUMN REFERENCE\n# ============================================================================\nprint(\"\\nüìã DATAFRAME COLUMN REFERENCE\")\nprint(\"=\"*80)\nprint(\"\"\"\nPIPELINE STAGES (dict objects):\n  - doctype_raw: {'raw_response', 'processing_time', 'pixel_values', 'history'}\n  - structure_raw: {'raw_response', 'processing_time', 'history'} or None\n  - extraction_raw: {'raw_response', 'processing_time', 'prompt_used'}\n  - parsed_fields: {field_name: value} - 17 fields\n  - cleaned_fields: {field_name: cleaned_value} - 17 fields\n  - evaluation: {metrics} or None\n\nEXTRACTED COMPONENTS (primitives):\n  - image_path, image_name: str\n  - document_type: 'INVOICE' | 'RECEIPT' | 'BANK_STATEMENT'\n  - structure_type: 'flat' | 'date_grouped' | 'N/A'\n  - doctype_response, extraction_response: str (raw VLM output)\n  - doctype_time, structure_time, extraction_time, total_time: float (seconds)\n  - fields_found, fields_cleaned: int (count of non-NOT_FOUND fields)\n  - overall_accuracy, fields_matched, fields_extracted: float/int (if evaluation)\n\nINTERNVL3-SPECIFIC COLUMNS (removed before checkpointing):\n  - pixel_values_after_stage0: Preprocessed image tensor (reused across stages)\n  - history_after_stage0, history_after_stage1: Conversation history\n\nACCESSING DATA:\n  - Full extraction: df.loc[0, 'cleaned_fields']\n  - Single field: df.loc[0, 'cleaned_fields']['SUPPLIER_NAME']\n  - List field as array: df.loc[0, 'cleaned_fields']['LINE_ITEM_DESCRIPTIONS'].split(' | ')\n\"\"\")\nprint(\"=\"*80)\n\nrprint(\"\\n[bold green]üéâ Pipeline processing complete! Use the debugging utilities above to inspect results.[/bold green]\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "x6yye43gie",
   "source": "## View Individual Extraction\n\nChange `image_to_view` to view detailed extraction for a specific image:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "fv1mwonhez8",
   "source": "#Cell 15\n# View detailed extraction for specific image (using pandas DataFrame)\nimage_to_view = \"image_003.png\"  # Change this\n\nif 'df' in dir() and len(df) > 0:\n    row = df[df['image_name'] == image_to_view]\n\n    if len(row) > 0:\n        row = row.iloc[0]\n        print(f\"\\nüîç Detailed Extraction: {image_to_view}\")\n        print(\"=\"*80)\n        print(f\"Document Type: {row['document_type']}\")\n        print(f\"Structure Type: {row['structure_type']}\")\n        print(f\"Prompt Used: {row['prompt_used']}\")\n        print(f\"\\nDocument Type Classification Response:\")\n        print(row['doctype_response'])\n        print(f\"\\nStructure Classification Response:\")\n        print(row['structure_response'])\n        print(f\"\\nExtraction Result:\")\n        print(row['extraction_response'])\n        print(\"=\"*80)\n    else:\n        print(f\"Image {image_to_view} not found in DataFrame\")\nelse:\n    print(\"‚ö†Ô∏è DataFrame 'df' not found - run Cell 11 (main pipeline) first\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}