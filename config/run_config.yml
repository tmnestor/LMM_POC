# Document Extraction Configuration
# All options explicitly stated - YAML takes priority over defaults
# CLI flags override YAML when explicitly provided

# Model configuration — switch type + path together
model:
  # type: internvl3
  # path: /home/jovyan/nfs_share/models/InternVL3_5-8B
  # type: llama
  # path: /home/jovyan/nfs_share/models/Llama-3.2-11B-Vision-Instruct
  type: glm_ocr
  path: /home/jovyan/nfs_share/models/GLM-OCR
  max_tiles: 6          # Image tiles (H200: 18-36, V100: 14, L4: 18)
  flash_attn: true       # Use Flash Attention 2 (disable for V100)
  dtype: bfloat16        # Torch dtype (bfloat16, float16, float32)
  max_new_tokens: 2000   # Max tokens for generation




# Data paths
data:
  # dir: ../evaluation_data/travel/logbooks
  # ground_truth: ../evaluation_data/travel/logbooks/ground_truth_logbook.csv
  dir: ../evaluation_data/bank
  ground_truth: ../evaluation_data/bank/ground_truth_bank.csv
  # dir: ../evaluation_data/synthetic
  # ground_truth: ../evaluation_data/synthetic/ground_truth_synthetic.csv
  max_images: null       # null = process all images
  document_types: null   # null = all types, or comma-separated: INVOICE,RECEIPT,BANK_STATEMENT,TRAVEL_EXPENSE,VEHICLE_LOGBOOK

# Output settings
output:
  dir: ../evaluation_data/output
  skip_visualizations: false  # Skip visualization generation
  skip_reports: false         # Skip report generation

# Processing options
processing:
  batch_size: null             # Images per batch (null = auto-detect from VRAM, 1 = sequential)
  num_gpus: 0                  # 0 = auto-detect all GPUs, 1 = single GPU, N = use N GPUs
  bank_v2: true              # Use V2 bank statement extraction
  balance_correction: true   # Enable balance validation for bank statements
  verbose: false             # Verbose output (debug messages)

# Batch processing tuning
batch:
  default_sizes: {internvl3: 4, internvl3-2b: 4, internvl3-8b: 4}
  max_sizes: {internvl3: 8, internvl3-2b: 8, internvl3-8b: 16}
  conservative_sizes: {internvl3: 1, internvl3-2b: 2, internvl3-8b: 1}
  min_size: 1
  strategy: balanced            # conservative | balanced | aggressive
  auto_detect: true
  memory_safety_margin: 0.8     # fraction of VRAM to use
  clear_cache_after_batch: true
  timeout_seconds: 300
  fallback_enabled: true
  fallback_steps: [8, 4, 2, 1]

# Generation parameters
generation:
  max_new_tokens_base: 2000
  max_new_tokens_per_field: 50
  do_sample: false
  use_cache: true
  num_beams: 1
  repetition_penalty: 1.0
  token_limits:
    2b: null                    # use calculation
    8b: 800

# GPU memory management
gpu:
  memory_thresholds:
    low: 8                      # GB — conservative batching
    medium: 16                  # GB — balanced batching
    high: 24                    # GB — aggressive batching
    very_high: 64               # GB — maximum batching
  cuda_max_split_size_mb: 128
  fragmentation_threshold_gb: 0.5
  critical_fragmentation_threshold_gb: 1.0
  max_oom_retries: 3
  cudnn_benchmark: true

# Model loading options
model_loading:
  trust_remote_code: true
  use_fast_tokenizer: false
  low_cpu_mem_usage: true
  device_map: auto
  default_paths:
    internvl3: /home/jovyan/nfs_share/models/InternVL3_5-8B
    llama: /home/jovyan/nfs_share/models/Llama-3.2-11B-Vision-Instruct
    qwen3vl: /home/jovyan/nfs_share/models/Qwen3-VL-8B-Instruct
    glm_ocr: /home/jovyan/nfs_share/models/GLM-OCR
