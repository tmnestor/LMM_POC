{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, MllamaForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2\n",
    "def set_seed(seed=42):\n",
    "    \"\"\"Set random seeds for reproducibility\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "print(\"✅ Random seed set to 42 for reproducibility\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3\n",
    "# model_id = \"/home/jovyan/shared_PTM/Llama-3.2-11B-Vision-Instruct\"\n",
    "model_id = \"/home/jovyan/nfs_share/models/Llama-3.2-11B-Vision-Instruct\"\n",
    "\n",
    "print(\"🔧 Loading Llama-3.2-Vision model...\")\n",
    "model = MllamaForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# processor\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    allocated = torch.cuda.memory_allocated(i) / 1e9\n",
    "    reserved = torch.cuda.memory_reserved(i) / 1e9\n",
    "    print(f\"    GPU [{i}]: {{allocated:.2f}}GB allocated, {{reserved:.2f}}GB reserved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4\n",
    "import re\n",
    "\n",
    "\n",
    "def clean_llama_response(response: str) -> str:\n",
    "    \"\"\"Remove chat template artifacts and extract only the assistant's response.\"\"\"\n",
    "    start_marker = \"<|start_header_id|>assistant<|end_header_id|>\"\n",
    "    end_marker = \"<|eot_id|>\"\n",
    "    \n",
    "    start_idx = response.find(start_marker)\n",
    "    if start_idx != -1:\n",
    "        start_idx += len(start_marker)\n",
    "        end_idx = response.find(end_marker, start_idx)\n",
    "        if end_idx != -1:\n",
    "            return response[start_idx:end_idx].strip()\n",
    "    \n",
    "    return response.replace(\"***\",\"\").strip()\n",
    "\n",
    "\n",
    "def clean_markdown_table(markdown_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Replace empty cells in markdown table with NOT_FOUND.\n",
    "    \n",
    "    Handles patterns like:\n",
    "    - \"|  |\" → \"| NOT_FOUND |\"\n",
    "    - \"| |\"  → \"| NOT_FOUND |\"\n",
    "    - \"|   |\" → \"| NOT_FOUND |\"\n",
    "    \n",
    "    Only processes table rows, skips header separator lines (| --- | --- |).\n",
    "    \"\"\"\n",
    "    lines = markdown_text.split('\\n')\n",
    "    cleaned_lines = []\n",
    "    \n",
    "    for line in lines:\n",
    "        # Skip header separator lines (like \"| --- | --- | --- |\")\n",
    "        if re.match(r'^\\|\\s*-+\\s*\\|', line):\n",
    "            cleaned_lines.append(line)\n",
    "            continue\n",
    "        \n",
    "        # Replace empty cells in data rows\n",
    "        if '|' in line:\n",
    "            # Pattern: pipe followed by only whitespace followed by pipe\n",
    "            cleaned_line = re.sub(r'\\|\\s+\\|', '| NOT_FOUND |', line)\n",
    "            cleaned_lines.append(cleaned_line)\n",
    "        else:\n",
    "            cleaned_lines.append(line)\n",
    "    \n",
    "    return '\\n'.join(cleaned_lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5\n",
    "# imageName = \"/home/jovyan/_LMM_POC/evaluation_data/image_008.png\"\n",
    "imageName = \"/home/jovyan/nfs_share/tod/LMM_POC/evaluation_data/image_008.png\"\n",
    "\n",
    "print(\"📂 Loading image...\")\n",
    "image = Image.open(imageName)\n",
    "print(f\"✅ Image loaded: {image.size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6\n",
    "# basic flat 5 column [\"Date\", \"Description\", \"Withdrawal\", \"Credit\", \"Balance\"] transaction table prompt\n",
    "# prompt_text = \"\"\"\n",
    "# You are an expert document analyzer specializing in bank statement extraction.\n",
    "# Extract structured data from this flat table bank statement for taxpayer expense claims.\n",
    "\n",
    "# CONVERSATION PROTOCOL:\n",
    "# - Start your response immediately with \"DOCUMENT_TYPE: BANK_STATEMENT\"\n",
    "# - Do NOT include conversational text like \"I'll extract...\" or \"Based on the document...\"\n",
    "# - Do NOT use bullet points, numbered lists, asterisks, or markdown formatting (no **, no ##, no 1., no -)\n",
    "# - Output ONLY the structured extraction data below\n",
    "# - End immediately after \"TRANSACTION_AMOUNTS_PAID:\" with no additional text\n",
    "# - NO explanations, NO comments, NO additional text\n",
    "\n",
    "# CRITICAL:\n",
    "# - The transaction table in the image has a \"Date\", a \"Description\", a \"Withdrawal\", a \"Deposit\" and a \"Balance\" column\n",
    "# - Specifically, it has a \"Date\" column, a \"Description\" column, a \"Withdrawal\" column, a \"Deposit\" column and a \"Balance\" column\n",
    "\n",
    "# ANTI-HALLUCINATION RULES:\n",
    "# - YOU MUST NOT GUESS values you are unsure of\n",
    "# - Rows may have missing values\n",
    "# - Rows NEVER HAVE REPEATED AMOUNTS, SO YOU MUST NOT REPEAT VALUES THAT YOU ARE UNSURE OF\n",
    "# - If a value is unclear or missing, use \"NOT_FOUND\" instead of guessing\n",
    "\n",
    "# STEP 1:\n",
    "# - Extract the Transaction Table formatted as markdown.\n",
    "\n",
    "# \"\"\"\n",
    "\n",
    "prompt_text = \"\"\"\n",
    "You are an expert document analyzer specializing in bank statement extraction.\n",
    "\n",
    "Step 1\n",
    "  - Extract the Transaction Table formatted as markdown.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0beb9a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7\n",
    "# Create message structure for Llama chat template\n",
    "messageDataStructure = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\"},\n",
    "            {\"type\": \"text\", \"text\": prompt_text},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Process the input\n",
    "textInput = processor.apply_chat_template(\n",
    "    messageDataStructure, add_generation_prompt=True\n",
    ")\n",
    "inputs = processor(image, textInput, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Generate response with deterministic parameters\n",
    "output = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=4000,\n",
    "    do_sample=False,\n",
    "    temperature=None,\n",
    "    top_p=None,\n",
    ")\n",
    "generatedOutput = processor.decode(output[0])\n",
    "\n",
    "# Clean the response to remove chat and markdown artifacts\n",
    "cleanedOutput = clean_llama_response(generatedOutput)\n",
    "cleanedOutput = cleanedOutput.replace(\"\\\"**\", \"\") # remove \"**\" markdown formatting\n",
    "\n",
    "# Clean markdown table: replace empty cells with NOT_FOUND\n",
    "if '|' in cleanedOutput:  # Check if it contains a table\n",
    "    cleanedOutput = clean_markdown_table(cleanedOutput)\n",
    "    print(\"✅ Empty cells replaced with NOT_FOUND\")\n",
    "\n",
    "print(\"✅ Response generated successfully!\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CLEANED EXTRACTION:\")\n",
    "print(\"=\" * 60)\n",
    "print(cleanedOutput)\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Save the cleaned response to a file\n",
    "output_path = Path(\"llama_grouped_bank_statement_output.txt\")\n",
    "\n",
    "with output_path.open(\"w\", encoding=\"utf-8\") as text_file:\n",
    "    text_file.write(cleanedOutput)\n",
    "\n",
    "print(f\"✅ Response saved to: {output_path}\")\n",
    "print(f\"📁 File size: {output_path.stat().st_size} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "id": "lfy15cyglti",
   "source": "# Cell 8\n# Transform markdown table into structured extraction format\nimport re\n\n# Parse markdown table from cleanedOutput\nlines = cleanedOutput.split('\\n')\n\n# Find table rows (skip header and separator lines)\ntable_rows = []\nfound_separator = False\n\nfor line in lines:\n    line = line.strip()\n    if not line or '|' not in line:\n        continue\n    \n    # Skip separator line (| --- | --- |)\n    if re.match(r'^\\|\\s*-+\\s*\\|', line):\n        found_separator = True\n        continue\n    \n    # Only process rows after separator\n    if found_separator:\n        # Parse data row: split by | and clean\n        cells = [cell.strip() for cell in line.split('|')]\n        # Remove empty cells from leading/trailing |\n        cells = [c for c in cells if c]\n        \n        if len(cells) >= 5:  # Date, Description, Withdrawal, Deposit, Balance\n            table_rows.append(cells)\n\nprint(f\"📊 Parsed {len(table_rows)} transaction rows from markdown table\\n\")\n\n# Extract columns (0=Date, 1=Description, 2=Withdrawal)\ndates = [row[0] for row in table_rows]\ndescriptions = [row[1] for row in table_rows]\nwithdrawals = [row[2] for row in table_rows]\n\n# STEP 2: Extract date range\nif dates:\n    first_date = dates[0]\n    last_date = dates[-1]\n    date_range = f\"STATEMENT_DATE_RANGE: [ {first_date} - {last_date} ]\"\nelse:\n    date_range = \"STATEMENT_DATE_RANGE: [ NOT_FOUND ]\"\n\n# STEP 3: Extract all dates\ndates_str = \" | \".join(dates)\ntransaction_dates = f\"TRANSACTION_DATES: [ {dates_str} ]\"\n\n# STEP 4: Extract all descriptions  \ndescriptions_str = \" | \".join(descriptions)\nline_item_descriptions = f\"LINE_ITEM_DESCRIPTIONS: [ {descriptions_str} ]\"\n\n# STEP 5: Extract all withdrawals\nwithdrawals_str = \" | \".join(withdrawals)\ntransaction_amounts = f\"TRANSACTION_AMOUNTS_PAID: [ {withdrawals_str} ]\"\n\n# Display structured output\nprint(\"=\" * 60)\nprint(\"STRUCTURED EXTRACTION OUTPUT:\")\nprint(\"=\" * 60)\nprint(\"DOCUMENT_TYPE: BANK_STATEMENT\\n\")\nprint(date_range)\nprint()\nprint(transaction_dates)\nprint()\nprint(line_item_descriptions)\nprint()\nprint(transaction_amounts)\nprint(\"=\" * 60)\n\n# Save structured output\nstructured_output = f\"\"\"DOCUMENT_TYPE: BANK_STATEMENT\n\n{date_range}\n\n{transaction_dates}\n\n{line_item_descriptions}\n\n{transaction_amounts}\n\"\"\"\n\nstructured_path = Path(\"llama_structured_extraction_output.txt\")\nwith structured_path.open(\"w\", encoding=\"utf-8\") as f:\n    f.write(structured_output)\n\nprint(f\"\\n✅ Structured output saved to: {structured_path}\")\nprint(f\"📁 File size: {structured_path.stat().st_size} bytes\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}