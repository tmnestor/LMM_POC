{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": "#Cell 0: Imports\nimport sys\nfrom pathlib import Path\n\nimport torch\nimport yaml\nfrom PIL import Image\nfrom transformers import AutoProcessor, MllamaForConditionalGeneration\n\n# Add project root to path for imports\nsys.path.insert(0, '/home/jovyan/nfs_share/tod/LMM_POC')\nfrom common.header_mapping import (\n    generate_extraction_instruction,\n    map_headers_to_fields,\n    validate_mapping,\n)\n",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": "#Cell 1: Load both classifier prompts\ndoc_type_prompt_path = Path(\"/home/jovyan/nfs_share/tod/LMM_POC/prompts/document_type_classifier.yaml\")\nheader_prompt_path = Path(\"/home/jovyan/nfs_share/tod/LMM_POC/prompts/bank_statement_structure_classifier_simple.yaml\")\n\nprint(\"üìÑ Loading document type classifier...\")\nwith doc_type_prompt_path.open(\"r\", encoding=\"utf-8\") as f:\n    doc_type_config = yaml.safe_load(f)\nprint(f\"‚úÖ {doc_type_config['name']} v{doc_type_config['version']}\")\n\nprint(\"\\nüìÑ Loading header classifier...\")\nwith header_prompt_path.open(\"r\", encoding=\"utf-8\") as f:\n    header_config = yaml.safe_load(f)\nprint(f\"‚úÖ {header_config['name']} v{header_config['version']}\")\n",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "#Cell 2: Load Llama-3.2-Vision model\n",
    "model_id = \"/home/jovyan/nfs_share/models/Llama-3.2-11B-Vision-Instruct\"\n",
    "\n",
    "print(\"üîß Loading Llama-3.2-Vision model...\")\n",
    "model = MllamaForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "print(\"‚úÖ Model loaded successfully!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "#Cell 3: Load test bank statement image\n",
    "image_path = \"/home/jovyan/nfs_share/tod/LMM_POC/evaluation_data/image_003.png\"\n",
    "\n",
    "print(\"üì∑ Loading bank statement image...\")\n",
    "image = Image.open(image_path)\n",
    "print(f\"‚úÖ Image loaded: {image.size}\")\n",
    "print(f\"üìÅ Image path: {image_path}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": "#Cell 4: STAGE 1 - Document Type Classification\nprint(\"=\"*60)\nprint(\"STAGE 1: DOCUMENT TYPE CLASSIFICATION\")\nprint(\"=\"*60)\n\n# Build document type classification prompt\ndoc_type_prompt = f\"{doc_type_config['instruction']}\\n\\n{doc_type_config['output_format']}\"\n\n# Generate classification\ndoc_type_message = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\"},\n            {\"type\": \"text\", \"text\": doc_type_prompt},\n        ],\n    }\n]\n\nprint(\"ü§ñ Classifying document type...\")\ndoc_type_text_input = processor.apply_chat_template(\n    doc_type_message, add_generation_prompt=True\n)\ndoc_type_inputs = processor(image, doc_type_text_input, return_tensors=\"pt\").to(model.device)\ndoc_type_output = model.generate(**doc_type_inputs, max_new_tokens=100)\ndoc_type_result = processor.decode(doc_type_output[0])\n\n# Extract clean result\nif \"<|start_header_id|>assistant<|end_header_id|>\" in doc_type_result:\n    doc_type_clean = doc_type_result.split(\"<|start_header_id|>assistant<|end_header_id|>\")[1]\n    doc_type_clean = doc_type_clean.replace(\"<|eot_id|>\", \"\").strip()\nelse:\n    doc_type_clean = doc_type_result\n\n# Determine classification\nif \"Mobile_APP\" in doc_type_clean:\n    document_type = \"Mobile_APP\"\nelif \"BANK_STATEMENT\" in doc_type_clean:\n    document_type = \"BANK_STATEMENT\"\nelse:\n    document_type = \"UNKNOWN\"\n\nprint(f\"‚úÖ Document Type: {document_type}\")\nprint(f\"üìù Raw response: {doc_type_clean}\")\n",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": "#Cell 5: STAGE 2 - Conditional Header Extraction\nprint(\"\\n\" + \"=\"*60)\nprint(\"STAGE 2: HEADER EXTRACTION\")\nprint(\"=\"*60)\n\nheaders_pipe_separated = None\n\nif document_type == \"BANK_STATEMENT\":\n    print(\"‚úÖ Bank statement detected - extracting headers...\")\n    \n    # Build header extraction prompt\n    header_prompt = f\"{header_config['instruction']}\\n\\n{header_config['output_format']}\"\n    \n    # Generate header extraction\n    header_message = [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"image\"},\n                {\"type\": \"text\", \"text\": header_prompt},\n            ],\n        }\n    ]\n    \n    print(\"ü§ñ Extracting transaction table headers...\")\n    header_text_input = processor.apply_chat_template(\n        header_message, add_generation_prompt=True\n    )\n    header_inputs = processor(image, header_text_input, return_tensors=\"pt\").to(model.device)\n    header_output = model.generate(**header_inputs, max_new_tokens=2000)\n    header_result = processor.decode(header_output[0])\n    \n    # Extract clean response\n    if \"<|start_header_id|>assistant<|end_header_id|>\" in header_result:\n        header_clean = header_result.split(\"<|start_header_id|>assistant<|end_header_id|>\")[1]\n        header_clean = header_clean.replace(\"<|eot_id|>\", \"\").strip()\n    else:\n        header_clean = header_result\n    \n    # Parse headers from response\n    if \"NO_HEADERS\" in header_clean:\n        headers_pipe_separated = \"NO_HEADERS\"\n    else:\n        # Look for header keywords\n        header_keywords = ['Date', 'Transaction', 'Description', 'Amount', 'Debit', 'Credit', 'Balance', 'Particulars']\n        \n        header_text = None\n        for line in header_clean.split('\\n'):\n            line = line.strip()\n            if not line:\n                continue\n            if ',' in line and any(keyword in line for keyword in header_keywords):\n                header_text = line\n                break\n        \n        if not header_text:\n            lines = [l.strip() for l in header_clean.split('\\n') if l.strip()]\n            if lines:\n                header_text = lines[-1]\n            else:\n                header_text = \"\"\n        \n        # Clean up and convert to pipe-separated\n        header_text = header_text.rstrip('.')\n        headers_list = [h.strip() for h in header_text.split(',') if h.strip()]\n        headers_pipe_separated = \" | \".join(headers_list)\n    \n    print(f\"‚úÖ Headers extracted: {headers_pipe_separated}\")\n    \nelif document_type == \"Mobile_APP\":\n    print(\"‚ÑπÔ∏è  Mobile app detected - skipping header extraction\")\n    headers_pipe_separated = \"N/A\"\nelse:\n    print(\"‚ö†Ô∏è  Unknown document type - skipping header extraction\")\n    headers_pipe_separated = \"UNKNOWN\"\n",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": "#Cell 6: Display Final Results\nprint(\"\\n\" + \"=\"*60)\nprint(\"FINAL CLASSIFICATION RESULTS\")\nprint(\"=\"*60)\nprint(f\"üìÑ Image: {image_path}\")\nprint(f\"üè∑Ô∏è  Document Type: {document_type}\")\nprint(f\"üìã Headers: {headers_pipe_separated}\")\nprint(\"=\"*60)\n\n# Store results for potential saving\nclassification_result = {\n    \"image_path\": str(image_path),\n    \"document_type\": document_type,\n    \"headers\": headers_pipe_separated\n}\n",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": "#Cell 7: Save Classification Results (Optional)\nimport json\n\noutput_dir = Path(\"classification_results\")\noutput_dir.mkdir(exist_ok=True)\n\n# Save as JSON\njson_path = output_dir / \"classification_result.json\"\nwith json_path.open(\"w\", encoding=\"utf-8\") as f:\n    json.dump(classification_result, f, indent=2)\n\nprint(f\"‚úÖ Results saved to: {json_path}\")\n\n# Also save as simple text format\ntext_path = output_dir / \"classification_result.txt\"\nwith text_path.open(\"w\", encoding=\"utf-8\") as f:\n    f.write(f\"Document Type: {document_type}\\n\")\n    f.write(f\"Headers: {headers_pipe_separated}\\n\")\n\nprint(f\"‚úÖ Text results saved to: {text_path}\")\n",
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "#Cell 8: STAGE 3 - Smart Header Mapping (Only if BANK_STATEMENT)\nprint(\"\\n\" + \"=\"*60)\nprint(\"STAGE 3: SMART HEADER MAPPING\")\nprint(\"=\"*60)\n\nif document_type == \"BANK_STATEMENT\" and headers_pipe_separated not in [\"NO_HEADERS\", \"N/A\", \"UNKNOWN\"]:\n    print(\"üß† Mapping headers to semantic fields...\")\n    \n    # Map headers to fields\n    field_mapping = map_headers_to_fields(headers_pipe_separated)\n    \n    print(\"\\nüìã Header Mapping Results:\")\n    for field, column_name in field_mapping.items():\n        status = \"‚úÖ\" if column_name else \"‚ùå\"\n        print(f\"  {status} {field}: {column_name or 'NOT FOUND'}\")\n    \n    # Validate that we have the required fields for extraction\n    is_valid, missing_fields = validate_mapping(field_mapping, required_fields=['DATE', 'DESCRIPTION', 'DEBIT'])\n    \n    if is_valid:\n        print(\"\\n‚úÖ All required fields mapped successfully!\")\n        can_extract = True\n    else:\n        print(f\"\\n‚ö†Ô∏è  WARNING: Missing required fields: {missing_fields}\")\n        print(\"   Extraction will proceed but may be incomplete.\")\n        can_extract = True  # Still try extraction with available fields\nelse:\n    print(\"‚ÑπÔ∏è  Skipping header mapping (not a bank statement or no headers detected)\")\n    field_mapping = None\n    can_extract = False\n",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "#Cell 9: STAGE 4 - Transaction Extraction (Only if mapping successful)\nprint(\"\\n\" + \"=\"*60)\nprint(\"STAGE 4: TRANSACTION EXTRACTION\")\nprint(\"=\"*60)\n\nextracted_transactions = None\n\nif can_extract and field_mapping:\n    print(\"üí∞ Extracting transactions (Date, Description, Debit)...\")\n    \n    # Load extraction template\n    extraction_template_path = Path(\"/home/jovyan/nfs_share/tod/LMM_POC/prompts/transaction_extraction_template.yaml\")\n    with extraction_template_path.open(\"r\", encoding=\"utf-8\") as f:\n        extraction_config = yaml.safe_load(f)\n    \n    print(f\"‚úÖ Loaded: {extraction_config['name']}\")\n    \n    # Generate dynamic instruction using mapped headers\n    extraction_instruction = generate_extraction_instruction(field_mapping, headers_pipe_separated)\n    \n    # Note: Anti-hallucination rules are now embedded in generate_extraction_instruction()\n    # but we can also include them separately if defined in the YAML\n    extraction_output_format = extraction_config['output_format']\n    \n    extraction_prompt = f\"{extraction_instruction}\\n\\n{extraction_output_format}\"\n    \n    print(f\"üìè Prompt length: {len(extraction_prompt)} characters\")\n    \n    # Generate extraction\n    extraction_message = [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"image\"},\n                {\"type\": \"text\", \"text\": extraction_prompt},\n            ],\n        }\n    ]\n    \n    print(\"ü§ñ Generating extraction with Llama-3.2-Vision...\")\n    extraction_text_input = processor.apply_chat_template(\n        extraction_message, add_generation_prompt=True\n    )\n    extraction_inputs = processor(image, extraction_text_input, return_tensors=\"pt\").to(model.device)\n    extraction_output = model.generate(**extraction_inputs, max_new_tokens=3000)\n    extraction_result = processor.decode(extraction_output[0])\n    \n    # Extract clean response\n    if \"<|start_header_id|>assistant<|end_header_id|>\" in extraction_result:\n        extracted_transactions = extraction_result.split(\"<|start_header_id|>assistant<|end_header_id|>\")[1]\n        extracted_transactions = extracted_transactions.replace(\"<|eot_id|>\", \"\").strip()\n    else:\n        extracted_transactions = extraction_result\n    \n    print(\"‚úÖ Extraction complete!\")\n    \nelse:\n    print(\"‚ÑπÔ∏è  Skipping extraction (not applicable for this document type)\")\n",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "#Cell 10: Display and Save Extracted Transactions\nimport json\n\nif extracted_transactions:\n    print(\"\\n\" + \"=\"*60)\n    print(\"EXTRACTED TRANSACTIONS (PIPE-SEPARATED FORMAT)\")\n    print(\"=\"*60)\n    print(extracted_transactions)\n    print(\"=\"*60)\n    \n    # Count transactions (exclude header line)\n    transaction_lines = [line for line in extracted_transactions.split('\\n') if line.strip() and not line.startswith('Date | Description | Debit')]\n    transaction_count = len(transaction_lines)\n    print(f\"\\nüìä Total transactions extracted: {transaction_count}\")\n    \n    # Save to file\n    output_dir = Path(\"classification_results\")\n    output_dir.mkdir(exist_ok=True)\n    \n    # Save as pipe-separated file\n    psv_path = output_dir / \"extracted_transactions.psv\"\n    with psv_path.open(\"w\", encoding=\"utf-8\") as f:\n        f.write(extracted_transactions)\n    print(f\"‚úÖ Pipe-separated file saved to: {psv_path}\")\n    \n    # Save complete results as JSON\n    complete_results = {\n        \"image_path\": str(image_path),\n        \"document_type\": document_type,\n        \"headers_detected\": headers_pipe_separated,\n        \"field_mapping\": field_mapping,\n        \"transaction_count\": transaction_count,\n        \"transactions_psv\": extracted_transactions\n    }\n    \n    json_path = output_dir / \"complete_extraction_results.json\"\n    with json_path.open(\"w\", encoding=\"utf-8\") as f:\n        json.dump(complete_results, f, indent=2)\n    print(f\"‚úÖ Complete results saved to: {json_path}\")\n    \nelse:\n    print(\"\\n\" + \"=\"*60)\n    print(\"NO TRANSACTIONS EXTRACTED\")\n    print(\"=\"*60)\n    print(\"Reason: Document is not a bank statement or extraction was skipped\")\n",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}