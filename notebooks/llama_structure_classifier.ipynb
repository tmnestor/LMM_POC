{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "#Cell 0: Imports\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import yaml\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, MllamaForConditionalGeneration"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": "#Cell 1: Load bank statement structure classifier prompt\n# Try the simpler version first\nprompt_path = Path(\"/home/jovyan/nfs_share/tod/LMM_POC/prompts/bank_statement_structure_classifier_simple.yaml\")\n\nprint(\"üìÑ Loading classification prompt...\")\nwith prompt_path.open(\"r\", encoding=\"utf-8\") as f:\n    classifier_config = yaml.safe_load(f)\n\nprint(f\"‚úÖ Loaded classifier: {classifier_config['name']}\")\nprint(f\"üìã Version: {classifier_config['version']}\")\nprint(f\"üéØ Task: {classifier_config['task']}\")",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "#Cell 2: Load Llama-3.2-Vision model\n",
    "model_id = \"/home/jovyan/nfs_share/models/Llama-3.2-11B-Vision-Instruct\"\n",
    "\n",
    "print(\"üîß Loading Llama-3.2-Vision model...\")\n",
    "model = MllamaForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "print(\"‚úÖ Model loaded successfully!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "#Cell 3: Load test bank statement image\n",
    "image_path = \"/home/jovyan/nfs_share/tod/LMM_POC/evaluation_data/image_003.png\"\n",
    "\n",
    "print(\"üì∑ Loading bank statement image...\")\n",
    "image = Image.open(image_path)\n",
    "print(f\"‚úÖ Image loaded: {image.size}\")\n",
    "print(f\"üìÅ Image path: {image_path}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": "#Cell 4: Build classification prompt from YAML\nclassification_instruction = classifier_config['instruction']\noutput_format = classifier_config['output_format']\n\nfull_prompt = f\"{classification_instruction}\\n\\n{output_format}\"\n\nprint(\"üìù Classification prompt constructed\")\nprint(f\"üìè Prompt length: {len(full_prompt)} characters\")\nprint(\"\\n\" + \"=\"*60)\nprint(\"FULL CLASSIFICATION PROMPT:\")\nprint(\"=\"*60)\nprint(full_prompt)\nprint(\"=\"*60)",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "#Cell 5: Generate classification response\n",
    "messageDataStructure = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\"},\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": full_prompt,\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"ü§ñ Generating classification with Llama-3.2-Vision...\")\n",
    "\n",
    "# Process the input\n",
    "textInput = processor.apply_chat_template(\n",
    "    messageDataStructure, add_generation_prompt=True\n",
    ")\n",
    "inputs = processor(image, textInput, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Generate response\n",
    "output = model.generate(**inputs, max_new_tokens=2000)\n",
    "generatedOutput = processor.decode(output[0])\n",
    "\n",
    "print(\"‚úÖ Classification generated successfully!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": "#Cell 6: Display and classify results\nprint(\"\\n\" + \"=\"*60)\nprint(\"LLAMA-3.2-VISION STRUCTURE CLASSIFICATION:\")\nprint(\"=\"*60)\nprint(generatedOutput)\nprint(\"=\"*60)\n\n# Extract clean response (remove chat template artifacts)\nif \"<|start_header_id|>assistant<|end_header_id|>\" in generatedOutput:\n    clean_response = generatedOutput.split(\"<|start_header_id|>assistant<|end_header_id|>\")[1]\n    clean_response = clean_response.replace(\"<|eot_id|>\", \"\").strip()\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"EXTRACTED HEADERS:\")\n    print(\"=\"*60)\n    print(clean_response)\n    print(\"=\"*60)\n    \n    # Python-based counting and classification\n    if \"NO_HEADERS\" in clean_response:\n        column_count = 0\n        structure_type = \"MOBILE_APP_LIGHT_INLINE\"\n        headers = []\n    else:\n        # Extract the header line\n        for line in clean_response.split('\\n'):\n            if ':' in line and 'TRANSACTION TABLE HEADERS' in line.upper():\n                header_text = line.split(':', 1)[1].strip()\n                break\n        else:\n            # Fallback: use the whole clean response\n            header_text = clean_response.strip()\n        \n        # Split by comma and clean\n        headers = [h.strip() for h in header_text.split(',') if h.strip()]\n        column_count = len(headers)\n        \n        # Classification based on Python count\n        if column_count == 3:\n            structure_type = \"TABLE_3COL_SIMPLE\"\n        elif column_count == 4:\n            structure_type = \"TABLE_4COL_STANDARD\"\n        elif column_count == 5:\n            structure_type = \"TABLE_5COL_STANDARD\"\n        elif column_count >= 6:\n            structure_type = \"TABLE_EXTENDED_LOCATION\"\n        else:\n            structure_type = \"UNKNOWN\"\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"PYTHON-BASED CLASSIFICATION:\")\n    print(\"=\"*60)\n    print(f\"Headers found: {headers}\")\n    print(f\"Column count: {column_count}\")\n    print(f\"Structure type: {structure_type}\")\n    print(\"=\"*60)",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "#Cell 7: Save classification results\n",
    "output_dir = Path(\"classification_results\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save full output\n",
    "output_path = output_dir / \"llama_structure_classification.txt\"\n",
    "with output_path.open(\"w\", encoding=\"utf-8\") as text_file:\n",
    "    text_file.write(generatedOutput)\n",
    "\n",
    "# Save clean output\n",
    "if \"<|start_header_id|>assistant<|end_header_id|>\" in generatedOutput:\n",
    "    clean_output_path = output_dir / \"llama_structure_classification_clean.txt\"\n",
    "    with clean_output_path.open(\"w\", encoding=\"utf-8\") as text_file:\n",
    "        text_file.write(clean_response)\n",
    "    print(f\"‚úÖ Clean response saved to: {clean_output_path}\")\n",
    "\n",
    "print(f\"‚úÖ Full response saved to: {output_path}\")\n",
    "print(f\"üìÑ File size: {output_path.stat().st_size} bytes\")"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}