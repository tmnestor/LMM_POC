{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama 3.2 Vision Multi-Turn Debit Extractor\n",
    "\n",
    "This notebook demonstrates multi-turn conversational extraction using Llama 3.2 Vision.\n",
    "Uses the `chat_with_mllm` pattern for clean, maintainable multi-turn conversations.\n",
    "\n",
    "**Reference**: [Chat with Your Images Using Multimodal LLMs](https://medium.com/data-science/chat-with-your-images-using-multimodal-llms-60af003e8bfa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Add project root to path for common/ imports\nimport sys\nfrom pathlib import Path\nsys.path.insert(0, str(Path.cwd().parent))\n\nimport random\nimport numpy as np\nimport torch\nfrom PIL import Image\nfrom transformers import AutoProcessor, MllamaForConditionalGeneration\nfrom transformers.image_utils import load_image\nfrom tqdm.notebook import tqdm\nfrom IPython.display import display, Markdown"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Random Seed for Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.reproducibility import set_seed\n",
    "set_seed(42)\n",
    "print(\"✅ Random seed set to 42 for reproducibility\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"/home/jovyan/shared_PTM/Llama-3.2-11B-Vision-Instruct\"\n",
    "\n",
    "print(\"🔧 Loading Llama-3.2-Vision model...\")\n",
    "\n",
    "from common.llama_model_loader_robust import load_llama_model_robust\n",
    "\n",
    "model, processor = load_llama_model_robust(\n",
    "    model_path=model_id,\n",
    "    use_quantization=False,\n",
    "    device_map='auto',\n",
    "    max_new_tokens=2000,\n",
    "    torch_dtype='bfloat16',\n",
    "    low_cpu_mem_usage=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Add tie_weights() call\n",
    "try:\n",
    "    model.tie_weights()\n",
    "    print(\"✅ Model weights tied successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ tie_weights() warning: {e}\")\n",
    "\n",
    "print(\"✅ Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define chat_with_mllm Function\n",
    "\n",
    "This function encapsulates the multi-turn conversation pattern, making it easy to have back-and-forth conversations with the model about an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_mllm(model, processor, prompt, images_path=[], do_sample=False, \n",
    "                   temperature=0.1, show_image=False, max_new_tokens=2000, \n",
    "                   messages=[], images=[]):\n",
    "    \"\"\"Chat with Llama vision model in multi-turn conversation mode.\n",
    "    \n",
    "    Args:\n",
    "        model: Loaded Llama vision model\n",
    "        processor: AutoProcessor for the model\n",
    "        prompt: User's text prompt\n",
    "        images_path: Path(s) to image files (string or list)\n",
    "        do_sample: Enable sampling (if True, uses temperature)\n",
    "        temperature: Sampling temperature (default 0.1)\n",
    "        show_image: Display image in notebook (default False)\n",
    "        max_new_tokens: Maximum tokens to generate (default 2000)\n",
    "        messages: Conversation history (empty list for new conversation)\n",
    "        images: Loaded image objects (empty list to load from paths)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (generated_text, updated_messages, images)\n",
    "    \"\"\"\n",
    "    # Ensure list\n",
    "    if not isinstance(images_path, list):\n",
    "        images_path = [images_path]\n",
    "\n",
    "    # Load images\n",
    "    if len(images) == 0 and len(images_path) > 0:\n",
    "        for image_path in tqdm(images_path, desc=\"Loading images\"):\n",
    "            image = load_image(image_path)\n",
    "            images.append(image)\n",
    "            if show_image:\n",
    "                display(image)\n",
    "\n",
    "    # If starting a new conversation about an image\n",
    "    if len(messages) == 0:\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": [\n",
    "                    {\"type\": \"image\"}, \n",
    "                    {\"type\": \"text\", \"text\": prompt}\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "\n",
    "    # If continuing conversation on the image\n",
    "    else:\n",
    "        messages.append({\n",
    "            \"role\": \"user\", \n",
    "            \"content\": [{\"type\": \"text\", \"text\": prompt}]\n",
    "        })\n",
    "\n",
    "    # Process input data\n",
    "    text = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "    inputs = processor(images=images, text=text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Generate response\n",
    "    generation_args = {\"max_new_tokens\": max_new_tokens, \"do_sample\": do_sample}\n",
    "    if do_sample:\n",
    "        generation_args[\"temperature\"] = temperature\n",
    "    else:\n",
    "        generation_args[\"temperature\"] = None\n",
    "        generation_args[\"top_p\"] = None\n",
    "    \n",
    "    generate_ids = model.generate(**inputs, **generation_args)\n",
    "    \n",
    "    # Trim input tokens from output\n",
    "    generate_ids = generate_ids[:, inputs['input_ids'].shape[1]:-1]\n",
    "    generated_texts = processor.decode(generate_ids[0], clean_up_tokenization_spaces=False)\n",
    "\n",
    "    # Append the model's response to the conversation history\n",
    "    messages.append({\n",
    "        \"role\": \"assistant\", \n",
    "        \"content\": [{\"type\": \"text\", \"text\": generated_texts}]\n",
    "    })\n",
    "\n",
    "    return generated_texts, messages, images\n",
    "\n",
    "print(\"✅ chat_with_mllm function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Extraction (Turn 1)\n",
    "\n",
    "Extract all transaction data from the bank statement image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image path\n",
    "imageName = \"/home/jovyan/nfs_share/tod/LMM_POC/evaluation_data/image_008.png\"\n",
    "\n",
    "# Initial extraction prompt\n",
    "prompt = \"\"\"\n",
    "You are an expert document analyser specializing in Date Grouped Australian Bank Statement extraction.\n",
    "Date Grouped Bank Statements are date ordered, with one or more transactions for each date header.\n",
    "Every transaction for a given date heading has a description, a debit/credit amount and finally a balance amount with a ' CR' suffix.\n",
    "Extract all balance amounts along with their ' CR' suffix, the transaction dates (from the date heading) and transaction descriptions,\n",
    "maintaining the same date ordering as the image, with every transaction appearing on its own row and remembering that some date headings have more than one balance.\n",
    "\"\"\"\n",
    "\n",
    "# Initialize conversation\n",
    "messages = []\n",
    "images = []\n",
    "\n",
    "print(\"📸 Processing bank statement image...\")\n",
    "response1, messages, images = chat_with_mllm(\n",
    "    model, processor, prompt,\n",
    "    images_path=[imageName],\n",
    "    do_sample=False,\n",
    "    max_new_tokens=2000,\n",
    "    show_image=True,\n",
    "    messages=messages,\n",
    "    images=images\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TURN 1 - INITIAL EXTRACTION:\")\n",
    "print(\"=\" * 60)\n",
    "print(response1)\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Save initial extraction\n",
    "Path(\"llama_debit_extractor_initial.txt\").write_text(response1)\n",
    "print(\"\\n✅ Initial extraction saved to llama_debit_extractor_initial.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turn 2: Count Transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response2, messages, images = chat_with_mllm(\n",
    "    model, processor,\n",
    "    \"How many transactions are shown in this bank statement?\",\n",
    "    messages=messages, \n",
    "    images=images,\n",
    "    max_new_tokens=500\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TURN 2 - TRANSACTION COUNT:\")\n",
    "print(\"=\" * 60)\n",
    "print(response2)\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turn 3: Extract Debit/Withdrawal Amounts Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response3, messages, images = chat_with_mllm(\n",
    "    model, processor,\n",
    "    \"From your first response, extract ONLY the debit/withdrawal amounts (amounts paid out). List them in order, one per line.\",\n",
    "    messages=messages,\n",
    "    images=images,\n",
    "    max_new_tokens=1000\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TURN 3 - DEBIT AMOUNTS ONLY:\")\n",
    "print(\"=\" * 60)\n",
    "print(response3)\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Save debit amounts\n",
    "Path(\"llama_debit_amounts.txt\").write_text(response3)\n",
    "print(\"\\n✅ Debit amounts saved to llama_debit_amounts.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turn 4: Verify Debit Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response4, messages, images = chat_with_mllm(\n",
    "    model, processor,\n",
    "    \"How many debit/withdrawal transactions did you extract in your previous response?\",\n",
    "    messages=messages,\n",
    "    images=images,\n",
    "    max_new_tokens=500\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TURN 4 - DEBIT COUNT:\")\n",
    "print(\"=\" * 60)\n",
    "print(response4)\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turn 5: Total Debit Amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response5, messages, images = chat_with_mllm(\n",
    "    model, processor,\n",
    "    \"What is the total sum of all debit/withdrawal amounts in this statement?\",\n",
    "    messages=messages,\n",
    "    images=images,\n",
    "    max_new_tokens=500\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TURN 5 - TOTAL DEBITS:\")\n",
    "print(\"=\" * 60)\n",
    "print(response5)\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turn 6: Date Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response6, messages, images = chat_with_mllm(\n",
    "    model, processor,\n",
    "    \"What is the date range covered by this bank statement?\",\n",
    "    messages=messages,\n",
    "    images=images,\n",
    "    max_new_tokens=500\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TURN 6 - DATE RANGE:\")\n",
    "print(\"=\" * 60)\n",
    "print(response6)\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turn 7: Verification - Cross-check First Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response7, messages, images = chat_with_mllm(\n",
    "    model, processor,\n",
    "    \"In your very first response, you extracted all transactions. Can you verify that the debit amounts you listed in turn 3 match the debit amounts from your first extraction?\",\n",
    "    messages=messages,\n",
    "    images=images,\n",
    "    max_new_tokens=1000\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TURN 7 - VERIFICATION:\")\n",
    "print(\"=\" * 60)\n",
    "print(response7)\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug: View Conversation Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🔍 Current conversation structure:\")\n",
    "print(\"=\" * 60)\n",
    "for i, msg in enumerate(messages, 1):\n",
    "    print(f\"\\nMessage {i} ({msg['role']}):\")\n",
    "    for content in msg['content']:\n",
    "        if content['type'] == 'text':\n",
    "            preview = content['text'][:100] + \"...\" if len(content['text']) > 100 else content['text']\n",
    "            print(f\"  [text]: {preview}\")\n",
    "        else:\n",
    "            print(f\"  [{content['type']}]\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n📊 Total messages: {len(messages)}\")\n",
    "print(f\"📊 User messages: {sum(1 for m in messages if m['role'] == 'user')}\")\n",
    "print(f\"📊 Assistant messages: {sum(1 for m in messages if m['role'] == 'assistant')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Full Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the entire conversation to a file\n",
    "output_path = Path(\"llama_multiturn_debit_conversation.txt\")\n",
    "\n",
    "with output_path.open(\"w\", encoding=\"utf-8\") as text_file:\n",
    "    text_file.write(\"=\" * 60 + \"\\n\")\n",
    "    text_file.write(\"MULTI-TURN DEBIT EXTRACTION CONVERSATION\\n\")\n",
    "    text_file.write(\"Llama-3.2-Vision-11B\\n\")\n",
    "    text_file.write(\"=\" * 60 + \"\\n\\n\")\n",
    "    \n",
    "    for i, msg in enumerate(messages, 1):\n",
    "        role = msg[\"role\"].upper()\n",
    "        text_file.write(f\"\\n{'-' * 60}\\n\")\n",
    "        text_file.write(f\"MESSAGE {i} - {role}\\n\")\n",
    "        text_file.write(f\"{'-' * 60}\\n\\n\")\n",
    "        \n",
    "        for content in msg[\"content\"]:\n",
    "            if content[\"type\"] == \"text\":\n",
    "                text_file.write(content[\"text\"] + \"\\n\")\n",
    "            elif content[\"type\"] == \"image\":\n",
    "                text_file.write(\"[IMAGE]\\n\")\n",
    "    \n",
    "    text_file.write(\"\\n\" + \"=\" * 60 + \"\\n\")\n",
    "    text_file.write(f\"Total messages: {len(messages)}\\n\")\n",
    "    text_file.write(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "print(f\"✅ Full conversation saved to: {output_path}\")\n",
    "print(f\"📊 File size: {output_path.stat().st_size} bytes\")\n",
    "print(f\"💬 Total messages in conversation: {len(messages)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrates:\n",
    "- Clean multi-turn conversations using `chat_with_mllm`\n",
    "- Focused debit/withdrawal extraction\n",
    "- Self-verification by referencing previous responses\n",
    "- Easy-to-extend conversation structure\n",
    "\n",
    "**Key advantages over manual approach**:\n",
    "- 3-5 lines per turn (vs ~25 lines)\n",
    "- Automatic message management\n",
    "- No boilerplate code\n",
    "- Easy to add new turns\n",
    "- Less error-prone"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}