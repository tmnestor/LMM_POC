{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ground Truth Preparation - Improved Version\n",
    "\n",
    "**Purpose**: Convert block-level annotation data into field-level ground truth CSV for model evaluation.\n",
    "\n",
    "**Improvements over original**:\n",
    "- Configurable paths (easy to switch between environments)\n",
    "- Comprehensive validation and error reporting\n",
    "- Modular functions for reusability\n",
    "- Better handling of boolean fields\n",
    "- Detailed logging of transformations\n",
    "- Automatic backup of existing ground truth\n",
    "- Summary statistics and quality checks\n",
    "\n",
    "**Data Flow**:\n",
    "1. Load and merge annotation files\n",
    "2. Filter out 'other' annotations and questions\n",
    "3. Apply semantic chunking (group by image and annotator)\n",
    "4. Transform from tall to wide format\n",
    "5. Map annotation fields to standard field names\n",
    "6. Clean and normalize values\n",
    "7. Validate and save ground truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "# Configure pandas display\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "\n",
    "print(\"✅ Imports loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# CONFIGURATION - EDIT THESE PATHS IF NEEDED\n# ============================================================================\n\n# Base paths\nANNOTATIONS_DIR = Path('/efs/shared/annotations')\nOUTPUT_DIR = Path('/efs/shared/PoC_data/evaluation_data')\n\n# Input files\nINPUT_FILES = {\n    'annotator1': ANNOTATIONS_DIR / 'annotator1_block_ids.csv',\n    'layoutlm': ANNOTATIONS_DIR / 'LayoutLM_annotation.csv'\n}\n\n# Output files\nOUTPUT_FILES = {\n    'merged': ANNOTATIONS_DIR / 'annotations_merged_block_ids.csv',\n    'filtered': ANNOTATIONS_DIR / 'annotations_filtered.csv',\n    'grouped': ANNOTATIONS_DIR / 'grouped_annotations_merged_block_ids.csv',\n    'ground_truth': OUTPUT_DIR / 'ground_truth.csv'\n}\n\n# Expected field names (for validation)\nEXPECTED_FIELDS = [\n    'image_name', 'DOCUMENT_TYPE', 'BUSINESS_ABN', 'SUPPLIER_NAME', 'BUSINESS_ADDRESS',\n    'PAYER_NAME', 'PAYER_ADDRESS', 'INVOICE_DATE', 'LINE_ITEM_DESCRIPTIONS',\n    'LINE_ITEM_QUANTITIES', 'LINE_ITEM_PRICES', 'LINE_ITEM_TOTAL_PRICES',\n    'IS_GST_INCLUDED', 'GST_AMOUNT', 'TOTAL_AMOUNT', 'STATEMENT_DATE_RANGE',\n    'TRANSACTION_DATES', 'TRANSACTION_AMOUNTS_PAID', 'TRANSACTION_AMOUNTS_RECEIVED',\n    'ACCOUNT_BALANCE'\n]\n\n# Ensure output directory exists\nOUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n\nprint(f\"✅ Configuration loaded\")\nprint(f\"📂 Annotations directory: {ANNOTATIONS_DIR}\")\nprint(f\"📁 Output directory: {OUTPUT_DIR}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def validate_input_files():\n    \"\"\"Validate that all required input files exist.\"\"\"\n    print(\"🔍 Validating input files...\")\n    missing_files = []\n    \n    for name, path in INPUT_FILES.items():\n        if path.exists():\n            print(f\"  ✅ {name}: {path.name}\")\n        else:\n            print(f\"  ❌ {name}: {path} NOT FOUND\")\n            missing_files.append(name)\n    \n    if missing_files:\n        raise FileNotFoundError(f\"Missing required files: {', '.join(missing_files)}\")\n    \n    print(\"✅ All input files validated\")\n\n\ndef backup_existing_ground_truth():\n    \"\"\"Backup existing ground truth file if it exists.\"\"\"\n    gt_path = OUTPUT_FILES['ground_truth']\n    \n    if gt_path.exists():\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        backup_path = gt_path.parent / f\"ground_truth_backup_{timestamp}.csv\"\n        shutil.copy2(gt_path, backup_path)\n        print(f\"💾 Backed up existing ground truth to: {backup_path.name}\")\n    else:\n        print(\"📝 No existing ground truth to backup\")\n\n\ndef dedupe_join(series, separator=' | '):\n    \"\"\"Join unique values maintaining insertion order.\"\"\"\n    return separator.join(dict.fromkeys(series.astype(str)))\n\n\ndef remove_duplicate_strings(text):\n    \"\"\"Remove duplicate strings separated by ' | ' while preserving order.\"\"\"\n    # Handle NaN and NOT_FOUND values\n    if pd.isna(text):\n        return text\n    if text == 'NOT_FOUND':\n        return text\n    \n    # Convert to string to check if it contains a separator\n    text_str = str(text)\n    if ' | ' not in text_str:\n        return text\n    \n    # Split by separator\n    parts = text_str.split(' | ')\n    \n    # Remove duplicates while preserving order\n    unique_parts = list(dict.fromkeys(parts))\n    \n    # Rejoin with separator\n    return ' | '.join(unique_parts)\n\n\ndef clean_abn(text):\n    \"\"\"Extract and format 11-digit Australian Business Number.\n    \n    Removes common prefixes like 'abn', 'ABN', 'a.b.n.', pipes, colons, etc.\n    and formats as 'XX XXX XXX XXX'.\n    \"\"\"\n    import re\n    \n    # Handle NaN and NOT_FOUND values\n    if pd.isna(text):\n        return text\n    if text == 'NOT_FOUND':\n        return text\n    \n    # Convert to string and normalize\n    text_str = str(text).strip()\n    \n    # Remove common ABN prefixes (case-insensitive)\n    text_str = re.sub(r'\\b(abn|a\\.b\\.n\\.?)\\b', '', text_str, flags=re.IGNORECASE)\n    \n    # Remove pipes, colons, and extra whitespace\n    text_str = text_str.replace('|', '').replace(':', '').strip()\n    \n    # Extract all digits\n    digits = re.sub(r'\\D', '', text_str)\n    \n    # Check if we have exactly 11 digits\n    if len(digits) == 11:\n        # Format as XX XXX XXX XXX\n        return f\"{digits[0:2]} {digits[2:5]} {digits[5:8]} {digits[8:11]}\"\n    elif len(digits) > 0:\n        # Return digits as-is if not exactly 11 (for debugging)\n        return digits\n    else:\n        # No digits found, return NOT_FOUND\n        return 'NOT_FOUND'\n\n\ndef normalize_single_date(text):\n    \"\"\"Normalize single date to DD/MM/YYYY format.\n    \n    Matches extraction_parser.py _normalize_date() function.\n    Handles formats: DD/MM/YY, DD/MM/YYYY, DD mon YYYY, YYYY-MM-DD, etc.\n    Strips timezone info like \"(UTC+10:00)\".\n    \n    For fields with multiple dates separated by ' | ', takes FIRST date only.\n    Use this for: INVOICE_DATE, STATEMENT_DATE_RANGE\n    \"\"\"\n    from dateutil import parser\n    \n    # Handle NaN and NOT_FOUND values\n    if pd.isna(text):\n        return text\n    if text == 'NOT_FOUND':\n        return text\n    \n    text_str = str(text).strip()\n    \n    # Handle multiple dates - take first one\n    if ' | ' in text_str:\n        dates = text_str.split(' | ')\n        # Remove duplicates while preserving order\n        unique_dates = list(dict.fromkeys(dates))\n        # Take the first date\n        text_str = unique_dates[0].strip()\n    \n    try:\n        # Remove timezone info and extra content for cleaner parsing\n        # Strip anything after ( like \"(UTC+10:00)\"\n        clean_str = text_str.split('(')[0].strip()\n        \n        # Parse with dayfirst=True for Australian DD/MM/YYYY preference\n        parsed_date = parser.parse(clean_str, dayfirst=True)\n        \n        # Format as DD/MM/YYYY (matches extraction_parser.py)\n        return parsed_date.strftime('%d/%m/%Y')\n    except (ValueError, parser.ParserError):\n        # If parsing fails, return original\n        return text_str\n\n\ndef normalize_transaction_dates(text):\n    \"\"\"Normalize multiple dates to DD/MM/YYYY format with pipe separator.\n    \n    Matches extraction_parser.py handling of TRANSACTION_DATES.\n    Keeps ALL dates (including duplicates - legitimate repeated transactions).\n    Normalizes each date individually, maintains pipe separation.\n    Use this for: TRANSACTION_DATES\n    \n    IMPORTANT: Does NOT remove duplicate dates - multiple transactions \n    on the same date are legitimate (e.g., two purchases on the same day).\n    \"\"\"\n    from dateutil import parser\n    \n    # Handle NaN and NOT_FOUND values\n    if pd.isna(text):\n        return text\n    if text == 'NOT_FOUND':\n        return text\n    \n    text_str = str(text).strip()\n    \n    # Split by pipe separator\n    if ' | ' in text_str:\n        dates = [d.strip() for d in text_str.split(' | ')]\n    else:\n        # Single date, wrap in list for consistent handling\n        dates = [text_str]\n    \n    # DO NOT remove duplicates - they may be legitimate repeated transactions\n    \n    normalized_dates = []\n    for date_str in dates:\n        try:\n            # Remove timezone info\n            clean_str = date_str.split('(')[0].strip()\n            \n            # Parse with dayfirst=True\n            parsed_date = parser.parse(clean_str, dayfirst=True)\n            \n            # Format as DD/MM/YYYY\n            normalized_dates.append(parsed_date.strftime('%d/%m/%Y'))\n        except (ValueError, parser.ParserError):\n            # If parsing fails, keep original\n            normalized_dates.append(date_str)\n    \n    # Return pipe-separated dates (including duplicates)\n    return ' | '.join(normalized_dates)\n\n\ndef display_dataframe_summary(df, df_name=\"DataFrame\"):\n    \"\"\"Display summary statistics for a DataFrame.\"\"\"\n    print(f\"\\n📊 {df_name} Structure:\")\n    print(\"=\" * 70)\n    print(f\"Shape: {df.shape}\")\n    print(f\"Columns: {list(df.columns)}\")\n    \n    print(f\"\\n📋 {df_name} Head (first 5 rows):\")\n    print(\"=\" * 70)\n    print(df.head().to_string(index=False))\n    \n    print(f\"\\n📈 {df_name} Column Summary:\")\n    print(\"=\" * 70)\n    \n    # Percentage filled (excluding 'NOT_FOUND')\n    for col in df.columns:\n        if col != 'image_name':\n            non_empty = (df[col] != 'NOT_FOUND').sum()\n            total = len(df)\n            pct = (non_empty / total) * 100\n            print(f\"{col:30s}: {non_empty:3d}/{total:3d} filled ({pct:5.1f}%)\")\n\n\nprint(\"✅ Utility functions defined\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Validate Input Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_input_files()\n",
    "backup_existing_ground_truth()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load and Merge Annotation Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"📥 Loading annotation files...\\n\")\n\n# Load annotator1 annotations\nannotations = pd.read_csv(INPUT_FILES['annotator1'])\nprint(f\"  ✅ Loaded annotator1: {annotations.shape[0]} rows\")\n\n# Remove rows with missing annotators\nannotations = annotations.dropna(subset=['annotator'])\nprint(f\"  ✅ Removed rows with missing annotators: {annotations.shape[0]} rows remaining\")\n\n# Load LayoutLM annotation file\nLayoutLM_annotation = pd.read_csv(INPUT_FILES['layoutlm'])\nprint(f\"  ✅ Loaded LayoutLM annotations: {LayoutLM_annotation.shape[0]} rows\")\n\n# Select only required columns from LayoutLM to avoid unnecessary data\nlayout_subset = LayoutLM_annotation[['page_id', 'case_id']].drop_duplicates()\n\n# Perform left join to preserve all annotation data\nmerged_table = pd.merge(\n    annotations,\n    layout_subset,\n    left_on='image_id',\n    right_on='page_id',\n    how='left'\n).drop(columns=['page_id'])\n\nprint(f\"  ✅ Merged annotations with layout data: {merged_table.shape[0]} rows\")\n\n# Reorder columns to put case_id first\ncols = ['case_id'] + [col for col in merged_table.columns if col != 'case_id']\nmerged_table = merged_table[cols]\n\n# Find and report duplicate rows\nduplicates = merged_table[merged_table.duplicated()]\nnum_duplicates = len(duplicates)\nprint(f\"  📋 Number of duplicate rows: {num_duplicates}\")\n\n# Drop duplicate rows\ndf_final = (\n    merged_table\n    .drop_duplicates()\n    .assign(image_name=lambda x: x['case_id'].astype(str) + '_' + x['image_id'].astype(str))\n)\n\n# Keep only required columns\ncols_to_keep = ['image_name'] + [\n    col for col in df_final.columns if col not in ['case_id', 'image_id', 'image_name']\n]\ndf_final = df_final[cols_to_keep]\n\n# Save merged result\ndf_final.to_csv(OUTPUT_FILES['merged'], index=False)\nprint(f\"\\n💾 Saved merged annotations: {OUTPUT_FILES['merged'].name}\")\nprint(f\"   Shape: {df_final.shape}\")\n\n# Load back for verification\nannotations_merged_block_ids = pd.read_csv(OUTPUT_FILES['merged'])\nprint(f\"\\n✅ Verified merged file: {annotations_merged_block_ids.shape[0]} rows\")\nprint(f\"   Sample: {annotations_merged_block_ids.head(3)['image_name'].tolist()}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Filter Out 'Other' and Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🔍 Filtering annotations...\\n\")\n",
    "\n",
    "# Show unique annotators before filtering\n",
    "print(f\"  📋 Unique annotators before filtering: {sorted(annotations_merged_block_ids['annotator'].unique())}\")\n",
    "\n",
    "# Filter out 'other' annotations\n",
    "annotations_filtered = annotations_merged_block_ids[\n",
    "    annotations_merged_block_ids['annotator'] != 'other'\n",
    "]\n",
    "print(f\"  ✅ Removed 'other' annotations: {len(annotations_merged_block_ids) - len(annotations_filtered)} rows removed\")\n",
    "\n",
    "# Filter out question annotations (containing '_q_')\n",
    "annotations_filtered = annotations_filtered[\n",
    "    ~annotations_filtered['annotator'].str.contains('_q_', na=False)\n",
    "]\n",
    "print(f\"  ✅ Removed question annotations: {annotations_filtered.shape[0]} rows remaining\")\n",
    "\n",
    "# Show unique annotators after filtering\n",
    "print(f\"  📋 Unique annotators after filtering: {sorted(annotations_filtered['annotator'].unique())}\")\n",
    "\n",
    "# Save filtered result\n",
    "annotations_filtered.to_csv(OUTPUT_FILES['filtered'], index=False)\n",
    "print(f\"\\n💾 Saved filtered annotations: {OUTPUT_FILES['filtered'].name}\")\n",
    "print(f\"   Shape: {annotations_filtered.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Apply Semantic Chunking (Group by Image and Annotator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"🔄 Applying semantic chunking...\\n\")\n\n# Group and aggregate\ngrouped_df = (\n    annotations_filtered\n    .groupby(['image_name', 'block_ids', 'annotator'])\n    .agg({\n        'words': lambda x: ' '.join(x.astype(str).str.lower()),\n        'pred': dedupe_join\n    })\n    .reset_index(drop=False)\n)\n\nprint(f\"  ✅ Grouped by image_name, block_ids, annotator: {grouped_df.shape[0]} groups\")\n\n# Select columns and drop duplicates\nfinal_df = grouped_df[['image_name', 'block_ids', 'words', 'pred', 'annotator']].drop_duplicates(\n    subset=['image_name', 'block_ids', 'pred', 'annotator'], keep='first'\n)\n\nprint(f\"  ✅ Dropped duplicates: {final_df.shape[0]} unique rows\")\n\n# Save to new CSV\nfinal_df.to_csv(OUTPUT_FILES['grouped'], index=False)\nprint(f\"\\n💾 Saved grouped annotations: {OUTPUT_FILES['grouped'].name}\")\nprint(f\"   Shape: {final_df.shape}\")\nprint(f\"   Sample: {final_df.head(3)[['image_name', 'annotator']].to_dict('records')}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Transform from Tall to Wide Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🔄 Transforming from tall to wide format...\\n\")\n",
    "\n",
    "# Load grouped annotations\n",
    "annotations_filtered = pd.read_csv(OUTPUT_FILES['grouped'])\n",
    "\n",
    "# Group by image_name and annotator, then concatenate words\n",
    "result_df = (\n",
    "    annotations_filtered.groupby(['image_name', 'annotator'])['words']\n",
    "    .apply(lambda x: ' | '.join(x))\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "print(f\"  ✅ Concatenated words by image and annotator: {result_df.shape[0]} rows\")\n",
    "\n",
    "# Transform from \"tall\" to \"wide\"\n",
    "wide_result_df = result_df.pivot(index='image_name', columns='annotator', values='words')\n",
    "wide_result_df = wide_result_df.reset_index()\n",
    "wide_result_df.columns.name = None\n",
    "\n",
    "print(f\"  ✅ Pivoted to wide format: {wide_result_df.shape}\")\n",
    "print(f\"  📋 Columns: {list(wide_result_df.columns)}\")\n",
    "\n",
    "# Replace missing values with \"NOT_FOUND\"\n",
    "wide_result_df = wide_result_df.fillna(\"NOT_FOUND\")\n",
    "\n",
    "# Save intermediate result\n",
    "intermediate_path = OUTPUT_DIR / 'my_ground_truth_intermediate.csv'\n",
    "wide_result_df.to_csv(intermediate_path, index=False)\n",
    "print(f\"\\n💾 Saved intermediate wide format: {intermediate_path.name}\")\n",
    "\n",
    "# Load back for next step\n",
    "transformed_df = pd.read_csv(intermediate_path)\n",
    "print(f\"   Shape: {transformed_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Map Annotation Fields to Standard Field Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"🗺️  Mapping annotation fields to standard names...\\n\")\n\n# First, print available columns for debugging\nprint(f\"  📋 Available columns in transformed_df: {list(transformed_df.columns)}\")\n\n# Column mapping dictionary (one-to-one mappings)\ncolumn_mapping = {\n    \"header_a_pg\": 'DOCUMENT_TYPE',  # Will be normalized to INVOICE/RECEIPT/BANK_STATEMENT\n    \"supplierABN_a_pgs\": 'BUSINESS_ABN',\n    \"supplier_a_pgs\": 'SUPPLIER_NAME',\n    \"address_extra\": 'BUSINESS_ADDRESS',  # Will be duplicated to PAYER_ADDRESS\n    \"payer_a_pgs\": 'PAYER_NAME',\n    \"invDate_a_pgs\": 'INVOICE_DATE',\n    \"desc_a_li\": 'LINE_ITEM_DESCRIPTIONS',\n    \"quantity_a_li\": 'LINE_ITEM_QUANTITIES',\n    \"unit_price_a_li\": 'LINE_ITEM_PRICES',\n    \"total_a_li\": 'LINE_ITEM_TOTAL_PRICES',\n    \"tax_a_pg\": 'GST_AMOUNT',\n    \"total_a_pg\": 'TOTAL_AMOUNT',\n    \"date_a_li\": 'TRANSACTION_DATES',\n    \"due_a_li\": 'TRANSACTION_AMOUNTS_PAID',\n    \"received_a_li\": 'TRANSACTION_AMOUNTS_RECEIVED',\n    \"balance_a_li\": 'ACCOUNT_BALANCE'\n}\n\n# Apply column mapping\ncols_to_keep = [col for col in transformed_df.columns if col in column_mapping]\ntransformed_df = transformed_df[['image_name'] + cols_to_keep]\ntransformed_df = transformed_df.rename(columns=column_mapping)\n\nprint(f\"  ✅ Mapped {len(cols_to_keep)} columns to standard names\")\nprint(f\"  📋 Columns after mapping: {list(transformed_df.columns)}\")\n\n# Normalize DOCUMENT_TYPE to INVOICE/RECEIPT/BANK_STATEMENT\nif 'DOCUMENT_TYPE' in transformed_df.columns:\n    def normalize_document_type(text):\n        \"\"\"Extract and normalize document type.\"\"\"\n        import re\n        if pd.isna(text) or text == 'NOT_FOUND':\n            return 'NOT_FOUND'\n        \n        text_upper = str(text).upper()\n        \n        # Check for BANK STATEMENT patterns\n        if re.search(r'BANK.*STATEMENT|STATEMENT.*BANK|ACCOUNT.*STATEMENT', text_upper):\n            return 'BANK_STATEMENT'\n        # Check for INVOICE patterns\n        elif re.search(r'INVOICE|TAX.*INVOICE', text_upper):\n            return 'INVOICE'\n        # Check for RECEIPT patterns\n        elif re.search(r'RECEIPT|RCPT', text_upper):\n            return 'RECEIPT'\n        else:\n            # Return original if no pattern matched\n            return text\n    \n    transformed_df['DOCUMENT_TYPE'] = transformed_df['DOCUMENT_TYPE'].apply(normalize_document_type)\n    print(f\"  ✅ Normalized DOCUMENT_TYPE to INVOICE/RECEIPT/BANK_STATEMENT\")\n\n# Duplicate BUSINESS_ADDRESS to PAYER_ADDRESS if it exists\nif 'BUSINESS_ADDRESS' in transformed_df.columns:\n    transformed_df['PAYER_ADDRESS'] = transformed_df['BUSINESS_ADDRESS']\n    print(f\"  ✅ Duplicated BUSINESS_ADDRESS to PAYER_ADDRESS\")\n\n# Add STATEMENT_DATE_RANGE as duplicate of INVOICE_DATE if it exists\nif 'INVOICE_DATE' in transformed_df.columns:\n    transformed_df['STATEMENT_DATE_RANGE'] = transformed_df['INVOICE_DATE']\n    print(f\"  ✅ Duplicated INVOICE_DATE to STATEMENT_DATE_RANGE\")\n\n# Derive IS_GST_INCLUDED from GST_AMOUNT\nif 'GST_AMOUNT' in transformed_df.columns:\n    transformed_df['IS_GST_INCLUDED'] = transformed_df['GST_AMOUNT'].apply(\n        lambda x: 'false' if str(x).strip().upper() in ['NOT_FOUND', 'NAN', ''] else 'true'\n    )\n    print(f\"  ✅ Derived IS_GST_INCLUDED from GST_AMOUNT presence\")\nelse:\n    # If GST_AMOUNT column doesn't exist, create it as NOT_FOUND and set IS_GST_INCLUDED to 'false'\n    transformed_df['GST_AMOUNT'] = 'NOT_FOUND'\n    transformed_df['IS_GST_INCLUDED'] = 'false'\n    print(f\"  ⚠️  GST_AMOUNT column not found, created with 'NOT_FOUND' and IS_GST_INCLUDED='false'\")\n\n# Reorder columns to match expected field order\nfinal_columns = ['image_name'] + [\n    col for col in EXPECTED_FIELDS if col in transformed_df.columns and col != 'image_name'\n]\ntransformed_df = transformed_df[final_columns]\n\nprint(f\"\\n  ✅ Reordered columns to match expected format\")\nprint(f\"   Shape: {transformed_df.shape}\")\nprint(f\"   Final columns: {list(transformed_df.columns)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Clean and Normalize Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"🧹 Cleaning and normalizing values...\\n\")\n\n# Clean and format BUSINESS_ABN field\nif 'BUSINESS_ABN' in transformed_df.columns:\n    transformed_df['BUSINESS_ABN'] = transformed_df['BUSINESS_ABN'].apply(clean_abn)\n    print(f\"  ✅ Cleaned and formatted BUSINESS_ABN\")\n    # Show sample of cleaned ABNs\n    sample_abns = transformed_df[transformed_df['BUSINESS_ABN'] != 'NOT_FOUND']['BUSINESS_ABN'].head(3).tolist()\n    if sample_abns:\n        print(f\"     Sample: {sample_abns}\")\n\n# Normalize single date fields (INVOICE_DATE, STATEMENT_DATE_RANGE)\n# Takes first date if multiple dates present\nsingle_date_fields = ['INVOICE_DATE', 'STATEMENT_DATE_RANGE']\nfor field in single_date_fields:\n    if field in transformed_df.columns:\n        transformed_df[field] = transformed_df[field].apply(normalize_single_date)\n        print(f\"  ✅ Normalized {field} to DD/MM/YYYY format (first date only)\")\n        # Show sample\n        sample_dates = transformed_df[transformed_df[field] != 'NOT_FOUND'][field].head(3).tolist()\n        if sample_dates:\n            print(f\"     Sample: {sample_dates}\")\n\n# Normalize TRANSACTION_DATES (keeps ALL dates pipe-separated)\nif 'TRANSACTION_DATES' in transformed_df.columns:\n    transformed_df['TRANSACTION_DATES'] = transformed_df['TRANSACTION_DATES'].apply(normalize_transaction_dates)\n    print(f\"  ✅ Normalized TRANSACTION_DATES to DD/MM/YYYY format (all dates preserved)\")\n    # Show sample\n    sample_dates = transformed_df[transformed_df['TRANSACTION_DATES'] != 'NOT_FOUND']['TRANSACTION_DATES'].head(3).tolist()\n    if sample_dates:\n        print(f\"     Sample: {sample_dates}\")\n\n# Apply deduplication to PAYER_NAME field\nif 'PAYER_NAME' in transformed_df.columns:\n    transformed_df['PAYER_NAME'] = transformed_df['PAYER_NAME'].apply(remove_duplicate_strings)\n    print(f\"  ✅ Applied deduplication to PAYER_NAME\")\n\n# Replace pipe separators with spaces in text fields\ntext_fields = ['DOCUMENT_TYPE', 'PAYER_NAME', 'PAYER_ADDRESS', 'SUPPLIER_NAME', 'BUSINESS_ADDRESS']\nfor field in text_fields:\n    if field in transformed_df.columns:\n        transformed_df[field] = transformed_df[field].astype(str).str.replace(r\"\\s*\\|\\s*\", \" \", regex=True)\n        print(f\"  ✅ Replaced pipe separators in {field}\")\n\nprint(f\"\\n✅ Cleaning complete\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Validate and Save Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🔍 Validating ground truth data...\\n\")\n",
    "\n",
    "# Check for expected fields\n",
    "missing_fields = [field for field in EXPECTED_FIELDS if field not in transformed_df.columns]\n",
    "extra_fields = [field for field in transformed_df.columns if field not in EXPECTED_FIELDS]\n",
    "\n",
    "if missing_fields:\n",
    "    print(f\"  ⚠️  Missing expected fields: {missing_fields}\")\n",
    "else:\n",
    "    print(f\"  ✅ All expected fields present\")\n",
    "\n",
    "if extra_fields:\n",
    "    print(f\"  ⚠️  Extra fields found: {extra_fields}\")\n",
    "else:\n",
    "    print(f\"  ✅ No extra fields\")\n",
    "\n",
    "# Check for duplicate image names\n",
    "duplicates = transformed_df[transformed_df['image_name'].duplicated()]\n",
    "if len(duplicates) > 0:\n",
    "    print(f\"  ⚠️  WARNING: {len(duplicates)} duplicate image names found:\")\n",
    "    print(f\"     {duplicates['image_name'].tolist()}\")\n",
    "else:\n",
    "    print(f\"  ✅ No duplicate image names\")\n",
    "\n",
    "# Save final ground truth\n",
    "transformed_df.to_csv(OUTPUT_FILES['ground_truth'], index=False)\n",
    "print(f\"\\n💾 Saved ground truth: {OUTPUT_FILES['ground_truth']}\")\n",
    "print(f\"   Shape: {transformed_df.shape}\")\n",
    "print(f\"   Columns: {len(transformed_df.columns)}\")\n",
    "\n",
    "# Reload and verify\n",
    "verified_df = pd.read_csv(OUTPUT_FILES['ground_truth'])\n",
    "print(f\"\\n✅ Verified saved file: {verified_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Display Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_dataframe_summary(transformed_df, \"Ground Truth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n🔍 Quality Checks:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Check 1: Field coverage\n",
    "print(\"\\n1. Field Coverage (non-NOT_FOUND values):\")\n",
    "for col in transformed_df.columns:\n",
    "    if col != 'image_name':\n",
    "        non_empty = (transformed_df[col] != 'NOT_FOUND').sum()\n",
    "        pct = (non_empty / len(transformed_df)) * 100\n",
    "        status = \"✅\" if pct > 50 else \"⚠️ \"\n",
    "        print(f\"  {status} {col:30s}: {pct:5.1f}% ({non_empty}/{len(transformed_df)})\")\n",
    "\n",
    "# Check 2: Document type distribution\n",
    "if 'DOCUMENT_TYPE' in transformed_df.columns:\n",
    "    print(\"\\n2. Document Type Distribution:\")\n",
    "    doc_types = transformed_df['DOCUMENT_TYPE'].value_counts()\n",
    "    for doc_type, count in doc_types.items():\n",
    "        pct = (count / len(transformed_df)) * 100\n",
    "        print(f\"  • {doc_type}: {count} ({pct:.1f}%)\")\n",
    "\n",
    "# Check 3: Boolean field validation\n",
    "if 'IS_GST_INCLUDED' in transformed_df.columns:\n",
    "    print(\"\\n3. Boolean Field Validation (IS_GST_INCLUDED):\")\n",
    "    value_counts = transformed_df['IS_GST_INCLUDED'].value_counts()\n",
    "    for value, count in value_counts.items():\n",
    "        print(f\"  • {value}: {count}\")\n",
    "    \n",
    "    # Check for invalid boolean values\n",
    "    invalid = transformed_df[\n",
    "        ~transformed_df['IS_GST_INCLUDED'].isin(['true', 'false', 'NOT_FOUND'])\n",
    "    ]\n",
    "    if len(invalid) > 0:\n",
    "        print(f\"  ⚠️  Invalid boolean values found: {invalid['IS_GST_INCLUDED'].unique()}\")\n",
    "    else:\n",
    "        print(f\"  ✅ All boolean values are valid (true/false/NOT_FOUND)\")\n",
    "\n",
    "print(\"\\n✅ Quality checks complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary\n\nGround truth preparation complete! The final ground truth CSV has been saved to:\n\n**Output file**: `{OUTPUT_FILES['ground_truth']}`\n\n**Next steps**:\n1. Review the quality check results above\n2. Verify field coverage is acceptable for your use case\n3. Use the ground truth CSV for model evaluation\n\n**Intermediate files saved** (for debugging):\n- Merged annotations: `{OUTPUT_FILES['merged']}`\n- Filtered annotations: `{OUTPUT_FILES['filtered']}`\n- Grouped annotations: `{OUTPUT_FILES['grouped']}`\n\n---\n\n## Data Transformations Applied\n\nThe following transformations were applied to `annotator1_block_ids.csv`:\n\n### **1. Data Loading & Merging**\n- Loaded `annotator1_block_ids.csv` as primary annotation source\n- Removed rows with missing annotators\n- Merged with `LayoutLM_annotation.csv` to add `case_id` mapping\n- Created `image_name` as `{case_id}_{image_id}`\n- Removed duplicate rows\n\n### **2. Filtering**\n- Removed annotations where `annotator == 'other'`\n- Removed question annotations (containing `_q_`)\n- Retained only valid annotation types\n\n### **3. Semantic Chunking**\n- Grouped by `image_name`, `block_ids`, and `annotator`\n- Concatenated `words` within each group (lowercase, space-separated)\n- Deduplicated `pred` values using pipe separator\n- Removed duplicate groups\n\n### **4. Data Reshaping**\n- Transformed from \"tall\" format (multiple rows per image) to \"wide\" format (one row per image)\n- Concatenated words by `image_name` and `annotator` using pipe separator\n- Pivoted annotator fields into columns\n- Filled missing values with `\"NOT_FOUND\"`\n\n### **5. Field Mapping & Derivation**\n**Mapped annotation fields to standard names:**\n- `header_a_pg` → `DOCUMENT_TYPE` (normalized to INVOICE/RECEIPT/BANK_STATEMENT)\n- `supplierABN_a_pgs` → `BUSINESS_ABN`\n- `supplier_a_pgs` → `SUPPLIER_NAME`\n- `address_extra` → `BUSINESS_ADDRESS`\n- `payer_a_pgs` → `PAYER_NAME`\n- `invDate_a_pgs` → `INVOICE_DATE`\n- `desc_a_li` → `LINE_ITEM_DESCRIPTIONS`\n- `quantity_a_li` → `LINE_ITEM_QUANTITIES`\n- `unit_price_a_li` → `LINE_ITEM_PRICES`\n- `total_a_li` → `LINE_ITEM_TOTAL_PRICES`\n- `tax_a_pg` → `GST_AMOUNT`\n- `total_a_pg` → `TOTAL_AMOUNT`\n- `date_a_li` → `TRANSACTION_DATES`\n- `due_a_li` → `TRANSACTION_AMOUNTS_PAID`\n- `received_a_li` → `TRANSACTION_AMOUNTS_RECEIVED`\n- `balance_a_li` → `ACCOUNT_BALANCE`\n\n**Derived fields:**\n- `PAYER_ADDRESS` = duplicate of `BUSINESS_ADDRESS`\n- `STATEMENT_DATE_RANGE` = duplicate of `INVOICE_DATE`\n- `IS_GST_INCLUDED` = `\"true\"` if `GST_AMOUNT` has value, else `\"false\"`\n\n### **6. Data Cleaning & Normalization**\n\n**BUSINESS_ABN:**\n- Removed prefixes: `abn`, `ABN`, `a.b.n.`, etc.\n- Removed pipes (`|`), colons (`:`), extra whitespace\n- Extracted 11 digits and formatted as `XX XXX XXX XXX`\n\n**Date Fields - UPDATED to match extraction_parser.py:**\n\n**Single Date Fields (INVOICE_DATE, STATEMENT_DATE_RANGE):**\n- Takes **FIRST date only** if multiple dates present\n- Strips timezone info like `(UTC+10:00)`\n- Strips day names (Monday, Tuesday, Wednesday, etc.) - handled by dateutil\n- Strips ordinal indicators (1st, 2nd, 24th, etc.) - handled by dateutil\n- Standardized to **`DD/MM/YYYY`** format (Australian format)\n- Supported input formats: `DD/MM/YY`, `DD/MM/YYYY`, `DD mon YYYY`, `YYYY-MM-DD`, `Wednesday, 24th August 2022`\n- Uses `dayfirst=True` for Australian date parsing preference\n- Examples:\n  - `26 Apr 2023` → `26/04/2023`\n  - `2023-04-14 11:22 AM (UTC+10:00)` → `14/04/2023`\n  - `Wednesday, 24th August 2022` → `24/08/2022`\n\n**Multi-Date Field (TRANSACTION_DATES):**\n- Keeps **ALL dates** (including duplicates - legitimate repeated transactions)\n- Each date normalized individually to **`DD/MM/YYYY`** format\n- Maintains pipe separation: `DD/MM/YYYY | DD/MM/YYYY`\n- Strips timezone info and day names from each date\n- **DOES NOT remove duplicate dates** - multiple transactions on same date are valid\n- Example: `15/03/2024, 15/03/2024, 20/03/2024` → `15/03/2024 | 15/03/2024 | 20/03/2024`\n\n**DOCUMENT_TYPE:**\n- Normalized using regex patterns:\n  - `BANK.*STATEMENT|STATEMENT.*BANK|ACCOUNT.*STATEMENT` → `BANK_STATEMENT`\n  - `INVOICE|TAX.*INVOICE` → `INVOICE`\n  - `RECEIPT|RCPT` → `RECEIPT`\n\n**PAYER_NAME:**\n- Removed duplicate strings separated by `|`\n\n**Text Fields (DOCUMENT_TYPE, PAYER_NAME, PAYER_ADDRESS, SUPPLIER_NAME, BUSINESS_ADDRESS):**\n- Replaced pipe separators (`|`) with spaces\n\n### **7. Validation**\n- Checked for all expected fields\n- Checked for duplicate image names\n- Verified field coverage percentages\n- Validated boolean field values (IS_GST_INCLUDED)\n\n### **Final Output**\n- 20 standardized fields per image\n- One row per image (identified by `image_name`)\n- Consistent data types and formats\n- Missing values marked as `\"NOT_FOUND\"`\n- **Date format matches extraction_parser.py**: `DD/MM/YYYY` (Australian format)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}