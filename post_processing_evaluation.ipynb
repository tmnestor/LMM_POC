{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Post-Processing Evaluation Notebook\n",
        "\n",
        "**Purpose:** Re-evaluate existing batch processing results using configurable evaluation metrics.\n",
        "\n",
        "**Workflow:**\n",
        "1. Load batch results CSV files (contain extracted field values)\n",
        "2. Load ground truth data\n",
        "3. Re-evaluate using selected metric(s):\n",
        "   - Position-aware F1 (strict position matching)\n",
        "   - Correlation-aware F1 (validates alignment across related fields)\n",
        "   - Position-agnostic F1 (ignores order)\n",
        "4. Generate new CSV files compatible with model_comparison.ipynb\n",
        "\n",
        "**Benefits:**\n",
        "- Test different evaluation approaches without re-running model inference\n",
        "- Compare metric sensitivities\n",
        "- Identify which metric best reflects true extraction quality"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Add project root to path (notebook is in root directory)\n",
        "project_root = Path.cwd()\n",
        "sys.path.insert(0, str(project_root))\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Tuple\n",
        "from rich.console import Console\n",
        "from rich.table import Table\n",
        "from rich.progress import track\n",
        "\n",
        "# Import evaluation utilities\n",
        "from common.evaluation_metrics import (\n",
        "    load_ground_truth,\n",
        "    calculate_field_accuracy_with_method,\n",
        "    calculate_field_accuracy_f1,\n",
        ")\n",
        "from common.config import get_document_type_fields\n",
        "\n",
        "console = Console()\n",
        "console.print(\"[green]âœ… Imports successful[/green]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration\n",
        "\n",
        "**Configure the specific batch result CSV files you want to evaluate.**\n",
        "\n",
        "Specify exact file paths instead of using auto-discovery."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# INPUT CONFIGURATION - EDIT THESE PATHS\n",
        "# ============================================================================\n",
        "\n",
        "# Batch result CSV files to evaluate\n",
        "# Format: {'model_label': 'path/to/batch_results.csv'}\n",
        "# Set path to None or remove entry to skip that model\n",
        "BATCH_RESULT_FILES = {\n",
        "    'llama': 'output/csv/llama_batch_results_20251024_224027.csv',\n",
        "    'internvl3_quantized': 'output/csv/internvl3_batch_results_20251024_224904.csv',\n",
        "    # 'internvl3_non_quantized': 'output/csv/internvl3_non_quantized_batch_results_20251024_224549.csv',\n",
        "}\n",
        "\n",
        "# Ground truth CSV path\n",
        "GROUND_TRUTH_CSV = 'evaluation_data/ground_truth.csv'\n",
        "\n",
        "# Evaluation method selection\n",
        "# Options: 'order_aware_f1', 'f1' (position-agnostic), 'correlation', 'all'\n",
        "EVALUATION_METHOD = 'all'  # Change to specific method or keep 'all' for comparison\n",
        "\n",
        "# Output directory\n",
        "OUTPUT_DIR = project_root / 'output' / 'csv' / 'post_evaluation'\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Timestamp for output files\n",
        "TIMESTAMP = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "# ============================================================================\n",
        "\n",
        "console.print(f\"[cyan]Ground truth CSV: {GROUND_TRUTH_CSV}[/cyan]\")\n",
        "console.print(f\"[cyan]Output directory: {OUTPUT_DIR}[/cyan]\")\n",
        "console.print(f\"[cyan]Evaluation method: {EVALUATION_METHOD}[/cyan]\")\n",
        "console.print(f\"[cyan]Timestamp: {TIMESTAMP}[/cyan]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load Batch Result Files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_batch_results(file_path: str | Path) -> pd.DataFrame:\n",
        "    \"\"\"Load batch results CSV and parse list fields.\"\"\"\n",
        "    file_path = Path(file_path)\n",
        "    \n",
        "    if not file_path.is_absolute():\n",
        "        file_path = project_root / file_path\n",
        "    \n",
        "    if not file_path.exists():\n",
        "        raise FileNotFoundError(f\"Batch result file not found: {file_path}\")\n",
        "    \n",
        "    df = pd.read_csv(file_path, dtype=str)\n",
        "    \n",
        "    # Replace NaN with \"NOT_FOUND\" for consistency\n",
        "    df = df.fillna(\"NOT_FOUND\")\n",
        "    \n",
        "    console.print(f\"[cyan]Loaded {len(df)} rows from {file_path.name}[/cyan]\")\n",
        "    \n",
        "    return df\n",
        "\n",
        "\n",
        "# Load all configured batch results\n",
        "batch_dataframes = {}\n",
        "\n",
        "for model_name, file_path in BATCH_RESULT_FILES.items():\n",
        "    if file_path is None:\n",
        "        console.print(f\"[yellow]Skipping {model_name} (no file path configured)[/yellow]\")\n",
        "        continue\n",
        "    \n",
        "    try:\n",
        "        batch_dataframes[model_name] = load_batch_results(file_path)\n",
        "        console.print(f\"[green]âœ… Loaded {model_name} from {Path(file_path).name}[/green]\")\n",
        "    except FileNotFoundError as e:\n",
        "        console.print(f\"[red]âŒ {e}[/red]\")\n",
        "    except Exception as e:\n",
        "        console.print(f\"[red]âŒ Error loading {model_name}: {e}[/red]\")\n",
        "\n",
        "if not batch_dataframes:\n",
        "    console.print(\"[red]âŒ No batch result files loaded![/red]\")\n",
        "else:\n",
        "    console.print(f\"\\n[green]âœ… Loaded {len(batch_dataframes)} batch result dataframes[/green]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Load Ground Truth Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load ground truth\n",
        "ground_truth_path = Path(GROUND_TRUTH_CSV)\n",
        "if not ground_truth_path.is_absolute():\n",
        "    ground_truth_path = project_root / ground_truth_path\n",
        "\n",
        "if not ground_truth_path.exists():\n",
        "    console.print(f\"[red]âŒ Ground truth file not found: {ground_truth_path}[/red]\")\n",
        "    ground_truth_data = None\n",
        "else:\n",
        "    ground_truth_data = load_ground_truth(\n",
        "        str(ground_truth_path),\n",
        "        show_sample=True,\n",
        "        verbose=True\n",
        "    )\n",
        "    console.print(f\"[green]âœ… Loaded ground truth for {len(ground_truth_data)} images[/green]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Re-Evaluation Engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def re_evaluate_extraction(\n",
        "    extracted_fields: Dict[str, str],\n",
        "    ground_truth: Dict[str, str],\n",
        "    document_type: str,\n",
        "    evaluation_method: str = 'order_aware_f1'\n",
        ") -> Dict:\n",
        "    \"\"\"\n",
        "    Re-evaluate extracted fields against ground truth using specified method.\n",
        "    \n",
        "    Args:\n",
        "        extracted_fields: Dictionary of {field_name: value}\n",
        "        ground_truth: Dictionary of {field_name: value}\n",
        "        document_type: Document type (for field selection)\n",
        "        evaluation_method: 'order_aware_f1', 'f1', or 'correlation'\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary with overall metrics and field-level scores\n",
        "    \"\"\"\n",
        "    # Get document-specific fields\n",
        "    doc_type_normalized = document_type.lower().replace(' ', '_')\n",
        "    fields_to_evaluate = get_document_type_fields(doc_type_normalized)\n",
        "    \n",
        "    field_scores = {}\n",
        "    total_f1 = 0.0\n",
        "    total_precision = 0.0\n",
        "    total_recall = 0.0\n",
        "    fields_evaluated = 0\n",
        "    fields_matched = 0\n",
        "    \n",
        "    for field_name in fields_to_evaluate:\n",
        "        extracted_value = extracted_fields.get(field_name, \"NOT_FOUND\")\n",
        "        ground_truth_value = ground_truth.get(field_name, \"NOT_FOUND\")\n",
        "        \n",
        "        # Skip if both are NOT_FOUND\n",
        "        if extracted_value == \"NOT_FOUND\" and ground_truth_value == \"NOT_FOUND\":\n",
        "            continue\n",
        "        \n",
        "        fields_evaluated += 1\n",
        "        \n",
        "        # Calculate field accuracy with chosen method\n",
        "        try:\n",
        "            metrics = calculate_field_accuracy_with_method(\n",
        "                extracted_value,\n",
        "                ground_truth_value,\n",
        "                field_name,\n",
        "                method=evaluation_method,\n",
        "                debug=False\n",
        "            )\n",
        "        except Exception as e:\n",
        "            console.print(f\"[yellow]âš  Error evaluating {field_name}: {e}[/yellow]\")\n",
        "            metrics = {'f1_score': 0.0, 'precision': 0.0, 'recall': 0.0}\n",
        "        \n",
        "        field_scores[field_name] = metrics\n",
        "        total_f1 += metrics.get('f1_score', 0.0)\n",
        "        total_precision += metrics.get('precision', 0.0)\n",
        "        total_recall += metrics.get('recall', 0.0)\n",
        "        \n",
        "        # Count as matched if F1 > 0.9\n",
        "        if metrics.get('f1_score', 0.0) > 0.9:\n",
        "            fields_matched += 1\n",
        "    \n",
        "    # Calculate overall metrics\n",
        "    overall_accuracy = (total_f1 / fields_evaluated * 100) if fields_evaluated > 0 else 0.0\n",
        "    overall_precision = (total_precision / fields_evaluated) if fields_evaluated > 0 else 0.0\n",
        "    overall_recall = (total_recall / fields_evaluated) if fields_evaluated > 0 else 0.0\n",
        "    \n",
        "    return {\n",
        "        'overall_accuracy': overall_accuracy,\n",
        "        'overall_precision': overall_precision,\n",
        "        'overall_recall': overall_recall,\n",
        "        'fields_evaluated': fields_evaluated,\n",
        "        'fields_matched': fields_matched,\n",
        "        'field_scores': field_scores\n",
        "    }\n",
        "\n",
        "\n",
        "console.print(\"[green]âœ… Re-evaluation function defined[/green]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_batch_with_method(\n",
        "    df: pd.DataFrame,\n",
        "    ground_truth_data: Dict,\n",
        "    method: str,\n",
        "    model_name: str\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Re-evaluate entire batch using specified method.\n",
        "    \n",
        "    Args:\n",
        "        df: Batch results dataframe\n",
        "        ground_truth_data: Ground truth dictionary\n",
        "        method: Evaluation method name\n",
        "        model_name: Model identifier\n",
        "    \n",
        "    Returns:\n",
        "        DataFrame with updated evaluation metrics\n",
        "    \"\"\"\n",
        "    # Create a copy to avoid modifying original\n",
        "    result_df = df.copy()\n",
        "    \n",
        "    # Field columns to extract\n",
        "    field_columns = [\n",
        "        'DOCUMENT_TYPE', 'BUSINESS_ABN', 'SUPPLIER_NAME', 'BUSINESS_ADDRESS',\n",
        "        'PAYER_NAME', 'PAYER_ADDRESS', 'INVOICE_DATE', 'LINE_ITEM_DESCRIPTIONS',\n",
        "        'LINE_ITEM_QUANTITIES', 'LINE_ITEM_PRICES', 'LINE_ITEM_TOTAL_PRICES',\n",
        "        'IS_GST_INCLUDED', 'GST_AMOUNT', 'TOTAL_AMOUNT', 'STATEMENT_DATE_RANGE',\n",
        "        'TRANSACTION_DATES', 'TRANSACTION_AMOUNTS_PAID', 'TRANSACTION_AMOUNTS_RECEIVED',\n",
        "        'ACCOUNT_BALANCE'\n",
        "    ]\n",
        "    \n",
        "    console.print(f\"\\n[bold cyan]Re-evaluating {model_name} with {method}[/bold cyan]\")\n",
        "    \n",
        "    # Process each row\n",
        "    for idx in track(range(len(result_df)), description=f\"Processing {model_name}\"):\n",
        "        row = result_df.iloc[idx]\n",
        "        image_name = row['image_name']\n",
        "        document_type = row.get('document_type', 'INVOICE')\n",
        "        \n",
        "        # Get ground truth for this image\n",
        "        gt = ground_truth_data.get(image_name)\n",
        "        if not gt:\n",
        "            console.print(f\"[yellow]âš  No ground truth for {image_name}[/yellow]\")\n",
        "            continue\n",
        "        \n",
        "        # Extract fields from row\n",
        "        extracted_fields = {}\n",
        "        for field in field_columns:\n",
        "            if field in row.index:\n",
        "                extracted_fields[field] = row[field]\n",
        "        \n",
        "        # Re-evaluate\n",
        "        evaluation = re_evaluate_extraction(\n",
        "            extracted_fields,\n",
        "            gt,\n",
        "            document_type,\n",
        "            evaluation_method=method\n",
        "        )\n",
        "        \n",
        "        # Update metrics in dataframe\n",
        "        result_df.at[idx, 'overall_accuracy'] = f\"{evaluation['overall_accuracy']:.2f}\"\n",
        "        result_df.at[idx, 'fields_matched'] = str(evaluation['fields_matched'])\n",
        "        result_df.at[idx, 'total_fields'] = str(evaluation['fields_evaluated'])\n",
        "    \n",
        "    return result_df\n",
        "\n",
        "\n",
        "console.print(\"[green]âœ… Batch processing function defined[/green]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Execute Re-Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define evaluation methods to run\n",
        "if EVALUATION_METHOD == 'all':\n",
        "    methods_to_run = ['order_aware_f1', 'f1', 'correlation']\n",
        "    method_labels = {\n",
        "        'order_aware_f1': 'Position-Aware F1',\n",
        "        'f1': 'Position-Agnostic F1',\n",
        "        'correlation': 'Correlation-Aware F1'\n",
        "    }\n",
        "else:\n",
        "    methods_to_run = [EVALUATION_METHOD]\n",
        "    method_labels = {EVALUATION_METHOD: EVALUATION_METHOD.replace('_', ' ').title()}\n",
        "\n",
        "console.print(f\"[bold green]Running evaluation with methods: {methods_to_run}[/bold green]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Store results for each model and method\n",
        "evaluated_results = {}\n",
        "\n",
        "if ground_truth_data is None:\n",
        "    console.print(\"[red]âŒ Cannot evaluate without ground truth data[/red]\")\n",
        "else:\n",
        "    for model_name, df in batch_dataframes.items():\n",
        "        evaluated_results[model_name] = {}\n",
        "        \n",
        "        for method in methods_to_run:\n",
        "            console.rule(f\"[bold]{model_name} - {method_labels.get(method, method)}[/bold]\")\n",
        "            \n",
        "            # Re-evaluate with this method\n",
        "            result_df = process_batch_with_method(\n",
        "                df,\n",
        "                ground_truth_data,\n",
        "                method,\n",
        "                model_name\n",
        "            )\n",
        "            \n",
        "            evaluated_results[model_name][method] = result_df\n",
        "            \n",
        "            # Show summary\n",
        "            avg_accuracy = result_df['overall_accuracy'].astype(float).mean()\n",
        "            console.print(f\"[green]âœ… {method_labels.get(method, method)}: Average accuracy = {avg_accuracy:.2f}%[/green]\")\n",
        "\n",
        "console.print(\"\\n[bold green]ðŸŽ‰ Re-evaluation complete![/bold green]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Generate Output CSV Files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save re-evaluated results\n",
        "output_files = {}\n",
        "\n",
        "for model_name, method_results in evaluated_results.items():\n",
        "    output_files[model_name] = {}\n",
        "    \n",
        "    for method, result_df in method_results.items():\n",
        "        # Generate output filename\n",
        "        method_suffix = method.replace('_', '-')\n",
        "        output_filename = f\"{model_name}_{method_suffix}_evaluated_{TIMESTAMP}.csv\"\n",
        "        output_path = OUTPUT_DIR / output_filename\n",
        "        \n",
        "        # Save CSV\n",
        "        result_df.to_csv(output_path, index=False)\n",
        "        output_files[model_name][method] = output_path\n",
        "        \n",
        "        console.print(f\"[green]ðŸ’¾ Saved: {output_filename}[/green]\")\n",
        "\n",
        "console.print(f\"\\n[bold green]âœ… All output files saved to: {OUTPUT_DIR}[/bold green]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Comparison Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compare_evaluation_methods(\n",
        "    model_name: str,\n",
        "    method_results: Dict[str, pd.DataFrame]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Compare accuracy across different evaluation methods for a model.\n",
        "    \"\"\"\n",
        "    comparison_data = []\n",
        "    \n",
        "    for method, df in method_results.items():\n",
        "        accuracies = df['overall_accuracy'].astype(float)\n",
        "        \n",
        "        comparison_data.append({\n",
        "            'Method': method_labels.get(method, method),\n",
        "            'Mean Accuracy': f\"{accuracies.mean():.2f}%\",\n",
        "            'Median Accuracy': f\"{accuracies.median():.2f}%\",\n",
        "            'Std Dev': f\"{accuracies.std():.2f}\",\n",
        "            'Min': f\"{accuracies.min():.2f}%\",\n",
        "            'Max': f\"{accuracies.max():.2f}%\"\n",
        "        })\n",
        "    \n",
        "    return pd.DataFrame(comparison_data)\n",
        "\n",
        "\n",
        "# Generate comparison tables\n",
        "if EVALUATION_METHOD == 'all':\n",
        "    console.print(\"\\n[bold cyan]Evaluation Method Comparison[/bold cyan]\")\n",
        "    \n",
        "    for model_name, method_results in evaluated_results.items():\n",
        "        console.rule(f\"[bold]{model_name}[/bold]\")\n",
        "        \n",
        "        comparison_df = compare_evaluation_methods(model_name, method_results)\n",
        "        \n",
        "        # Display as Rich table\n",
        "        table = Table(title=f\"{model_name} - Method Comparison\")\n",
        "        for col in comparison_df.columns:\n",
        "            table.add_column(col, style=\"cyan\")\n",
        "        \n",
        "        for _, row in comparison_df.iterrows():\n",
        "            table.add_row(*[str(v) for v in row.values])\n",
        "        \n",
        "        console.print(table)\n",
        "        \n",
        "        # Save comparison to CSV\n",
        "        comparison_file = OUTPUT_DIR / f\"{model_name}_method_comparison_{TIMESTAMP}.csv\"\n",
        "        comparison_df.to_csv(comparison_file, index=False)\n",
        "        console.print(f\"[green]ðŸ’¾ Saved comparison: {comparison_file.name}[/green]\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Document Type Breakdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_by_document_type(\n",
        "    model_name: str,\n",
        "    method: str,\n",
        "    df: pd.DataFrame\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Analyze accuracy breakdown by document type.\n",
        "    \"\"\"\n",
        "    if 'document_type' not in df.columns:\n",
        "        return None\n",
        "    \n",
        "    doc_type_stats = []\n",
        "    \n",
        "    for doc_type in df['document_type'].unique():\n",
        "        type_df = df[df['document_type'] == doc_type]\n",
        "        accuracies = type_df['overall_accuracy'].astype(float)\n",
        "        \n",
        "        doc_type_stats.append({\n",
        "            'Document Type': doc_type,\n",
        "            'Count': len(type_df),\n",
        "            'Mean Accuracy': f\"{accuracies.mean():.2f}%\",\n",
        "            'Median Accuracy': f\"{accuracies.median():.2f}%\",\n",
        "            'Std Dev': f\"{accuracies.std():.2f}\"\n",
        "        })\n",
        "    \n",
        "    return pd.DataFrame(doc_type_stats)\n",
        "\n",
        "\n",
        "# Generate document type analysis\n",
        "console.print(\"\\n[bold cyan]Document Type Breakdown[/bold cyan]\")\n",
        "\n",
        "for model_name, method_results in evaluated_results.items():\n",
        "    for method, result_df in method_results.items():\n",
        "        console.rule(f\"[bold]{model_name} - {method_labels.get(method, method)}[/bold]\")\n",
        "        \n",
        "        doctype_df = analyze_by_document_type(model_name, method, result_df)\n",
        "        \n",
        "        if doctype_df is not None:\n",
        "            # Display as Rich table\n",
        "            table = Table(title=f\"{model_name} - {method_labels.get(method, method)}\")\n",
        "            for col in doctype_df.columns:\n",
        "                table.add_column(col, style=\"cyan\")\n",
        "            \n",
        "            for _, row in doctype_df.iterrows():\n",
        "                table.add_row(*[str(v) for v in row.values])\n",
        "            \n",
        "            console.print(table)\n",
        "            console.print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Summary and Next Steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "console.rule(\"[bold green]Summary[/bold green]\")\n",
        "\n",
        "console.print(f\"\\n[cyan]Processed models: {', '.join(evaluated_results.keys())}[/cyan]\")\n",
        "console.print(f\"[cyan]Evaluation methods: {', '.join([method_labels.get(m, m) for m in methods_to_run])}[/cyan]\")\n",
        "console.print(f\"[cyan]Output directory: {OUTPUT_DIR}[/cyan]\")\n",
        "\n",
        "console.print(\"\\n[bold green]Next Steps:[/bold green]\")\n",
        "console.print(\"1. Review the generated CSV files in the output directory\")\n",
        "console.print(\"2. Use these files with model_comparison.ipynb for visualization\")\n",
        "console.print(\"3. Compare how different evaluation metrics affect model rankings\")\n",
        "console.print(\"4. Identify which metric best aligns with true extraction quality\")\n",
        "\n",
        "console.print(\"\\n[bold green]ðŸŽ‰ Post-processing evaluation complete![/bold green]\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
