{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# InternVL3-8B: Independent Single-Turn Bank Statement Extraction\n",
    "\n",
    "**Protocol**: Two independent single-turn prompts + Python parsing/filtering\n",
    "\n",
    "**Key Insight**: Multi-turn conversation degrades accuracy. LLM filtering mixes up rows. Use Python for filtering!\n",
    "\n",
    "**Model**: InternVL3-8B with bfloat16 and multi-GPU split_model() for compatibility\n",
    "\n",
    "---\n",
    "\n",
    "## Complete Workflow\n",
    "\n",
    "```\n",
    "Turn 0: Image + Prompt ‚Üí Headers (fresh context)\n",
    "        ‚Üì (Python pattern matching)\n",
    "Turn 1: Image + Prompt ‚Üí Full Table (fresh context, dynamic markdown example)\n",
    "        ‚Üì (Python parsing + filtering)\n",
    "Schema Fields: TRANSACTION_DATES, LINE_ITEM_DESCRIPTIONS, TRANSACTION_AMOUNTS_PAID\n",
    "```\n",
    "\n",
    "### Pipeline Stages:\n",
    "1. **Turn 0 (LLM)**: Identify column headers from image\n",
    "2. **Pattern Matching (Python)**: Map headers to concepts (Date, Description, Debit, Credit)\n",
    "3. **Turn 1 (LLM)**: Extract full markdown table using **dynamic example** matching detected column structure\n",
    "4. **Python Parsing**: Parse markdown ‚Üí Filter for debits ‚Üí Extract schema fields\n",
    "\n",
    "### Critical Features:\n",
    "- ‚ùå **No Turn 2** - LLM filtering mixes up rows!\n",
    "- ‚úÖ **Python filtering** - Reliable debit/credit separation\n",
    "- ‚úÖ **Dynamic examples** - Adapt to 3/4/5 column formats\n",
    "- ‚úÖ **Markdown teaching** - InternVL3 understands markdown format for alignment\n",
    "- ‚úÖ **Tax accuracy** - Correct Debit/Credit separation critical for identifying purchases\n",
    "\n",
    "### Why This Works:\n",
    "- **Turn 0**: Clean context ‚Üí accurate header identification\n",
    "- **Turn 1**: Clean context + dynamic example ‚Üí accurate table extraction\n",
    "- **Python**: Reliable filtering for debit transactions (what taxpayer PAID)\n",
    "\n",
    "### Model: InternVL3-8B\n",
    "- **bfloat16 precision** with official multi-GPU split_model() pattern\n",
    "- Higher capacity than 2B variant\n",
    "- Strong vision-language capabilities\n",
    "- Simple API with `model.chat()` method\n",
    "- Multi-GPU: First and last LLM layers anchored to GPU 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 1: Imports and Configuration\n\n# Add parent directory to path AND change working directory for config file resolution\nimport sys\nimport os\nfrom pathlib import Path\n\n# Get project root (parent of bank_statement/)\nproject_root = Path.cwd().parent\nsys.path.insert(0, str(project_root))\n\n# Change working directory to project root so config/field_definitions.yaml is found\nos.chdir(project_root)\nprint(f\"üìÅ Working directory: {os.getcwd()}\")\n\nimport random\nimport math\n\nimport numpy as np\nimport torch\nfrom PIL import Image\nfrom transformers import AutoModel, AutoTokenizer, AutoConfig, BitsAndBytesConfig\nimport torchvision.transforms as T\nfrom torchvision.transforms.functional import InterpolationMode\n\n# IPython display for rendering markdown\nfrom IPython.display import display, Markdown\n\n# ============================================================================\n# CONFIGURATION\n# ============================================================================\nCONFIG = {\n    # Model path - update for your environment\n    \"MODEL_PATH\": \"/home/jovyan/nfs_share/models/InternVL3-8B\",\n    \n    # Generation settings\n    \"MAX_NEW_TOKENS\": 4096,\n    \n    # Image processing - V100 optimized\n    \"MAX_TILES\": 14,  # V100 optimized (use 18 for A10G, 36 for H200)\n}\n\nprint(f\"‚úÖ Configuration loaded:\")\nprint(f\"   Model: {CONFIG['MODEL_PATH']}\")\nprint(f\"   Max tiles: {CONFIG['MAX_TILES']}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Random Seed for Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Random seed set to 42 for reproducibility\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Set random seed\n",
    "\n",
    "from common.reproducibility import set_seed\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Loading InternVL3-8B with memory-aware strategy...\n",
      "üñ•Ô∏è  Detected 2 GPU(s)\n",
      "üíæ GPU 0 free memory: 47.8 GB\n",
      "üíæ GPU 1 free memory: 47.8 GB\n",
      "üíæ Total free memory: 95.6 GB\n",
      "‚úÖ Sufficient memory for bfloat16 multi-GPU mode\n",
      "üì• Loading model with official multi-GPU split_model() pattern...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'CONFIG' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 78\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚úÖ Sufficient memory for bfloat16 multi-GPU mode\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     77\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müì• Loading model with official multi-GPU split_model() pattern...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m device_map = split_model(\u001b[43mCONFIG\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mMODEL_PATH\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     79\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33müîÑ Custom device map created with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(device_map)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m entries\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     81\u001b[39m model = AutoModel.from_pretrained(\n\u001b[32m     82\u001b[39m     CONFIG[\u001b[33m'\u001b[39m\u001b[33mMODEL_PATH\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     83\u001b[39m     torch_dtype=torch.bfloat16,\n\u001b[32m   (...)\u001b[39m\u001b[32m     87\u001b[39m     device_map=device_map\n\u001b[32m     88\u001b[39m ).eval()\n",
      "\u001b[31mNameError\u001b[39m: name 'CONFIG' is not defined"
     ]
    }
   ],
   "source": [
    "# Cell 5: Load InternVL3-8B model with memory-aware loading strategy\n",
    "# Supports: Multi-GPU bfloat16 OR Single-GPU 8-bit quantization\n",
    "\n",
    "from transformers import AutoConfig, BitsAndBytesConfig\n",
    "\n",
    "print(\"üîß Loading InternVL3-8B with memory-aware strategy...\")\n",
    "\n",
    "def split_model(model_path):\n",
    "    \"\"\"\n",
    "    Official InternVL3 multi-GPU device mapping function.\n",
    "    \n",
    "    Creates a custom device map that ensures the first and last layers of the\n",
    "    language model stay on the same device (GPU 0) to prevent tensor placement\n",
    "    mismatches during generation.\n",
    "    \n",
    "    From: https://internvl.readthedocs.io/en/latest/internvl3.0/quick_start.html\n",
    "    \"\"\"\n",
    "    device_map = {}\n",
    "    world_size = torch.cuda.device_count()\n",
    "    config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)\n",
    "    num_layers = config.llm_config.num_hidden_layers\n",
    "    \n",
    "    # Since the first GPU will be used for ViT, treat it as half a GPU\n",
    "    num_layers_per_gpu = math.ceil(num_layers / (world_size - 0.5))\n",
    "    num_layers_per_gpu = [num_layers_per_gpu] * world_size\n",
    "    num_layers_per_gpu[0] = math.ceil(num_layers_per_gpu[0] * 0.5)\n",
    "    \n",
    "    layer_cnt = 0\n",
    "    for i, num_layer in enumerate(num_layers_per_gpu):\n",
    "        for _j in range(num_layer):\n",
    "            device_map[f'language_model.model.layers.{layer_cnt}'] = i\n",
    "            layer_cnt += 1\n",
    "    \n",
    "    # Critical components must stay on GPU 0\n",
    "    device_map['vision_model'] = 0\n",
    "    device_map['mlp1'] = 0\n",
    "    device_map['language_model.model.tok_embeddings'] = 0\n",
    "    device_map['language_model.model.embed_tokens'] = 0\n",
    "    device_map['language_model.output'] = 0\n",
    "    device_map['language_model.model.norm'] = 0\n",
    "    device_map['language_model.model.rotary_emb'] = 0\n",
    "    device_map['language_model.lm_head'] = 0\n",
    "    \n",
    "    # CRITICAL: Force last layer back to GPU 0 to prevent device mismatch\n",
    "    device_map[f'language_model.model.layers.{num_layers - 1}'] = 0\n",
    "    \n",
    "    return device_map\n",
    "\n",
    "def get_gpu_free_memory(gpu_id=0):\n",
    "    \"\"\"Get free GPU memory in GB.\"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        return 0\n",
    "    free_mem = torch.cuda.get_device_properties(gpu_id).total_memory - torch.cuda.memory_allocated(gpu_id)\n",
    "    return free_mem / 1e9\n",
    "\n",
    "# Check available GPU memory\n",
    "world_size = torch.cuda.device_count()\n",
    "print(f\"üñ•Ô∏è  Detected {world_size} GPU(s)\")\n",
    "\n",
    "gpu0_free = get_gpu_free_memory(0)\n",
    "gpu1_free = get_gpu_free_memory(1) if world_size > 1 else 0\n",
    "total_free = gpu0_free + gpu1_free\n",
    "\n",
    "print(f\"üíæ GPU 0 free memory: {gpu0_free:.1f} GB\")\n",
    "if world_size > 1:\n",
    "    print(f\"üíæ GPU 1 free memory: {gpu1_free:.1f} GB\")\n",
    "print(f\"üíæ Total free memory: {total_free:.1f} GB\")\n",
    "\n",
    "# Decision: Use bfloat16 multi-GPU if enough memory, otherwise 8-bit quantization\n",
    "# InternVL3-8B needs ~16GB in bfloat16, ~8GB in 8-bit\n",
    "BFLOAT16_REQUIRED = 16.0\n",
    "QUANTIZED_REQUIRED = 10.0\n",
    "\n",
    "if total_free >= BFLOAT16_REQUIRED and world_size > 1:\n",
    "    # Multi-GPU bfloat16 mode\n",
    "    print(f\"‚úÖ Sufficient memory for bfloat16 multi-GPU mode\")\n",
    "    print(\"üì• Loading model with official multi-GPU split_model() pattern...\")\n",
    "    device_map = split_model(CONFIG['MODEL_PATH'])\n",
    "    print(f\"üîÑ Custom device map created with {len(device_map)} entries\")\n",
    "    \n",
    "    model = AutoModel.from_pretrained(\n",
    "        CONFIG['MODEL_PATH'],\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        low_cpu_mem_usage=True,\n",
    "        use_flash_attn=False,  # V100 compatible\n",
    "        trust_remote_code=True,\n",
    "        device_map=device_map\n",
    "    ).eval()\n",
    "    \n",
    "    MODEL_DTYPE = torch.bfloat16\n",
    "    print(\"‚úÖ Loaded in bfloat16 with multi-GPU split\")\n",
    "\n",
    "elif total_free >= QUANTIZED_REQUIRED:\n",
    "    # Single-GPU 8-bit quantization mode (lower memory)\n",
    "    print(f\"‚ö†Ô∏è  Limited memory - using 8-bit quantization on single GPU\")\n",
    "    \n",
    "    # Find GPU with most free memory\n",
    "    if world_size > 1 and gpu1_free > gpu0_free:\n",
    "        target_gpu = 1\n",
    "        print(f\"üì• Loading on GPU 1 (more free memory: {gpu1_free:.1f} GB)\")\n",
    "    else:\n",
    "        target_gpu = 0\n",
    "        print(f\"üì• Loading on GPU 0 ({gpu0_free:.1f} GB free)\")\n",
    "    \n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_8bit=True,\n",
    "        llm_int8_enable_fp32_cpu_offload=False  # Keep on GPU\n",
    "    )\n",
    "    \n",
    "    model = AutoModel.from_pretrained(\n",
    "        CONFIG['MODEL_PATH'],\n",
    "        torch_dtype=torch.float16,\n",
    "        low_cpu_mem_usage=True,\n",
    "        use_flash_attn=False,\n",
    "        trust_remote_code=True,\n",
    "        quantization_config=quantization_config,\n",
    "        device_map={\"\": target_gpu}\n",
    "    ).eval()\n",
    "    \n",
    "    MODEL_DTYPE = torch.float16\n",
    "    print(f\"‚úÖ Loaded in 8-bit quantization on GPU {target_gpu}\")\n",
    "\n",
    "else:\n",
    "    # Not enough memory - fail with helpful message\n",
    "    raise RuntimeError(\n",
    "        f\"‚ùå Insufficient GPU memory!\\n\"\n",
    "        f\"   Available: {total_free:.1f} GB\\n\"\n",
    "        f\"   Required: {QUANTIZED_REQUIRED:.1f} GB (8-bit) or {BFLOAT16_REQUIRED:.1f} GB (bfloat16)\\n\"\n",
    "        f\"   Please free GPU memory by stopping other processes.\"\n",
    "    )\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    CONFIG['MODEL_PATH'],\n",
    "    trust_remote_code=True,\n",
    "    use_fast=False\n",
    ")\n",
    "\n",
    "# Set generation config on model\n",
    "model.config.max_new_tokens = CONFIG['MAX_NEW_TOKENS']\n",
    "\n",
    "# Fix pad_token_id to suppress warnings\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Display model information\n",
    "print(\"‚úÖ Model and tokenizer loaded successfully!\")\n",
    "\n",
    "# GPU memory check\n",
    "if torch.cuda.is_available():\n",
    "    device_count = torch.cuda.device_count()\n",
    "    \n",
    "    if device_count > 1 and hasattr(model, 'hf_device_map'):\n",
    "        # Multi-GPU detailed breakdown\n",
    "        print(f\"üîÑ Multi-GPU Distribution Analysis ({device_count} GPUs):\")\n",
    "        \n",
    "        total_allocated = 0\n",
    "        for gpu_id in range(device_count):\n",
    "            gpu_allocated = torch.cuda.memory_allocated(gpu_id) / 1e9\n",
    "            gpu_capacity = torch.cuda.get_device_properties(gpu_id).total_memory / 1e9\n",
    "            gpu_name = torch.cuda.get_device_name(gpu_id)\n",
    "            \n",
    "            total_allocated += gpu_allocated\n",
    "            usage_pct = (gpu_allocated / gpu_capacity) * 100 if gpu_capacity > 0 else 0\n",
    "            print(f\"   GPU {gpu_id} ({gpu_name}): {gpu_allocated:.1f}GB/{gpu_capacity:.0f}GB ({usage_pct:.1f}%)\")\n",
    "        \n",
    "        from collections import Counter\n",
    "        device_distribution = Counter(model.hf_device_map.values())\n",
    "        print(f\"‚úÖ Model distributed: {dict(device_distribution)}\")\n",
    "    else:\n",
    "        # Single GPU\n",
    "        device = next(model.parameters()).device\n",
    "        gpu_id = device.index if device.index is not None else 0\n",
    "        allocated = torch.cuda.memory_allocated(gpu_id) / 1e9\n",
    "        total = torch.cuda.get_device_properties(gpu_id).total_memory / 1e9\n",
    "        gpu_name = torch.cuda.get_device_name(gpu_id)\n",
    "        usage_pct = (allocated / total) * 100 if total > 0 else 0\n",
    "        print(f\"üìä GPU {gpu_id} ({gpu_name}): {allocated:.1f}GB/{total:.0f}GB ({usage_pct:.1f}%)\")\n",
    "\n",
    "# Model parameters\n",
    "param_count = sum(p.numel() for p in model.parameters())\n",
    "print(f\"üî¢ Model parameters: {param_count:,}\")\n",
    "print(f\"üéØ Data type: {MODEL_DTYPE}\")\n",
    "print(f\"üñ•Ô∏è  Device: {next(model.parameters()).device}\")\n",
    "print(f\"üî≤ Max Tiles: {CONFIG['MAX_TILES']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Load bank statement image\n",
    "# Update this path to your test image\n",
    "# imageName = \"/home/jovyan/nfs_share/tod/LMM_POC/evaluation_data/synthetic_bank_images/cba_amount_balance.png\"\n",
    "# imageName = \"/home/jovyan/nfs_share/tod/LMM_POC/evaluation_data/synthetic_bank_images/cba_date_grouped_cont.png\"\n",
    "# imageName = \"/home/jovyan/nfs_share/tod/LMM_POC/evaluation_data/synthetic_bank_images/cba_debit_credit.png\"\n",
    "# imageName = \"/home/jovyan/nfs_share/tod/LMM_POC/evaluation_data/synthetic_bank_images/cba_highligted.png\"\n",
    "# imageName = \"/home/jovyan/nfs_share/tod/LMM_POC/evaluation_data/synthetic_bank_images/low_contrast_fixed.png\"\n",
    "# imageName = \"/home/jovyan/nfs_share/tod/LMM_POC/evaluation_data/synthetic_bank_images/nab_classic_highligted.png\"\n",
    "# imageName = \"/home/jovyan/nfs_share/tod/LMM_POC/evaluation_data/synthetic_bank_images/westpac_debit_credit.png\"\n",
    "# imageName = \"/home/jovyan/nfs_share/tod/LMM_POC/evaluation_data/synthetic_bank_images/transaction_summary.png\"\n",
    "imageName = \"/home/jovyan/nfs_share/tod/LMM_POC/evaluation_data/cba_date_grouped.png\"\n",
    "\n",
    "\n",
    "# imageName = \"/home/jovyan/nfs_share/tod/LMM_POC/evaluation_data/images/image_009.png\"\n",
    "print(\"üìÅ Loading image...\")\n",
    "image = Image.open(imageName)\n",
    "\n",
    "# CRITICAL: Store as list for multi-turn compatibility\n",
    "images = [image]\n",
    "\n",
    "print(f\"‚úÖ Image loaded: {image.size}\")\n",
    "print(f\"‚úÖ Images list created with {len(images)} image(s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the loaded image for visual verification\n",
    "print(\"üñºÔ∏è  Bank statement image:\")\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Image preprocessing for InternVL3 (Official implementation)\n",
    "\n",
    "# Official InternVL3 image preprocessing (from docs)\n",
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD = (0.229, 0.224, 0.229)\n",
    "\n",
    "def build_transform(input_size):\n",
    "    \"\"\"Build image transformation pipeline with ImageNet normalization.\"\"\"\n",
    "    transform = T.Compose([\n",
    "        T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
    "        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "    ])\n",
    "    return transform\n",
    "\n",
    "def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):\n",
    "    \"\"\"Find the closest aspect ratio from target ratios based on image dimensions.\"\"\"\n",
    "    best_ratio_diff = float('inf')\n",
    "    best_ratio = (1, 1)\n",
    "    area = width * height\n",
    "    for ratio in target_ratios:\n",
    "        target_aspect_ratio = ratio[0] / ratio[1]\n",
    "        ratio_diff = abs(aspect_ratio - target_aspect_ratio)\n",
    "        if ratio_diff < best_ratio_diff:\n",
    "            best_ratio_diff = ratio_diff\n",
    "            best_ratio = ratio\n",
    "        elif ratio_diff == best_ratio_diff:\n",
    "            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\n",
    "                best_ratio = ratio\n",
    "    return best_ratio\n",
    "\n",
    "def dynamic_preprocess(image, min_num=1, max_num=None, image_size=448, use_thumbnail=False):\n",
    "    \"\"\"\n",
    "    Dynamically preprocess image by splitting into tiles based on aspect ratio.\n",
    "    \n",
    "    Args:\n",
    "        image: PIL Image\n",
    "        min_num: Minimum number of tiles\n",
    "        max_num: Maximum number of tiles (from CONFIG['MAX_TILES'])\n",
    "        image_size: Size of each tile (448 for InternVL3)\n",
    "        use_thumbnail: Whether to include thumbnail image\n",
    "    \n",
    "    Returns:\n",
    "        List of PIL Image tiles\n",
    "    \"\"\"\n",
    "    # Use CONFIG if max_num not specified\n",
    "    if max_num is None:\n",
    "        max_num = CONFIG['MAX_TILES']\n",
    "    \n",
    "    orig_width, orig_height = image.size\n",
    "    aspect_ratio = orig_width / orig_height\n",
    "\n",
    "    # Generate target aspect ratios\n",
    "    target_ratios = set(\n",
    "        (i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if\n",
    "        i * j <= max_num and i * j >= min_num)\n",
    "    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n",
    "\n",
    "    # Find best aspect ratio\n",
    "    target_aspect_ratio = find_closest_aspect_ratio(\n",
    "        aspect_ratio, target_ratios, orig_width, orig_height, image_size)\n",
    "\n",
    "    # Calculate target dimensions\n",
    "    target_width = image_size * target_aspect_ratio[0]\n",
    "    target_height = image_size * target_aspect_ratio[1]\n",
    "    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]\n",
    "\n",
    "    # Resize and split into tiles\n",
    "    resized_img = image.resize((target_width, target_height))\n",
    "    processed_images = []\n",
    "    for i in range(blocks):\n",
    "        box = (\n",
    "            (i % (target_width // image_size)) * image_size,\n",
    "            (i // (target_width // image_size)) * image_size,\n",
    "            ((i % (target_width // image_size)) + 1) * image_size,\n",
    "            ((i // (target_width // image_size)) + 1) * image_size\n",
    "        )\n",
    "        split_img = resized_img.crop(box)\n",
    "        processed_images.append(split_img)\n",
    "    \n",
    "    assert len(processed_images) == blocks\n",
    "    \n",
    "    # Add thumbnail if requested\n",
    "    if use_thumbnail and len(processed_images) != 1:\n",
    "        thumbnail_img = image.resize((image_size, image_size))\n",
    "        processed_images.append(thumbnail_img)\n",
    "    \n",
    "    return processed_images\n",
    "\n",
    "def load_image(image_file, input_size=448, max_num=None):\n",
    "    \"\"\"\n",
    "    Load and preprocess image for InternVL3.\n",
    "    \n",
    "    Args:\n",
    "        image_file: Path to image or PIL Image object\n",
    "        input_size: Size of each tile (448 for InternVL3)\n",
    "        max_num: Max number of tiles (uses CONFIG['MAX_TILES'] if None)\n",
    "    \n",
    "    Returns:\n",
    "        pixel_values: Preprocessed tensor ready for model.chat()\n",
    "    \"\"\"\n",
    "    # Use CONFIG if max_num not specified\n",
    "    if max_num is None:\n",
    "        max_num = CONFIG['MAX_TILES']\n",
    "    \n",
    "    # Handle both path string and PIL Image\n",
    "    if isinstance(image_file, str):\n",
    "        image = Image.open(image_file).convert('RGB')\n",
    "    else:\n",
    "        image = image_file\n",
    "    \n",
    "    # Build transform and preprocess\n",
    "    transform = build_transform(input_size=input_size)\n",
    "    images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)\n",
    "    pixel_values = [transform(img) for img in images]\n",
    "    pixel_values = torch.stack(pixel_values)\n",
    "    \n",
    "    return pixel_values\n",
    "\n",
    "print(\"‚úÖ InternVL3 image preprocessing functions defined\")\n",
    "print(f\"üî≤ Using max_num={CONFIG['MAX_TILES']} tiles (from CONFIG)\")\n",
    "print(f\"üí° Image preprocessing: ImageNet normalization + dynamic tiling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bank Statement Extraction Protocol\n",
    "- Turn 0: Identify actual table headers\n",
    "- Turn 1: Extract full table using dynamic markdown example\n",
    "- Python: Parse, filter, and extract schema fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Turn 0 - Identify table headers (prompt)\n",
    "# TURN 0: Identify Table Headers\n",
    "# First, identify the actual column headers used in this specific bank statement\n",
    "\n",
    "prompt = \"\"\"\n",
    "Look at the transaction table in this bank statement image.\n",
    "\n",
    "IMPORTANT STRUCTURAL NOTE:\n",
    "Some bank statements show dates as section headings with multiple transactions underneath.\n",
    "If you see this structure, remember that each transaction needs its explicit date in the final output.\n",
    "\n",
    "What are the exact column header names used in the transaction table?\n",
    "\n",
    "List each column header exactly as it appears, in order from left to right.\n",
    "Do not interpret or rename them - use the EXACT text from the image.\n",
    "\"\"\"\n",
    "\n",
    "# Create message structure for Llama\n",
    "messageDataStructure = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\"},\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": prompt,\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"üí¨ TURN 0: Identifying actual table headers\")\n",
    "print(\"ü§ñ Generating response with Llama-3.2-Vision...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Turn 0 - Execute and parse headers (InternVL3-8B version)\n",
    "\n",
    "print(\"üí¨ TURN 0: Identifying actual table headers\")\n",
    "print(\"ü§ñ Generating response with InternVL3-8B...\")\n",
    "\n",
    "# Load and preprocess image for InternVL3 (uses CONFIG['MAX_TILES'])\n",
    "pixel_values = load_image(imageName, input_size=448)\n",
    "\n",
    "# Move to correct device and dtype (MODEL_DTYPE set in cell-5 based on loading mode)\n",
    "# Vision model is always on GPU 0\n",
    "vision_device = 'cuda:0'\n",
    "pixel_values = pixel_values.to(dtype=MODEL_DTYPE, device=vision_device)\n",
    "\n",
    "# Generate response using chat() method\n",
    "cleanedOutput = model.chat(\n",
    "    tokenizer=tokenizer,\n",
    "    pixel_values=pixel_values,\n",
    "    question=prompt,\n",
    "    generation_config={\n",
    "        \"max_new_tokens\": CONFIG['MAX_NEW_TOKENS'],\n",
    "        \"do_sample\": False  # Greedy decoding for consistency\n",
    "    }\n",
    ")\n",
    "\n",
    "# Clean InternVL3 artifacts:\n",
    "# 1. Remove image markdown placeholder (![...])\n",
    "# 2. Remove markdown code fences (```markdown and ```)\n",
    "lines = cleanedOutput.split(\"\\n\")\n",
    "cleaned_lines = []\n",
    "for line in lines:\n",
    "    stripped = line.strip()\n",
    "    # Skip image markdown, code fences, and empty fence markers\n",
    "    if stripped.startswith(\"![\"):\n",
    "        continue\n",
    "    if stripped in [\"```markdown\", \"```\", \"```md\"]:\n",
    "        continue\n",
    "    cleaned_lines.append(line)\n",
    "\n",
    "cleanedOutput = \"\\n\".join(cleaned_lines)\n",
    "\n",
    "print(\"‚úÖ Response generated successfully!\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TURN 0 - IDENTIFIED TABLE HEADERS:\")\n",
    "print(\"=\" * 60)\n",
    "print(cleanedOutput)\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# CRITICAL: Parse the identified headers for use in subsequent turns\n",
    "# Extract column names from the response\n",
    "header_lines = [line.strip() for line in cleanedOutput.split('\\n') if line.strip()]\n",
    "identified_headers = []\n",
    "\n",
    "# Look for numbered list or bullet points\n",
    "for line in header_lines:\n",
    "    # Remove common list markers\n",
    "    cleaned = line.lstrip('0123456789.-‚Ä¢* ').strip()\n",
    "    \n",
    "    # Strip markdown bold formatting\n",
    "    cleaned = cleaned.replace('**', '').replace('__', '')\n",
    "    \n",
    "    # Skip section headers (lines ending with colon)\n",
    "    if cleaned.endswith(':'):\n",
    "        continue\n",
    "    \n",
    "    # Skip long sentences (likely explanatory text, not headers)\n",
    "    if len(cleaned) > 40:\n",
    "        continue\n",
    "        \n",
    "    if cleaned and len(cleaned) > 2:  # Ignore very short strings\n",
    "        identified_headers.append(cleaned)\n",
    "\n",
    "print(f\"\\nüìã Parsed {len(identified_headers)} column headers:\")\n",
    "for i, header in enumerate(identified_headers, 1):\n",
    "    print(f\"  {i}. '{header}'\")\n",
    "\n",
    "# Store headers for use in subsequent turns\n",
    "table_headers = identified_headers\n",
    "\n",
    "# Save the table headers\n",
    "output_path = Path(\"ivl3_8b_table_headers.txt\")\n",
    "with output_path.open(\"w\", encoding=\"utf-8\") as text_file:\n",
    "    text_file.write(cleanedOutput)\n",
    "\n",
    "print(f\"\\n‚úÖ Table headers saved to: {output_path}\")\n",
    "print(\"üí° These LITERAL header names will be used in Turn 1 prompts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 11: Turn 0 - Execute and parse headers\n",
    "# # Process the input using the CORRECT multi-turn pattern\n",
    "# # Based on: https://medium.com/data-science/chat-with-your-images-using-multimodal-llms-60af003e8bfa\n",
    "\n",
    "# textInput = processor.apply_chat_template(\n",
    "#     messageDataStructure, add_generation_prompt=True\n",
    "# )\n",
    "\n",
    "# # CRITICAL: Use named parameter 'images=' with list\n",
    "# inputs = processor(images=images, text=textInput, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# # Generate response with deterministic parameters\n",
    "# output = model.generate(\n",
    "#     **inputs,\n",
    "#     max_new_tokens=2000,\n",
    "#     do_sample=False,\n",
    "#     temperature=None,\n",
    "#     top_p=None,\n",
    "# )\n",
    "\n",
    "# # CRITICAL: Trim input tokens from output (this is the key to clean responses!)\n",
    "# generate_ids = output[:, inputs['input_ids'].shape[1]:-1]\n",
    "# cleanedOutput = processor.decode(generate_ids[0], clean_up_tokenization_spaces=False)\n",
    "\n",
    "# print(\"‚úÖ Response generated successfully!\")\n",
    "# print(\"\\n\" + \"=\" * 60)\n",
    "# print(\"TURN 0 - IDENTIFIED TABLE HEADERS:\")\n",
    "# print(\"=\" * 60)\n",
    "# print(cleanedOutput)\n",
    "# print(\"=\" * 60)\n",
    "\n",
    "# # CRITICAL: Parse the identified headers for use in subsequent turns\n",
    "# # Extract column names from the response\n",
    "# header_lines = [line.strip() for line in cleanedOutput.split('\\n') if line.strip()]\n",
    "# identified_headers = []\n",
    "\n",
    "# # Look for numbered list or bullet points\n",
    "# for line in header_lines:\n",
    "#     # Remove common list markers\n",
    "#     cleaned = line.lstrip('0123456789.-‚Ä¢* ').strip()\n",
    "    \n",
    "#     # Strip markdown bold formatting\n",
    "#     cleaned = cleaned.replace('**', '').replace('__', '')\n",
    "    \n",
    "#     # Skip section headers (lines ending with colon)\n",
    "#     if cleaned.endswith(':'):\n",
    "#         continue\n",
    "    \n",
    "#     # Skip long sentences (likely explanatory text, not headers)\n",
    "#     if len(cleaned) > 40:\n",
    "#         continue\n",
    "        \n",
    "#     if cleaned and len(cleaned) > 2:  # Ignore very short strings\n",
    "#         identified_headers.append(cleaned)\n",
    "\n",
    "# print(f\"\\nüìã Parsed {len(identified_headers)} column headers:\")\n",
    "# for i, header in enumerate(identified_headers, 1):\n",
    "#     print(f\"  {i}. '{header}'\")\n",
    "\n",
    "# # Store headers for use in subsequent turns\n",
    "# table_headers = identified_headers\n",
    "\n",
    "# # Save the table headers\n",
    "# output_path = Path(\"llama_table_headers.txt\")\n",
    "# with output_path.open(\"w\", encoding=\"utf-8\") as text_file:\n",
    "#     text_file.write(cleanedOutput)\n",
    "\n",
    "# print(f\"\\n‚úÖ Table headers saved to: {output_path}\")\n",
    "# print(\"üí° These LITERAL header names will be used in Turn 1 & 2 prompts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern Matching: Map Generic Concepts to Actual Headers\n",
    "\n",
    "Different bank statements use different column names. Use pattern matching to identify:\n",
    "- Which header represents **Date**\n",
    "- Which header represents **Description/Details**  \n",
    "- Which header represents **Debit/Withdrawal**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Pattern Matching - Map headers to generic columns\n",
    "# Pattern Matching: Map extracted headers to generic concepts\n",
    "# This handles variety in bank statement column naming conventions\n",
    "\n",
    "# Pattern keywords for each concept (in priority order)\n",
    "DATE_PATTERNS = ['date', 'day', 'transaction date', 'trans date']\n",
    "DESCRIPTION_PATTERNS = [\n",
    "    'description', 'details', 'transaction details', 'trans details',\n",
    "    'particulars', 'narrative', 'transaction', 'trans'\n",
    "]\n",
    "DEBIT_PATTERNS = ['debit', 'withdrawal', 'withdrawals', 'paid', 'paid out', 'spent', 'dr']\n",
    "CREDIT_PATTERNS = ['credit', 'deposit', 'deposits', 'received', 'cr']\n",
    "BALANCE_PATTERNS = ['balance', 'bal', 'running balance']\n",
    "\n",
    "# NEW: Pattern for single-column transaction formats (e.g., \"Amount\" instead of separate Debit/Credit)\n",
    "AMOUNT_PATTERNS = ['amount', 'amt', 'value', 'total']\n",
    "\n",
    "def match_header(headers, patterns, fallback=None):\n",
    "    \"\"\"Match a header using pattern keywords.\n",
    "    \n",
    "    Matching strategy:\n",
    "    1. Exact match (case-insensitive)\n",
    "    2. Substring match (only for patterns with length > 2 to avoid false positives)\n",
    "    \"\"\"\n",
    "    headers_lower = [h.lower() for h in headers]\n",
    "    \n",
    "    # Try exact match first\n",
    "    for pattern in patterns:\n",
    "        for i, header_lower in enumerate(headers_lower):\n",
    "            if pattern == header_lower:\n",
    "                return headers[i]\n",
    "    \n",
    "    # Try substring match (only for patterns longer than 2 chars)\n",
    "    for pattern in patterns:\n",
    "        if len(pattern) > 2:  # Avoid false positives like 'cr' matching 'description'\n",
    "            for i, header_lower in enumerate(headers_lower):\n",
    "                if pattern in header_lower:\n",
    "                    return headers[i]\n",
    "    \n",
    "    return fallback\n",
    "\n",
    "# Perform pattern matching on extracted headers\n",
    "date_col = match_header(table_headers, DATE_PATTERNS, fallback=table_headers[0] if table_headers else 'Date')\n",
    "desc_col = match_header(table_headers, DESCRIPTION_PATTERNS, fallback=table_headers[1] if len(table_headers) > 1 else 'Description')\n",
    "\n",
    "# NEW: First try to match a generic \"Amount\" column (for 4-column formats)\n",
    "amount_col = match_header(table_headers, AMOUNT_PATTERNS, fallback=None)\n",
    "\n",
    "# Use amount_col as fallback if no separate debit/credit columns exist\n",
    "# This handles formats like: Date | Description | Amount | Balance\n",
    "debit_col = match_header(table_headers, DEBIT_PATTERNS, fallback=amount_col if amount_col else 'Debit')\n",
    "credit_col = match_header(table_headers, CREDIT_PATTERNS, fallback=amount_col if amount_col else 'Credit')\n",
    "balance_col = match_header(table_headers, BALANCE_PATTERNS, fallback='Balance')\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PATTERN MATCHING RESULTS:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"üìã Extracted Headers: {table_headers}\")\n",
    "print(f\"\\nüîç Mapped Columns:\")\n",
    "print(f\"  Date        ‚Üí '{date_col}'\")\n",
    "print(f\"  Description ‚Üí '{desc_col}'\")\n",
    "print(f\"  Debit       ‚Üí '{debit_col}'\")\n",
    "print(f\"  Credit      ‚Üí '{credit_col}'\")\n",
    "print(f\"  Balance     ‚Üí '{balance_col}'\")\n",
    "if amount_col:\n",
    "    print(f\"\\nüí° Single-column format detected: '{amount_col}' used for both debit and credit\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n‚úÖ These literal column names will be used in Turn 1 and Turn 2\")\n",
    "print(\"üí° Adjust patterns above if matching fails for your bank statement format\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîë Independent Single-Turn Pattern (NOT Multi-Turn Conversation)\n",
    "\n",
    "**CRITICAL INSIGHT**: Multi-turn conversation accumulates context and degrades accuracy.\n",
    "\n",
    "We use **two independent single-turn prompts**, each with fresh context:\n",
    "\n",
    "#### Key Principles:\n",
    "\n",
    "1. **No Conversation History**: Each turn is completely independent\n",
    "2. **Fresh Image Attention**: Each turn processes the image directly\n",
    "3. **No Context Accumulation**: Prevents attention dilution\n",
    "4. **Headers as Parameters**: Turn 0 headers used to generate dynamic examples for Turn 1\n",
    "5. **Python Filtering**: LLM filtering mixes up rows - Python is reliable\n",
    "\n",
    "#### Message Structure for Each Turn:\n",
    "\n",
    "Every turn uses fresh structure:\n",
    "```python\n",
    "messageDataStructure = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\"},\n",
    "            {\"type\": \"text\", \"text\": \"<prompt with dynamic example>\"}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "```\n",
    "\n",
    "**No assistant responses in history. No conversation accumulation.**\n",
    "\n",
    "#### Why This Works Better:\n",
    "\n",
    "- **Turn 0**: Clean context ‚Üí accurate header identification\n",
    "- **Turn 1**: Clean context + dynamic example ‚Üí accurate table extraction  \n",
    "- **Python**: Reliable parsing and filtering (no row mixing!)\n",
    "\n",
    "Each turn has **full attention** on the image, not diluted by conversation history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15: NO conversation history (independent turns)\n",
    "# \n",
    "# CRITICAL: We do NOT use conversation history in this notebook.\n",
    "# Each turn is completely independent with fresh context.\n",
    "#\n",
    "# Why? Multi-turn conversation accumulates context and degrades accuracy:\n",
    "# - Turn 0: ~50 tokens ‚Üí accurate\n",
    "# - Turn 1 with history: ~350 tokens ‚Üí attention diluted ‚Üí less accurate\n",
    "# - Turn 2 with history: ~2000 tokens ‚Üí attention heavily diluted ‚Üí row mixing!\n",
    "#\n",
    "# Instead: \n",
    "# - Turn 0: Fresh context ‚Üí headers\n",
    "# - Turn 1: Fresh context + dynamic example ‚Üí full table\n",
    "# - Python: Parse and filter (no LLM confusion!)\n",
    "\n",
    "print(\"‚úÖ Independent turn approach - NO conversation history\")\n",
    "print(\"üí° Each turn has fresh context with direct image access\")\n",
    "print(\"üêç Python handles all filtering - no LLM row mixing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Column Aware Extraction Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 17: Generate Column Aware Extraction Prompt\n",
    "\n",
    "# Build the header string using LITERAL names from Turn 0\n",
    "header_string = \" | \".join(table_headers)\n",
    "\n",
    "# Build separator row with proper alignment indicators\n",
    "# Date and Description columns: left-aligned (:---)\n",
    "# Debit, Credit, Balance columns: right-aligned (---:)\n",
    "separator_parts = []\n",
    "for h in table_headers:\n",
    "    h_lower = h.lower()\n",
    "    # Right-align numeric columns\n",
    "    if any(keyword in h_lower for keyword in ['debit', 'credit', 'balance', 'amount', 'total']):\n",
    "        separator_parts.append('---:')\n",
    "    else:\n",
    "        # Left-align text columns (Date, Transaction, Description, etc.)\n",
    "        separator_parts.append(':---')\n",
    "\n",
    "separator_row = \" | \".join(separator_parts)\n",
    "\n",
    "# Build dynamic example rows based on detected column structure\n",
    "# CRITICAL: Examples must emphasize correct Debit/Credit column alignment!\n",
    "def build_dynamic_example(headers, date_col, desc_col, debit_col, credit_col, balance_col):\n",
    "    \"\"\"Generate example rows matching detected column structure.\n",
    "    \n",
    "    Emphasizes correct Debit/Credit alignment for tax purposes:\n",
    "    - Debits (purchases/withdrawals) = money OUT = what taxpayer PAID\n",
    "    - Credits (deposits/income) = money IN = NOT purchases\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if we have separate debit/credit columns\n",
    "    has_separate_debit_credit = (debit_col in headers and credit_col in headers and debit_col != credit_col)\n",
    "    \n",
    "    rows = []\n",
    "    \n",
    "    if has_separate_debit_credit:\n",
    "        # 5-column format: Emphasize Debit vs Credit with clear examples\n",
    "        \n",
    "        # Example 1: DEBIT transaction (purchase/withdrawal) - money OUT\n",
    "        row1 = []\n",
    "        for h in headers:\n",
    "            if h == date_col:\n",
    "                row1.append(\"15 Jan\")\n",
    "            elif h == desc_col:\n",
    "                row1.append(\"ATM Withdrawal City Branch\")\n",
    "            elif h == debit_col:\n",
    "                row1.append(\"200.00\")  # Amount in DEBIT column\n",
    "            elif h == credit_col:\n",
    "                row1.append(\"\")  # Credit column EMPTY\n",
    "            elif h == balance_col:\n",
    "                row1.append(\"$1,500.00 CR\")\n",
    "            else:\n",
    "                row1.append(\"\")\n",
    "        rows.append(\" | \".join(row1))\n",
    "        \n",
    "        # Example 2: CREDIT transaction (deposit) - money IN\n",
    "        row2 = []\n",
    "        for h in headers:\n",
    "            if h == date_col:\n",
    "                row2.append(\"16 Jan\")\n",
    "            elif h == desc_col:\n",
    "                row2.append(\"Salary Employer Name Ref 12345\")\n",
    "            elif h == debit_col:\n",
    "                row2.append(\"\")  # Debit column EMPTY\n",
    "            elif h == credit_col:\n",
    "                row2.append(\"3,500.00\")  # Amount in CREDIT column\n",
    "            elif h == balance_col:\n",
    "                row2.append(\"$5,000.00 CR\")\n",
    "            else:\n",
    "                row2.append(\"\")\n",
    "        rows.append(\" | \".join(row2))\n",
    "        \n",
    "        # Example 3: Another DEBIT transaction (purchase)\n",
    "        row3 = []\n",
    "        for h in headers:\n",
    "            if h == date_col:\n",
    "                row3.append(\"17 Jan\")\n",
    "            elif h == desc_col:\n",
    "                row3.append(\"Online Purchase Store Name\")\n",
    "            elif h == debit_col:\n",
    "                row3.append(\"150.00\")  # Amount in DEBIT column\n",
    "            elif h == credit_col:\n",
    "                row3.append(\"\")  # Credit column EMPTY\n",
    "            elif h == balance_col:\n",
    "                row3.append(\"$4,850.00 CR\")\n",
    "            else:\n",
    "                row3.append(\"\")\n",
    "        rows.append(\" | \".join(row3))\n",
    "        \n",
    "    else:\n",
    "        # 4-column format: Single Amount column\n",
    "        row1 = []\n",
    "        for h in headers:\n",
    "            if h == date_col:\n",
    "                row1.append(\"15 Jan\")\n",
    "            elif h == desc_col:\n",
    "                row1.append(\"ATM Withdrawal City Branch\")\n",
    "            elif h == debit_col:  # This is the Amount column\n",
    "                row1.append(\"200.00\")\n",
    "            elif h == balance_col:\n",
    "                row1.append(\"$1,500.00 CR\")\n",
    "            else:\n",
    "                row1.append(\"\")\n",
    "        rows.append(\" | \".join(row1))\n",
    "        \n",
    "        row2 = []\n",
    "        for h in headers:\n",
    "            if h == date_col:\n",
    "                row2.append(\"16 Jan\")\n",
    "            elif h == desc_col:\n",
    "                row2.append(\"Salary Employer Name Ref 12345\")\n",
    "            elif h == debit_col:  # This is the Amount column\n",
    "                row2.append(\"3,500.00\")\n",
    "            elif h == balance_col:\n",
    "                row2.append(\"$5,000.00 CR\")\n",
    "            else:\n",
    "                row2.append(\"\")\n",
    "        rows.append(\" | \".join(row2))\n",
    "    \n",
    "    return rows\n",
    "\n",
    "def build_multiline_rule(headers):\n",
    "    \"\"\"Generate multi-line extraction rule using ACTUAL column structure from Turn 0.\"\"\"\n",
    "    num_cols = len(headers)\n",
    "    \n",
    "    # Find actual Debit, Credit, and Balance columns by name\n",
    "    debit_idx = None\n",
    "    credit_idx = None\n",
    "    balance_idx = None\n",
    "    \n",
    "    for i, header in enumerate(headers):\n",
    "        h_lower = header.lower()\n",
    "        if any(p in h_lower for p in [\"debit\", \"withdrawal\", \"paid\", \"spent\", \"dr\"]):\n",
    "            debit_idx = i\n",
    "        if any(p in h_lower for p in [\"credit\", \"deposit\", \"received\", \"cr\"]):\n",
    "            credit_idx = i\n",
    "        if any(p in h_lower for p in [\"balance\", \"bal\"]):\n",
    "            balance_idx = i\n",
    "    \n",
    "    if debit_idx is None or credit_idx is None:\n",
    "        return \"Multi-line: combine description lines into single row.\"\n",
    "    \n",
    "    def format_aligned_table(rows):\n",
    "        \"\"\"Format rows with properly aligned vertical pipes.\"\"\"\n",
    "        if not rows:\n",
    "            return []\n",
    "        \n",
    "        num_cols = len(rows[0])\n",
    "        \n",
    "        # Calculate max width for each column\n",
    "        col_widths = [0] * num_cols\n",
    "        for row in rows:\n",
    "            for i, val in enumerate(row):\n",
    "                col_widths[i] = max(col_widths[i], len(val))\n",
    "        \n",
    "        # Find last non-empty column index\n",
    "        last_col = 0\n",
    "        for row in rows:\n",
    "            for i, val in enumerate(row):\n",
    "                if val:\n",
    "                    last_col = max(last_col, i)\n",
    "        \n",
    "        # Ensure empty MIDDLE columns have minimum width\n",
    "        for i in range(1, last_col):  # Skip first column, only middle columns\n",
    "            if col_widths[i] == 0:\n",
    "                col_widths[i] = 7\n",
    "        \n",
    "        # Format each row with proper alignment\n",
    "        formatted = []\n",
    "        for row in rows:\n",
    "            # Determine how many columns to include\n",
    "            end_col = last_col + 2 if last_col < len(row) - 1 else last_col + 1\n",
    "            end_col = min(end_col, len(row))\n",
    "            \n",
    "            # Pad each column value to its width\n",
    "            parts = []\n",
    "            for i in range(end_col):\n",
    "                val = row[i] if i < len(row) else \"\"\n",
    "                parts.append(val.ljust(col_widths[i]))\n",
    "            \n",
    "            line = \" | \".join(parts)\n",
    "            \n",
    "            # CRITICAL: If first column is empty, add leading spaces to align pipes\n",
    "            if not row[0]:\n",
    "                line = \" \" * col_widths[0] + \" | \" + \" | \".join(parts[1:])\n",
    "            \n",
    "            formatted.append(line)\n",
    "        \n",
    "        return formatted\n",
    "    \n",
    "    # Create example rows using ACTUAL column positions\n",
    "    # Credit example (amount in credit_idx position)\n",
    "    credit_rows = [[\"\"] * num_cols for _ in range(3)]\n",
    "    credit_rows[0][0] = \"a date\"\n",
    "    credit_rows[0][1] = \"line 1\"\n",
    "    credit_rows[0][credit_idx] = \"85.50\"\n",
    "    # Add Balance column value if it exists\n",
    "    if balance_idx is not None:\n",
    "        credit_rows[0][balance_idx] = \"$1,085.50 CR\"\n",
    "    credit_rows[1][0] = \"\"  # Empty date for continuation\n",
    "    credit_rows[1][1] = \"line 2\"\n",
    "    credit_rows[2][0] = \"a date\"\n",
    "    credit_rows[2][1] = \"line 1 line 2\"\n",
    "    credit_rows[2][credit_idx] = \"85.50\"\n",
    "    # Add Balance column value if it exists\n",
    "    if balance_idx is not None:\n",
    "        credit_rows[2][balance_idx] = \"$1,085.50 CR\"\n",
    "    \n",
    "    # Debit example (amount in debit_idx position)\n",
    "    debit_rows = [[\"\"] * num_cols for _ in range(3)]\n",
    "    debit_rows[0][0] = \"a date\"\n",
    "    debit_rows[0][1] = \"line 1\"\n",
    "    debit_rows[0][debit_idx] = \"150.00\"\n",
    "    # Add Balance column value if it exists\n",
    "    if balance_idx is not None:\n",
    "        debit_rows[0][balance_idx] = \"$850.00 CR\"\n",
    "    debit_rows[1][0] = \"\"  # Empty date for continuation\n",
    "    debit_rows[1][1] = \"line 2\"\n",
    "    debit_rows[2][0] = \"a date\"\n",
    "    debit_rows[2][1] = \"line 1 line 2\"\n",
    "    debit_rows[2][debit_idx] = \"150.00\"\n",
    "    # Add Balance column value if it exists\n",
    "    if balance_idx is not None:\n",
    "        debit_rows[2][balance_idx] = \"$850.00 CR\"\n",
    "    \n",
    "    # Format both examples\n",
    "    credit_fmt = format_aligned_table(credit_rows)\n",
    "    debit_fmt = format_aligned_table(debit_rows)\n",
    "    \n",
    "    # Build rule with LABELED examples using actual header names\n",
    "    rule = f\"\"\"  {headers[credit_idx]} example:\n",
    "       {credit_fmt[0]}\n",
    "       {credit_fmt[1]}\n",
    "    you must extract it as:\n",
    "       {credit_fmt[2]}\n",
    "\n",
    "  {headers[debit_idx]} example:\n",
    "       {debit_fmt[0]}\n",
    "       {debit_fmt[1]}\n",
    "    you must extract it as:\n",
    "       {debit_fmt[2]}\"\"\"\n",
    "    \n",
    "    return rule\n",
    "\n",
    "# Generate dynamic example rows\n",
    "example_rows = build_dynamic_example(table_headers, date_col, desc_col, debit_col, credit_col, balance_col)\n",
    "\n",
    "# Build complete example table with proper alignment\n",
    "example_table = f\"\"\"| {header_string} |\n",
    "| {separator_row} |\n",
    "\"\"\" + \"\\n\".join([f\"| {row} |\" for row in example_rows])\n",
    "\n",
    "# Generate dynamic multi-line rule\n",
    "multiline_rule = build_multiline_rule(table_headers)\n",
    "\n",
    "# OPTION D: STRONGER COLUMN REINFORCEMENT\n",
    "follow_up_prompt = f\"\"\"\n",
    "Extract the transaction table from this bank statement image in markdown format.\n",
    "\n",
    "Example showing the format I want:\n",
    "\n",
    "{example_table}\n",
    "\n",
    "## CRITICAL: COLUMN ALIGNMENT\n",
    "\n",
    "Before extracting ANY row, locate the header row with these column names:\n",
    "{\" | \".join(table_headers)}\n",
    "\n",
    "For EACH transaction, you must check which column the amount appears under:\n",
    "\n",
    "**Step-by-step process:**\n",
    "1. Find the header row\n",
    "2. Look at the transaction row\n",
    "3. Draw an imaginary vertical line from the amount UP to the header\n",
    "4. Read which header the amount aligns with\n",
    "5. Put the amount in that SAME column in your markdown table\n",
    "\n",
    "**Column placement rules:**\n",
    "- Amount aligns with \"{debit_col}\" header ‚Üí put amount in {debit_col} column, leave {credit_col} EMPTY\n",
    "- Amount aligns with \"{credit_col}\" header ‚Üí put amount in {credit_col} column, leave {debit_col} EMPTY\n",
    "\n",
    "**Do NOT guess based on description text. Use visual alignment ONLY.**\n",
    "\n",
    "## OTHER RULES\n",
    "\n",
    "**Multi-line transactions:** Combine description lines into single row:\n",
    "{multiline_rule}\n",
    "\n",
    "**Empty columns:** Leave empty (|  |)\n",
    "\n",
    "**Output:** Markdown table only, no explanations\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Turn 1 Extraction Prompt (Option D):\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n{follow_up_prompt}\")\n",
    "print(\"=\" * 60 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 17.5: Turn 1 - Extract full table (InternVL3-8B version, INDEPENDENT fresh context)\n",
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "def markdown_table_to_html(markdown_text):\n",
    "    \"\"\"Convert markdown table to HTML for reliable Jupyter rendering.\n",
    "\n",
    "    CRITICAL: Markdown tables with empty cells render incorrectly in Jupyter.\n",
    "    HTML tables preserve empty cells and column alignment properly.\n",
    "    \"\"\"\n",
    "    lines = [line.strip() for line in markdown_text.strip().split('\\n') if line.strip()]\n",
    "\n",
    "    # Find table lines (contain pipes)\n",
    "    table_lines = [line for line in lines if '|' in line]\n",
    "    if not table_lines:\n",
    "        return f\"<pre>{markdown_text}</pre>\"\n",
    "\n",
    "    html_parts = ['<table border=\"1\" style=\"border-collapse: collapse;\">']\n",
    "\n",
    "    for i, line in enumerate(table_lines):\n",
    "        # Skip separator rows (contain only pipes, hyphens, colons, spaces)\n",
    "        cleaned = line.replace('|', '').replace('-', '').replace(':', '').replace(' ', '')\n",
    "        if not cleaned:\n",
    "            continue\n",
    "\n",
    "        # Parse cells\n",
    "        cells = [c.strip() for c in line.split('|')]\n",
    "        # Remove leading/trailing empty strings from pipe delimiters\n",
    "        if cells and cells[0] == '':\n",
    "            cells = cells[1:]\n",
    "        if cells and cells[-1] == '':\n",
    "            cells = cells[:-1]\n",
    "\n",
    "        # First non-separator row is header\n",
    "        if i == 0:\n",
    "            html_parts.append('<tr style=\"background-color: #f0f0f0;\">')\n",
    "            for cell in cells:\n",
    "                html_parts.append(f'<th style=\"padding: 8px; text-align: left;\">{cell}</th>')\n",
    "            html_parts.append('</tr>')\n",
    "        else:\n",
    "            html_parts.append('<tr>')\n",
    "            for cell in cells:\n",
    "                html_parts.append(f'<td style=\"padding: 8px;\">{cell}</td>')\n",
    "            html_parts.append('</tr>')\n",
    "\n",
    "    html_parts.append('</table>')\n",
    "    return '\\n'.join(html_parts)\n",
    "\n",
    "print(\"ü§ñ Generating response with InternVL3-8B...\")\n",
    "\n",
    "# CRITICAL: Reload image for fresh context (independent turn, not continuing conversation)\n",
    "# Uses CONFIG['MAX_TILES'] via load_image() default\n",
    "pixel_values = load_image(imageName, input_size=448)\n",
    "\n",
    "# Move to correct device and dtype (MODEL_DTYPE set in cell-5 based on loading mode)\n",
    "# Vision model is always on GPU 0\n",
    "pixel_values = pixel_values.to(dtype=MODEL_DTYPE, device='cuda:0')\n",
    "\n",
    "# Generate response using chat() method with dynamic prompt\n",
    "cleanedOutput2 = model.chat(\n",
    "    tokenizer=tokenizer,\n",
    "    pixel_values=pixel_values,\n",
    "    question=follow_up_prompt,  # Dynamic prompt with column-aware examples from Cell 19\n",
    "    generation_config={\n",
    "        \"max_new_tokens\": CONFIG['MAX_NEW_TOKENS'],\n",
    "        \"do_sample\": False\n",
    "    }\n",
    ")\n",
    "\n",
    "# Clean InternVL3 artifacts:\n",
    "# 1. Remove image markdown placeholder (![...])\n",
    "# 2. Remove markdown code fences (```markdown and ```)\n",
    "lines = cleanedOutput2.split(\"\\n\")\n",
    "cleaned_lines = []\n",
    "for line in lines:\n",
    "    stripped = line.strip()\n",
    "    # Skip image markdown, code fences, and empty fence markers\n",
    "    if stripped.startswith(\"![\"):\n",
    "        continue\n",
    "    if stripped in [\"```markdown\", \"```\", \"```md\"]:\n",
    "        continue\n",
    "    cleaned_lines.append(line)\n",
    "\n",
    "cleanedOutput2 = \"\\n\".join(cleaned_lines)\n",
    "\n",
    "print(\"\\n‚úÖ Turn 1 extraction complete!\")\n",
    "\n",
    "# Display the extracted table as HTML for reliable rendering with empty cells\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TURN 1 - EXTRACTED TABLE (HTML Rendered):\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Render as HTML table (fixes empty cell rendering issues with Markdown)\n",
    "html_table = markdown_table_to_html(cleanedOutput2)\n",
    "display(HTML(html_table))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TURN 1 - RAW MARKDOWN TEXT:\")\n",
    "print(\"=\" * 60)\n",
    "print(cleanedOutput2)\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Save the markdown table\n",
    "output_path = Path(\"ivl3_8b_markdown_table_extraction.txt\")\n",
    "with output_path.open(\"w\", encoding=\"utf-8\") as text_file:\n",
    "    text_file.write(cleanedOutput2)\n",
    "\n",
    "print(f\"\\n‚úÖ Markdown table saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Parsing and Filtering\n",
    "\n",
    "Parse the Turn 1 markdown table, filter for debit transactions, and extract schema fields using Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 19: Parse Turn 1 markdown table and filter for debits (Python)\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "def parse_markdown_table(markdown_text):\n",
    "    \"\"\"Parse markdown table into list of dictionaries.\n",
    "    \n",
    "    CRITICAL: Must preserve empty columns for correct Debit/Credit alignment!\n",
    "    \"\"\"\n",
    "    lines = [line.strip() for line in markdown_text.strip().split('\\n') if line.strip()]\n",
    "    \n",
    "    # Find header row (first line with pipes)\n",
    "    header_idx = None\n",
    "    for i, line in enumerate(lines):\n",
    "        if '|' in line:\n",
    "            # Skip separator rows (contain only pipes, hyphens, and spaces)\n",
    "            cleaned = line.replace('|', '').replace('-', '').replace(' ', '')\n",
    "            if cleaned:  # Has actual content, not just separators\n",
    "                header_idx = i\n",
    "                break\n",
    "    \n",
    "    if header_idx is None:\n",
    "        return []\n",
    "    \n",
    "    # Parse headers - KEEP empty values to preserve column positions\n",
    "    header_line = lines[header_idx]\n",
    "    header_parts = [h.strip() for h in header_line.split('|')]\n",
    "    # Remove leading/trailing empty strings from pipe delimiters\n",
    "    if header_parts and header_parts[0] == '':\n",
    "        header_parts = header_parts[1:]\n",
    "    if header_parts and header_parts[-1] == '':\n",
    "        header_parts = header_parts[:-1]\n",
    "    # Filter out any remaining empty headers\n",
    "    headers = [h for h in header_parts if h]\n",
    "    \n",
    "    print(f\"üîç Debug: Parsed {len(headers)} headers: {headers}\")\n",
    "    \n",
    "    # Parse data rows (skip header and separator)\n",
    "    rows = []\n",
    "    for idx, line in enumerate(lines[header_idx + 1:], start=header_idx+1):\n",
    "        if '|' not in line:\n",
    "            continue\n",
    "            \n",
    "        # Skip separator rows\n",
    "        cleaned = line.replace(\"|\", \"\").replace(\"-\", \"\").replace(\" \", \"\").replace(\":\", \"\")\n",
    "        if not cleaned:\n",
    "            continue\n",
    "        \n",
    "        # Parse values - KEEP empty values to preserve column positions!\n",
    "        value_parts = [v.strip() for v in line.split('|')]\n",
    "        # Remove leading/trailing empty strings from pipe delimiters\n",
    "        if value_parts and value_parts[0] == '':\n",
    "            value_parts = value_parts[1:]\n",
    "        if value_parts and value_parts[-1] == '':\n",
    "            value_parts = value_parts[:-1]\n",
    "        \n",
    "        print(f\"üîç Debug row {idx}: {len(value_parts)} values: {value_parts}\")\n",
    "        \n",
    "        # Match to headers length\n",
    "        if len(value_parts) == len(headers):\n",
    "            rows.append(dict(zip(headers, value_parts)))\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  Row {idx} mismatch: {len(value_parts)} values vs {len(headers)} headers - SKIPPED\")\n",
    "    \n",
    "    return rows\n",
    "\n",
    "def filter_debit_transactions(rows, debit_col):\n",
    "    \"\"\"Filter rows to only those with debit (purchase) amounts.\n",
    "    \n",
    "    CRITICAL: For tax purposes, we only want transactions where taxpayer PAID money (debits).\n",
    "    \"\"\"\n",
    "    debit_rows = []\n",
    "    for row in rows:\n",
    "        debit_value = row.get(debit_col, '').strip()\n",
    "        # Include row if debit column has a value (not empty)\n",
    "        if debit_value:\n",
    "            debit_rows.append(row)\n",
    "    \n",
    "    return debit_rows\n",
    "\n",
    "def extract_schema_fields(rows, date_col, desc_col, debit_col):\n",
    "    \"\"\"Extract fields in universal.yaml schema format.\"\"\"\n",
    "    if not rows:\n",
    "        return {\n",
    "            'TRANSACTION_DATES': 'NOT_FOUND',\n",
    "            'LINE_ITEM_DESCRIPTIONS': 'NOT_FOUND',\n",
    "            'TRANSACTION_AMOUNTS_PAID': 'NOT_FOUND',\n",
    "            'STATEMENT_DATE_RANGE': 'NOT_FOUND'\n",
    "        }\n",
    "    \n",
    "    # Extract lists\n",
    "    dates = []\n",
    "    descriptions = []\n",
    "    amounts = []\n",
    "    \n",
    "    for row in rows:\n",
    "        date = row.get(date_col, '').strip()\n",
    "        desc = row.get(desc_col, '').strip()\n",
    "        amount = row.get(debit_col, '').strip()\n",
    "        \n",
    "        if date:\n",
    "            dates.append(date)\n",
    "        if desc:\n",
    "            descriptions.append(desc)\n",
    "        if amount:\n",
    "            amounts.append(amount)\n",
    "    \n",
    "    # Calculate statement date range - use literal date format from image\n",
    "    # No parsing, no year assumption - just \"earliest date - latest date\"\n",
    "    date_range = 'NOT_FOUND'\n",
    "    if dates:\n",
    "        # Use first and last date as-is (same format as in the image)\n",
    "        date_range = f\"{dates[0]} - {dates[-1]}\"\n",
    "    \n",
    "    return {\n",
    "        'TRANSACTION_DATES': ' | '.join(dates) if dates else 'NOT_FOUND',\n",
    "        'LINE_ITEM_DESCRIPTIONS': ' | '.join(descriptions) if descriptions else 'NOT_FOUND',\n",
    "        'TRANSACTION_AMOUNTS_PAID': ' | '.join(amounts) if amounts else 'NOT_FOUND',\n",
    "        'STATEMENT_DATE_RANGE': date_range\n",
    "    }\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PARSING TURN 1 MARKDOWN TABLE:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Parse the full markdown table from Turn 1\n",
    "all_rows = parse_markdown_table(cleanedOutput2)\n",
    "\n",
    "print(f\"\\nüìä Parsed {len(all_rows)} total transactions from Turn 1 markdown table\")\n",
    "\n",
    "if all_rows:\n",
    "    # Show sample parsed row\n",
    "    print(f\"\\nüîç Sample parsed row:\")\n",
    "    for key, value in all_rows[0].items():\n",
    "        print(f\"  {key}: '{value}'\")\n",
    "\n",
    "# Filter to only debit (purchase) transactions - Python filtering, not LLM!\n",
    "debit_rows = filter_debit_transactions(all_rows, debit_col)\n",
    "\n",
    "print(f\"\\nüí∞ Filtered to {len(debit_rows)} debit transactions (taxpayer purchases)\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DEBIT TRANSACTIONS (WHAT TAXPAYER PAID):\")\n",
    "print(\"=\" * 60)\n",
    "for i, row in enumerate(debit_rows, 1):\n",
    "    print(f\"\\nTransaction {i}:\")\n",
    "    print(f\"  {date_col}: {row.get(date_col, '')}\")\n",
    "    print(f\"  {desc_col}: {row.get(desc_col, '')}\")\n",
    "    print(f\"  {debit_col}: {row.get(debit_col, '')}\")\n",
    "\n",
    "# Extract schema fields using the LITERAL column names from pattern matching\n",
    "schema_fields = extract_schema_fields(debit_rows, date_col, desc_col, debit_col)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EXTRACTED SCHEMA FIELDS (TAX-RELEVANT DATA):\")\n",
    "print(\"=\" * 60)\n",
    "for field, value in schema_fields.items():\n",
    "    print(f\"{field}: {value}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Save to file with ivl3_8b prefix\n",
    "output_path = Path(\"ivl3_8b_extracted_fields.txt\")\n",
    "with output_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    for field, value in schema_fields.items():\n",
    "        f.write(f\"{field}: {value}\\n\")\n",
    "\n",
    "print(f\"\\n‚úÖ Schema fields saved to: {output_path}\")\n",
    "print(f\"üí° Fields extracted from columns: '{date_col}' | '{desc_col}' | '{debit_col}'\")\n",
    "print(f\"üéØ Success: Python parsing + filtering from Turn 1 markdown table\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (LMM_POC)",
   "language": "python",
   "name": "unified_vision_processor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}