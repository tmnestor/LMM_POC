{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# InternVL3-8B: 2-Turn Balance-Description Bank Statement Extraction\n",
    "\n",
    "**Protocol**: Two independent single-turn prompts + Python parsing/filtering\n",
    "\n",
    "**Key Insight**: Balance-description prompt works for BOTH date-per-row AND date-grouped formats!\n",
    "\n",
    "---\n",
    "\n",
    "## Complete Workflow\n",
    "\n",
    "```\n",
    "Turn 0: Image + Prompt ‚Üí Headers (fresh context)\n",
    "        ‚Üì (Python pattern matching)\n",
    "        ‚Üì (Check if Balance column exists)\n",
    "Turn 1: Image + Prompt ‚Üí Balance-Description extraction (fresh context)\n",
    "        ‚Üì (Python parsing + filtering)\n",
    "Schema Fields: TRANSACTION_DATES, LINE_ITEM_DESCRIPTIONS, TRANSACTION_AMOUNTS_PAID\n",
    "```\n",
    "\n",
    "### Why Balance-Description Works:\n",
    "- **Anchors extraction to Balance column** - unambiguous reference point\n",
    "- **Works for date-per-row**: Each transaction gets its date\n",
    "- **Works for date-grouped**: Date headers naturally map to transactions\n",
    "- **No format classification needed** - eliminates Turn 0.5 entirely!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and Configuration\n",
    "\n",
    "# Add parent directory to path AND change working directory for config file resolution\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Get project root (parent of bank_statement/)\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Change working directory to project root so config/field_definitions.yaml is found\n",
    "os.chdir(project_root)\n",
    "print(f\"üìÅ Working directory: {os.getcwd()}\")\n",
    "\n",
    "\n",
    "import random\n",
    "import re\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from PIL import Image\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig, BitsAndBytesConfig\n",
    "from IPython.display import display, Markdown, HTML\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "CONFIG = {\n",
    "    # Model path - update for your environment\n",
    "    \"MODEL_PATH\": \"/home/jovyan/nfs_share/models/InternVL3-8B\",\n",
    "    \n",
    "    # Generation settings\n",
    "    \"MAX_NEW_TOKENS\": 4096,\n",
    "    \n",
    "    # Image processing - V100 optimized\n",
    "    \"MAX_TILES\": 24,  # V100 optimized (use 18 for A10G, 36 for H200)\n",
    "}\n",
    "\n",
    "print(f\"‚úÖ Configuration loaded:\")\n",
    "print(f\"   Model: {CONFIG['MODEL_PATH']}\")\n",
    "print(f\"   Max tiles: {CONFIG['MAX_TILES']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Random Seed for Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Set random seed\n",
    "\n",
    "from common.reproducibility import set_seed\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Load InternVL3-8B model with memory-aware loading strategy\n",
    "\n",
    "MODEL_PATH = \"/home/jovyan/nfs_share/models/InternVL3-8B\"\n",
    "MAX_TILES = 14  # V100 optimized\n",
    "\n",
    "# Image preprocessing constants\n",
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "def build_transform(input_size):\n",
    "    \"\"\"Build image transformation pipeline.\"\"\"\n",
    "    return T.Compose([\n",
    "        T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
    "        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "    ])\n",
    "\n",
    "def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):\n",
    "    \"\"\"Find closest aspect ratio from target ratios.\"\"\"\n",
    "    best_ratio_diff = float('inf')\n",
    "    best_ratio = (1, 1)\n",
    "    area = width * height\n",
    "    for ratio in target_ratios:\n",
    "        target_aspect_ratio = ratio[0] / ratio[1]\n",
    "        ratio_diff = abs(aspect_ratio - target_aspect_ratio)\n",
    "        if ratio_diff < best_ratio_diff:\n",
    "            best_ratio_diff = ratio_diff\n",
    "            best_ratio = ratio\n",
    "        elif ratio_diff == best_ratio_diff:\n",
    "            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\n",
    "                best_ratio = ratio\n",
    "    return best_ratio\n",
    "\n",
    "def dynamic_preprocess(image, min_num=1, max_num=MAX_TILES, image_size=448, use_thumbnail=False):\n",
    "    \"\"\"Dynamically preprocess image by splitting into tiles.\"\"\"\n",
    "    orig_width, orig_height = image.size\n",
    "    aspect_ratio = orig_width / orig_height\n",
    "    \n",
    "    target_ratios = set(\n",
    "        (i, j) for n in range(min_num, max_num + 1)\n",
    "        for i in range(1, n + 1) for j in range(1, n + 1)\n",
    "        if i * j <= max_num and i * j >= min_num\n",
    "    )\n",
    "    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n",
    "    \n",
    "    target_aspect_ratio = find_closest_aspect_ratio(\n",
    "        aspect_ratio, target_ratios, orig_width, orig_height, image_size\n",
    "    )\n",
    "    \n",
    "    target_width = image_size * target_aspect_ratio[0]\n",
    "    target_height = image_size * target_aspect_ratio[1]\n",
    "    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]\n",
    "    \n",
    "    resized_img = image.resize((target_width, target_height))\n",
    "    processed_images = []\n",
    "    for i in range(blocks):\n",
    "        box = (\n",
    "            (i % (target_width // image_size)) * image_size,\n",
    "            (i // (target_width // image_size)) * image_size,\n",
    "            ((i % (target_width // image_size)) + 1) * image_size,\n",
    "            ((i // (target_width // image_size)) + 1) * image_size,\n",
    "        )\n",
    "        split_img = resized_img.crop(box)\n",
    "        processed_images.append(split_img)\n",
    "    \n",
    "    if use_thumbnail and len(processed_images) != 1:\n",
    "        thumbnail_img = image.resize((image_size, image_size))\n",
    "        processed_images.append(thumbnail_img)\n",
    "    \n",
    "    return processed_images\n",
    "\n",
    "def load_image(image_file, input_size=448, max_num=MAX_TILES):\n",
    "    \"\"\"Load and preprocess image for InternVL3.\"\"\"\n",
    "    if isinstance(image_file, str):\n",
    "        image = Image.open(image_file).convert('RGB')\n",
    "    else:\n",
    "        image = image_file\n",
    "    \n",
    "    transform = build_transform(input_size=input_size)\n",
    "    images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)\n",
    "    pixel_values = [transform(img) for img in images]\n",
    "    pixel_values = torch.stack(pixel_values)\n",
    "    return pixel_values\n",
    "\n",
    "def split_model(model_path):\n",
    "    \"\"\"Official InternVL3 multi-GPU device mapping.\"\"\"\n",
    "    device_map = {}\n",
    "    world_size = torch.cuda.device_count()\n",
    "    config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)\n",
    "    num_layers = config.llm_config.num_hidden_layers\n",
    "    \n",
    "    num_layers_per_gpu = math.ceil(num_layers / (world_size - 0.5))\n",
    "    num_layers_per_gpu = [num_layers_per_gpu] * world_size\n",
    "    num_layers_per_gpu[0] = math.ceil(num_layers_per_gpu[0] * 0.5)\n",
    "    \n",
    "    layer_cnt = 0\n",
    "    for i, num_layer in enumerate(num_layers_per_gpu):\n",
    "        for _ in range(num_layer):\n",
    "            device_map[f'language_model.model.layers.{layer_cnt}'] = i\n",
    "            layer_cnt += 1\n",
    "    \n",
    "    device_map['vision_model'] = 0\n",
    "    device_map['mlp1'] = 0\n",
    "    device_map['language_model.model.tok_embeddings'] = 0\n",
    "    device_map['language_model.model.embed_tokens'] = 0\n",
    "    device_map['language_model.output'] = 0\n",
    "    device_map['language_model.model.norm'] = 0\n",
    "    device_map['language_model.model.rotary_emb'] = 0\n",
    "    device_map['language_model.lm_head'] = 0\n",
    "    device_map[f'language_model.model.layers.{num_layers - 1}'] = 0\n",
    "    \n",
    "    return device_map\n",
    "\n",
    "print(\"üîß Loading InternVL3-8B model...\")\n",
    "\n",
    "world_size = torch.cuda.device_count()\n",
    "print(f\"  Detected {world_size} GPU(s)\")\n",
    "\n",
    "# Memory-aware loading\n",
    "if world_size > 1:\n",
    "    print(\"  Using multi-GPU bfloat16 mode\")\n",
    "    device_map = split_model(MODEL_PATH)\n",
    "    model = AutoModel.from_pretrained(\n",
    "        MODEL_PATH,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        low_cpu_mem_usage=True,\n",
    "        use_flash_attn=False,\n",
    "        trust_remote_code=True,\n",
    "        device_map=device_map,\n",
    "    ).eval()\n",
    "    model_dtype = torch.bfloat16\n",
    "else:\n",
    "    print(\"  Using single-GPU 8-bit quantization mode\")\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_8bit=True,\n",
    "        llm_int8_enable_fp32_cpu_offload=False\n",
    "    )\n",
    "    model = AutoModel.from_pretrained(\n",
    "        MODEL_PATH,\n",
    "        torch_dtype=torch.float16,\n",
    "        low_cpu_mem_usage=True,\n",
    "        use_flash_attn=False,\n",
    "        trust_remote_code=True,\n",
    "        quantization_config=quantization_config,\n",
    "        device_map={\"\":0},\n",
    "    ).eval()\n",
    "    model_dtype = torch.float16\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True, use_fast=False)\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "print(f\"‚úÖ Model loaded successfully!\")\n",
    "print(f\"  Data type: {model_dtype}\")\n",
    "print(f\"  Max Tiles: {MAX_TILES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Load bank statement image\n",
    "\n",
    "# imageName = \"/home/jovyan/nfs_share/tod/LMM_POC/evaluation_data/bank/minimal/image_003.png\"\n",
    "imageName = \"/home/jovyan/nfs_share/tod/LMM_POC/evaluation_data/bank/minimal/image_008.png\"\n",
    "# imageName = \"/home/jovyan/nfs_share/tod/LMM_POC/evaluation_data/bank/minimal/image_009.png\"\n",
    "\n",
    "print(\"üìÅ Loading image...\")\n",
    "image = Image.open(imageName).convert('RGB')\n",
    "\n",
    "print(f\"‚úÖ Image loaded: {image.size}\")\n",
    "print(\"üñºÔ∏è  Bank statement image:\")\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bank Statement Extraction Protocol (2-Turn Balance-Description)\n",
    "- Turn 0: Identify actual table headers\n",
    "- Turn 1: Extract using balance-description prompt\n",
    "- Python: Parse, filter, and extract schema fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Turn 0 - Identify table headers\n",
    "\n",
    "turn0_prompt = \"\"\"Look at the transaction table in this bank statement image.\n",
    "\n",
    "What are the exact column header names used in the transaction table?\n",
    "\n",
    "List each column header exactly as it appears, in order from left to right.\n",
    "Do not interpret or rename them - use the EXACT text from the image.\n",
    "\"\"\"\n",
    "\n",
    "print(\"üí¨ TURN 0: Identifying actual table headers\")\n",
    "print(\"ü§ñ Generating response with InternVL3-8B...\")\n",
    "\n",
    "# Preprocess image\n",
    "pixel_values = load_image(imageName, input_size=448)\n",
    "pixel_values = pixel_values.to(dtype=model_dtype, device='cuda:0')\n",
    "\n",
    "# Generate response using chat method\n",
    "turn0_response = model.chat(\n",
    "    tokenizer=tokenizer,\n",
    "    pixel_values=pixel_values,\n",
    "    question=turn0_prompt,\n",
    "    generation_config={'max_new_tokens': 500, 'do_sample': False}\n",
    ")\n",
    "\n",
    "# Free memory\n",
    "del pixel_values\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"‚úÖ Response generated successfully!\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TURN 0 - IDENTIFIED TABLE HEADERS:\")\n",
    "print(\"=\" * 60)\n",
    "print(turn0_response)\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Parse headers from Turn 0 response\n",
    "\n",
    "def parse_headers_from_response(response_text):\n",
    "    \"\"\"Parse column headers from Turn 0 response.\"\"\"\n",
    "    header_lines = [line.strip() for line in response_text.split('\\n') if line.strip()]\n",
    "    identified_headers = []\n",
    "    \n",
    "    for line in header_lines:\n",
    "        cleaned = line.lstrip('0123456789.-‚Ä¢* ').strip()\n",
    "        cleaned = cleaned.replace('**', '').replace('__', '')\n",
    "        if cleaned.endswith(':'):\n",
    "            continue\n",
    "        if len(cleaned) > 40:\n",
    "            continue\n",
    "        if cleaned and len(cleaned) > 2:\n",
    "            identified_headers.append(cleaned)\n",
    "    \n",
    "    return identified_headers\n",
    "\n",
    "table_headers = parse_headers_from_response(turn0_response)\n",
    "\n",
    "print(f\"\\nüìã Parsed {len(table_headers)} column headers:\")\n",
    "for i, header in enumerate(table_headers, 1):\n",
    "    print(f\"  {i}. '{header}'\")\n",
    "\n",
    "print(f\"\\n‚úÖ Stored table_headers: {table_headers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern Matching: Map Generic Concepts to Actual Headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Pattern Matching\n",
    "\n",
    "DATE_PATTERNS = ['date', 'day', 'transaction date', 'trans date']\n",
    "DESCRIPTION_PATTERNS = [\n",
    "    'description', 'details', 'transaction details', 'trans details',\n",
    "    'particulars', 'narrative', 'transaction', 'trans'\n",
    "]\n",
    "DEBIT_PATTERNS = ['debit', 'debits', 'withdrawal', 'withdrawals', 'paid', 'paid out', 'spent', 'dr']\n",
    "CREDIT_PATTERNS = ['credit', 'credits', 'deposit', 'deposits', 'received', 'cr']\n",
    "BALANCE_PATTERNS = ['balance', 'bal', 'running balance']\n",
    "AMOUNT_PATTERNS = ['amount', 'amt', 'value', 'total']\n",
    "\n",
    "def match_header(headers, patterns, fallback=None):\n",
    "    \"\"\"Match a header using pattern keywords.\"\"\"\n",
    "    headers_lower = [h.lower() for h in headers]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        for i, header_lower in enumerate(headers_lower):\n",
    "            if pattern == header_lower:\n",
    "                return headers[i]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        if len(pattern) > 2:\n",
    "            for i, header_lower in enumerate(headers_lower):\n",
    "                if pattern in header_lower:\n",
    "                    return headers[i]\n",
    "    \n",
    "    return fallback\n",
    "\n",
    "# Perform pattern matching\n",
    "date_col = match_header(table_headers, DATE_PATTERNS, fallback=table_headers[0] if table_headers else 'Date')\n",
    "desc_col = match_header(table_headers, DESCRIPTION_PATTERNS, fallback=table_headers[1] if len(table_headers) > 1 else 'Description')\n",
    "amount_col = match_header(table_headers, AMOUNT_PATTERNS, fallback=None)\n",
    "debit_col = match_header(table_headers, DEBIT_PATTERNS, fallback=amount_col if amount_col else 'Debit')\n",
    "credit_col = match_header(table_headers, CREDIT_PATTERNS, fallback=amount_col if amount_col else 'Credit')\n",
    "balance_col = match_header(table_headers, BALANCE_PATTERNS, fallback=None)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PATTERN MATCHING RESULTS:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"üìã Extracted Headers: {table_headers}\")\n",
    "print(f\"\\nüîç Mapped Columns:\")\n",
    "print(f\"  Date        ‚Üí '{date_col}'\")\n",
    "print(f\"  Description ‚Üí '{desc_col}'\")\n",
    "print(f\"  Debit       ‚Üí '{debit_col}'\")\n",
    "print(f\"  Credit      ‚Üí '{credit_col}'\")\n",
    "print(f\"  Balance     ‚Üí '{balance_col}'\")\n",
    "\n",
    "has_balance = balance_col is not None and balance_col in table_headers\n",
    "print(f\"\\nüéØ Balance column detected: {'‚úÖ YES' if has_balance else '‚ùå NO'}\")\n",
    "\n",
    "if not has_balance:\n",
    "    print(\"‚ö†Ô∏è  WARNING: No balance column found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turn 1: Balance-Description Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Generate extraction prompt\n",
    "\n",
    "if has_balance:\n",
    "    extraction_prompt = f\"\"\"List all the balances in the {balance_col} column, including:\n",
    "- Date from the Date Header of the balance\n",
    "- {desc_col}\n",
    "- {debit_col} Amount or \"NOT_FOUND\"\n",
    "- {credit_col} Amount or \"NOT_FOUND\"\n",
    "\n",
    "Format each balance entry like this:\n",
    "1. **[Date]**\n",
    "   - {desc_col}: [description text]\n",
    "   - {debit_col}: [amount or NOT_FOUND]\n",
    "   - {credit_col}: [amount or NOT_FOUND]\n",
    "   - {balance_col}: [balance amount]\n",
    "\n",
    "CRITICAL RULES:\n",
    "1. List EVERY balance entry in order from top to bottom\n",
    "2. EVERY balance entry has a date, either on the same row, or above\n",
    "3. Include the FULL description text, not abbreviated\n",
    "4. If amount is in {debit_col} column, put it there and use NOT_FOUND for {credit_col}\n",
    "5. If amount is in {credit_col} column, put it there and use NOT_FOUND for {debit_col}\n",
    "6. Do NOT skip any transactions\n",
    "\"\"\"\n",
    "    \n",
    "    print(\"üìù TURN 1: Balance-Description Extraction\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Extraction Prompt:\")\n",
    "    print(extraction_prompt)\n",
    "    print(\"=\" * 60)\n",
    "else:\n",
    "    print(\"‚ùå Cannot proceed - no balance column detected.\")\n",
    "    extraction_prompt = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Execute Turn 1 extraction\n",
    "\n",
    "if extraction_prompt:\n",
    "    print(\"ü§ñ Generating response with InternVL3-8B...\")\n",
    "    \n",
    "    # Reload image for fresh context\n",
    "    pixel_values = load_image(imageName, input_size=448)\n",
    "    pixel_values = pixel_values.to(dtype=model_dtype, device='cuda:0')\n",
    "    \n",
    "    extraction_response = model.chat(\n",
    "        tokenizer=tokenizer,\n",
    "        pixel_values=pixel_values,\n",
    "        question=extraction_prompt,\n",
    "        generation_config={'max_new_tokens': 4096, 'do_sample': False}\n",
    "    )\n",
    "    \n",
    "    # Free memory\n",
    "    del pixel_values\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    print(\"\\n‚úÖ Turn 1 extraction complete!\")\n",
    "    print(f\"\\nüìä Response length: {len(extraction_response)} characters\")\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"TURN 1 - BALANCE-DESCRIPTION EXTRACTION:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(extraction_response)\n",
    "    print(\"=\" * 60)\n",
    "else:\n",
    "    extraction_response = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Parsing: Balance-Description Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Parse balance-description response\n",
    "\n",
    "def parse_balance_description_response(response_text, date_col, desc_col, debit_col, credit_col, balance_col):\n",
    "    \"\"\"Parse the hierarchical balance-description response into transaction rows.\n",
    "\n",
    "    Handles multiple output formats:\n",
    "    - InternVL3: \"1. **Date:** 03/05/2025\" with \"**Field:** value\"\n",
    "    - Llama: \"**03/05/2025**\" with \"* Field: value\"\n",
    "    - Standard: \"1. **date**\" with \"- Field: value\"\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    current_date = None\n",
    "    current_transaction = {}\n",
    "\n",
    "    lines = response_text.strip().split(\"\\n\")\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        # ============================================================\n",
    "        # DATE DETECTION PATTERNS\n",
    "        # ============================================================\n",
    "        date_found = None\n",
    "\n",
    "        # Pattern 1: InternVL3 format \"1. **Date:** 03/05/2025\" or \"**Date:** 03/05/2025\"\n",
    "        date_field_match = re.match(r\"^\\d*\\.?\\s*\\*?\\*?Date:?\\*?\\*?\\s*(.+)$\", line, re.IGNORECASE)\n",
    "        if date_field_match:\n",
    "            date_found = date_field_match.group(1).strip().strip('*').strip()\n",
    "\n",
    "        # Pattern 2: Bold date header \"**03/05/2025**\" (Llama style)\n",
    "        if not date_found:\n",
    "            bold_date_match = re.match(r\"^\\*\\*(\\d{1,2}/\\d{1,2}/\\d{4})\\*\\*$\", line)\n",
    "            if bold_date_match:\n",
    "                date_found = bold_date_match.group(1)\n",
    "\n",
    "        # Pattern 3: Numbered bold date \"1. **Thu 04 Sep 2025**\"\n",
    "        if not date_found:\n",
    "            date_match = re.match(r\"^\\d+\\.\\s*\\*?\\*?([A-Za-z]{3}\\s+\\d{1,2}\\s+[A-Za-z]{3}\\s+\\d{4})\\*?\\*?\", line)\n",
    "            if date_match:\n",
    "                date_found = date_match.group(1).strip()\n",
    "\n",
    "        # Pattern 4: Numbered date without day \"1. **04 Sep 2025**\"\n",
    "        if not date_found:\n",
    "            date_match = re.match(r\"^\\d+\\.\\s*\\*?\\*?(\\d{1,2}\\s+[A-Za-z]{3}\\s+\\d{4})\\*?\\*?\", line)\n",
    "            if date_match:\n",
    "                date_found = date_match.group(1).strip()\n",
    "\n",
    "        # Pattern 5: Numbered DD/MM/YYYY \"1. **03/05/2025**\"\n",
    "        if not date_found:\n",
    "            date_match = re.match(r\"^\\d+\\.\\s*\\*?\\*?(\\d{1,2}/\\d{1,2}/\\d{4})\\*?\\*?\", line)\n",
    "            if date_match:\n",
    "                date_found = date_match.group(1).strip()\n",
    "\n",
    "        if date_found:\n",
    "            # Save previous transaction if exists\n",
    "            if current_transaction and current_date:\n",
    "                current_transaction[date_col] = current_date\n",
    "                rows.append(current_transaction)\n",
    "                current_transaction = {}\n",
    "            current_date = date_found\n",
    "            continue\n",
    "\n",
    "        # ============================================================\n",
    "        # FIELD DETECTION PATTERNS\n",
    "        # ============================================================\n",
    "        field_name = None\n",
    "        field_value = None\n",
    "\n",
    "        # Pattern 1: \"**Description:** value\" (InternVL3 style)\n",
    "        bold_field_match = re.match(r\"^\\s*\\*\\*([^*:]+)(?:\\s*Amount)?:?\\*\\*\\s*(.+)$\", line, re.IGNORECASE)\n",
    "        if bold_field_match:\n",
    "            field_name = bold_field_match.group(1).strip().lower()\n",
    "            field_value = bold_field_match.group(2).strip()\n",
    "\n",
    "        # Pattern 2: \"* Description: value\" (Llama style with asterisk bullet)\n",
    "        if not field_name:\n",
    "            asterisk_field_match = re.match(r\"^\\s*\\*\\s*([^:]+):\\s*(.+)$\", line)\n",
    "            if asterisk_field_match:\n",
    "                field_name = asterisk_field_match.group(1).strip().lower()\n",
    "                field_value = asterisk_field_match.group(2).strip()\n",
    "\n",
    "        # Pattern 3: \"- Description: value\" (standard style)\n",
    "        if not field_name:\n",
    "            dash_field_match = re.match(r\"^\\s*-\\s*([^:]+):\\s*(.+)$\", line)\n",
    "            if dash_field_match:\n",
    "                field_name = dash_field_match.group(1).strip().lower()\n",
    "                field_value = dash_field_match.group(2).strip()\n",
    "\n",
    "        if field_name and field_value:\n",
    "            # Normalize field names (remove \"amount\" suffix, handle variants)\n",
    "            field_name = field_name.replace(\" amount\", \"\").strip()\n",
    "\n",
    "            # Map to appropriate column\n",
    "            if field_name in [\"description\", \"transaction\", \"details\", \"particulars\", desc_col.lower()]:\n",
    "                # If we already have a description, this is a new transaction under same date\n",
    "                if desc_col in current_transaction and current_transaction[desc_col]:\n",
    "                    if current_date:\n",
    "                        current_transaction[date_col] = current_date\n",
    "                    rows.append(current_transaction)\n",
    "                    current_transaction = {}\n",
    "                current_transaction[desc_col] = field_value\n",
    "\n",
    "            elif field_name in [\"debit\", \"withdrawal\", \"withdrawwal\", \"dr\", debit_col.lower()]:\n",
    "                current_transaction[debit_col] = field_value\n",
    "\n",
    "            elif field_name in [\"credit\", \"deposit\", \"cr\", credit_col.lower()]:\n",
    "                current_transaction[credit_col] = field_value\n",
    "\n",
    "            elif field_name == \"balance\":\n",
    "                current_transaction[balance_col] = field_value\n",
    "\n",
    "            elif field_name == \"amount\":\n",
    "                # Generic amount - put in debit by default\n",
    "                if debit_col not in current_transaction:\n",
    "                    current_transaction[debit_col] = field_value\n",
    "\n",
    "    # Don't forget the last transaction\n",
    "    if current_transaction and current_date:\n",
    "        current_transaction[date_col] = current_date\n",
    "        rows.append(current_transaction)\n",
    "\n",
    "    return rows\n",
    "\n",
    "\n",
    "def parse_markdown_table(markdown_text):\n",
    "    \"\"\"Fallback: Parse markdown table.\"\"\"\n",
    "    lines = [line.strip() for line in markdown_text.strip().split('\\n') if line.strip()]\n",
    "    \n",
    "    header_idx = None\n",
    "    for i, line in enumerate(lines):\n",
    "        if '|' in line:\n",
    "            cleaned = line.replace('|', '').replace('-', '').replace(' ', '')\n",
    "            if cleaned:\n",
    "                header_idx = i\n",
    "                break\n",
    "    \n",
    "    if header_idx is None:\n",
    "        return []\n",
    "    \n",
    "    header_line = lines[header_idx]\n",
    "    header_parts = [h.strip() for h in header_line.split('|')]\n",
    "    if header_parts and header_parts[0] == '':\n",
    "        header_parts = header_parts[1:]\n",
    "    if header_parts and header_parts[-1] == '':\n",
    "        header_parts = header_parts[:-1]\n",
    "    headers = [h for h in header_parts if h]\n",
    "    \n",
    "    rows = []\n",
    "    for line in lines[header_idx + 1:]:\n",
    "        if '|' not in line:\n",
    "            continue\n",
    "        cleaned = line.replace('|', '').replace('-', '').replace(' ', '').replace(':', '')\n",
    "        if not cleaned:\n",
    "            continue\n",
    "        value_parts = [v.strip() for v in line.split('|')]\n",
    "        if value_parts and value_parts[0] == '':\n",
    "            value_parts = value_parts[1:]\n",
    "        if value_parts and value_parts[-1] == '':\n",
    "            value_parts = value_parts[:-1]\n",
    "        if len(value_parts) == len(headers):\n",
    "            rows.append(dict(zip(headers, value_parts)))\n",
    "    \n",
    "    return rows\n",
    "\n",
    "\n",
    "# Parse the extraction response\n",
    "if extraction_response:\n",
    "    all_rows = parse_balance_description_response(\n",
    "        extraction_response, date_col, desc_col, debit_col, credit_col, balance_col\n",
    "    )\n",
    "    \n",
    "    if not all_rows and \"|\" in extraction_response:\n",
    "        print(\"‚ö†Ô∏è  Fallback: parsing as markdown table\")\n",
    "        all_rows = parse_markdown_table(extraction_response)\n",
    "    \n",
    "    print(f\"\\nüìä Parsed {len(all_rows)} total rows\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"PARSED TRANSACTIONS:\")\n",
    "    print(\"=\" * 60)\n",
    "    for i, row in enumerate(all_rows[:10]):\n",
    "        print(f\"\\n{i+1}. {row}\")\n",
    "    if len(all_rows) > 10:\n",
    "        print(f\"\\n... and {len(all_rows) - 10} more rows\")\n",
    "else:\n",
    "    all_rows = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter for Debit Transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Filter for debit transactions\n",
    "\n",
    "def parse_amount(value):\n",
    "    \"\"\"Extract numeric value from formatted currency string.\"\"\"\n",
    "    if not value or value.strip() == \"\":\n",
    "        return 0.0\n",
    "    cleaned = value.replace(\"$\", \"\").replace(\",\", \"\").replace(\"CR\", \"\").replace(\"DR\", \"\").strip()\n",
    "    try:\n",
    "        return float(cleaned)\n",
    "    except ValueError:\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "def is_non_transaction_row(row, desc_col):\n",
    "    \"\"\"Check if this row is NOT an actual transaction.\"\"\"\n",
    "    desc = row.get(desc_col, \"\").strip().upper()\n",
    "    return any(x in desc for x in [\"OPENING BALANCE\", \"CLOSING BALANCE\", \"BROUGHT FORWARD\", \"CARRIED FORWARD\"])\n",
    "\n",
    "\n",
    "def filter_debit_transactions(rows, debit_col, desc_col=None):\n",
    "    \"\"\"Filter rows to only those with actual debit transactions.\"\"\"\n",
    "    debit_rows = []\n",
    "    for row in rows:\n",
    "        debit_value = row.get(debit_col, \"\").strip()\n",
    "        \n",
    "        if not debit_value or debit_value.upper() == \"NOT_FOUND\":\n",
    "            continue\n",
    "        \n",
    "        amount = parse_amount(debit_value)\n",
    "        if amount <= 0:\n",
    "            continue\n",
    "        \n",
    "        if desc_col and is_non_transaction_row(row, desc_col):\n",
    "            continue\n",
    "        \n",
    "        debit_rows.append(row)\n",
    "    \n",
    "    return debit_rows\n",
    "\n",
    "\n",
    "debit_rows = filter_debit_transactions(all_rows, debit_col, desc_col)\n",
    "\n",
    "print(f\"\\nüìä Filtered to {len(debit_rows)} debit transactions\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DEBIT TRANSACTIONS ONLY:\")\n",
    "print(\"=\" * 60)\n",
    "for i, row in enumerate(debit_rows):\n",
    "    date = row.get(date_col, \"N/A\")\n",
    "    desc = row.get(desc_col, \"N/A\")\n",
    "    amount = row.get(debit_col, \"N/A\")\n",
    "    print(f\"{i+1}. [{date}] {desc} - {amount}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Schema Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Extract schema fields\n",
    "\n",
    "def extract_schema_fields(debit_rows, date_col, desc_col, debit_col, all_rows=None):\n",
    "    \"\"\"Extract fields in universal.yaml schema format.\"\"\"\n",
    "    if not debit_rows:\n",
    "        return {\n",
    "            \"DOCUMENT_TYPE\": \"BANK_STATEMENT\",\n",
    "            \"STATEMENT_DATE_RANGE\": \"NOT_FOUND\",\n",
    "            \"TRANSACTION_DATES\": \"NOT_FOUND\",\n",
    "            \"LINE_ITEM_DESCRIPTIONS\": \"NOT_FOUND\",\n",
    "            \"TRANSACTION_AMOUNTS_PAID\": \"NOT_FOUND\",\n",
    "        }\n",
    "    \n",
    "    debit_dates = [row.get(date_col, \"\").strip() for row in debit_rows if row.get(date_col)]\n",
    "    descriptions = [row.get(desc_col, \"\").strip() for row in debit_rows if row.get(desc_col)]\n",
    "    amounts = [row.get(debit_col, \"\").strip() for row in debit_rows if row.get(debit_col)]\n",
    "    \n",
    "    rows_for_range = all_rows if all_rows is not None else debit_rows\n",
    "    all_dates = [row.get(date_col, \"\").strip() for row in rows_for_range if row.get(date_col)]\n",
    "    date_range = f\"{all_dates[0]} - {all_dates[-1]}\" if all_dates else \"NOT_FOUND\"\n",
    "    \n",
    "    return {\n",
    "        \"DOCUMENT_TYPE\": \"BANK_STATEMENT\",\n",
    "        \"STATEMENT_DATE_RANGE\": date_range,\n",
    "        \"TRANSACTION_DATES\": \" | \".join(debit_dates) if debit_dates else \"NOT_FOUND\",\n",
    "        \"LINE_ITEM_DESCRIPTIONS\": \" | \".join(descriptions) if descriptions else \"NOT_FOUND\",\n",
    "        \"TRANSACTION_AMOUNTS_PAID\": \" | \".join(amounts) if amounts else \"NOT_FOUND\",\n",
    "    }\n",
    "\n",
    "\n",
    "schema_fields = extract_schema_fields(debit_rows, date_col, desc_col, debit_col, all_rows=all_rows)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EXTRACTED SCHEMA FIELDS:\")\n",
    "print(\"=\" * 60)\n",
    "for field, value in schema_fields.items():\n",
    "    display_value = str(value)[:100] + \"...\" if len(str(value)) > 100 else str(value)\n",
    "    print(f\"\\n{field}:\")\n",
    "    print(f\"  {display_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Load Ground Truth and Evaluate (using batch_v2 metrics)\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from dateutil import parser as date_parser\n",
    "\n",
    "# Import evaluation metrics from common module (same as batch_v2 scripts)\n",
    "from common.evaluation_metrics import (\n",
    "    calculate_field_accuracy_f1,\n",
    "    calculate_field_accuracy_f1_position_agnostic,\n",
    "    calculate_field_accuracy_kieval,\n",
    "    calculate_correlation_aware_f1,\n",
    "    load_ground_truth,\n",
    ")\n",
    "\n",
    "# Bank statement fields to evaluate (same as batch_v2)\n",
    "BANK_STATEMENT_FIELDS = [\n",
    "    \"DOCUMENT_TYPE\",\n",
    "    \"STATEMENT_DATE_RANGE\",\n",
    "    \"TRANSACTION_DATES\",\n",
    "    \"LINE_ITEM_DESCRIPTIONS\",\n",
    "    \"TRANSACTION_AMOUNTS_PAID\",\n",
    "]\n",
    "\n",
    "# Ground truth configuration\n",
    "GROUND_TRUTH_PATH = \"/home/jovyan/nfs_share/tod/LMM_POC/evaluation_data/bank/ground_truth_bank.csv\"\n",
    "\n",
    "# Evaluation method: \"order_aware_f1\", \"position_agnostic_f1\", \"kieval\", or \"correlation\"\n",
    "EVALUATION_METHOD = \"order_aware_f1\"\n",
    "\n",
    "# ============================================================================\n",
    "# SEMANTIC NORMALIZATION (same as batch_v2)\n",
    "# ============================================================================\n",
    "def normalize_date(date_str):\n",
    "    \"\"\"Normalize date string to canonical format YYYY-MM-DD for semantic comparison.\"\"\"\n",
    "    if not date_str or pd.isna(date_str):\n",
    "        return \"\"\n",
    "    date_str = str(date_str).strip()\n",
    "    if not date_str:\n",
    "        return \"\"\n",
    "    try:\n",
    "        parsed = date_parser.parse(date_str, dayfirst=True)\n",
    "        return parsed.strftime(\"%Y-%m-%d\")\n",
    "    except (ValueError, TypeError):\n",
    "        return date_str\n",
    "\n",
    "\n",
    "def normalize_amount(amount_str):\n",
    "    \"\"\"Normalize amount string for semantic comparison.\"\"\"\n",
    "    if not amount_str or pd.isna(amount_str):\n",
    "        return \"\"\n",
    "    amount_str = str(amount_str).strip()\n",
    "    if not amount_str:\n",
    "        return \"\"\n",
    "    # Remove currency symbols and whitespace\n",
    "    cleaned = re.sub(r\"[$¬£‚Ç¨¬•‚Çπ\\s]\", \"\", amount_str)\n",
    "    cleaned = cleaned.replace(\",\", \"\")\n",
    "    try:\n",
    "        value = float(cleaned)\n",
    "        value = abs(value)  # Ignore sign for matching\n",
    "        return f\"{value:.2f}\".rstrip(\"0\").rstrip(\".\")\n",
    "    except ValueError:\n",
    "        return cleaned\n",
    "\n",
    "\n",
    "def normalize_pipe_delimited(value, normalizer_fn):\n",
    "    \"\"\"Apply normalizer function to each item in a pipe-delimited string.\"\"\"\n",
    "    if not value or pd.isna(value):\n",
    "        return \"\"\n",
    "    value = str(value).strip()\n",
    "    if not value:\n",
    "        return \"\"\n",
    "    items = [item.strip() for item in value.split(\"|\")]\n",
    "    normalized = [normalizer_fn(item) for item in items]\n",
    "    return \" | \".join(normalized)\n",
    "\n",
    "\n",
    "def normalize_field_for_comparison(field_name, value):\n",
    "    \"\"\"Normalize a field value based on its type for semantic comparison.\"\"\"\n",
    "    if not value or pd.isna(value):\n",
    "        return \"\"\n",
    "    value = str(value).strip()\n",
    "    if field_name == \"TRANSACTION_DATES\":\n",
    "        return normalize_pipe_delimited(value, normalize_date)\n",
    "    elif field_name == \"TRANSACTION_AMOUNTS_PAID\":\n",
    "        return normalize_pipe_delimited(value, normalize_amount)\n",
    "    elif field_name == \"STATEMENT_DATE_RANGE\":\n",
    "        if \" - \" in value:\n",
    "            parts = value.split(\" - \")\n",
    "            if len(parts) == 2:\n",
    "                start = normalize_date(parts[0].strip())\n",
    "                end = normalize_date(parts[1].strip())\n",
    "                return f\"{start} - {end}\"\n",
    "        return value\n",
    "    else:\n",
    "        return value\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# EVALUATION FUNCTIONS (same as batch_v2)\n",
    "# ============================================================================\n",
    "def evaluate_field(extracted_value, gt_value, field_name, method):\n",
    "    \"\"\"Route to appropriate evaluation function.\"\"\"\n",
    "    if method == \"order_aware_f1\":\n",
    "        return calculate_field_accuracy_f1(extracted_value, gt_value, field_name)\n",
    "    elif method == \"position_agnostic_f1\":\n",
    "        return calculate_field_accuracy_f1_position_agnostic(extracted_value, gt_value, field_name)\n",
    "    elif method == \"kieval\":\n",
    "        return calculate_field_accuracy_kieval(extracted_value, gt_value, field_name)\n",
    "    elif method == \"correlation\":\n",
    "        return None\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown evaluation method: {method}\")\n",
    "\n",
    "\n",
    "def evaluate_extraction(schema_fields, image_name, ground_truth_map, method):\n",
    "    \"\"\"Evaluate extracted schema fields against ground truth.\"\"\"\n",
    "    gt_data = ground_truth_map.get(image_name, {})\n",
    "    \n",
    "    if not gt_data:\n",
    "        return {\"error\": \"No ground truth found\", \"image_name\": image_name}\n",
    "    \n",
    "    if method == \"correlation\":\n",
    "        normalized_extracted = {\n",
    "            field: normalize_field_for_comparison(field, schema_fields.get(field, \"\"))\n",
    "            for field in BANK_STATEMENT_FIELDS\n",
    "        }\n",
    "        normalized_gt = {\n",
    "            field: normalize_field_for_comparison(field, gt_data.get(field, \"\"))\n",
    "            for field in BANK_STATEMENT_FIELDS\n",
    "        }\n",
    "        result = calculate_correlation_aware_f1(\n",
    "            extracted_data=normalized_extracted,\n",
    "            ground_truth_data=normalized_gt,\n",
    "            document_type=\"bank_statement\",\n",
    "            debug=False,\n",
    "        )\n",
    "        return {\n",
    "            \"image_name\": image_name,\n",
    "            \"method\": method,\n",
    "            \"overall_accuracy\": result.get(\"combined_f1\", 0.0),\n",
    "            \"standard_f1\": result.get(\"standard_f1\", 0.0),\n",
    "            \"alignment_score\": result.get(\"alignment_score\", 0.0),\n",
    "            \"field_scores\": result.get(\"field_f1_scores\", {}),\n",
    "        }\n",
    "    \n",
    "    field_scores = {}\n",
    "    total_f1 = 0.0\n",
    "    \n",
    "    for field in BANK_STATEMENT_FIELDS:\n",
    "        extracted_value = schema_fields.get(field, \"NOT_FOUND\")\n",
    "        gt_value = gt_data.get(field, \"NOT_FOUND\")\n",
    "        \n",
    "        if pd.isna(gt_value):\n",
    "            gt_value = \"NOT_FOUND\"\n",
    "        \n",
    "        normalized_extracted = normalize_field_for_comparison(field, extracted_value)\n",
    "        normalized_gt = normalize_field_for_comparison(field, gt_value)\n",
    "        \n",
    "        result = evaluate_field(normalized_extracted, normalized_gt, field, method)\n",
    "        \n",
    "        if result:\n",
    "            field_scores[field] = {\n",
    "                \"f1_score\": result.get(\"f1_score\", 0.0),\n",
    "                \"precision\": result.get(\"precision\", 0.0),\n",
    "                \"recall\": result.get(\"recall\", 0.0),\n",
    "                \"extracted\": str(extracted_value)[:100],\n",
    "                \"ground_truth\": str(gt_value)[:100],\n",
    "            }\n",
    "            total_f1 += result.get(\"f1_score\", 0.0)\n",
    "    \n",
    "    overall_accuracy = total_f1 / len(BANK_STATEMENT_FIELDS) if BANK_STATEMENT_FIELDS else 0.0\n",
    "    \n",
    "    return {\n",
    "        \"image_name\": image_name,\n",
    "        \"method\": method,\n",
    "        \"overall_accuracy\": overall_accuracy,\n",
    "        \"field_scores\": field_scores,\n",
    "    }\n",
    "\n",
    "\n",
    "def display_field_comparison(schema_fields, ground_truth_map, image_name, eval_result):\n",
    "    \"\"\"Display stacked comparison of extracted vs ground truth fields.\"\"\"\n",
    "    gt_data = ground_truth_map.get(image_name, {})\n",
    "    field_scores = eval_result.get(\"field_scores\", {})\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 90)\n",
    "    print(f\"üìä FIELD COMPARISON ({EVALUATION_METHOD}) - {image_name}\")\n",
    "    print(\"=\" * 90)\n",
    "    \n",
    "    print(f\"\\n{'Status':<8} {'Field':<30} {'F1':>8} {'Prec':>8} {'Recall':>8}\")\n",
    "    print(\"-\" * 90)\n",
    "    \n",
    "    for field in BANK_STATEMENT_FIELDS:\n",
    "        extracted_val = schema_fields.get(field, \"NOT_FOUND\")\n",
    "        ground_val = gt_data.get(field, \"NOT_FOUND\")\n",
    "        \n",
    "        if pd.isna(ground_val):\n",
    "            ground_val = \"NOT_FOUND\"\n",
    "        \n",
    "        if isinstance(field_scores.get(field), dict):\n",
    "            f1_score = field_scores[field].get(\"f1_score\", 0.0)\n",
    "            precision = field_scores[field].get(\"precision\", 0.0)\n",
    "            recall = field_scores[field].get(\"recall\", 0.0)\n",
    "        else:\n",
    "            f1_score = field_scores.get(field, 0.0)\n",
    "            precision = 0.0\n",
    "            recall = 0.0\n",
    "        \n",
    "        if f1_score == 1.0:\n",
    "            status = \"‚úÖ OK\"\n",
    "        elif f1_score >= 0.5:\n",
    "            status = \"‚ö†Ô∏è PART\"\n",
    "        else:\n",
    "            status = \"‚ùå FAIL\"\n",
    "        \n",
    "        print(f\"{status:<8} {field:<30} {f1_score:>7.1%} {precision:>7.1%} {recall:>7.1%}\")\n",
    "    \n",
    "    print(\"-\" * 90)\n",
    "    \n",
    "    # Show detailed values\n",
    "    print(\"\\nüìã DETAILED VALUES:\")\n",
    "    print(\"-\" * 90)\n",
    "    for field in BANK_STATEMENT_FIELDS:\n",
    "        extracted_val = schema_fields.get(field, \"NOT_FOUND\")\n",
    "        gt_val = gt_data.get(field, \"NOT_FOUND\")\n",
    "        if pd.isna(gt_val):\n",
    "            gt_val = \"NOT_FOUND\"\n",
    "        \n",
    "        print(f\"\\n{field}:\")\n",
    "        print(f\"  Extracted:    {str(extracted_val)[:80]}{'...' if len(str(extracted_val)) > 80 else ''}\")\n",
    "        print(f\"  Ground Truth: {str(gt_val)[:80]}{'...' if len(str(gt_val)) > 80 else ''}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# RUN EVALUATION\n",
    "# ============================================================================\n",
    "print(\"üìÅ Loading ground truth...\")\n",
    "ground_truth_map = load_ground_truth(GROUND_TRUTH_PATH, verbose=True)\n",
    "\n",
    "# Get image filename for lookup\n",
    "image_filename = Path(imageName).name\n",
    "print(f\"\\nüîç Looking up ground truth for: {image_filename}\")\n",
    "\n",
    "# Add DOCUMENT_TYPE to schema_fields if not present\n",
    "if \"DOCUMENT_TYPE\" not in schema_fields:\n",
    "    schema_fields[\"DOCUMENT_TYPE\"] = \"BANK_STATEMENT\"\n",
    "\n",
    "# Check if ground truth exists for this image\n",
    "if image_filename in ground_truth_map:\n",
    "    print(f\"‚úÖ Ground truth found!\")\n",
    "    \n",
    "    # Evaluate using batch_v2 metrics\n",
    "    eval_result = evaluate_extraction(schema_fields, image_filename, ground_truth_map, EVALUATION_METHOD)\n",
    "    \n",
    "    # Display comparison\n",
    "    display_field_comparison(schema_fields, ground_truth_map, image_filename, eval_result)\n",
    "    \n",
    "    overall_f1 = eval_result.get(\"overall_accuracy\", 0.0)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 90)\n",
    "    print(f\"üìä OVERALL F1 SCORE ({EVALUATION_METHOD}): {overall_f1:.1%}\")\n",
    "    print(\"=\" * 90)\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  No ground truth found for: {image_filename}\")\n",
    "    print(f\"   Available images: {list(ground_truth_map.keys())[:5]}...\")\n",
    "    overall_f1 = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Summary\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìä EXTRACTION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nüîß Method: 2-Turn Balance-Description\")\n",
    "print(f\"üìã Headers detected: {len(table_headers)}\")\n",
    "print(f\"üí∞ Balance column: {balance_col}\")\n",
    "print(f\"üìù Total transactions parsed: {len(all_rows)}\")\n",
    "print(f\"üí∏ Debit transactions: {len(debit_rows)}\")\n",
    "if 'overall_f1' in dir():\n",
    "    print(f\"üìä Overall F1 Score: {overall_f1:.1%}\")\n",
    "print(f\"\\n‚úÖ Pipeline complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (LMM_POC)",
   "language": "python",
   "name": "unified_vision_processor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
