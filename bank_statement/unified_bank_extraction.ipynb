{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Unified Bank Statement Extraction\n",
    "\n",
    "**Protocol**: Automatic strategy selection based on document characteristics\n",
    "\n",
    "This notebook uses the `UnifiedBankExtractor` which automatically selects the optimal extraction strategy:\n",
    "\n",
    "- **2-Turn Balance-Description**: When Balance column is detected (simpler, more reliable)\n",
    "- **3-Turn Table Extraction**: When no Balance column exists (fallback)\n",
    "\n",
    "---\n",
    "\n",
    "## Workflow\n",
    "\n",
    "```\n",
    "Turn 0: Image → Header Detection (shared)\n",
    "    ↓\n",
    "Python: Pattern matching → Strategy selection\n",
    "    ↓\n",
    "Turn 1: Image → Extraction (strategy-specific)\n",
    "    ↓\n",
    "Python: Parsing → Filtering → Schema fields\n",
    "```\n",
    "\n",
    "## Configuration\n",
    "\n",
    "- Model configs: `config/model_config.yaml`\n",
    "- Prompts: `config/bank_statement_prompts.yaml`\n",
    "- Column patterns: `config/column_patterns.yaml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /home/jovyan/nfs_share/tod/LMM_POC\n",
      "✅ Random seed set to 42 for reproducibility\n",
      "✅ PyTorch deterministic mode enabled\n",
      "⚠️  Note: May reduce performance\n",
      "PyTorch version: 2.5.1\n",
      "CUDA available: True\n",
      "CUDA version: 11.8\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Deterministic setup and imports\n",
    "# MUST be set before importing torch for deterministic CuBLAS\n",
    "import os\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Get project root (parent of bank_statement/)\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Change working directory to project root\n",
    "os.chdir(project_root)\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "\n",
    "# Standard imports\n",
    "import torch\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "# Reproducibility\n",
    "from common.reproducibility import set_seed, configure_deterministic_mode\n",
    "set_seed(42)\n",
    "configure_deterministic_mode(True)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Select model and image path below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected model: InternVL3.5-8B (bfloat16)\n",
      "Model type: internvl3\n",
      "Model path: /home/jovyan/nfs_share/models/InternVL3_5-8B\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Configuration\n",
    "import yaml\n",
    "\n",
    "# ============================================================================\n",
    "# SELECT MODEL\n",
    "# ============================================================================\n",
    "# Options: \"llama_3_2_vision\", \"internvl3_8b\", \"internvl3_2b\", \"internvl3_5_8b\"\n",
    "MODEL_KEY = \"internvl3_5_8b\"\n",
    "\n",
    "# ============================================================================\n",
    "# SELECT IMAGE\n",
    "# ============================================================================\n",
    "IMAGE_PATH = \"evaluation_data/bank/minimal/image_003.png\"\n",
    "# IMAGE_PATH = \"evaluation_data/bank/minimal/image_008.png\"\n",
    "# IMAGE_PATH = \"evaluation_data/bank/minimal/image_009.png\"\n",
    "\n",
    "# ============================================================================\n",
    "# GROUND TRUTH (optional, for evaluation)\n",
    "# ============================================================================\n",
    "GROUND_TRUTH_PATH = \"evaluation_data/bank/ground_truth_bank.csv\"\n",
    "\n",
    "# ============================================================================\n",
    "# Load model config\n",
    "# ============================================================================\n",
    "with open(\"config/model_config.yaml\") as f:\n",
    "    models_config = yaml.safe_load(f)\n",
    "\n",
    "model_config = models_config[\"models\"][MODEL_KEY]\n",
    "print(f\"Selected model: {model_config['name']}\")\n",
    "print(f\"Model type: {model_config['type']}\")\n",
    "print(f\"Model path: {model_config['default_path']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Load Model\n",
    "\n",
    "Model loading is model-type specific. The cell below handles both Llama and InternVL3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading InternVL3 model from: /home/jovyan/nfs_share/models/InternVL3_5-8B\n",
      "Detected 1 GPU(s)\n",
      "Using single-GPU 8-bit quantization mode\n",
      "FlashAttention2 is not installed.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Using `bitsandbytes` 8-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 99\u001b[39m\n\u001b[32m     94\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mUsing single-GPU 8-bit quantization mode\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     95\u001b[39m     quantization_config = BitsAndBytesConfig(\n\u001b[32m     96\u001b[39m         load_in_8bit=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     97\u001b[39m         llm_int8_enable_fp32_cpu_offload=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     98\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m     model = \u001b[43mAutoModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[43m        \u001b[49m\u001b[43mMODEL_PATH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloading_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlow_cpu_mem_usage\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_flash_attn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloading_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muse_flash_attn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloading_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrust_remote_code\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m.eval()\n\u001b[32m    108\u001b[39m     model_dtype = torch.float16\n\u001b[32m    110\u001b[39m tokenizer = AutoTokenizer.from_pretrained(\n\u001b[32m    111\u001b[39m     MODEL_PATH, \n\u001b[32m    112\u001b[39m     trust_remote_code=\u001b[38;5;28;01mTrue\u001b[39;00m, \n\u001b[32m    113\u001b[39m     use_fast=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    114\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/ivl35_env/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:597\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    595\u001b[39m         model_class.register_for_auto_class(auto_class=\u001b[38;5;28mcls\u001b[39m)\n\u001b[32m    596\u001b[39m     model_class = add_generation_mixin_to_remote_model(model_class)\n\u001b[32m--> \u001b[39m\u001b[32m597\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    598\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    599\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    600\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._model_mapping:\n\u001b[32m    601\u001b[39m     model_class = _get_model_class(config, \u001b[38;5;28mcls\u001b[39m._model_mapping)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/ivl35_env/lib/python3.11/site-packages/transformers/modeling_utils.py:277\u001b[39m, in \u001b[36mrestore_default_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    275\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    276\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    279\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/ivl35_env/lib/python3.11/site-packages/transformers/modeling_utils.py:4881\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4872\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m transformers_explicit_filename.endswith(\n\u001b[32m   4873\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m.safetensors\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4874\u001b[39m     ) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m transformers_explicit_filename.endswith(\u001b[33m\"\u001b[39m\u001b[33m.safetensors.index.json\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   4875\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   4876\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mThe transformers file in the config seems to be incorrect: it is neither a safetensors file \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4877\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m(*.safetensors) nor a safetensors index file (*.safetensors.index.json): \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4878\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtransformers_explicit_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   4879\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m4881\u001b[39m hf_quantizer, config, dtype, device_map = \u001b[43mget_hf_quantizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_agent\u001b[49m\n\u001b[32m   4883\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4885\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m gguf_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   4886\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   4887\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou cannot combine Quantization and loading a model from a GGUF file, try again by making sure you did not passed a `quantization_config` or that you did not load a quantized model from the Hub.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4888\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/ivl35_env/lib/python3.11/site-packages/transformers/quantizers/auto.py:319\u001b[39m, in \u001b[36mget_hf_quantizer\u001b[39m\u001b[34m(config, quantization_config, dtype, from_tf, from_flax, device_map, weights_only, user_agent)\u001b[39m\n\u001b[32m    316\u001b[39m     hf_quantizer = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m     \u001b[43mhf_quantizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    321\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    322\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    323\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    324\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    325\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    326\u001b[39m     dtype = hf_quantizer.update_dtype(dtype)\n\u001b[32m    327\u001b[39m     device_map = hf_quantizer.update_device_map(device_map)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/ivl35_env/lib/python3.11/site-packages/transformers/quantizers/quantizer_bnb_8bit.py:73\u001b[39m, in \u001b[36mBnb8BitHfQuantizer.validate_environment\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     69\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     70\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUsing `bitsandbytes` 8-bit quantization requires Accelerate: `pip install \u001b[39m\u001b[33m'\u001b[39m\u001b[33maccelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     71\u001b[39m     )\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_bitsandbytes_available(check_library_only=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     74\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUsing `bitsandbytes` 8-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     75\u001b[39m     )\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_available():\n\u001b[32m     77\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     78\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe bitsandbytes library requires PyTorch but it was not found in your environment. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     79\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou can install it with `pip install torch`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     80\u001b[39m     )\n",
      "\u001b[31mImportError\u001b[39m: Using `bitsandbytes` 8-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`"
     ]
    }
   ],
   "source": [
    "# Cell 3: Load model based on type\n",
    "import math\n",
    "\n",
    "MODEL_PATH = model_config[\"default_path\"]\n",
    "MODEL_TYPE = model_config[\"type\"]\n",
    "\n",
    "if MODEL_TYPE == \"llama\":\n",
    "    # ========================================================================\n",
    "    # LLAMA MODEL LOADING\n",
    "    # ========================================================================\n",
    "    from common.llama_model_loader_robust import load_llama_model_robust\n",
    "    \n",
    "    loading_config = model_config.get(\"loading\", {})\n",
    "    gen_config = model_config.get(\"generation\", {})\n",
    "    \n",
    "    print(f\"Loading Llama model from: {MODEL_PATH}\")\n",
    "    \n",
    "    model, processor = load_llama_model_robust(\n",
    "        model_path=MODEL_PATH,\n",
    "        use_quantization=loading_config.get(\"use_quantization\", False),\n",
    "        device_map=loading_config.get(\"device_map\", \"auto\"),\n",
    "        max_new_tokens=gen_config.get(\"max_new_tokens\", 4096),\n",
    "        torch_dtype=loading_config.get(\"torch_dtype\", \"bfloat16\"),\n",
    "        low_cpu_mem_usage=loading_config.get(\"low_cpu_mem_usage\", True),\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    # Tie weights if configured\n",
    "    if loading_config.get(\"tie_weights\", True):\n",
    "        try:\n",
    "            model.tie_weights()\n",
    "            print(\"Model weights tied\")\n",
    "        except Exception as e:\n",
    "            print(f\"tie_weights() warning: {e}\")\n",
    "    \n",
    "    tokenizer = None  # Llama uses processor\n",
    "    model_dtype = torch.bfloat16\n",
    "\n",
    "else:\n",
    "    # ========================================================================\n",
    "    # INTERNVL3 MODEL LOADING\n",
    "    # ========================================================================\n",
    "    from transformers import AutoModel, AutoTokenizer, AutoConfig, BitsAndBytesConfig\n",
    "    \n",
    "    loading_config = model_config.get(\"loading\", {})\n",
    "    \n",
    "    print(f\"Loading InternVL3 model from: {MODEL_PATH}\")\n",
    "    \n",
    "    def split_model(model_path):\n",
    "        \"\"\"Official InternVL3 multi-GPU device mapping.\"\"\"\n",
    "        device_map = {}\n",
    "        world_size = torch.cuda.device_count()\n",
    "        config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)\n",
    "        num_layers = config.llm_config.num_hidden_layers\n",
    "        \n",
    "        num_layers_per_gpu = math.ceil(num_layers / (world_size - 0.5))\n",
    "        num_layers_per_gpu = [num_layers_per_gpu] * world_size\n",
    "        num_layers_per_gpu[0] = math.ceil(num_layers_per_gpu[0] * 0.5)\n",
    "        \n",
    "        layer_cnt = 0\n",
    "        for i, num_layer in enumerate(num_layers_per_gpu):\n",
    "            for _ in range(num_layer):\n",
    "                device_map[f'language_model.model.layers.{layer_cnt}'] = i\n",
    "                layer_cnt += 1\n",
    "        \n",
    "        device_map['vision_model'] = 0\n",
    "        device_map['mlp1'] = 0\n",
    "        device_map['language_model.model.tok_embeddings'] = 0\n",
    "        device_map['language_model.model.embed_tokens'] = 0\n",
    "        device_map['language_model.output'] = 0\n",
    "        device_map['language_model.model.norm'] = 0\n",
    "        device_map['language_model.model.rotary_emb'] = 0\n",
    "        device_map['language_model.lm_head'] = 0\n",
    "        device_map[f'language_model.model.layers.{num_layers - 1}'] = 0\n",
    "        \n",
    "        return device_map\n",
    "    \n",
    "    world_size = torch.cuda.device_count()\n",
    "    print(f\"Detected {world_size} GPU(s)\")\n",
    "    \n",
    "    if world_size > 1:\n",
    "        print(\"Using multi-GPU bfloat16 mode\")\n",
    "        device_map = split_model(MODEL_PATH)\n",
    "        model = AutoModel.from_pretrained(\n",
    "            MODEL_PATH,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            low_cpu_mem_usage=loading_config.get(\"low_cpu_mem_usage\", True),\n",
    "            use_flash_attn=loading_config.get(\"use_flash_attn\", False),\n",
    "            trust_remote_code=loading_config.get(\"trust_remote_code\", True),\n",
    "            device_map=device_map,\n",
    "        ).eval()\n",
    "        model_dtype = torch.bfloat16\n",
    "    else:\n",
    "        print(\"Using single-GPU 8-bit quantization mode\")\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_8bit=True,\n",
    "            llm_int8_enable_fp32_cpu_offload=False\n",
    "        )\n",
    "        model = AutoModel.from_pretrained(\n",
    "            MODEL_PATH,\n",
    "            torch_dtype=torch.float16,\n",
    "            low_cpu_mem_usage=loading_config.get(\"low_cpu_mem_usage\", True),\n",
    "            use_flash_attn=loading_config.get(\"use_flash_attn\", False),\n",
    "            trust_remote_code=loading_config.get(\"trust_remote_code\", True),\n",
    "            quantization_config=quantization_config,\n",
    "            device_map={\"\":0},\n",
    "        ).eval()\n",
    "        model_dtype = torch.float16\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        MODEL_PATH, \n",
    "        trust_remote_code=True, \n",
    "        use_fast=False\n",
    "    )\n",
    "    if tokenizer.pad_token_id is None:\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    \n",
    "    processor = None  # InternVL3 doesn't use processor\n",
    "\n",
    "print(f\"\\nModel loaded successfully!\")\n",
    "print(f\"Model dtype: {model_dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Load Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Load image\n",
    "print(f\"Loading image: {IMAGE_PATH}\")\n",
    "image = Image.open(IMAGE_PATH).convert(\"RGB\")\n",
    "print(f\"Image size: {image.size}\")\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Create Extractor and Extract\n",
    "\n",
    "The `UnifiedBankExtractor` automatically:\n",
    "1. Detects column headers (Turn 0)\n",
    "2. Maps headers to semantic types\n",
    "3. Selects optimal strategy based on Balance column presence\n",
    "4. Executes extraction\n",
    "5. Parses and filters results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Create extractor and run extraction\n",
    "from common.unified_bank_extractor import UnifiedBankExtractor\n",
    "\n",
    "# Create extractor\n",
    "extractor = UnifiedBankExtractor(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    processor=processor,\n",
    "    model_type=MODEL_TYPE,\n",
    "    model_dtype=model_dtype,\n",
    ")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"UNIFIED BANK STATEMENT EXTRACTION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Run extraction\n",
    "result = extractor.extract(image)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXTRACTION COMPLETE\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Display extraction results\n",
    "print(\"=\"*60)\n",
    "print(\"EXTRACTION METADATA\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Strategy used: {result.strategy_used}\")\n",
    "print(f\"Turns executed: {result.turns_executed}\")\n",
    "print(f\"Headers detected: {result.headers_detected}\")\n",
    "if result.column_mapping:\n",
    "    print(f\"\\nColumn mapping:\")\n",
    "    print(f\"  Date: {result.column_mapping.date}\")\n",
    "    print(f\"  Description: {result.column_mapping.description}\")\n",
    "    print(f\"  Debit: {result.column_mapping.debit}\")\n",
    "    print(f\"  Credit: {result.column_mapping.credit}\")\n",
    "    print(f\"  Balance: {result.column_mapping.balance}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXTRACTED SCHEMA FIELDS\")\n",
    "print(\"=\"*60)\n",
    "schema = result.to_schema_dict()\n",
    "for field, value in schema.items():\n",
    "    display_value = str(value)[:100] + \"...\" if len(str(value)) > 100 else str(value)\n",
    "    print(f\"\\n{field}:\")\n",
    "    print(f\"  {display_value}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRANSACTION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total debit transactions: {len(result.transaction_dates)}\")\n",
    "print(f\"Date range: {result.statement_date_range}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Display individual transactions\n",
    "print(\"=\"*60)\n",
    "print(\"DEBIT TRANSACTIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, (date, desc, amount) in enumerate(zip(\n",
    "    result.transaction_dates,\n",
    "    result.line_item_descriptions,\n",
    "    result.transaction_amounts_paid\n",
    "), 1):\n",
    "    print(f\"{i}. [{date}] {desc[:50]}{'...' if len(desc) > 50 else ''} - {amount}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Evaluation (Optional)\n",
    "\n",
    "Compare extracted results against ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Load ground truth and evaluate\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Import evaluation metrics\n",
    "from common.evaluation_metrics import (\n",
    "    calculate_field_accuracy_f1,\n",
    "    load_ground_truth,\n",
    ")\n",
    "\n",
    "# Bank statement fields to evaluate\n",
    "BANK_STATEMENT_FIELDS = [\n",
    "    \"DOCUMENT_TYPE\",\n",
    "    \"STATEMENT_DATE_RANGE\",\n",
    "    \"TRANSACTION_DATES\",\n",
    "    \"LINE_ITEM_DESCRIPTIONS\",\n",
    "    \"TRANSACTION_AMOUNTS_PAID\",\n",
    "]\n",
    "\n",
    "# Load ground truth\n",
    "print(\"Loading ground truth...\")\n",
    "try:\n",
    "    ground_truth_map = load_ground_truth(GROUND_TRUTH_PATH, verbose=True)\n",
    "    \n",
    "    # Get image filename\n",
    "    image_filename = Path(IMAGE_PATH).name\n",
    "    print(f\"Looking up: {image_filename}\")\n",
    "    \n",
    "    if image_filename in ground_truth_map:\n",
    "        gt_data = ground_truth_map[image_filename]\n",
    "        print(\"Ground truth found!\")\n",
    "        \n",
    "        # Calculate F1 scores\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"EVALUATION RESULTS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        total_f1 = 0.0\n",
    "        for field in BANK_STATEMENT_FIELDS:\n",
    "            extracted = schema.get(field, \"NOT_FOUND\")\n",
    "            ground_truth = gt_data.get(field, \"NOT_FOUND\")\n",
    "            \n",
    "            if pd.isna(ground_truth):\n",
    "                ground_truth = \"NOT_FOUND\"\n",
    "            \n",
    "            result_eval = calculate_field_accuracy_f1(extracted, ground_truth, field)\n",
    "            f1 = result_eval.get(\"f1_score\", 0.0)\n",
    "            total_f1 += f1\n",
    "            \n",
    "            status = \"OK\" if f1 == 1.0 else (\"PARTIAL\" if f1 >= 0.5 else \"FAIL\")\n",
    "            print(f\"{status:7} {field:30} F1: {f1:.1%}\")\n",
    "        \n",
    "        overall_f1 = total_f1 / len(BANK_STATEMENT_FIELDS)\n",
    "        print(\"\\n\" + \"-\"*60)\n",
    "        print(f\"OVERALL F1 SCORE: {overall_f1:.1%}\")\n",
    "        print(\"=\"*60)\n",
    "    else:\n",
    "        print(f\"No ground truth found for: {image_filename}\")\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print(f\"Ground truth file not found: {GROUND_TRUTH_PATH}\")\n",
    "    print(\"Skipping evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## View Raw Responses (Debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: View raw LLM responses (for debugging)\n",
    "print(\"=\"*60)\n",
    "print(\"RAW TURN 0 RESPONSE (Header Detection)\")\n",
    "print(\"=\"*60)\n",
    "print(result.raw_responses.get(\"turn0\", \"N/A\"))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RAW TURN 1 RESPONSE (Extraction)\")\n",
    "print(\"=\"*60)\n",
    "print(result.raw_responses.get(\"turn1\", \"N/A\")[:10000])\n",
    "if len(result.raw_responses.get(\"turn1\", \"\")) > 10000:\n",
    "    print(\"... [truncated]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Final summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXTRACTION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nModel: {model_config['name']}\")\n",
    "print(f\"Image: {IMAGE_PATH}\")\n",
    "print(f\"Strategy: {result.strategy_used}\")\n",
    "print(f\"Headers: {len(result.headers_detected)} detected\")\n",
    "print(f\"Balance column: {result.column_mapping.balance if result.column_mapping else 'N/A'}\")\n",
    "print(f\"Transactions extracted: {len(result.transaction_dates)} debits\")\n",
    "print(f\"Date range: {result.statement_date_range}\")\n",
    "print(f\"\\nPipeline complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c29ea4-8d86-4079-8803-220ccee426c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (InternVL3.5)",
   "language": "python",
   "name": "ivl35_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
