{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# Unified Bank Statement Extraction\n\n**Protocol**: Automatic strategy selection based on document characteristics\n\nThis notebook uses the `UnifiedBankExtractor` which automatically selects the optimal extraction strategy:\n\n- **2-Turn Balance-Description**: When Balance column is detected (simpler, more reliable)\n- **3-Turn Table Extraction**: When no Balance column exists (fallback)\n\n---\n\n## Workflow\n\n```\nTurn 0: Image → Header Detection (shared)\n    ↓\nPython: Pattern matching → Strategy selection\n    ↓\nTurn 1: Image → Extraction (strategy-specific)\n    ↓\nPython: Parsing → Filtering → Schema fields\n```\n\n## Configuration\n\nAll configs in `config/`:\n- `model_config.yaml` - Model configurations (shared)\n- `bank_prompts.yaml` - Bank statement extraction prompts\n- `bank_column_patterns.yaml` - Column header pattern matching"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /home/jovyan/nfs_share/tod/LMM_POC\n",
      "✅ Random seed set to 42 for reproducibility\n",
      "✅ PyTorch deterministic mode enabled\n",
      "⚠️  Note: May reduce performance\n",
      "PyTorch version: 2.5.1\n",
      "CUDA available: True\n",
      "CUDA version: 11.8\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Deterministic setup and imports\n",
    "# MUST be set before importing torch for deterministic CuBLAS\n",
    "import os\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Get project root (parent of bank_statement/)\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Change working directory to project root\n",
    "os.chdir(project_root)\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "\n",
    "# Standard imports\n",
    "import torch\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "# Reproducibility\n",
    "from common.reproducibility import set_seed, configure_deterministic_mode\n",
    "set_seed(42)\n",
    "configure_deterministic_mode(True)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Select model and image path below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 2: Configuration\nimport yaml\n\n# ============================================================================\n# SELECT MODEL - Choose from config/model_config.yaml\n# ============================================================================\n# Options (bfloat16 - no quantization):\n#   \"llama_3_2_vision\", \"internvl3_8b\", \"internvl3_2b\", \"internvl3_5_8b\"\n# Options (8-bit quantized):\n#   \"llama_3_2_vision_8bit\", \"internvl3_8b_8bit\", \"internvl3_2b_8bit\", \"internvl3_5_8b_8bit\"\nMODEL_KEY = \"internvl3_5_8b\"\n\n# ============================================================================\n# SELECT IMAGE\n# ============================================================================\nIMAGE_PATH = \"evaluation_data/bank/minimal/image_003.png\"\n# IMAGE_PATH = \"evaluation_data/bank/minimal/image_008.png\"\n# IMAGE_PATH = \"evaluation_data/bank/minimal/image_009.png\"\n\n# ============================================================================\n# GROUND TRUTH (optional, for evaluation)\n# ============================================================================\nGROUND_TRUTH_PATH = \"evaluation_data/bank/ground_truth_bank.csv\"\n\n# ============================================================================\n# Load model config\n# ============================================================================\nwith open(\"config/model_config.yaml\") as f:\n    models_config = yaml.safe_load(f)\n\nmodel_config = models_config[\"models\"][MODEL_KEY]\nloading_config = model_config.get(\"loading\", {})\nimage_processing_config = model_config.get(\"image_processing\", {})\n\nprint(f\"Selected model: {model_config['name']}\")\nprint(f\"Model type: {model_config['type']}\")\nprint(f\"Model path: {model_config['default_path']}\")\nprint(f\"Quantization: {loading_config.get('quantization', 'none')}\")\nprint(f\"Torch dtype: {loading_config.get('torch_dtype', 'bfloat16')}\")\nif image_processing_config:\n    print(f\"Max tiles: {image_processing_config.get('max_tiles', 14)}\")\n    print(f\"Input size: {image_processing_config.get('input_size', 448)}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Load Model\n",
    "\n",
    "Model loading is model-type specific. The cell below handles both Llama and InternVL3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 3: Load model based on EXPLICIT config (no silent fallbacks)\nimport math\n\nMODEL_PATH = model_config[\"default_path\"]\nMODEL_TYPE = model_config[\"type\"]\nQUANTIZATION = loading_config.get(\"quantization\", \"none\")\nTORCH_DTYPE_STR = loading_config.get(\"torch_dtype\", \"bfloat16\")\n\n# Map string dtype to torch dtype\nDTYPE_MAP = {\n    \"bfloat16\": torch.bfloat16,\n    \"float16\": torch.float16,\n    \"float32\": torch.float32,\n}\nTORCH_DTYPE = DTYPE_MAP.get(TORCH_DTYPE_STR, torch.bfloat16)\n\nprint(f\"Loading model: {model_config['name']}\")\nprint(f\"  Path: {MODEL_PATH}\")\nprint(f\"  Quantization: {QUANTIZATION}\")\nprint(f\"  Dtype: {TORCH_DTYPE_STR}\")\n\nif MODEL_TYPE == \"llama\":\n    # ========================================================================\n    # LLAMA MODEL LOADING\n    # ========================================================================\n    from common.llama_model_loader_robust import load_llama_model_robust\n    \n    gen_config = model_config.get(\"generation\", {})\n    \n    model, processor = load_llama_model_robust(\n        model_path=MODEL_PATH,\n        use_quantization=(QUANTIZATION == \"8bit\"),\n        device_map=loading_config.get(\"device_map\", \"auto\"),\n        max_new_tokens=gen_config.get(\"max_new_tokens\", 4096),\n        torch_dtype=TORCH_DTYPE_STR,\n        low_cpu_mem_usage=loading_config.get(\"low_cpu_mem_usage\", True),\n        verbose=True\n    )\n    \n    if loading_config.get(\"tie_weights\", True):\n        try:\n            model.tie_weights()\n            print(\"Model weights tied\")\n        except Exception as e:\n            print(f\"tie_weights() warning: {e}\")\n    \n    tokenizer = None\n    model_dtype = TORCH_DTYPE\n\nelse:\n    # ========================================================================\n    # INTERNVL3 MODEL LOADING - EXPLICIT CONFIG\n    # ========================================================================\n    from transformers import AutoModel, AutoTokenizer, AutoConfig, BitsAndBytesConfig\n    \n    print(f\"Loading InternVL3 model...\")\n    \n    # Multi-GPU device mapping helper\n    def split_model(model_path):\n        \"\"\"Official InternVL3 multi-GPU device mapping.\"\"\"\n        device_map = {}\n        world_size = torch.cuda.device_count()\n        config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)\n        num_layers = config.llm_config.num_hidden_layers\n        \n        num_layers_per_gpu = math.ceil(num_layers / (world_size - 0.5))\n        num_layers_per_gpu = [num_layers_per_gpu] * world_size\n        num_layers_per_gpu[0] = math.ceil(num_layers_per_gpu[0] * 0.5)\n        \n        layer_cnt = 0\n        for i, num_layer in enumerate(num_layers_per_gpu):\n            for _ in range(num_layer):\n                device_map[f'language_model.model.layers.{layer_cnt}'] = i\n                layer_cnt += 1\n        \n        device_map['vision_model'] = 0\n        device_map['mlp1'] = 0\n        device_map['language_model.model.tok_embeddings'] = 0\n        device_map['language_model.model.embed_tokens'] = 0\n        device_map['language_model.output'] = 0\n        device_map['language_model.model.norm'] = 0\n        device_map['language_model.model.rotary_emb'] = 0\n        device_map['language_model.lm_head'] = 0\n        device_map[f'language_model.model.layers.{num_layers - 1}'] = 0\n        \n        return device_map\n    \n    if QUANTIZATION == \"8bit\":\n        # 8-bit quantized loading (explicit config)\n        print(\"  Mode: 8-bit quantization\")\n        quantization_config = BitsAndBytesConfig(\n            load_in_8bit=True,\n            llm_int8_enable_fp32_cpu_offload=False\n        )\n        model = AutoModel.from_pretrained(\n            MODEL_PATH,\n            torch_dtype=TORCH_DTYPE,\n            low_cpu_mem_usage=loading_config.get(\"low_cpu_mem_usage\", True),\n            use_flash_attn=loading_config.get(\"use_flash_attn\", False),\n            trust_remote_code=loading_config.get(\"trust_remote_code\", True),\n            quantization_config=quantization_config,\n            device_map={\"\": 0},\n        ).eval()\n    else:\n        # Non-quantized loading (bfloat16/float16)\n        print(f\"  Mode: {TORCH_DTYPE_STR} (no quantization)\")\n        world_size = torch.cuda.device_count()\n        \n        if world_size > 1:\n            print(f\"  Multi-GPU: {world_size} GPUs detected\")\n            device_map = split_model(MODEL_PATH)\n        else:\n            print(\"  Single GPU\")\n            device_map = {\"\": 0}\n        \n        model = AutoModel.from_pretrained(\n            MODEL_PATH,\n            torch_dtype=TORCH_DTYPE,\n            low_cpu_mem_usage=loading_config.get(\"low_cpu_mem_usage\", True),\n            use_flash_attn=loading_config.get(\"use_flash_attn\", False),\n            trust_remote_code=loading_config.get(\"trust_remote_code\", True),\n            device_map=device_map,\n        ).eval()\n    \n    tokenizer = AutoTokenizer.from_pretrained(\n        MODEL_PATH, \n        trust_remote_code=True, \n        use_fast=False\n    )\n    if tokenizer.pad_token_id is None:\n        tokenizer.pad_token_id = tokenizer.eos_token_id\n    \n    processor = None\n    model_dtype = TORCH_DTYPE\n\nprint(f\"\\nModel loaded successfully!\")\nprint(f\"  Final dtype: {model_dtype}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Load Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Load image\n",
    "print(f\"Loading image: {IMAGE_PATH}\")\n",
    "image = Image.open(IMAGE_PATH).convert(\"RGB\")\n",
    "print(f\"Image size: {image.size}\")\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Create Extractor and Extract\n",
    "\n",
    "The `UnifiedBankExtractor` automatically:\n",
    "1. Detects column headers (Turn 0)\n",
    "2. Maps headers to semantic types\n",
    "3. Selects optimal strategy based on Balance column presence\n",
    "4. Executes extraction\n",
    "5. Parses and filters results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 5: Create extractor and run extraction\nfrom common.unified_bank_extractor import UnifiedBankExtractor\n\n# Create extractor with image processing config from YAML\nextractor = UnifiedBankExtractor(\n    model=model,\n    tokenizer=tokenizer,\n    processor=processor,\n    model_type=MODEL_TYPE,\n    model_dtype=model_dtype,\n    image_processing_config=image_processing_config,\n)\n\nprint(\"=\"*60)\nprint(\"UNIFIED BANK STATEMENT EXTRACTION\")\nprint(\"=\"*60)\nprint(f\"Max tiles: {extractor.max_tiles}\")\nprint(f\"Input size: {extractor.input_size}\")\n\n# Run extraction\nresult = extractor.extract(image)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"EXTRACTION COMPLETE\")\nprint(\"=\"*60)"
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Display extraction results\n",
    "print(\"=\"*60)\n",
    "print(\"EXTRACTION METADATA\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Strategy used: {result.strategy_used}\")\n",
    "print(f\"Turns executed: {result.turns_executed}\")\n",
    "print(f\"Headers detected: {result.headers_detected}\")\n",
    "if result.column_mapping:\n",
    "    print(f\"\\nColumn mapping:\")\n",
    "    print(f\"  Date: {result.column_mapping.date}\")\n",
    "    print(f\"  Description: {result.column_mapping.description}\")\n",
    "    print(f\"  Debit: {result.column_mapping.debit}\")\n",
    "    print(f\"  Credit: {result.column_mapping.credit}\")\n",
    "    print(f\"  Balance: {result.column_mapping.balance}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXTRACTED SCHEMA FIELDS\")\n",
    "print(\"=\"*60)\n",
    "schema = result.to_schema_dict()\n",
    "for field, value in schema.items():\n",
    "    display_value = str(value)[:100] + \"...\" if len(str(value)) > 100 else str(value)\n",
    "    print(f\"\\n{field}:\")\n",
    "    print(f\"  {display_value}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRANSACTION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total debit transactions: {len(result.transaction_dates)}\")\n",
    "print(f\"Date range: {result.statement_date_range}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Display individual transactions\n",
    "print(\"=\"*60)\n",
    "print(\"DEBIT TRANSACTIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, (date, desc, amount) in enumerate(zip(\n",
    "    result.transaction_dates,\n",
    "    result.line_item_descriptions,\n",
    "    result.transaction_amounts_paid\n",
    "), 1):\n",
    "    print(f\"{i}. [{date}] {desc[:50]}{'...' if len(desc) > 50 else ''} - {amount}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Evaluation (Optional)\n",
    "\n",
    "Compare extracted results against ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Load ground truth and evaluate\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Import evaluation metrics\n",
    "from common.evaluation_metrics import (\n",
    "    calculate_field_accuracy_f1,\n",
    "    load_ground_truth,\n",
    ")\n",
    "\n",
    "# Bank statement fields to evaluate\n",
    "BANK_STATEMENT_FIELDS = [\n",
    "    \"DOCUMENT_TYPE\",\n",
    "    \"STATEMENT_DATE_RANGE\",\n",
    "    \"TRANSACTION_DATES\",\n",
    "    \"LINE_ITEM_DESCRIPTIONS\",\n",
    "    \"TRANSACTION_AMOUNTS_PAID\",\n",
    "]\n",
    "\n",
    "# Load ground truth\n",
    "print(\"Loading ground truth...\")\n",
    "try:\n",
    "    ground_truth_map = load_ground_truth(GROUND_TRUTH_PATH, verbose=True)\n",
    "    \n",
    "    # Get image filename\n",
    "    image_filename = Path(IMAGE_PATH).name\n",
    "    print(f\"Looking up: {image_filename}\")\n",
    "    \n",
    "    if image_filename in ground_truth_map:\n",
    "        gt_data = ground_truth_map[image_filename]\n",
    "        print(\"Ground truth found!\")\n",
    "        \n",
    "        # Calculate F1 scores\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"EVALUATION RESULTS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        total_f1 = 0.0\n",
    "        for field in BANK_STATEMENT_FIELDS:\n",
    "            extracted = schema.get(field, \"NOT_FOUND\")\n",
    "            ground_truth = gt_data.get(field, \"NOT_FOUND\")\n",
    "            \n",
    "            if pd.isna(ground_truth):\n",
    "                ground_truth = \"NOT_FOUND\"\n",
    "            \n",
    "            result_eval = calculate_field_accuracy_f1(extracted, ground_truth, field)\n",
    "            f1 = result_eval.get(\"f1_score\", 0.0)\n",
    "            total_f1 += f1\n",
    "            \n",
    "            status = \"OK\" if f1 == 1.0 else (\"PARTIAL\" if f1 >= 0.5 else \"FAIL\")\n",
    "            print(f\"{status:7} {field:30} F1: {f1:.1%}\")\n",
    "        \n",
    "        overall_f1 = total_f1 / len(BANK_STATEMENT_FIELDS)\n",
    "        print(\"\\n\" + \"-\"*60)\n",
    "        print(f\"OVERALL F1 SCORE: {overall_f1:.1%}\")\n",
    "        print(\"=\"*60)\n",
    "    else:\n",
    "        print(f\"No ground truth found for: {image_filename}\")\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print(f\"Ground truth file not found: {GROUND_TRUTH_PATH}\")\n",
    "    print(\"Skipping evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## View Raw Responses (Debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: View raw LLM responses (for debugging)\n",
    "print(\"=\"*60)\n",
    "print(\"RAW TURN 0 RESPONSE (Header Detection)\")\n",
    "print(\"=\"*60)\n",
    "print(result.raw_responses.get(\"turn0\", \"N/A\"))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RAW TURN 1 RESPONSE (Extraction)\")\n",
    "print(\"=\"*60)\n",
    "print(result.raw_responses.get(\"turn1\", \"N/A\")[:10000])\n",
    "if len(result.raw_responses.get(\"turn1\", \"\")) > 10000:\n",
    "    print(\"... [truncated]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Final summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXTRACTION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nModel: {model_config['name']}\")\n",
    "print(f\"Image: {IMAGE_PATH}\")\n",
    "print(f\"Strategy: {result.strategy_used}\")\n",
    "print(f\"Headers: {len(result.headers_detected)} detected\")\n",
    "print(f\"Balance column: {result.column_mapping.balance if result.column_mapping else 'N/A'}\")\n",
    "print(f\"Transactions extracted: {len(result.transaction_dates)} debits\")\n",
    "print(f\"Date range: {result.statement_date_range}\")\n",
    "print(f\"\\nPipeline complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c29ea4-8d86-4079-8803-220ccee426c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (InternVL3.5)",
   "language": "python",
   "name": "ivl35_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}